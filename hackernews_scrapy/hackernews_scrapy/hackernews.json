[
{"link_title": "Lyft riders will be able to hail a semi-autonomous car by the end of 2017", "url": "https://www.recode.net/platform/amp/2017/9/7/16262610/lyft-drive-ai-self-driving-pilot-bay-area", "text": "Lyft riders will be able to hail a semi-autonomous car in the Bay Area by the end of 2017\n\nLyft is adding another self-driving startup to the roster of software companies it\u2019s working with. The ride-hail company will be working with Drive.ai to bring its semi-autonomous cars to the Bay Area by the end of the year.\n\nLyft riders will be able to summon a car that drives itself part of the time. A driver will still be present to take over in situations where the software can\u2019t properly read the terrain or the presence of pedestrians.\n\nWhile the experience won\u2019t be significantly different to the customer, Lyft and its chief competitor, Uber, have been testing semi-autonomous vehicles in an effort to eventually deploy cars without drivers. Such vehicles could operate nearly continuously, and wouldn\u2019t require the company to split fares with the driver.\n\nDrive.ai, which recently raised $50 million and is backed by GGV Capital, is now the fourth autonomous software company Lyft has signed. In addition to Cruise, which is owned by GM, Lyft is also working with Alphabet\u2019s Waymo and the Boston-based startup nuTonomy, which expects to roll out its semi-autonomous cars on the Lyft platform in Boston by the end of the year.\n\nThe companies aren\u2019t sharing many details about the pilot \u2014 such as when exactly it will launch, or where in the Bay Area. What we know is that Lyft will invite customers to test the service for free, and the pilot will initially include the handful of cars Drive.ai has already been testing.\n\nIn addition to building out its own self-driving software, Lyft has created an open platform that is available to automakers and software companies.\n\nDrive.ai\u2019s garage of cars, which include a Lincoln sedan and an Audi sedan.\n\nWhile it\u2019s certainly a development for Lyft, this is by no means an indication that fully autonomous cars will be available anytime soon. The industry is still in the test phase.\n\nUber is trying out its autonomous network in Arizona, San Francisco and Pittsburgh, whileWaymo is testing an on-demand network in Arizona. Cruise employees have also been testing an internal app in downtown San Francisco, and nuTonomy has been testing its own services in both Singapore and Boston.\n\nKeep in mind, these are just tests, and aren\u2019t in and of themselves indicators of which companies are ahead in the race to getting a network of fully self-driving cars on the road. Progress is more dependent on how well cars can drive themselves in as many situations and geographies as possible.\n\nFor instance, Cruise\u2019s app \u2014 called Cruise Anywhere \u2014 allows staffers to take its semi-autonomous Chevy Bolts to anywhere in downtown San Francisco. Navigating city traffic and variables is no small feat.\n\nStill, all of these pilots are operating in thoroughly mapped-out, geofenced areas. The biggest tangible development for consumers is that more of them will actually be exposed to and experience autonomous technology for the first time.", "sentiment": 0.2481026785714286},
{"link_title": "The Myth of AI (2014)", "url": "https://www.edge.org/video/the-myth-of-ai", "text": "The idea that computers are people has a long and storied history. It goes back to the very origins of computers, and even from before. There's always been a question about whether a program is something alive or not since it intrinsically has some kind of autonomy at the very least, or it wouldn't be a program. There has been a domineering subculture\u2014that's been the most wealthy, prolific, and influential subculture in the technical world\u2014that for a long time has not only promoted the idea that there's an equivalence between algorithms and life, and certain algorithms and people, but a historical determinism that we're inevitably making computers that will be smarter and better than us and will take over from us. ...That mythology, in turn, has spurred a reactionary, perpetual spasm from people who are horrified by what they hear. You'll have a figure say, \"The computers will take over the Earth, but that's a good thing, because people had their chance and now we should give it to the machines.\" Then you'll have other people say, \"Oh, that's horrible, we must stop these computers.\" Most recently, some of the most beloved and respected figures in the tech and science world, including Stephen Hawking and Elon Musk, have taken that position of: \"Oh my God, these things are an existential threat. They must be stopped.\"\n\nIn the history of organized religion, it's often been the case that people have been disempowered precisely to serve what was perceived to be the needs of some deity or another, where in fact what they were doing was supporting an elite class that was the priesthood for that deity. ... That looks an awful lot like the new digital economy to me, where you have (natural language) translators and everybody else who contributes to the corpora that allows the data schemes to operate, contributing to the fortunes of whoever runs the computers. You're saying, \"Well, but they're helping the AI, it's not us, they're helping the AI.\" It reminds me of somebody saying, \"Oh, build these pyramids, it's in the service of this deity,\" and, on the ground, it's in the service of an elite. It's an economic effect of the new idea. The new religious idea of AI is a lot like the economic effect of the old idea, religion.\n\nJARON LANIER is a Computer Scientist; Musician; Author of Who Owns the Future? Jaron Lanier's Edge Bio Page", "sentiment": 0.11595361781076066},
{"link_title": "Continuous Delivery for DC/OS with Spinnaker", "url": "http://engineering.cerner.com/blog/continuous-delivery-for-dcos-with-spinnaker/", "text": "Last fall our team (Mike Tweten, Trevin Teacutter, and Zameer Sura) started working on the problem of automating DC/OS deployments in a way that wouldn\u2019t require multiple teams to duplicate effort and tie themselves directly to DC/OS APIs. While DC/OS certainly makes the act of deploying an application much easier than anything we\u2019ve used previously, there are still many different ways you could choose to layer on deployment strategies and integrate with continuous delivery systems. Additionally, there may be lots of teams with very similar needs in this space and there are certainly more efficient uses of their time than making them all solve the same problems. Furthermore, while DC/OS is a good choice today we need to make sure that we don\u2019t become locked in and can change our mind in the future without a big impact to those teams.\n\nBeing an engineering team, naturally our first approach was to try to write a service to manage a simple set of deployment strategies with an API that we could reimplement over other resource schedulers (like Kubernetes) in the future. However, after producing a prototype we really started to grasp that we were taking on a much bigger task than we should. Our API could handle some basic pre-built deployment workflows, but if it wasn\u2019t flexible enough for teams we\u2019d need to continue to adapt it. In addition, we would still have to work out how it would integrate with CI/CD tools like Jenkins, and we\u2019d have to do a lot of that work again each time we wanted to support another resource scheduler as a deployment target.\n\nAt this point we decided to take a deeper look at Spinnaker, a continuous delivery platform that was open-sourced by Netflix. It had originally been developed to orchestrate AWS deployments and had expanded to other providers like Azure, OpenStack, and more recently Kubernetes was added as the first container based deployment target. Spinnaker allows for building deployment pipelines that can be triggered by things like Github commits or Jenkins builds and can then run stages to do things like start other Jenkins jobs, deploy new versions of an application, scale up or down applications, and disable old application versions. With these capabilities we would be able to use Spinnaker to provide a consistent deployment abstraction while still providing the flexibility to handle deployment workflows that we didn\u2019t build ourselves.\n\nThe only minor problem was that it didn\u2019t support DC/OS, our preferred deployment target. After taking some time to dive in and get familiar with the details it quickly became apparent that adding support for DC/OS would be much easier than continuing to build our own system, especially since we had already become familiar with the DC/OS APIs while developing our prototype. Not only would we then gain the flexibility to allow more deployment strategies than we originally anticipated, but it would also buy us integration with Jenkins and the ability to convert pipelines over to Kubernetes without much effort. We checked with the Spinnaker community to make sure that no one was already working on DC/OS support and also with Mesosphere (the company behind DC/OS) to make sure they weren\u2019t planning to do it. When both of those came back negative we began working on the project in earnest, with the goal of building something that would benefit not only Cerner but also the entire Spinnaker and DC/OS communities.\n\nThis was the first time that any of us had tried to contribute significant changes to an established open-source project so there was some adjustment and learning along the way. We initially created our fork of the necessary repositories to our internal Github with the plan to finish the project before making anything visible publicly. Instead, we found that we wanted to more easily be able to share things with Mesosphere since they had agreed to give us any guidance we might need. It also became apparent that it was too easy to commit things like config that referenced internal resources, or comments that referred to our own JIRA issues. If we kept going that way we were only going to make more work for ourselves cleaning those things up when it came time to submit our changes. By moving our development to repos in Cerner\u2019s Github organization we were able to solve both of these problems at once.\n\nIn spite of moving our code into the public Github we still maintained an insular development approach, determined to get everything just right before submitting a perfect polished gem to the upstream project. In hindsight, we should have been less concerned about trying to get everything right all at once and submitted more incremental changes. We thought it would be easier for everyone not to have to deal with our messy work in progress, but after our first pull request for one of the services ended up with over 130 files and a history of 100+ commits we realized that it\u2019s too much to expect the maintainers to review so much at once, especially when they may not be that familiar with DC/OS. Ultimately, they wanted us to carve up that massive patch anyway so it would have been less work for us to do it from the start.\n\nAfter completing this effort we\u2019ve found that we get even more benefit from Spinnaker than we initially expected. Spinnaker also helps us manage deployments to multiple DC/OS clusters, use Chaos Monkey to test the resiliency of our applications, and even to deploy DC/OS itself on AWS. We\u2019re pleased to announce that our contribution has been accepted by the Spinnaker maintainers and is now available for anyone to use. The Spinnaker community has been great to work with and helped us by answering questions and inviting us to participate in the discussions with other companies that have contributed major functionality to Spinnaker. This is an exciting opportunity to contribute something back to the open-source community and we can\u2019t wait to see where it goes from here.", "sentiment": 0.17108957766174263},
{"link_title": "C# 7: Discards", "url": "http://www.devsanon.com/c/using-discards-feature-of-c-7/", "text": "Let\u2019s assume that you wish to call a method that has a return value and also accepts an out variable, but you do not wish to use the contents of the out variable that will be returned.\n\n So far we were creating a dummy variable that will later not be used, or discarded.\n\n With C# 7 you can now use Discards\n\nDiscards are local variables which you can assign a value to them and that value cannot be read (discarded). In essence they are \u201cwrite-only\u201d variables.\n\nThese discards do not have names, but rather they are represented with a _ (underscore).\n\nSo let\u2019s go with the following example.\n\n Assume we have a ConcurrentQueue of integers, from which we wish to dequeue something, without actually using that something.\n\nWhat would we do?\n\nNow, with C# 7 we can utilize discards.\n\nAnd the value that has been dequeued will not and can not be used.\n\nFor example the following piece of code\n\nwill not compile, nor will it appear in IntelliSense.\n\nPlease remember though, that since _ is a contextual keyword, if you declare a variable with the name _ the variable will be used.\n\nIn the above code, the value that will removed from the queue will be assigned to the variable x, as in the above case the underscore is used as a variable and not as a discard.\n\nThe discards in C# enables a way to ignore some local variables, it is a design time feature.\n\n At runtime a variable may be required and the compiler may generate a name for it.\n\n Since the _ keyword is a contextual keyword, you need to set a code policy to avoid declaring local variables with the name _ to reduce confusions.\n\n This feature is compatible with previous versions of .NET platforms as it does not require a CLR change.", "sentiment": -0.006060606060606059},
{"link_title": "YouTube shuts down North Korean propaganda channels", "url": "https://amp.theguardian.com/world/2017/sep/08/youtube-north-korea-propaganda-terminated", "text": "YouTube has shut down two North Korean propaganda channels that academics use to monitor and assess the regime\u2019s missile programs.\n\nStimmekoreas, the most popular YouTube channel on North Korea with more than 20,000 subscribers, and Uriminzokkiri, which had more than 18,000 subscribers, frequently posted of videos of state TV news clips and other footage, attracting millions of views.\n\nOn Friday they were terminated for violating YouTube\u2019s community guidelines.\n\nArms control expert Jeffrey Lewis uses these channels to analyze videos of missile launches and tours of factories, to better understand the regime\u2019s nuclear capabilities. He urged YouTube to revoke its decision in the interest of national security.\n\n\u201cNorth Korea is a country with thermonuclear weapons sitting on ICBMs [intercontinental ballistic missiles] that can reach the United States. It is really important to understand them even if we don\u2019t like them,\u201d he said. \u201cThat starts by analyzing their propaganda. Even though it\u2019s tendentious, you can learn a lot about a country from the lies they tell.\u201d\n\nStimmekoreas is believed to be operated by a supporter of North Korea who lives outside of the country and posts hi-resolution videos of state propaganda from the Korean Central News Agency in a variety of languages.\n\nUriminzokkiri is directly tied to North Korea\u2019s propaganda wing, and posts content that appears to target North Koreans living abroad.\n\nAcademics use official footage of missile launches to assess how powerful they are based on how quickly they are accelerating, he said. They can also learn about the weapons from machinery and parts visible in videos of factory tours by Kim Jong-un.\n\n\u201cWhen [Kim] visits a factory in the middle of nowhere and stares at machine tools, it provides an important insight into the progress they are making,\u201d added Lewis, the director of the East Asia Nonproliferation Program at the Middlebury Institute of International Studies.\n\nStimmekoreas\u2019 hi-resolution videos were particularly useful for this type of analysis, he said.\n\nBeyond the missile program, researchers can also learn about the people Kim Jong-un surrounds himself with, and which individuals are becoming more and less important in North Korea\u2019s precarious political sphere.\n\n\u201cNorth Korea uses YouTube as a primary distribution network for its propaganda,\u201d said Lewis. \u201cOften you don\u2019t know something is important until later, so having all that information available and searchable was incredibly valuable.\u201d\n\nScott Lafoy, a Washington-based satellite imagery analyst told NK News: \u201cTracking and digitally reconstructing events is going to be more difficult as these accounts get deleted.\u201d\n\nYouTube did not immediately respond to questions about why the channels were shut down, although it could be because the advertising revenue generated by the accounts would violate US trade sanctions.\n\n\u201cI know when I click on the videos I get ads,\u201d said Lewis. \u201cSo perhaps they are nervous about sending that money on to the North Koreans. But honestly the YouTube ad revenues are not going to make or break the missile program.\u201d\n\nIt\u2019s not the first time that YouTube has targeted North Korean propaganda. In November 2016, the video sharing platform closed down KoreanCentralTV1. Several other channels including Chosun TV, NK Propaganda and KCTV Stream were also axed, according to NK News.", "sentiment": 0.1588888888888889},
{"link_title": "Libre Barcode 39", "url": "https://fonts.google.com/specimen/Libre+Barcode+39+Text", "text": "", "sentiment": 0.0},
{"link_title": "Eisenhower and emoji: How I use Things.app to get focus", "url": "https://chrisheisel.com/2017/08/31/eisenhower-and-emoji-how-i-use-things-app/amp/", "text": "You know that anxiety you get when there\u2019s something you need to do and you\u2019re trying to keep it in your head? You write it down on your todo list and problem solved! Lather, rinse repeat over the course of a day and now you\u2019ve got anxiety about all the things you\u2019ve got to do on your sprawling todo list!\n\nLet me tell you about how I use a combination of Things.app, the Eisenhower matrix and emoji to capture and prioritize my tasks, and to stay focused.\n\nFor many years now I\u2019ve been using Things.app. I\u2019ve had dalliances with Trello, Asana, Omnifocus, Remember The Milk and Wunderlist, but I keep coming back to Things.app. I\u2019ve found that for me, it\u2019s the most frictionless experience to capture and organize my tasks. Especially with the release of version 3 adding features like task checklists, recurring todos within a project, and one of my favorites for balancing work and home: This Evening. Check out all the new features in Things 3.\n\nThat said, all the concepts I talk about can be done in just about any tool, including paper!\n\nAlmost every task starts its life in the Things Inbox. It\u2019s a great place to jot down a task \u2013 whether that\u2019s right after a meeting, during a hallway conversation, or while waiting in line at the coffee shop.\n\nItems in the Inbox shouldn\u2019t be there more than a few hours. If I find an item lingering there for days or more, it\u2019s usually a sign I should delete it.\n\nIf an item hasn\u2019t been prioritized, or if I think its priority has changed, then it\u2019s time to update it using Eisenhower and emoji.\n\nEvery item gets a Things tag to indicate where it falls in the Eisenhower matrix and a couple of other tags that I find useful. They started out life as words, but I switched to using emoji for each tag to make the UI easier and faster to scan.\n\n\ud83d\udd25 \u2013 Important and urgent \u2013 Do it today or there\u2019ll be hell to pay\n\n \u26a1\ufe0f \u2013 Important but not urgent \u2013 This is something that should be done, but it doesn\u2019t have to be today\n\n \u26be\ufe0f \u2013 Not important but urgent \u2013 I picked the baseball emoji, because these are usually things that come \u201cout of left field\u201d from someone else and I couldn\u2019t negotiate them away\n\n \ud83d\uddd1 \u2013 Not important and not urgent \u2013 Delete it!\n\n \u23f3 \u2013 Work in progress \u2013 I\u2019ve started work on this but it isn\u2019t done. I want to explicitly track this so I can limit my work in progress and deliver faster through focus\n\n \ud83d\uded1 \u2013 Blocked \u2013 This is something I can\u2019t move forward on without input/delivery from someone else.\n\n \ud83d\udc49 \u2013 Delegated \u2013 Someone else is working on this item, but it\u2019s important for me to track its progress and to not forget about it.\n\nThings.app lets you assign keyboard shortcuts to tags, which makes prioritizing and re-prioritizing soooo much faster. Go to Window > Tags or hit Shift-Command-T\n\n\u26a1\ufe0f \u2013 I \u2013 for important, but not urgent\n\n \ud83d\udd25 \u2013 U \u2013 for urgent\n\n \u26be\ufe0f \u2013 R \u2013 for \u201crequested\u201d\n\n \ud83d\uded1 \u2013 B \u2013 for blocked\n\n \u23f3 \u2013 W \u2013 for work in progress\n\n \ud83d\udc49 \u2013 D \u2013 for delegated\n\nAt least once, preferably twice, a day I go through a regular review and planning process:\n\nAfter I\u2019m done, I should have a view that looks something like this:\n\nWith a view like this for my day, and the capability to quickly prioritize new items that land on my plate, I\u2019m way less stressed, way more organized, and way more productive!", "sentiment": 0.15518025880929107},
{"link_title": "The good news about failing at absolutely everything in your 20s", "url": "https://qz.com/1072820/the-good-news-about-failing-at-absolutely-everything-in-your-20s/", "text": "A couple nights ago, I stumbled across a cache of rejection letters \u2013 possibly one of the largest deposits of rejection letters recorded in human history. They were all addressed to me.\n\nI made this discovery, not coincidentally, while binge-watching Insecure, an HBO show all about a 20-something woman who\u2019s questioning her life choices. I felt guilty about the binging, so I picked up my laptop and began cleaning out an old inbox that I\u2019d used between roughly the ages 22 and 30. That\u2019s when I hit the motherlode. Mixed in with the bills and emails from my dad were dozens of rejections \u2013 from jobs, graduate schools, literary agents, writing contests, and magazine and website editors.\n\nI couldn\u2019t believe I\u2019d forgotten about them. I\u2019d been disappointed, sometimes crushed, the first time I\u2019d read them. Now I was smiling. From the vantage point of my 30s, what I saw was not a bunch of rejections \u2013 it was a bunch of dodged bullets.\n\nTake the rejection letters from doctoral programs. When I was 26, I\u2019d already racked up $22,000 in debt getting a master\u2019s degree in creative writing. My plan was to spend even more money I didn\u2019t have on a doctorate\u2014except my inquiries and applications didn\u2019t go anywhere. Now I can see that, if I\u2019d been accepted by one of these programs, I almost surely would have come out of school even more unemployable, not to mention in hock to Sallie Mae.\n\nThen there were the rejections from adjunct-teaching jobs. Because I only had a master\u2019s degree, I couldn\u2019t convince anyone to let me teach English 101 for subsistence wages and no benefits. The one offer I did get \u2013 at a liberal-arts college 40 miles away, which paid its adjuncts a few thousand dollars per class while charging students nearly $50,000 in tuition \u2013 was rescinded when an existing staffer requested more hours. My applications to jobs in publishing and journalism also led nowhere.\n\nIn the end, I was forced to take a job in advertising, which came with plush health insurance (including some handy free therapy) and a decent salary. But it was not what I\u2019d hoped for. My disappointment was extreme. At least initially.\n\nCommencement speakers are keen to tell you that you should \u201cfollow your passion\u201d and \u201cembrace failure.\u201d In the interest of expedience, let\u2019s leave aside the grating fact that so many of these commencement speakers are boomers who did not come of age in a dire economic recession; they\u2019re like generals still fighting the last war. The problem is that \u201cembracing failure\u201d presents a false choice. Failure is going to happen whether we embrace it or not. What we need are ways to think about it that aren\u2019t facile.\n\nSo here are a few things that failing in my 20s taught me. They just might be useful to you if you\u2019re one of the millions of millennials uncertain about what the hell you\u2019re doing in life. Don\u2019t worry \u2013 I promise this won\u2019t devolve into inspirational quotes.\n\nYour 20s are a good time to \u201cfail early, fail often.\u201d In The Defining Decade: Why Your 20s Matter, Meg Jay writes, \u201cThe twentysomething years are real time and ought to be treated that way.\u201d So how can you treat them as real? You try stuff. Take advantage of the relatively low stakes. Chances are that mortgages and kids aren\u2019t a part of your problem set just yet (or if they are, you still have a lot of energy). This leaves you free to try on different lives, the way you\u2019d try on outfits. Sure, the uncertainty will drive you half-crazy. But as research and a wealth of anecdotal data suggest, we tend to regret the things we didn\u2019t do more than the things we did. The basic formula is: Trying and failing is > not trying.\n\nFailure is a useful data point. Think of yourself as a probe that\u2019s just landed on an alien planet called Adulthood. The territory is strange, but you must start driving anyway. Put your sensors out. See what you come across. Rejections and failures are sources of information. How else will you know what\u2019s possible and what isn\u2019t? (My version of an inspirational quote: You\u2019ll never know till you fail!)\n\nSometimes what you\u2019re encountering is not failure but a \u201cgravity problem.\u201d Dave Evans, who teaches \u201cdesign thinking\u201d at Stanford, suggests we make a distinction between fixable issues and immutable circumstances, which he calls \u201cgravity problems.\u201d One example of a gravity problem is how difficult it is to make a living as an artist. Once you accept the basic truth of the situation, you\u2019re in better position to adjust to the constraints. Maybe your art becomes a side hustle, or maybe you move to a much cheaper place that allows you to live on very little money. What you don\u2019t do is feel like a failure when you have a hard time making money with creative pursuits.\n\nThe straight path is often an optical illusion. When you read about any successful startup, what you inevitably discover is how many failures the company experienced along the way. The same holds true for talking to successful artists. From the outside, it can look as though a company or a person has achieved overnight success, and this is especially true when big achievements come at an early age. Still, overnight success is largely a myth. It\u2019s worth remembering that you probably don\u2019t know the gory details of the story. Trust me: that Instagrammer with the seemingly perfect life has also spent her fair share of nights crying and staring at the ceiling, worried out of her mind about money and love. It\u2019s just part of the human condition.\n\nRationalization can be a beautiful thing. In Stumbling on Happiness \u2013 which The New York Times praised as a \u201cpaean to delusion\u201d \u2013 Daniel Gilbert notes that humans aren\u2019t great at predicting what will actually make them happy. At the same time, we are pretty good at rationalizing what does happen. In other words, rationalization is an important, underrated coping skill.\n\nSome fantastic art has been made about failure, too. Is it any wonder that the best TV made by millennials, including Insecure and Girls, is all about one\u2019s 20s playing out as a confused, chaotic mess? Many artists have made great work out of their misspent youths. Just a few examples include Meghan Daum (see her essay: \u201cMy Misspent Youth\u201d) and nearly every poem Philip Larkin ever wrote. Your mistakes can give you something to say to the world. And nothing breaks the ice like admitting your life hasn\u2019t worked out quite according to plan.\n\nI tried putting this last point into action by polling my Facebook friends with the question: \u201cWhat\u2019s something you\u2019re glad you failed at in your 20s?\u201d If you ever want your notifications and inbox to blow up, you should ask this, too. One friend listed a whole series of fortuitous failures: \u201cGetting fired from my first job, homeownership, and not getting a couple of jobs that I REALLY wanted.\u201d\n\nAnother friend shared a story about how she\u2019d failed to get an ROTC scholarship \u2013 shortly before September 11, 2001. \u201cI\u2019m not at all looking negatively at those who do serve,\u201d she said. \u201cBut I know it wasn\u2019t for me, and it might have been a literal bullet that I dodged. I probably wouldn\u2019t have joined Peace Corps or traveled as much, wouldn\u2019t have my dog (from Peace Corps), or half my friends, or be myself,\u201d she explained.\n\nThe most common answer I heard, by far, was \u201cmy first marriage.\u201d This may seem like blatant bright-siding. But if we can eventually be happy about a marriage ending, it seems there\u2019s no limit on what kind of failures can make us happy\u2014or at least, happier.\n\nLearn how to write for Quartz Ideas. We welcome your comments at ideas@qz.com.", "sentiment": 0.12356007313393674},
{"link_title": "Spotify no longer streams music in Apple's Safari web browser", "url": "https://www.engadget.com/2017/09/09/spotify-drops-safari-streaming-support/", "text": "Mac Generation suspects that the decision might have something to do with the Widevine copy protection plugin. Spotify wants to use that for web-based music streaming, but macOS may flag it over concerns that its security isn't sufficiently airtight. And without reliable access to Widevine, Spotify can't guarantee that Safari will play properly even if it's technically possible.\n\nThis isn't likely to be a huge issue on the Mac, since it doesn't take much to fire up another app. However, this could be a headache if you use a Mac at work and aren't allowed to install your own software. Also, it illustrates one of the perils of copy protection on the web: it turns an ostensibly universal platform into a proprietary one where you have to support a given company's add-ons to get the full experience.", "sentiment": 0.19444444444444445},
{"link_title": "How to send your teddy bear to near-space\u2026 and get it back unhurt", "url": "http://bluesat.com.au/send-teddy-bear-near-space-get-back-unhurt/", "text": "Things have changed since the glory days of Yuri Gagarin when we first reached for the stars. Now, it is easier than ever to get involved in a space mission.\n\nThe High-Altitude Ballooning project at BLUEsat UNSW brings together passionate students from a wide range of degrees to build, test and launch equipment to altitudes over 30km.\n\nOur most recent mission was on the 5th of August, where we sent our custom-built scientific research payload up 33km to gather data on the atmosphere and capture video of the ascent.\n\nSo, you\u2019re probably thinking to yourself, how can I do this from home?\n\nDo you want to take stunning pictures of the curvature of the earth? Perhaps take a photo of your teddy bear against the backdrop of space?\n\nOur main objective was to study the atmosphere to gain knowledge for future missions. This involved sensors, an onboard computer, and code to log data. We used a Raspberry Pi for computing, with an accelerometer, gyroscope, magnetometer, barometer, temperature probes and humidity sensor recording data to an SD card.\n\nWe also captured video and photos with two cameras mounted on our payload.\n\nWithout knowing where your payload is during flight, you have just launched a glorified party balloon. We used 3 discrete self-contained systems to track our flight, pictured below.\n\nThe SPOT GPS Tracker simply transmits and uploads coordinates to a website, where we can view location updates on a map. An APRS radio device, the yellow box, transmits GPS coordinates over amateur radio bands which are uploaded to an online network. The third device is a short-range radio beacon which plays a continuous tune over radio frequencies, allowing us to pinpoint the landing area.\n\nTypically, a mission is ended when the balloon bursts, but if you are feeling fancy, you can build a device that can remotely cut down your payload. We have developed a mechanism that runs a current through a piece of nichrome wire, which is wrapped around the rope connecting the balloon to the payload. An on-board radio receiver decodes a termination signal which we can send from the ground, causing the nichrome to heat up and sever the rope.\n\nBefore you can send anything into the air, you must seek approval from CASA, the Civil Aviation Safety Authority. You will also need to perform predictions of your flight using online prediction software, such as that found at HabHub, which will allow you to visualise the estimated flight path and burst altitude based on weather forecasts. The figure below shows a predicted trajectory for our launch.\n\nYou now need to hire a tank of helium and drive all your equipment out to your launch site. Make sure you take a toolbox and spare parts with you for any last minute adjustments.\n\nOnce you have inflated the balloon with enough helium to attain the desired lift and attached your payload, you are ready for launch. Perform a final flight prediction, check your on-board systems, turn on your cameras and cross your fingers!\n\nWatch the promotional video of our launch below:\n\nIf you would like details on anything you have seen in this article, please contact us at info@bluesat.com.au", "sentiment": 0.05577777777777779},
{"link_title": "How Much Can You Make Mining Ether and How Much Does It Cost to Run?", "url": "http://www.learn2ai.com/2017/09/how-much-can-you-make-mining-ether-and.html", "text": "The first three scenario's are shown straight forward in the table below, we're basically not at the break-even point for any price of ETH that has occurred so far except for the theoretical high price. Theoretical because that price occurred a few days into mining and we can't sell coins we don't own yet, the Arrow of Time only goes one-way. Today's price is $340 though, so we aren't far off from the high were we to liquidate right now, we would receive about $428 USD for the 1.259 ETH we've mined. which put's us ($428 - $700) = but - the GPUs still have resale value right now and could be sold to recoup costs. We would need an ETH price of ~$586 to be at break-even with 1.259 ETH.\n\n \n\n \n\n \n\n To answer point #4, we simply need to check the price of ETH at each payout and sum the individual 'would-be' sell orders. Taking any arbitrary price within each day combined with the payout amount, the summed total would be $345.42 USD, which would be Skepticism has a cost.\n\n \n\n What if I sold Ethereum as soon as I got Payout? \n\n \n\n To answer point #5, What if you had a crystal ball? Take a look at the chart below showing Ethereum price in USD for the time period in question. If you knew the price today was coming, you never would've sold at any point historically, - except perhaps that 0.05 ETH on day 3 for $390. -Because you will have been waiting for this weeks' price, one of the higher prices recently, but there in lies the point, in the real world, you either believe in something, or you don't. If you're in on ETH, I suggest you go long, Long, and Hard on ETH as much as you can. When it reaches a value you feel comfortable with, perhaps take out a small % and let the rest go for a while longer, just like averaging the buys, you can average the sells. The only caveat to this, is if you know something we don't know... \n\n \n\n \n\n \n\n are shown straight forward in the table below, we're basically not at the break-even point for any price of ETH that has occurred so far except for thehigh price. Theoretical because that price occurred a few days into mining and we can't sell coins we don't own yet, theonly goes one-way. Today's price is $340 though, so we aren't far off from the high were we to liquidate right now, we would receive about $428 USD for the 1.259 ETH we've mined. which put's us ($428 - $700) =but - the GPUs still have resale value right now and could be sold to recoup costs. We would need an ETH price of ~$586 to be at break-even with 1.259 ETH.we simply need to check the price of ETH at each payout and sum the individual 'would-be' sell orders. Taking any arbitrary price within each day combined with the payout amount, the summed total would be $345.42 USD, which would beSkepticism has a cost.Take a look at the chart below showing Ethereum price in USD for the time period in question. If you knew the price today was coming, you never would've sold at any point historically, --Because you will have been waiting for this weeks' price, one of the higher prices recently, but there in lies the point, in the real world, you either believe in something, or you don't. If you're in on ETH, I suggest you go long,as much as you can. When it reaches a value you feel comfortable with, perhaps take out a small % and let the rest go for a while longer, just like averaging the buys, you can average the sells.\n\nshow you how much it costs to get up and running, how much you'll make and how much you can expect to pay for electricity, but first I want you to know that You're probably asking the wrong question. You shouldn't be asking \"?\"...Asking that, demonstrates that you might be getting into mining for the wrong reasons. If you're setting up a rig thinking you're going to strike it rich with a couple of GPUs in your parents basement and you've run up your credit card on 'case fans' like a hardware junkie, you might need to reconsider your goal. There is NO Ether mining 'get rich quick' scheme, nor is Ether day-trading a smooth route to a quick buck. It takes some work to get the basicsbut what it really takes to make it work, isIf you are setting up a rig for any other reason, then you might be a long haul believer, like me. As I explained in one of my first articles' How I Learned to Mine Ether... , Bitcoin never felt that exciting to me, but Ethereum held promise of a new \"form\" of internet,, its' technological background is fundamentally different, and better. That alone is worth it for me, but then you add to that, the fact that some of the most valuable AltCoins are based on Ethreum as well. - that's why I voted for Ethereum with my dollars. But not just buying in, I got GPUs and dedicated my PC to be a mining rig because I wanted to be a part of the support for Ethereum and learn first hand how it all fits together, I really only 'invest' in things if I can understand and see how it works -So, as much as I would have to pay for new GPUs, I also put some money directly into buying ETH coins with smaller purchases spread out over 2 months to try and take advantage of average buying prices rather than the extremes or timing the market.I already had a PC with a power supply which could easily handle a couple more GPUs, as it only had the one 980 GTX at the time. This PC was primarily used for work, editing, development, testing and the occasional Steam game, so it has a 12-core Intel i7-5930K on liquid cooling with 32 GB of Ram, which I had overclocked to 4.4GHz sometimes, as most people would. About 85% of the time the PC sat switched off though, nobody was using it. (). The cost for this was more of an upgrade fee, sold the 980 GTX for $320 (USD$) and picked up two new 1070 GTX's for a total of $1,100, taxes in. So that was the start-up cost $1,100, less $400 if you count the resale. Not bad, $700 to get up and running.Of course this expense will vary depending on your local power-flavor's price, here we are paying $0.07 USD per KWh. I've been using a power consumption meter to monitor the usage and help keep track of costs as well as to know the system Power specs, listed below for the last 75 days of operation, since mining began.Since Electricity costs scale with your operation, it's important to keep in mind this is for two 1070 GTX cards on a very overpowered system. it's not far off the mark though to say the rate per GPU would be near 200-220 Watts per hour or near $0.0155 per hour, a Penny and a half.That covers the cost side of the equation, now let's take a look at the Earnings side.SoWell I actually tested this extensively and the baseline hashing power you'll get from a fresh out of the box MSI Gaming X 8G 1070 GTX will be around 25-27 MH/s. If you overclock it and have the memory clock running up to 8800 MHz you could expect 29-30 MH/s , if you are lucky enough to have a card with Samsung memory your card could hit 9000+ MHz and get into the 32 MH/s region of power.That's the good news, getting the power is pretty efficient in terms of an electrical cost to mining productivity ratio, and it scales well if you have a dedicated low power mobo/cpu and run up to 6 GPUs per motherboard, from there it's only linear scalability.The bad news is that theis completely outside of your control, and is rising every day. In the future, more difficulty will bring output to a virtual crawl - When this rig started mining 75 days ago the average block time on ethstats.net was around 14 seconds, today it's 24 seconds. - which is why, everyone is always asking \"\" - the answer is IT DEPENDS!R.O.I. depends on the price of Ether and that really is the whole point, you cannot possibly predict what the price will be, whether or not ETH will be valuable at all ten years, one year or even one day down the road. Anything could happen, as it often does, which might dramatically change the value. The way I see it though,Being in ETH for the longmeans you believe in the underlying infrastructure, and if you believe in that, then you know the price today is irrelevant.There are still changes to come, what will Ether be worth whenarrives. You mine, buy and hold ETH to act as a sort of hedge against either scenario going too far in one direction, if the price goes very high quickly, then it's a good thing you have mining gear that you paid a fixed price for. Really, ETH is, by the very definition of the word, an, we don't know how big the future will be for that Asset, but we definitely believe it will be bigger in the future than it is right now, ETH has formidable growth potential.How-ever, if you're a skeptic, and would rather, we can look at what this all translates to for bottom line NET Profit ($). This all comes down to the Difficulty vs reward, over the last 75 days the average difficulty has risen 187% from 769 TH to 2,204 TH, whichSo I'm ''...Plotting the data from etherscan.io and extrapolating from the last 75 days,This doesn't look like good news at all for mining profits...via Nanopool Mining Pool. If the next 75 days follow the predicted difficulty growth, we could expect to see much less than what we made in the first 75 days. In truth, my intuition says the growth will be even faster than the exponential curve and even harsher for miners - but that's OK! Because it helps with inflation, to increase the value overall in the long run.So how would I answer the Question: \"No problem right, because thankfully the mining pool offers a dirty calc-u-later table that shows me \"experimental\" forecasts... :| Most people already know these tables don't factor in difficulty changes so it's a pretty useless table. Also, I've found the values to be off the mark quite a lot. When doing analysis, sometimes you would find the data you're trying to analyze isn't well behaved, this happens in the real world.Assuming all other things are constant, such as the mining pool payout remains the same, the share ratio the same, the fees the same, the efficiency of the code etc... then to answer this question I look for a proxy to the data I want, but can't use. In this case, I like the proxy of \"\", because the payouts are always fixed and controllable by me to occur at 0.2 ETH, and the data in this example is smooth because my rig typically has 98% up time and that's something that also is unlikely to change. Which means if everything else stays the same, I will observe a pattern in how many days it has been taking me to earn equal payouts, and should be able to use that data to estimate my future payouts -Ah, that's baked in to the analysis already because the growth delay rate of the payout frequency already includes factors that depend on the difficulty, so I don't need to go crazy trying to model future block difficulty rates when I can just measure and extrapolate the piece of data I'm most interested in.The one Caveat here is that if there is a difficulty bomb that dramatically slows production, then the model will be off, but we can't know the future until it happens, so this is the best approximation I think we'll be able to get, given the data available now.First, set up an X-Y Scatter plot, on the X-Axis goes the Independent Variable, in our case, on the Y-Axis we put the. We should expect to see an exponential growth curve because we know as difficulty increases it's going to take us more and more time to get that 0.2 ETH payout.where X is the Number of Days in Mining Operation and Y is the Number of Days since Last Payout.If we plug in an estimate for the next point to be approximately 19 days after the 75th day, the equation outputs the next point ''. This equation can be used to predict as far as you like, but I doubt it'll be very accurate beyond 75 days anyway, since we only have a low number of data points to work with. But for now, I am satisfied that this will very likely be my yield for the next 75 days, an additional 0.6166 ETH, bringing myWhat does that mean in Dollars though?Now,is a whole other can of worms because settling out your coins into fiat value is all about timing, and really depends on. So let's look at a few scenarios, what if you sell your ETH at the price of:no matter which why you slice it, our R.O.I. and Profit is tied to the Price of ETH, and the more ETH we're holding the more leveraged we become. Leverage is an interesting property because it allows smaller things to exert bigger forces. But just like real life, if things slip and start to go the wrong way, and you're leveraged when things go bad, they go much worse depending on the degree of leverage now acting, instead ofIf the price goes around $400, we should be at break-even in the next 75 days, so a total of 150 days to break-even IF the price is at $400, which isn't unreasonable. Everything after that is profit, minus the electrical cost, which account for such a small %. So far, Mining Ethereum is worth it, but it takes Patience to see the value, and Time to let the value grow, if you believe in that sort of thing.Good Hodling, and happy mining,Will update this post in 75 days and see how close the prediction was.Check back then,", "sentiment": 0.10370978915765645},
{"link_title": "PureScript News", "url": "http://purescript.news/", "text": "PureScript News is a curated publication of content relevant to anyone interested in PureScript.\n\nEach issue contains the most interesting developments in PureScript and functional programming from multiple sources, all in one place.\n\nIf you have content suggestions or other feedback, I'd love to hear from you via email at paul@purescript.news or on Twitter @py.", "sentiment": 0.28928571428571426},
{"link_title": "What the Rich Won't Tell You", "url": "https://www.nytimes.com/2017/09/08/opinion/sunday/what-the-rich-wont-tell-you.html", "text": "We often imagine that the wealthy are unconflicted about their advantages and in fact eager to display them. Since Thorstein Veblen coined the term \u201cconspicuous consumption\u201d more than a century ago, the rich have typically been represented as competing for status by showing off their wealth. Our current president is the conspicuous consumer in chief, the epitome of the rich person who displays his wealth in the glitziest way possible.\n\nYet we believe that wealthy people seek visibility because those we see are, by definition, visible. In contrast, the people I spoke with expressed a deep ambivalence about identifying as affluent. Rather than brag about their money or show it off, they kept quiet about their advantages. They described themselves as \u201cnormal\u201d people who worked hard and spent prudently, distancing themselves from common stereotypes of the wealthy as ostentatious, selfish, snobby and entitled. Ultimately, their accounts illuminate a moral stigma of privilege.\n\nThe ways these wealthy New Yorkers identify and avoid stigma matter not because we should feel sorry for uncomfortable rich people, but because they tell us something about how economic inequality is hidden, justified and maintained in American life.\n\nKeeping silent about social class, a norm that goes far beyond the affluent, can make Americans feel that class doesn\u2019t, or shouldn\u2019t, matter. And judging wealthy people on the basis of their individual behaviors \u2014 do they work hard enough, do they consume reasonably enough, do they give back enough \u2014 distracts us from other kinds of questions about the morality of vastly unequal distributions of wealth.\n\nTo hide the price tags is not to hide the privilege; the nanny is no doubt aware of the class gap whether or not she knows the price of her employer\u2019s bread. Instead, such moves help wealthy people manage their discomfort with inequality, which in turn makes that inequality impossible to talk honestly about \u2014 or to change.\n\nThe stigma of wealth showed up in my interviews first in literal silences about money. When I asked one very wealthy stay-at-home mother what her family\u2019s assets were, she was taken aback. \u201cNo one\u2019s ever asked me that, honestly,\u201d she said. \u201cNo one asks that question. It\u2019s up there with, like, \u2018Do you masturbate?\u2019 \u201d\n\nAnother woman, speaking of her wealth of over $50 million, which she and her husband generated through work in finance, and her home value of over $10 million, told me: \u201cThere\u2019s nobody who knows how much we spend. You\u2019re the only person I ever said those numbers to out loud.\u201d She was so uncomfortable with having shared this information that she contacted me later the same day to confirm exactly how I was going to maintain her anonymity. Several women I talked with mentioned that they would not tell their husbands that they had spoken to me at all, saying, \u201cHe would kill me,\u201d or \u201cHe\u2019s more private.\u201d\n\nThese conflicts often extended to a deep discomfort with displaying wealth. Scott, who had inherited wealth of more than $50 million, told me he and his wife were ambivalent about the Manhattan apartment they had recently bought for over $4 million. Asked why, he responded: \u201cDo we want to live in such a fancy place? Do we want to deal with the person coming in and being like, \u2018Wow!\u2019 That wears on you. We\u2019re just not the type of people who wear it on our sleeve. We don\u2019t want that \u2018Wow.\u2019 \u201d His wife, whom I interviewed separately, was so uneasy with the fact that they lived in a penthouse that she had asked the post office to change their mailing address so that it would include the floor number instead of \u201cPH,\u201d a term she found \u201celite and snobby.\u201d\n\nMy interviewees never talked about themselves as \u201crich\u201d or \u201cupper class,\u201d often preferring terms like \u201ccomfortable\u201d or \u201cfortunate.\u201d Some even identified as \u201cmiddle class\u201d or \u201cin the middle,\u201d typically comparing themselves with the super-wealthy, who are especially prominent in New York City, rather than to those with less.\n\nWhen I used the word \u201caffluent\u201d in an email to a stay-at-home mom with a $2.5 million household income, a house in the Hamptons and a child in private school, she almost canceled the interview, she told me later. Real affluence, she said, belonged to her friends who traveled on a private plane.\n\nOthers said that affluence meant never having to worry about money, which many of them, especially those in single-earner families dependent on work in finance, said they did, because earnings fluctuate and jobs are impermanent.\n\nAmerican culture has long been marked by questions about the moral caliber of wealthy people. Capitalist entrepreneurs are often celebrated, but they are also represented as greedy and ruthless. Inheritors of fortunes, especially women, are portrayed as glamorous, but also as self-indulgent.\n\nThe negative side of this portrayal may be more prominent in times of high inequality (think of the robber barons of the Gilded Age or the Gordon Gekko figures of the 1980s). In recent years, the Great Recession and Occupy Wall Street, which were in the background when I conducted these interviews, brought extreme income inequality onto the national stage again. The top 10 percent of earners now garner over 50 percent of income nationally, and the top 1 percent over 20 percent.\n\nIt is not surprising, then, that the people I talked with wanted to distance themselves from the increasingly vilified category of the 1 percent. But their unease with acknowledging their privilege also grows out of a decades-long shift in the composition of the wealthy. During most of the 20th century, the upper class was a homogeneous community. Nearly all white and Protestant, the top families belonged to the same exclusive clubs, were listed in the Social Register, educated their children at the same elite institutions.\n\nThis class has diversified, thanks largely to the opening of elite education to people of different ethnic and religious backgrounds starting after World War II, and to the more recent rise of astronomical compensation in finance. At the same time, the rise of finance and related fields means that many of the wealthiest are the \u201cworking rich,\u201d not the \u201cleisure class\u201d Veblen described. The quasi-aristocracy of the WASP upper class has been replaced by a \u201cmeritocracy\u201d of a more varied elite. Wealthy people must appear to be worthy of their privilege for that privilege to be seen as legitimate.\n\nBeing worthy means working hard, as we might expect. But being worthy also means spending money wisely. In both these ways, my interviewees strove to be \u201cnormal.\u201d\n\nTalia was a stay-at-home mom whose husband worked in finance and earned about $500,000 per year. They were combining two apartments in a renovation, and they rented a country home. \u201cWe have a pretty normal existence,\u201d she told me. When I asked what that meant, she responded: \u201cI don\u2019t know. Like, dinners at home with the family. The kids eat, we give them their bath, we read stories.\u201d It wasn\u2019t as if she was dining out at four-star restaurants every night, she said. \u201cWe walk to school every morning. And, you know, it\u2019s fun. It\u2019s a real neighborhood existence.\u201d\n\nScott and his wife had spent $600,000 in the year before our conversation. \u201cWe just can\u2019t understand how we spent that much money,\u201d he told me. \u201cThat\u2019s kind of a little spousal joke. You know, like: \u2018Hey. Do you feel like this is the $600,000 lifestyle? Whooo!\u2019 \u201d Rather than living the high life that he imagined would carry such a price tag, he described himself as \u201cfrenetic,\u201d asserting, \u201cI\u2019m running around, I\u2019m making peanut butter and jelly sandwiches.\u201d Having money does not mean, in his view, that he is not ordinary.\n\nThe people I talked with never bragged about the price of something because it was high; instead, they enthusiastically recounted snagging bargains on baby strollers, buying clothes at Target and driving old cars. They critiqued other wealthy people\u2019s expenditures, especially ostentatious ones such as giant McMansions or pricey resort vacations where workers, in one man\u2019s sarcastic words, \u201cmassage your toes.\u201d\n\nThey worried about how to raise children who would themselves be \u201cgood people\u201d rather than entitled brats. The context of New York City, especially its private schools, heightened their fear that their kids would never encounter the \u201creal world,\u201d or have \u201cfluency outside the bubble,\u201d in the words of one inheritor. Another woman told me about a child she knew of whose father had taken the family on a $10,000 vacation; afterward the child had said, \u201cIt was great, but next time we fly private like everyone else.\u201d\n\nTo be sure, these are New Yorkers with elite educations, and most are socially liberal. Wealthy people in other places or with other histories may feel more comfortable talking about their money and spending it in more obvious ways. And even the people I spoke with may be less reticent among their wealthy peers than they are in a formal interview.\n\nNonetheless, their ambivalence about recognizing privilege suggests a deep tension at the heart of the idea of American dream. While pursuing wealth is unequivocally desirable, having wealth is not simple and straightforward. Our ideas about egalitarianism make even the beneficiaries of inequality uncomfortable with it. And it is hard to know what they, as individuals, can do to change things.\n\nIn response to these tensions, silence allows for a kind of \u201csee no evil, hear no evil\u201d stance. By not mentioning money, my interviewees follow a seemingly neutral social norm that frowns on such talk. But this norm is one of the ways in which privileged people can obscure both their advantages and their conflicts about these advantages.\n\nAnd, as they try to be \u201cnormal,\u201d these wealthy and affluent people deflect the stigma of wealth. If they can see themselves as hard workers and reasonable consumers, they can belong symbolically to the broad and legitimate American \u201cmiddle,\u201d while remaining materially at the top.\n\nThese efforts respond to widespread judgments of the individual behaviors of wealthy people as morally meritorious or not. Yet what\u2019s crucial to see is that such judgments distract us from any possibility of thinking about redistribution. When we evaluate people\u2019s moral worth on the basis of where and how they live and work, we reinforce the idea that what matters is what people do, not what they have. With every such judgment, we reproduce a system in which being astronomically wealthy is acceptable as long as wealthy people are morally good.\n\nCalls from liberal and left social critics for advantaged people to recognize their privilege also underscore this emphasis on individual identities. For individual people to admit that they are privileged is not necessarily going to change an unequal system of accumulation and distribution of resources.\n\nInstead, we should talk not about the moral worth of individuals but about the moral worth of particular social arrangements. Is the society we want one in which it is acceptable for some people to have tens of millions or billions of dollars as long as they are hardworking, generous, not materialistic and down to earth? Or should there be some other moral rubric, that would strive for a society in which such high levels of inequality were morally unacceptable, regardless of how nice or moderate its beneficiaries are?", "sentiment": 0.14719131715019862},
{"link_title": "International Fellowship applications for Part 1 now open", "url": "http://www.fast.ai/2017/09/08/international-fellowship/", "text": "In both our previous deep learning courses at USF (which were recorded and formed the basis of our MOOCs), we allowed students that could not participate in person to attend via video and text chat through our International Fellowship. As Rachel described in discussing our last course, this (along with our Diversity Fellowship program) was an important part of our mission:\n\n\u201c\u2026we worked hard to curate a diverse group of participants, because we\u2019d observed that artificial intelligence is missing out because of its lack of diversity. A study of 366 companies found that ethnically diverse companies are 35% more likely to perform well financially, and teams with more women perform better on collective intelligence tests. Scientific papers written by diverse teams receive more citations and have higher impact factors.\u201d\n\nIn fact, many of our strongest students and most effective projects have come from the International Fellowship. By opening up the opportunity to learn deep learning in a collaborative environment, students have been able to apply this powerful technology to local problems in their area. For instance, past International Fellows are working to:\n\nThis year, we\u2019re presenting an entirely new version of part 1 of our deep learning course, and today we\u2019re launching the International Fellowship for it. The program allows those who can not get to San Francisco to attend virtual classes for free during the same time period as the in-person class and provides access to all the same online resources. (Note that International Fellowships do not provide an official completion certificate through USF). International fellows can come from anywhere on the planet other than San Francisco (including from the US), but need to be able to attend each class via Youtube Live at 6.30pm-9pm Pacific Time each Monday for 7 weeks from Oct 30, 2017 onwards. For many people that means getting up in the middle of the night\u2014but our past students tell us it\u2019s worth it!\n\nTo apply, email your resume to rachel@fast.ai, and:\n\nThe deadline to apply is 10/6.\n\nTo discuss this post at Hacker News, click here", "sentiment": 0.1303598484848485},
{"link_title": "Spotify no longer streams music in Apple's Safari web browser", "url": "https://community.spotify.com/t5/forums/v3_1/forumtopicpage/board-id/001/thread-id/194927/page/1", "text": "It seems that the Safari browser is no longer supported for the Web Player. I've been using it for years without a problem. The Spotify System requirements page even says it can run in Safari 6 or higher: https://support.spotify.com/us/article/spotify-system-requirements/\n\nI have Safari 10 on Mac OS 10.12.6, but when I try to open play.spotify.com now, I get the message, \"This browser doesn't support Spotify Web Player. Switch browsers or download Spotify for your desktop\" (see screen shot).\n\nI checked the Switch browsers link, and sure enough, Spotify Web Player no longer supports Safari!\n\nThanks a lot, Spotify, but I don't want to switch browsers or use your desktop player!\n\nEither restore Safari support, or I'm off to Apple Music, Amazon Music, Google Music, or something else. And while you're at it, dump Flash and use HTML5!\n\nEDIT: I thought I replied to @SebastVAIO but don't see it, so here it is again:\n\nplay.spotify.com redirects to open.spotify.com and then I get the error message. Deleting cache and cookies has no effect. I contacted Spotify Support and this is what they said:\n\n\"After taking a look backstage, we can confirm that after recent updates Safari is no longer a supported browser for Web Player.\n\nWe're always testing things by adding or removing features to make Spotify better overall. We\u2019re sorry that this means you\u2019re not able to use the Web Player like you could before.\n\nWe can't say if or when any specific features will be back. But as soon as we\u2019ve got anything to announce, we\u2019ll let everyone know via the Spotify Community.\n\nSorry again for any inconvenience caused, and please let us know if there is anything else we can do for you.\n\nBest wishes,\n\nRollie\n\nSpotify Customer Support\"\n\nIn other words, Safari users have no choice but to switch browsers or use the desktop app. I did a little experimenting (switched my user agent to Chrome), and I got a new message to \"Enable player in your browser,\" which leads to this page:\n\nApparently it has something to do with the Google Widevine content decryption module, which Apple doesn't support because it's not very secure. I tried enabling the Widevine plugin and got the attached message. Looks like Apple is having a pissing contest with Google, Spotify, and anyone else who uses Widevine. In the meantime, we users are caught in the crossfire.", "sentiment": 0.05434302539565696},
{"link_title": "How to steer a spacecraft into Saturn", "url": "https://www.washingtonpost.com/national/health-science/how-to-steer-a-spacecraft-into-saturn/2017/09/09/ce6a8d18-74af-11e7-8839-ec48ec4cae25_story.html", "text": "PASADENA, Calif. \u2014 A billion-dollar spacecraft named Cassini is about to burn up as it plunges into the atmosphere of Saturn this month. That\u2019s the plan, exquisitely crafted. Cassini will transmit data to Earth to the very end, squeezing out the last drips of science as a valediction for one of NASA\u2019s greatest missions.\n\nDreamed up when Ronald Reagan was president, and launched during the tenure of Bill Clinton, Cassini arrived at Saturn in the first term of George W. Bush. So it\u2019s old, as space hardware goes. It has fulfilled its mission goals and then some. It has sent back stunning images and troves of scientific data. It has discovered moons, and geysers spewing from the weird Saturn satellite Enceladus. It landed a probe on the moon Titan.\n\nIt has also run out of gas, basically, though precisely how much fuel is left is unknown. Program manager Earl Maize says, \u201cOne of our lessons learned, and it\u2019s a lesson learned by many missions, is to attach a gas gauge.\u201d\n\nThe spacecraft is tracked in the Charles Elachi Mission Control Center of the Jet Propulsion Laboratory in Pasadena, Calif. Mission Control is a darkened chamber with no external windows. The room (named after a retired JPL director) is dominated by glowing screens and people peering into consoles. Someone wandering into the place by accident would think: This looks like the kind of place where they fly spaceships.\n\nOn the far wall is a screen showing the operations of the three huge radio antennae \u2014 in the California desert; near Madrid; and in Canberra, Australia \u2014 that together make up NASA\u2019s Deep Space Network. As the Earth turns, there\u2019s always a big dish looking out for Cassini, and for JPL\u2019s other spacecraft roaming the solar system.\n\nThe navigators have a computer model that tells them where the spacecraft probably is and probably will be.\n\n\u201cWe need to be able to point instruments to objects. Nothing is static. Everything is moving. The timing is critical,\u201d said navigation team leader Duane Roth. \u201cWe don\u2019t know exactly where Titan is at any given moment, or where Saturn is, or where Cassini is. When you want to propagate that out to some future time, all our errors grow.\u201d\n\nBut they\u2019re getting it done.\n\nCassini\u2019s final orbits have taken it, amazingly, inside the rings of Saturn, where the spacecraft practically skims the tops of the planet\u2019s clouds. These orbits can plausibly be compared to Luke Skywalker flying into that narrow trench on the Death Star.\n\nThe navigators here at NASA\u2019s Jet Propulsion Laboratory do not boast of their prowess, however. For them, it\u2019s just . . . math.\n\n\u201cThe key is to calculate this change in velocity,\u201d said navigation team member Mar Vaquero as she explained a complex set of equations on a whiteboard in her workspace at the lab. \u201cSo you use math. You have matrices. And you have partials. Those are changes in your trajectories with respect to each parameter. So you use your matrices, your vectors, position and velocity and your partials to come up with this delta V that you see here.\u201d\n\nSo they\u2019ve done the calculations, and they\u2019ve plotted the trajectory. If the atmosphere is thicker than expected, they might have to send a slight course correction using small hydrazine thrusters. But really, there\u2019s not much to do other than let gravity handle everything, and watch the data come in, and clap, and maybe shed a few tears.\n\n\u201cWe\u2019re kind of going through the mourning cycle,\u201d said Julie Webster, head of spacecraft operations.\n\n\u201cYou form a family,\u201d said Linda Spilker, the Cassini project scientist, speaking of the team. \u201cYour kids grow up together.\u201d\n\nCassini closes out an era in NASA space science. This is hardly the end of solar system exploration, but it\u2019s essentially the end of the first, heroic phase \u2014 the initial reconnaissance of the planets.\n\nSixty years ago, the Soviet Union put the first satellite, Sputnik, into orbit. Within a few years, there were spacecraft flying by the moon, crashing into the moon, even landing softly on the moon. More would go winging by Mars to see for the first time the craters and canyons and volcanoes of that desert planet.\n\nForty-one years ago, NASA soft-landed the two Viking probes on Mars and scratched the surface looking for signs of life (the results are disputed, but the smart money says the surface is sterile).\n\nThis year, NASA marked the 40th anniversary of the astonishing Voyager program \u2014 two robotic spacecraft that explored the outer solar system, the first Voyager flying by Jupiter and Saturn, the second flying by Jupiter, Saturn, Uranus and Neptune \u2014 a solar system superfecta, to borrow a term from the horse track. The two Voyagers are now out in the exurbs of the solar system, far beyond the orbit of even the dwarf planet Pluto.\n\nThe colossal scale of Cassini is a legacy of the go-big mentality of the early days of space exploration. The United States put men on the moon with a jumbo rocket, and NASA for a long time skewed toward muscle-bound spacecraft even when humans weren\u2019t along for the ride.\n\nNo single event changed everything, but what happened to a spacecraft called Mars Observer in 1993 certainly had an impact. It was large and fully adorned with instruments. And then, one day shortly before it was to go into Mars orbit, it simply went silent.\n\nWebster was part of the Mars Observer team and remembers how, for many days, JPL staffers tried to reconnect with the spacecraft. But Mars Observer was never heard from again. Webster said that the fuel tanks were being pressurized with helium in advance of the Mars orbital insertion. \u201cProbably the pressurization system had a leak somewhere and it essentially blew up.\u201d\n\nSpace is hard. Space will break your heart.\n\n\u201cIt\u2019s like a loss of a family member,\u201d Webster said.\n\nBy that point, Cassini had already been conceived, the instruments already coming online, and so it was essentially grandfathered in to the old-fashioned go-big protocol. NASA Administrator Dan Goldin wasn\u2019t a fan. He had a name for Cassini: \u201cBattlestar Galactica.\u201d\n\nActually, it wasn\u2019t simply the \u201cCassini\u201d mission. It was the \u201cCassini-Huygens\u201d mission. The Europeans designed the Huygens probe, a separate vehicle that detached from Cassini when it passed close to Titan.\n\nAfter Cassini, launched in 1997, arrived at Saturn in 2004, Huygens disengaged from the main spacecraft and dropped through Titan\u2019s thick clouds. It sent back details of an alien world that possesses a stew of complex organic molecules, including liquid methane. Hydrocarbons rain from the sky. There are lakes and rivers.\n\nIt\u2019s the only place in the solar system other than Earth known to have rain and open bodies of liquid on the surface.\n\nCassini also discovered something amazing about Saturn\u2019s moon Enceladus: It has geysers spewing from its south pole. Almost certainly it has an interior ocean, sealed beneath ice, that contains great volumes of water and possibly hydrothermal vents.\n\nSomeday NASA or some other space agency is likely to send a probe to Enceladus to sample those geysers and test them for indications of life.\n\n\u201cThe legacy for which Cassini will be remembered will be Enceladus,\u201d said project scientist Spilker.\n\nExploration begets more exploration. Every mission drops a rope ladder in its wake.\n\nCassini has slowed down slightly in its final few orbits as it has passed through the outermost layers of Saturn\u2019s atmosphere. The drag on the spacecraft hastens the final plunge slightly.\n\nAt about 1:37 a.m. Pacific Daylight Time on Sept. 15, the spacecraft will roll into position to enable one of its instruments to sample Saturn\u2019s atmosphere as it gets closer and closer to the planet. It will stream data back to the Deep Space Network.\n\nIn the final minute of its life, Cassini will fire its thrusters in an attempt to keep its high-gain antenna pointing to Earth. But that is a battle Cassini is destined to lose.\n\nThe navigators at the Jet Propulsion Laboratory are still calculating precisely when the spacecraft will send its final signal on Sept. 15. At last report, it will be 4:55 a.m. Pacific Daylight Time, about 13 minutes earlier than the time calculated a couple of months ago.\n\nBut it will actually be already gone, in a sense. It will actually have been destroyed 83 minutes earlier. That\u2019s how long it takes at the speed of light for news to travel from Saturn to Pasadena.\n\nCassini won\u2019t exactly \u201ccrash\u201d into Saturn, because it\u2019s a gaseous planet and there\u2019s no surface to hit. In the last moments, the spacecraft will go into a tumble and lose contact with Earth. Then it will burn up as it plunges through Saturn\u2019s atmosphere. It will disintegrate.\n\nAnd then nothing will be left.\n\n\u201cIt\u2019ll just be vaporized and completely disassociated,\u201d said Maize.\n\n\u201cIt will become part of Saturn.\u201d", "sentiment": 0.1112333475056689},
{"link_title": "Understanding Moscow: The Mysteries of the Russian Mindset", "url": "http://www.spiegel.de/international/world/understanding-moscow-the-mysteries-of-the-russian-mindset-a-1162072.html", "text": "Recently, I came across a book at home that I hadn't thought about in a long time. It's called \"Russia: Faces of a Torn Country.\" The title may not be particularly original, but it is certainly apt. The book covers Russia as it was a quarter-century ago: a kind of madhouse. The Soviet Union had just collapsed, hopes for a new beginning had proven largely illusionary, ex-functionaries and cunning businessmen had seized the inheritance of the Soviet Union for themselves and were enjoying their sudden wealth as the rest of the country slid into poverty. Grandmothers stood in the wind and rain for hours at the tolkuchkas, flea markets, trying to sell their wedding china alongside students advertising their lovingly assembled stamp collections. Meanwhile, war raged at the periphery of the realm.\n\nBack in 1991, everyday Russians couldn't explain what Russia represented, where it was heading politically and how all its conflicts could be resolved. We journalists, of course, couldn't either.\n\nAll that is now history and, all things considered, Russia isn't doing so badly these days. The book mentioned above was written by me and includes profiles of 18 people trying to find their place in Russia. They were typical of the transition period: politicians and generals, businesspeople and artists, idealists, populists and criminals.\n\nSome of them are no longer alive -- a couple were killed while others left the country or climbed up the ranks of the government. Examining the profiles from today's perspective, it isn't difficult to understand how some were left behind while others went on to have successful careers. You can also see how Russia managed to regain its footing.\n\nOne of the book's heroes is Dzhokhar Dudayev, who declared Chechnya's independence from Russia in 1991 and called for the people of the Caucasus to resist the Moscow colonizers. Russia launched a war because of him, deploying 60,000 soldiers into the small republic. Three months after I spoke with the Chechen president, his office, in which we had met, was flattened. Fifteen months after that, a Russian missile took his life. Another 15 years later, there was peace in Chechnya. As many as 160,000 people are believed to have been killed in the war. Since then, no region has attempted to secede from Russia.\n\nAnother of the book's heroes was biochemist and skin specialist Sergei Debov. He too is now dead. Debov joined the secret \"Lenin Mausoleum\" unit in 1952, a group of scientists that embalmed the revolutionary leader, who has been lying in state in Moscow since his death in 1924. For almost 40 years, Debov, who also embalmed Stalin, freshened up Lenin's body with a secret solution twice a week.\n\nThen the Soviet Union and communism collapsed and President Boris Yeltsin slashed the secret unit's financing as well as the honor guard in front of the mausoleum. Lenin became a national pariah and citizen's initiatives began calling for him to be buried in a cemetery in St. Petersburg. Debov told me he was shocked: \"Removing Lenin from the history of Russia -- that's unacceptable.\"\n\nNothing To Regret in Russian History?\n\nThe fact that the revolutionary leader is still displayed on Red Square 25 years later, and that Debov's successors are still at work, also helps explain how Russia found its way back to stability. The leader of the October Revolution, who, like Stalin, didn't care how many of his people were sacrificed for the communist idea, is still an important political symbol. His continued presence calms the adherents of communism, but also represents the belief among the Kremlin's leadership that there is nothing to regret in Russia's history. It's reflective of the way history is viewed in Vladimir Putin's Russia.\n\nA third character in the book is still alive and is now 53 years old. He was born on Stalin's birthday, rose to become deputy prime minister and is today responsible for the Russian defense industry. I met Dmitry Rogozin when he was 31 years old. Just a short time earlier, he had been active with Komsomol, the youth wing of the Communist Party. He later became an ambassador to NATO and shocked the Western military with his off-the-cuff statements. We often met in Brussels.\n\nAfter the collapse of the Soviet Union, Rogozin was placed in charge of the fate of the 25 million ethnic Russians who now lived outside the country's borders in the other former Soviet republics. He founded the Congress of Russian Communities to safeguard their interests and became a thought leader on the concept of the \"Russian world\" -- a world which includes all corners of the globe where Russian people live and which, according to Putin, must be defended. Rogozin is now something of a nationalist mouthpiece for the government and just recently once again described Western politicians as \"scum.\"\n\nThe reclamation of Chechnya, the rehabilitation of Soviet history and the reinvocation of the \"Russian world\" -- similar to what Donald Trump is now doing in the United States under his \"America First\" motto -- all of that helped save young Russia. And Russian gratitude has been directed at Putin, which is reflected in the president's 80 percent approval rating.\n\nThe new Russia is visible in many places. A few weeks ago, I traveled through the small western Russian town of Gvardeysk, with only 13,000 inhabitants. The last time I had visited the town was in 1998, just after the great ruble crisis that brought the government to the verge of bankruptcy. The paper mill closed in the wake of that crisis, followed by the reinforced concrete factory and then the cheese factory. The heating plants no longer had any coal, and three-quarters of the inhabitants lived below the poverty line.\n\nIt was even cold in the hospital, which lacked medication and even gloves for the surgeons. The army barracks and the prison, which was located in an old castle, were running low on food. In the villages around Gvardeysk, insufficient nourishment and dirty drinking water led to cases of tuberculosis and meningitis.\n\nNow, in 2017, the houses on the central square are freshly painted, a furniture factory has opened as has a meat processing plant and a factory for packaging material. There is a youth center and a gym. And the castle/jail is to be transformed into a tourist attraction.\n\nIf you drive eastward, of course, deep into the more rural areas, a different reality emerges, with entire villages dying. But there's no sign of the national state of emergency that paralyzed Russia two decades ago. Especially not in Moscow, which has transformed itself into a modern metropolis, with pedestrian zones, giant supermarkets, jazz clubs and avant-garde theaters, with Wi-Fi in the streets and even below-ground in the metro.\n\nBut there is something that hasn't changed in either the capital or in the rest of the country. I recently came across it while reading a notice in my Moscow apartment building that symbolizes a phenomenon that has been part of Russia for centuries and that puts Putin's 80 percent approval rating into context.\n\nThe notice pertained to the announcement by the Moscow municipal government of its plan to demolish 4,500 dilapidated apartment buildings. The buildings are five-story prefabricated monstrosities, ugly and often crumbling. But around 1 million Moscow residents are affected by the plan, roughly one-twelfth of the city's overall population. And tthe city government has pushed ahead with its plan so ruthlessly, even shoving a relevant law through parliament at lightning speed, that a storm of indignation broke out -- even in my neighborhood, which isn't home to any of the buildings slated for demolition.\n\nThe notice in my building came from an initiative called Muscovites Against the Demolition. They argue that the plan is a disgraceful form of forced resettlement -- that it wasn't just about prefabricated high-rise buildings, but about providing real estate for the construction of profitable high-rises to construction companies with close ties to the government. Residents who refused to move would be forcibly relocated, the notice claimed, and there would be no compensation for renovations carried out by renters. Many of those who owned their apartments, the group argued, would not receive a new residence of equivalent value.\n\nThe uproar in Moscow is immense. The government, which had said it wanted to do something good for the city's inhabitants, seemed to be totally surprised by the pushback. Even Putin had to intervene and urge the Russian parliament, the Duma, to slightly modify the law, because a presidential election is scheduled for 2018 and he doesn't need protests from angry citizens.\n\nIt's a common story in Russia. Even when the leadership tries to do something good for its people, things go wrong -- because the government takes decisions on its own and then presents them to the people like a Christmas present. And because it tries to realize its projects in a Bolshevik manner. The notion that there could be objections among the people is not something that Russian politicians tend to consider.\n\nThe debate about the resettlement of those living in the apartment buildings shows yet again that there is still no feedback loop in the Russian political system. The government doesn't make any serious effort to include the people in its decision-making. Political resolutions are presented either as favors or prohibitions -- which also helps explain the new wave of protests in Moscow and other cities. The people and the government rarely come together in Russia.\n\nWriter Viktor Erofeyev once said that it is a country of barriers and \"the normal position of the barrier is 'closed.'\" He also asked: \"The homeland happily allows you to love her, but -- does she also love you back? Does Russia love us?\" Erofeyev believes that the Russians' love for Russia isn't based on reciprocity, which is something I've also repeatedly noticed in the past decades. But he believes that the Russians themselves are to blame -- because they don't take sufficient interest in the state.\n\nSeveral months ago, I had a dispute about this with respected filmmaker and theater director Andrei Konchalovsky. He is turning 80 this year, has made some of Russia's best films and has lived in Hollywood for a long time. Despite our disagreements, we were very close to one another in our views of many things. Konchalovsky says Russians have retained the soul of a peasant over the centuries, arguing that Russians never became citizens in the true sense of the word and always positioned themselves in opposition to the state, because the government is always trying to take something away from them. At the same time, he argues, Russians are so enormously patient that they can more easily accept injustices. He also argues that Russian thinking is Manichean -- that Russians only know black and white.\n\nAnd then Konchalovsky said that Putin initially thought like a Westerner, but ultimately realized why every Russian ruler struggles to lead this nation: Because its inhabitants, in accordance with an unshakable tradition, freely delegate all their power to a single person, and then wait for that power to take care of them, without doing anything themselves.\n\nIn that sense, the relationship between people and state in Russia is a vast misunderstanding. Is a foreigner allowed to say such a thing? I think so. I have been reporting on Russia for over 30 years and lived half of that time in the country. It's clear to me why the liberals associated with Boris Yeltsin failed in the 1990s. Liberalism has no chance in Russia. The people won't allow it.\n\nThe strange relationship between many Russians and their government is also manifest in myriad everyday details. Two or three years ago, Moscow's mayor tried solving the parking problem by introducing an online parking system. The charges were low, with an hour usually costing less than a euro. The problem was largely relieved and the system worked for everyone. And then what happened? Muscovites began covering their license-plate numbers so that the inspection vehicles couldn't scan the numbers as they drove, thus making it impossible for them to find any violators.\n\nAnother example: For decades, few new streets, let alone highways, have been built in Russia. But now there are plans to build a new highway between Moscow and St. Petersburg. The first stretch, which leads to Moscow's Sheremetyevo international airport, is already open. As a toll road, however, it is hardly getting used, despite the relatively low charges. Russia's drivers believe it is a government rip-off and prefer sitting in traffic jams on the old road.\n\nThe notion that citizens must do something for society and that they will get something back is only rarely encountered in Russia. The Russians may honor their actors and poets far more than the Germans do theirs, but they take a skeptical view of the truly creative people, who, in their own way, try to advance the debate about the future direction of the country.\n\nWriter Boris Akunin sells millions of books, but he lives outside the country because he can't stand his government's politics. The same is true of fellow writer Vladimir Sorokin, who was long harassed by political organizations close to the government. Internationally renowned director Kirill Serebrennikov has also been pressured, with police units recently having stormed his theater. His ballet \"Nureyev\" at the Bolshoi Theater was canceled three days before its premiere after having faced heavy resistance from conservative politicians. The cancellation also affected me, because I had managed to obtain one of the hard-to-get Bolshoi tickets for that evening.\n\nSuch overreach bothers only a small number of Russians. Apart from a few voices in the Moscow intelligentsia, there are no protests.\n\n\"We are a people, we love those who are similar to us, we don't need any dissimilar ones,\" writer Viktor Yerofeyev once stated sarcastically. A government prosecutor who frightens the people at large, he went on, is still closer to them than a reformed oligarch like Mikhail Khodorkovsky, who clear-sightedly criticizes the Putin system.\n\nWhy is that, I wondered recently when I found myself in a police station in central St. Petersburg. Nowhere is it more obvious how the state seeks to make its citizens feel unimportant. The officer on duty didn't even look up when people came to him with their concerns and heavy iron doors barred the entrance to the offices. They only opened every once in a while in accordance to some inexplicable logic. In the offices, records were taken by hand. And the walls were decorated with portraits of Felix Dzerzhinsky! He was the Soviet Union's first intelligence chief, the man who set off the Red Terror and had tens of thousands of people murdered. His monument in front of Moscow's Lubyanka building was the first one to be toppled after the end of the Soviet Union -- and now the police have hung his portrait back up again?\n\nWhy do the Russians accept all of this without saying a word?\n\nTheir passivity and their indifference unpleasantly combine with fatalism and a fear of responsibility and make it impossible for most of them to get to to the core of historical truths. Many are indifferent about the fact that new monuments are being built to Stalin. One Moscow journalist described it as being tantamount to Jews setting up monuments to Hitler.\n\nThe use of force by the state is also still experienced as metaphysical, as a matter of fate. Social philosopher Alexander Zipko argues that an overwhelming majority of the population still has trouble understanding that millions of people lost their lives in the Soviet Union as a result of the Red Terror.\n\nAfter years of research, Denis Karagodin, a 35-year-old from the Siberian city of Tomsk, recently found out which secret service officials were responsible for declaring his great-grandfather Stepan a Japanese spy in 1938, in order to then execute him. Armed with that information, Karagodin then filed a suit against those responsible even though they had long since died. He's the first citizen of Russia who wasn't satisfied with the authorities' formal rehabilitation notice. He wants to bring the executioners to account -- at least symbolically. But his persistence has been met with incomprehension and alienation. The argument being: You can't change anything that happened anyway.\n\nThe things I have written about here were not invented by Vladimir Putin. He merely discovered things that already existed and factored them into his calculations. Fear of personal responsibility? Marginalization of people who think differently? Resignation to fate? Feelings of inferiority toward the rest of the world? These are traits against which the state should be acting. Instead the government strengthens them, because it is useful for it. I only realized in the last few years how much it bothers me, even among Russian friends, most of whom have now succumbed to their president's demagogy.\n\nPutin fires up the Russians' feelings of contempt for Ukrainians, even though -- and I'm convinced of this -- the Russians are jealous that the Ukrainians will now succeed in getting closer to Europe. And he reinforces a feeling of moral and military superiority among the Russians over the West. It has little connection to reality, but isolates the state and the people more and more from the outside world. Putin is making Russia a dissident from the world order and the people are thrilled by it like it's a fairground attraction, even though for many Russians, Europe and America remain the primary reference point for their own lives.\n\nAs I said, Putin didn't invent any of this. He only learned how to masterfully exploit it and to serve this Russian mentality with demagogy, half-truths and lies. That, for me, is the most important realization 25 years after Russia's rebirth.", "sentiment": 0.05262929402663241},
{"link_title": "James Baldwin's Blues", "url": "https://lareviewofbooks.org/article/jimmys-blues", "text": "IN THE EARLY to mid-1960s, when he was nearing the age of 40, a little more than a decade after his first book was published, the novelist, essayist, and playwright James Baldwin \u2014 who died 30 years ago this fall, at age 63 \u2014 began referring to himself as a blues singer. His artistic forebears, he was suggesting, were not the likes of the writer Henry James \u2014 who had obviously influenced his style \u2014 but figures such as the vocalist Bessie Smith. He did not, of course, mean it literally: Baldwin did not begin physically singing the blues. He noted in one essay, \u201cDown at the Cross,\u201d that he \u201ccould not sing,\u201d and he admitted in another piece, \u201cThe Uses of the Blues,\u201d that \u201cI don\u2019t know anything about music.\u201d What he meant, rather, was that for him, writing had come to serve the same purpose as the work of a blues singer.\n\nA widespread idea about the blues is that it is the sad music of the down-and-out, which feeds the unconscious belief that those who sing the blues, literally or metaphorically, are a certain category of people, one whose ranks we will, with luck and proper care, never join. But in truth the blues is a testimony to the inescapable conditions of everyone\u2019s life, which of its very nature includes suffering. The blues is not the cry of those too dumb or unlucky to avoid unnecessary strife, but a response to human truths which we must all face sooner or later. Further, the blues is not really a \u201ccry\u201d at all. A cry suggests that one is so much in the grip of emotion that he or she has lost control, and perhaps the first requirement of art is that the artist be in control of the tools of the given medium \u2014 whether the tool is the brush, horn, voice, or written word. The blues, then, as an art form, is a disciplined, controlled, and thoughtful response to the conditions of human life, a process of transcending our troubles by facing them squarely. It is what the cultural critic Albert Murray called \u201cmusic for good times earned in adversity.\u201d\n\nWhat does it mean for a writer to say that he is a blues singer? Baldwin seems to have had in mind at least two things. One was to testify to the conditions not only of everyday human life, but to the peculiar and intolerable conditions facing American blacks, conditions from which, again, no amount of care on their own part was sufficient to save them \u2014 racism, to put it simply. But in a way, this mission was tied to the necessity of facing the truths of ordinary life. It was Baldwin\u2019s contention that many white Americans sought to enjoy an exalted status, and thus to triumph over, or at least distract themselves from, the condition we all face \u2014 ultimately, death. This necessitated someone to be exalted above, a role black Americans filled nicely. It was necessary to keep blacks in their place so that whites could enjoy theirs; thus could white racists avoid facing the facts of life. This was a self-perpetuating cycle: the more whites felt the need to avoid truths, the greater the effort that went into keeping blacks in their place, and the more brutal those efforts, the greater white guilt became; the stronger the guilt, the greater the necessity of avoiding truth, when doing so was the very root of the evil. So by saying that he was a blues singer, Baldwin meant that like all blues singers, he was concerned with facing the truth, and having his countrymen, of all colors, do the same.\n\nBut Baldwin also had something else in mind, which was to begin to bring the sensibility, rhythms, and beat of the blues \u2014 of black American music in general, including jazz \u2014 to the written word. He had, he wrote, long used the English language as it existed to imitate the styles of great white writers, but now he hoped to recreate that language for his own purposes, not to imitate anything but to use the blues sound to share his experiences. The irony here is that Baldwin\u2019s success in writing like a blues singer is, at best, arguable, while his other purpose was one he had been accomplishing all along: being a blues singer in the sense of facing his own truth and pain and those of his countrymen.\n\nBaldwin\u2019s own pain, his own blues, had many sources. He was born on August 2, 1924, in Harlem Hospital, in the neighborhood that was to shape him. He was illegitimate; his mother, Emma Berdis Jones, was not yet 20 years old, and for a short while, the boy who would become the greatest of all African-American writers was named James Jones. In 1927, Emma Jones married David Baldwin, a transplant from the South and a much older man, possibly born before the Emancipation of 1863. His stepson, James Jones, became James Arthur Baldwin. James Baldwin was a teenager when his mother told him that David was not his biological father, and James was never to learn who that man was. The news came as a great disappointment to him.\n\nBaldwin told an interviewer when he was about 50 that he \u201cnever had a childhood,\u201d and one meaning of that statement has to be that he was too busy helping raise other children to be one himself. The Baldwin family eventually included nine kids. David Baldwin worked in a factory, earning very low wages, and the family lived in appalling poverty, with barely enough to eat. Scarcely able to provide for his family, or to combat the racism and segregation of the era, or to control any element of his life, David Baldwin became embittered, hating the white world and keeping an emotional distance from his family. (His wife called him \u201cMr. Baldwin.\u201d) He often beat his children, and even his intermittent efforts to relate to his family ended in disaster: as Baldwin wrote in the essay \u201cNotes of a Native Son,\u201d whenever his father\n\ntook one of his children on his knee to play, the child always became fretful and began to cry; when he tried to help one of us with our homework the absolutely unabating tension which emanated from him caused our minds and our tongues to become paralyzed, so that he, scarcely knowing why, flew into a rage, and the child, not knowing why, was punished. [\u2026] I do not remember, in all those years, that one of his children was ever glad to see him come home.\n\nIronically, this dark-spirited man aimed to bring light to others as a preacher, though not with much success, going \u201cfrom church to smaller and more improbable church\u201d and finding himself \u201cin less and less demand,\u201d as Baldwin put it.\n\nWhen James Baldwin\u2019s perceptions moved beyond his wretched home life to the surrounding streets, he saw what awaited him and others his age. \u201cCrime became real [\u2026] \u2014 for the first time \u2014 not as a possibility but as the possibility,\u201d he wrote in \u201cDown at the Cross,\u201d the first of two essays in The Fire Next Time. \u201cOne would never defeat one\u2019s circumstances by working and saving one\u2019s pennies; one would never, by working, acquire that many pennies, and, besides, the social treatment accorded even the most successful Negroes proved that one needed, in order to be free, something more than a bank account.\u201d Baldwin went on to add that every boy in his situation\n\nrealizes, at once, profoundly, because he wants to live, that he stands in great peril and must find, with speed, a \u201cthing,\u201d a gimmick, to lift him out, to start him on his way. And it does not matter what the gimmick is. It was this last realization that terrified me and \u2014 since it revealed that the door opened on so many dangers \u2014 helped to hurl me into the church.\n\nAt 14, Baldwin became a boy preacher. Because he had a feel for it, and because his youth was a draw, he proved to be more popular in the pulpit than his own father, the first but not last father figure Baldwin would take satisfaction in besting.\n\nAmong James Baldwin\u2019s many gifts was that of facing and dealing with truths. Over his three years as a teenage preacher, he came to view the church as an institution founded not on love but on fear, self-protection, and exclusion, as a way of preparing for a world that was assuredly better than this one while remaining unconcerned about the fate of those outside one\u2019s narrow circle. Baldwin had revealed himself by this time as an extremely bright boy and attended the prestigious public high school DeWitt Clinton, where many of his classmates were Jews; the belief that his friends there would suffer eternal damnation because of the accident of their heritage seemed to him both false and wrong. He also saw that the church to which he belonged had no love or use for whites generally, Christian or otherwise. And while Baldwin could have easily remained in a comfortable niche in the church, whose music he loved and whose texts would help inspire the fire and cadence of his writing, he knew that to be true to himself and his own beliefs he had to leave the church. He felt himself to be a writer, not a preacher, and as he often put it, he \u201cleft the pulpit to preach the gospel.\u201d\n\nBaldwin was similarly honest with himself with regard to other matters, including his sexuality and what awaited him in a country defined by racial animosity. As a young man he dated women but also became involved with men, and he seems never to have made an attempt to hide that \u2014 \u201cI\u2019ve loved a few men; I loved a few women\u201d was how he put it. What seems to have been the harder adjustment was the recognition that his father had been right regarding one thing, which Baldwin referred to in \u201cNotes of a Native Son\u201d as \u201cthe weight of white people in the world.\u201d Baldwin left home in his late teens to find work in a defense plant in New Jersey during World War II, and he wrote of simply not believing, initially, how he was treated in the whites-only restaurants and other establishments he encountered. He recalled vividly the bitterness and anger these experiences created in him, particularly on one night, when he became violent in a restaurant where he was told for the umpteenth time, \u201cWe don\u2019t serve Negroes here.\u201d After coming perilously close to getting himself killed, Baldwin realized, as he put it, that his life was in real danger, \u201cnot from anything other people might do but from the hatred I carried in my own heart.\u201d And it was this realization that led him, with no knowledge of French and nearly no money, to Paris.\n\nBy the time he left the United States, in November 1948, Baldwin had begun writing for the literary journals, such as Commentary, that would help make his reputation before the publication of his first novel, but these short pieces did not bring him any fortune. In Paris, Baldwin found personal poverty to rival what he had known in the United States, and once that was relieved somewhat by the publication of his first two novels, he discovered something harder to escape: that as much as Europe was a sanctuary in some ways from the evils of America, he was, in fact, an American. This was brought home to him, in part, by news of the civil rights struggles he was missing in his time overseas. Baldwin could not truly live as a free man in the country of his birth, but in Europe he could not escape the feeling that he was in hiding and was not where he really ought to be.\n\nAnd so we begin to see the sources of James Baldwin\u2019s blues: being a man without a country, being gay at a time when homosexuality was not only widely decried but could be punished by law, being a despised outsider in his native land, a minority within a minority. These were the blues he set out to sing, in stories, essays, and novels, and as befits the definition of the blues discussed earlier, he did so with great control and with a detachment and wisdom that let him recognize his troubles for what they were, and begin to transcend them. And he attempted to do the same for his native country.\n\nA core fact about the blues is that, while a creation of black Americans, it is also a hybrid form; it combines elements of European melody with black vocal styles, sensibilities, rhythms, and the presence of bent, or \u201cblue,\u201d notes \u2014 the flatted fifths and sevenths, for example, that account in part for the music\u2019s special sound. From the beginning of his career, James Baldwin was a literary blues singer in the sense that the rhythms of his prose combined the formal control of writers of European descent, whose works he had read from childhood, with the rhythms, repetition, and rising cadence of sermons in the black church. Consider a passage from the very first page of Baldwin\u2019s first novel, Go Tell It on the Mountain, published in 1953, when he was living in France. The novel is about the Grimeses, a Harlem family much like Baldwin\u2019s, and its first page describes the family\u2019s routine on Sundays:\n\nThey all rose together on that day; his father, who did not have to go to work, and led them in prayer before breakfast; his mother, who dressed up on that day, and looked almost young, with her hair straightened, and on her head the close-fitting white cap that was the uniform of holy women; his younger brother, Roy, who was silent that day because his father was home.\n\nThat passage is a single sentence, containing three semicolons and eight commas, a long, flowing river of words that would have made Baldwin\u2019s beloved Henry James proud and that is representative of Baldwin\u2019s style. Notice, at the same time, the repetition of \u201cthat day,\u201d which is not strictly necessary for the sake of providing information but evokes church rhythms; one can imagine this passage being spoken from a pulpit, where each uttering of \u201cthat day\u201d would add an almost indefinable dimension to the words, allowing the listener to reflect each time on a different aspect of their meaning.\n\nBaldwin, as already noted, was a proponent of facing the truths of one\u2019s life, of a singing of the blues that involved confrontations, not with others but with oneself. Many of the most significant passages in both his fiction and nonfiction recount either these confrontations or their tragic absence. The character John Grimes in Go Tell It on the Mountain, whose experiences were inspired by those of Baldwin, undergoes a confrontation with himself in the form of a religious conversion at church, feeling physically helpless while the forces of God and Satan struggle for command of his soul and his fellow believers pray loudly in order to \u201cpull him through.\u201d A passage from the near the end of the novel captures, at once, both the notion of confrontation and the blues-like literary hybridization that was Baldwin\u2019s hallmark. This music-like passage contains three sentences; the third one fills nine of the passage\u2019s 11 printed lines and repeats words strategically, with certain repetitions succeeding others that die away:\n\nAnd something moved in John\u2019s body which was not John. He was invaded, set at naught, possessed. This power had struck John, in the head or in the heart; and, in a moment, wholly, filling him with an anguish that he could never in his life have imagined, that he surely could not endure, that even now he could not believe, had opened him up; had cracked him open, as wood beneath the axe cracks down the middle, as rocks break up; had ripped him and felled him in a moment, so that John had not felt the wound, but only the agony, had not felt the fall, but only the fear; and lay here, now, helpless, screaming, at the very bottom of darkness.\n\nWhereas John Grimes of Go Tell It on the Mountain undergoes a confrontation with the self to preserve his soul, David \u2014 the protagonist of Baldwin\u2019s second novel, Giovanni\u2019s Room \u2014 avoids confronting himself, leading to the death of his soul and the physical death of his male lover, the title character. At a time when book companies hesitated to publish novels with homosexual themes for fear of lawsuits, when there were quite a few writers who were gay but very few gay writers, Baldwin boldly took on the subject of homosexual love with Giovanni\u2019s Room, published in 1956, while he was abroad. (In another daring move, he followed up the success of his debut novel not with another book about black characters but with one in which there are none.) David, a white American who is engaged to be married, comes to Paris, where he falls into an affair with Giovanni. When David\u2019s fianc\u00e9e comes to Paris, David distances himself from Giovanni; the distraught Giovanni sinks to prostitution and, eventually, murder, for which he is guillotined. David, meanwhile, indulges his homosexual yearnings in secret while on the surface embracing a socially acceptable lifestyle. The novel\u2019s ending passage, however, suggests the impossibility of outrunning the self. David receives a letter informing him of Giovanni\u2019s execution; Baldwin writes,\n\nI take the blue envelope which Jacques has sent me and tear it slowly into many pieces, watching them dance in the wind, watching the wind carry them away. Yet, as I turn and begin walking toward the waiting people, the wind blows some of them back on me.\n\nDavid fails to sing his own blues, with tragic consequences.\n\nBetween his first two novels, Baldwin had published his first book of nonfiction, the 1955 essay collection Notes of a Native Son. The title piece is an achingly honest and unsentimental nonfiction blues song about Baldwin\u2019s relationship with his father, the realizations at which he arrives with regard to race in the United States, and the lesson he took from his father\u2019s life, which is that, as Baldwin put it, \u201cnothing is ever escaped.\u201d That is the truth behind the ending of Giovanni\u2019s Room; it is, indeed, the animating idea at the center of Baldwin\u2019s greatest works. We can escape nothing, but we can transcend much, provided we take on the task of simply confronting the truth. Baldwin knew this to be a very difficult task \u2014 there is probably no word that recurs more often in Baldwin\u2019s work than \u201cterrifying\u201d \u2014 but he also knew it to be necessary. And for Baldwin, the second part of that task was to infuse his writing with that truth. He wrote in the introduction to his second essay collection, Nobody Knows My Name (1961), \u201c[S]elf-delusion, in the service of no matter what small or lofty cause, is a price no writer can afford. His subject is himself and the world and it requires every ounce of stamina he can summon to attempt to look on himself and the world as they are.\u201d\n\nAs the 1950s wore on, Baldwin felt another self-confrontation looming. As James Campbell notes in his excellent 1991 biography of Baldwin, Talking at the Gates, two occurrences in 1956 went a long way toward leading the writer back to his native land. In Paris, Baldwin attended the Conference of Negro-African Writers and Artists. The preeminent, outspoken black scholar, writer, and civil rights activist W. E. B. Du Bois had been invited to the conference but could not attend, instead sending a message that was read aloud to the conferees: \u201cI am not present at your meeting because the US government will not give me a passport. Any American travelling abroad today must either not care about Negroes or say what the State Department wishes him to say.\u201d\n\nThe very next day, walking down a Paris street, Baldwin passed newsstands that all carried papers with the image of a 15-year-old black girl in Charlotte, North Carolina, who had tried in the wake of Brown v. Board of Education to enter what was up until then an all-white public school. Barring her way into the school was a mob of white men, women, and children who threatened violence and screamed taunts. The girl\u2019s brave action, carried out while Baldwin wrote in Paris cafes, eventually led him to conclude that it was time to go home.\n\nIn 1957, Baldwin returned to the United States, where he got assignments from high-profile magazines to cover civil rights activities and general conditions for blacks in the South. This constituted the writer\u2019s first visit to the region that had produced his stepfather and that had served as the setting for portions of Go Tell It on the Mountain. There, greeted by rank-and-file whites who did not know who he was, treated not as a famous writer but as just another black man and, therefore, just another second-class citizen, he was shocked though not surprised by what he observed and experienced. The bravery of the activists he met would inspire his own activism.\n\nAnd yet, having faced his need to return home, Baldwin did not, as others might have, allow his pride in having accepted that truth to blind him to others; he did not permit his new fellowship with other blacks to cloud his view of them. His gaze continued to penetrate, and as a result his reportage went beyond the constraints, false authority, and pigeonholing that mars the work of many journalists. He plumbed below the surface, he acknowledged uncertainty where he felt it, and he acknowledged that blacks, as embattled and as in-the-right as they might be, were human beings and were not admirable simply by nature of being black. This could be an inconvenient truth, but it was one Baldwin faced; here was the blues singer as reporter. Consider a passage from \u201cThe Dangerous Road Before Martin Luther King,\u201d published in Harper\u2019s Magazine in February 1961. Even in his very positive profile of Martin Luther King Jr., Baldwin \u2014 in praising King \u2014 also tells the truth as he sees it with regard to other blacks of King\u2019s profession and stature:\n\n[T]he Reverend King is not like any preacher I have ever met before. For one thing, to state it baldly, I liked him. It is rare that one likes a world-famous man \u2014 by the time they become world-famous they rarely like themselves, which may account for this antipathy. Yet King is immediately and tremendously winning, there is really no other word for it. [\u2026]\n\nI was not there in a professional capacity, and the questions I wanted to ask him had less to do with his public role than with his private life. When I say \u201cprivate life\u201d I am not referring to those maliciously juicy tidbits, those meaningless details, which clutter up the gossip columns and muddy everybody\u2019s mind and obliterate the humanity of the subject as well as that of the reader. I wanted to ask him how it felt to be standing where he stood, how he bore it, what complex of miracles had prepared him for it. But such questions can scarcely be asked, they can scarcely be answered.\n\nAnd King does not like to talk about himself. I have described him as winning, but he does not give the impression of being particularly outgoing or warm. His restraint is not, on the other hand, of that icily uneasy, nerve-wracking kind to be encountered in so many famous Negroes who have allowed their aspirations and notoriety to destroy their identities and who always seem to be giving an uncertain imitation of some extremely improbable white man. No, King impressed me then and he impresses me now as a man solidly anchored in those spiritual realities concerning which he can be so eloquent. This divests him of the hideous piety which is so prevalent in his profession, and it also saves him from the ghastly self-importance which, until recently, was all that allowed one to be certain one was addressing a Negro leader. [\u2026] What he says to Negroes he will say to whites; and what he says to whites he will say to Negroes. He is the first Negro leader in my experience, or the first in many generations, of whom this can be said; most of his predecessors were in the extraordinary position of saying to white men, Hurry, while saying to black men, Wait.\n\nBaldwin\u2019s activities back in the United States did not qualify him as a civil rights worker, but he was involved with the movement in his own way, debating the subject on television, for example, with personalities as different as Malcolm X and the conservative intellectual William F. Buckley. As befit a writer, his main contribution to the cause was literary. And a theme he often sounded in the late 1950s and early 1960s had its parallels with the blues. As has been discussed, Baldwin\u2019s style, like the blues, was a hybrid form, a product of those of African descent and those of European ancestry. Baldwin saw the United States itself in much the same way; in his view, given the miscegenation that had taken place for centuries in America, racial hatred and discrimination were less an instance of one people mistreating another than an instance of a family abusing its members. Interracial friendships and romances were very much a part of Baldwin\u2019s third, highly celebrated novel, Another Country (1962). And the idea of a shared black and white fate was a central idea in what is perhaps Baldwin\u2019s most powerful piece of writing, the short nonfiction book The Fire Next Time. The book\u2019s first section, \u201cMy Dungeon Shook,\u201d called on black Americans to respond to whites with love, not for moral reasons but because blacks, in his view, constituted whites\u2019 only means of seeing themselves and their lives clearly. It was through love, Baldwin felt, that blacks could perhaps make their white countrymen see the wrong of what they were doing and had long done to others, and it was love that might cure whites of what, in Baldwin\u2019s view, was the greatest of all sins: innocence \u2014 not innocence in the sense of doing nothing wrong, but in the sense of being unaware. This kind of innocence had to do, as well, with an innocence concerning the true facts of life, the truth that one could not escape one\u2019s ultimate fate \u2014 death \u2014 through a status that depended on the subjugation of others. Baldwin writes in the book\u2019s second section, \u201cDown at the Cross\u201d:\n\nPerhaps the whole root of our trouble, the human trouble, is that we will sacrifice all the beauty of our lives, will imprison ourselves in totems, taboos, crosses, blood sacrifices, steeples, mosques, races, armies, flags, nations, in order to deny the fact of death, which is the only fact we have. It seems to me that one ought to rejoice in the fact of death \u2014 ought to decide, indeed, to earn one\u2019s death by confronting with passion the conundrum of life. [\u2026] It is the responsibility of free men to trust and to celebrate what is constant \u2014 birth, struggle, and death are constant, and so is love.\n\nAnd, indeed, another term for this confrontation with, and celebration of, the constants of life, is \u201csinging the blues.\u201d\n\nThe 1960s is often remembered as a decade of great and positive change, but for many who lived through it, the change came at a great cost and was at times hard to discern. It must have seemed that for every stirring and uplifting line spoken by Martin Luther King, there was a corresponding tragedy, an act of senseless and sickening violence. For those hearing of the murder of Medgar Evers; the bombing of the Birmingham church in which four black girls perished; the killings of the civil rights workers James Chaney, Andrew Goodman, and Michael Schwerner; and other brutal acts, the cumulative emotional toll was great. Evers had been a friend of Baldwin\u2019s, and Baldwin had other friends who were severely beaten as a result of their civil rights work. All of this not only saddened and angered Baldwin, but also led him to question his commitment to the course of nonviolence and the possibility of achieving change through love. It also seems to have led him to question the wisdom of seeking to become one with those who were dead-set against changing their ways.\n\nThis shift in Baldwin had implications for his writing. He began to question, as well, his association with symbols of the racist society in which he lived, including the English language itself. He would remain, of course, a writer, and one who wrote in English. He had already begun referring to himself as a blues singer when, in 1964, he published the essay \u201cWhy I Stopped Hating Shakespeare,\u201d in which he wrote:\n\nMy quarrel with the English language has been that the language reflected none of my experience. But now I began to see the matter in quite another way. If the language was not my own, it might be the fault of the language; but it might also be my fault. Perhaps the language was not my own because I had never attempted to use it, had only learned to imitate it. If this were so, then it might be made to bear the burden of my experience if I could find the stamina to challenge it, and me, to such a test.\n\nIn support of this possibility, I had two mighty witnesses: my black ancestors, who evolved the sorrow songs, the blues, and jazz, and created an entirely new idiom in an overwhelmingly hostile place; and Shakespeare, who was the last bawdy writer in the English language. [\u2026]\n\nI was listening very hard to jazz and hoping, one day, to translate it into language, and Shakespeare\u2019s bawdiness became very important to me since bawdiness was one of the elements of jazz.\n\nBaldwin refers here to blues and jazz, two different but closely related forms. Much of jazz is based on the blues, but I would argue that where blues represents an approach to living, jazz carries the rhythm and bent notes of the blues to an approach to music. Jazz is often, in addition, an investigation of music: jazz versions of pop tunes, for example, involve a remaking of songs by taking them apart and putting them back together in a different way, in the process seeing what they are made of, even as new elements are added as the result of improvisation. By writing that the English language \u201creflected none of my experience,\u201d Baldwin was perhaps suggesting the need for a remaking of language that would show what his experiences were, what they were made of.\n\nHow well did Baldwin succeed in the task he set for himself? The answer is complicated. In some cases his sentences took on an improvisatory freedom, with shorter sentences and fewer commas holding the various clauses in place; as a result the rhythm was faster, much like up-tempo jazz solos. Listen to the beginning of Tell Me How Long the Train\u2019s Been Gone, from 1968, the first novel Baldwin began writing after announcing his intention to write with the beat of jazz and blues:\n\nThe heart attack was strange \u2014 fear is strange. I knew I had been working too hard. I had been warned. But I have always worked too hard. I came offstage at the end of the second act. I felt hot and I was having trouble catching my breath. But I knew that I was tired. I went to my dressing room and poured myself a drink and put my feet up. Then I felt better. I knew I had about twenty-five minutes before I was due onstage. I felt very bitterly nauseous and I went to the bathroom but nothing happened. Then I began to be afraid, rather, to sit or lie down again and I poured myself another drink and left my dressing room to stand in the wings. I had begun to sweat and I was freezing cold. The nausea came back, making me feel that my belly was about to rise to the roof of my head. The stage manager looked at me just as I heard my cue. I carried his face onstage with me. It had looked white and horrified and disembodied in the eerie backstage light. I wondered what had frightened him. Then I realized that I was having trouble finding my positions and having trouble hearing lines. Barbara delivered her lines. I knew the lines, I knew what she was saying, but I did not know how to relate to it, and it took an eternity before I could reply. Then I began to be frightened and this, of course, created and compounded the nightmare, made me realize that I was in the middle of a nightmare. I moved about that stage, I don\u2019t know how, dragging my lines up from the crypt of memory, praying that my moves were right \u2014 for I had lost any sense of depth or distance \u2014 feeling that I was sinking deeper and deeper into some icy void. \u201cShall we ring down the curtain?\u201d Barbara whispered, and \u201cNo!\u201d I shouted or whispered back. At one point in the scene I was called upon to laugh and when I laughed I began to cough. I was afraid the cough would never stop, some horrible-tasting stuff came up, which I was forced to swallow, and then, suddenly, everything passed, everything became as clear and still and luminous as day. I got through a few more lines, and I thought, Hell, it\u2019s over, I\u2019m all right, and then something hit me in the chest, tore through my chest to my backbone and almost knocked me down. I couldn\u2019t catch my breath to deliver my lines. They covered for me. I knew we were approaching the end of the act. I prayed that I could stand up that long. I made a few more moves, I delivered a few more lines. [\u2026] The curtain came down. I heard the crash of applause, like the roar of a cataract far away, and for the first time I heard the sound of my own breathing, it was louder than the cataract. I took a step and fell to my knees, then I was on the floor, then I was being carried, then I was in my dressing room. I was trying to speak, but I couldn\u2019t speak. It was Barbara\u2019s face above me that told me how ill I was. Her brown hair fell over her face, half hiding it, and her storm-colored eyes stared into mine with the intention of communicating something which I had to know, but did not know. \u201cBe still,\u201d she said, \u201cdon\u2019t move. Don\u2019t speak.\u201d\n\nBut I wanted to ask her to forgive me for so many errors, so many fears.\n\nNote the difference in rhythm between that passage and the passages I cited earlier. Here we see the vitality of an improvised piece of music, as opposed to the formal beauty of one that is composed note for note \u2014 a tradeoff.\n\nTell Me How Long the Train\u2019s Been Gone is one of Baldwin\u2019s most critically reviled works, lambasted by reviewers, then and since, and perhaps justifiably, for having little or nothing in the way of a plot, among other faults. And yet I would argue that of all of Baldwin\u2019s works of fiction, it comes the closest to an examination of the demons of his adult life. Leo Proudhammer, the novel\u2019s narrator, as the just-cited passage makes clear, is an actor. Baldwin himself did not ultimately become an actor, though he had some theatrical experience and was temperamentally suited to the profession. He was \u2014 and I do not mean this term in its homophobic, pejorative sense \u2014 something of a drama queen (he also seems to have been a bit of a hypochondriac). He was small and not conventionally handsome but a natural entertainer, and he drew people to him easily. As already noted, he claimed to have loved a few men and loved a few women \u2014 indeed, there were more than a few, of different races \u2014 but he did not create a life with any of them, and a man of Baldwin\u2019s reflective nature must have given some thought as to why. Tell Me How Long the Train\u2019s Been Gone finds the narrator, Leo, looking back on three loves of his life: Barbara, a white woman and fellow actor; Christopher, a younger black man; and Leo\u2019s own older brother, Caleb, with whom, at one point, Leo engages in a physical act of love \u2014 not lust, but love, an outward manifestation of a very close emotional connection. The \u201cerrors\u201d and \u201cfears\u201d for which Leo wants Barbara\u2019s forgiveness are no doubt reflections of what Baldwin perceived as his own personal failures. The novel is thus a jazzlike reconfiguration of Baldwin\u2019s own life, with existing parts examined and rearranged and new parts added.\n\nIronically, as the jazz musician in Baldwin came to the fore, the blues singer he claimed to be suffered a setback. It was not so much that he abdicated the blues singer\u2019s role \u2014 looking squarely at the truth \u2014 as that the terrible events of the 1960s led him to view that truth, or whatever can be said to be truth, through the lens of his own bitterness. The Fire Next Time, published in 1963, had called for blacks to love whites, in spite of everything; Tell Me How Long the Train\u2019s Been Gone, appearing five years later, ends with a passage in which one black character, Christopher, says, \u201c[W]e need us some guns,\u201d and Leo replies, \u201cYes. [\u2026] I see that.\u201d\n\nThe shift in Baldwin\u2019s thinking from love to guns might seem at first glance to mirror the shift in the black liberation struggle of the time, the move from the love-and-nonviolence message preached by Martin Luther King to the \u201cby any means necessary\u201d call uttered by Malcolm X and taken up by Stokely Carmichael, the Black Panthers, and the Black Power movement. But whereas the black revolutionaries of the late 1960s who advocated violence, or at least were not opposed to it, had rejected love in favor of an energizing anger, Baldwin felt rejected by love and felt himself in danger of an exhausted despair. His nonfiction work No Name in the Street, a memoir of sorts published in 1972, is a very good, often angry book that is nonetheless written in the key of resignation. Its ending is similar to that of The Fire Next Time, but with this crucial difference: Fire warned of what was to come if America did not change its ways; No Name in the Street seemed to suggest that the die was cast. Black people \u201chave always seen,\u201d he wrote, \u201cspinning above the thoughtless American head, the shape of the wrath to come.\u201d\n\nBaldwin had one more good nonfiction book in him: The Devil Finds Work, from 1976, which focuses on film. That work, along with No Name in the Street and pages from Baldwin\u2019s never-completed book \u201cRemember This House,\u201d provides much of the voice-over \u2014 spoken by Samuel L. Jackson \u2014 in Raoul Peck\u2019s excellent 2016 documentary about Baldwin, I Am Not Your Negro. But Baldwin\u2019s days as a novelist of any power ended with Tell Me How Long the Train\u2019s Been Gone, and, indeed, some would have said they ended earlier than that. If Beale Street Could Talk, from 1974, is a thin work whose poor, 19-year-old, female narrator often seems to speak in the voice of a certain world-famous middle-aged writer; Just Above My Head, from 1979, is a lumbering, ponderous affair, which, published by someone else, would have seen its author accused of plagiarizing Baldwin\u2019s earlier works, as a friend of mine put it. Baldwin\u2019s final full-length nonfiction effort, The Evidence of Things Not Seen, published in 1985 \u2014 two years before his death \u2014 is the work of a writer who has lost his way, a singer whose voice is gone.\n\nIt may be better, then, especially with regard to Baldwin\u2019s fiction, to remember his earlier work. A fitting place to conclude reflections on a writer who self-identified as a blues singer is with a passage from one of his most celebrated short stories, \u201cSonny\u2019s Blues.\u201d In that tale of a jazz piano player, Baldwin has his narrator tell us,\n\nAll I know about music is that not many people ever really hear it. And even then, on the rare occasions when something opens within, and the music enters, what we mainly hear, or hear corroborated, are personal, private, vanishing evocations. But the man who creates the music is hearing something else, is dealing with the roar rising from the void and imposing order on it as it hits the air. What is evoked in him, then, is of another order, more terrible because it has no words, and triumphant, too, for that same reason. And his triumph, when he triumphs, is ours.\n\nAnd Baldwin\u2019s triumph, when he triumphed, was ours.\n\nClifford Thompson is the author of Twin of Blackness: A Memoir, Love for Sale and Other Essays, and a novel, Signifying Nothing.", "sentiment": 0.09303769659684101},
{"link_title": "Key West Live Webcam", "url": "https://www.youtube.com/watch?v=hGD1byu7gJc", "text": "", "sentiment": 0.0},
{"link_title": "How will the growth of AI impact the HR and recruitment sectors?", "url": "https://code.likeagirl.io/how-will-the-growth-of-ai-impact-the-hr-and-recruitment-sectors-f06008dd41b", "text": "Answering the question on many people in the recruitment industry\u2019s lips: will AI mean it\u2019s the end of the line for recruiters?\n\nArtificial intelligence (AI) is well and truly on the rise across many sectors and industries. In fact, there\u2019s probably been a point where you\u2019ve asked yourself: \u201ccould my job be taken over by a robot?\u201d.\n\nFor recruiters, this could be an extremely likely possibility.\n\nResearch into the HR and recruitment industries has found that 70% of HR managers believe the recruitment process would be more effective if it were more data-driven and, because of the technological advances in the industry, an increase in the use of AI would be an obvious solution to this problem.\n\n>See also: Making business smarter: 3 misconceptions about AI\n\nHowever, many could argue that an increase in AI will lead to jobs being put at risk \u2014 but should we simply embrace this change and prepare for new roles to open up within the sector?\n\nAdview expects AI to transform the industry in three ways:\n\nA human recruiter\u2019s manual ability to carry out candidate searches is much more limited than AI\u2019s. All recruiters know that an effective way of finding out about a candidate\u2019s attitudes, interests and values is through their social media profiles but AI can take this one step further.\n\nAI technology can analyse a wide variety of words used in any given candidate\u2019s social media posts, making it an extremely useful tool for narrowing down the talent pool during the early stages of the recruitment process. But, this doesn\u2019t mean that a recruiter\u2019s role becomes futile \u2014 it simply allows them to spend more time on worthwhile and valuable activities whilst AI undertakes candidate screening tasks.\n\nBy using AI in the candidate search process, any risk of unconscious bias on the recruiter\u2019s behalf is reduced. Instead, AI can ensure that recruiters focus on the candidate\u2019s expertise and skills so the most talented applicants shine through, benefiting both the individual and the recruiter.\n\nWhilst it could be argued that AI will mean it\u2019s the end of the line for recruiters as the technology will be enhancing and improving efficiency in the recruitment process, it\u2019s important to remember that AI doesn\u2019t have the advantage of experiencing emotions that humans do. Humans have an innate ability to judge character and personality, meaning the need for manual screening when selecting the right candidates will always be necessary.\n\nThe process of searching for a job, applying for it and waiting to hear back \u2014 and often getting radio silence \u2014 can be painfully long and stressful for job seekers, especially if the business or recruitment agency they are applying through has an inefficient process.\n\nIt is equally as important for a job candidate to feel impressed by the business they are applying to, as it is for the company to feel like the candidate is perfect for the role. If a job applicant has to wait two weeks to find out their application has been accepted, another two weeks to schedule an interview and a further three weeks to find out if they\u2019ve landed the job, there\u2019s a strong chance they\u2019ll be discouraged.\n\nAt the very least, they\u2019ll now have a negative perception of the company but they could also have spent that time talking themselves out of wanting the job. However, AI can reduce this processing time; with the use of technology, businesses and recruitment agencies can improve the candidate experience and prevent them from becoming disengaged.\n\nAccording to findings from a recent survey by Software Advice, 41% of job hunters have put their negative candidate experience down to being unable to contact a recruiter. Chatbot software enables job applications to be reviewed for the mandatory criteria immediately; instead of waiting two weeks to hear feedback from a recruiter, candidates can find out in a matter of minutes whether they have been accepted for the next stage of the process.\n\nDue to the high volume of CVs and applications recruiters receive, it means that it is almost impossible to provide feedback to an applicant in such a short space of time. AI technology means that the time of uncertainty for the candidate, whilst they wait to be contacted, is significantly reduced.\n\nEven before the initial candidate screening stage, AI is already hard at work. Through analysing data, algorithms and trends, AI can pick up on the behaviour of active job seekers. If someone is spending a significant amount of time searching for \u2018marketing jobs\u2019 on a job board website, AI will track and learn this pattern and target the job seeker with relevant marketing jobs.\n\nNot only can AI reach active job seekers, it also has the ability to target those who may not be actively searching for a new job or career; AI software can analyse data from social media to learn when a user might be leaving their job, looking for work or changing career.\n\nFor human recruiters, staying on top of job hunters\u2019 trends and patterns can be a time consuming process, but AI can take on this role and reduce the manual investment. A recruiter\u2019s ability to follow up with candidates can only go as far as phone calls and emails (apart from stalking LinkedIn and Twitter profiles) which, again, is a timely process, and often not appreciated by the candidate.\n\nAI\u2019s ability to discreetly track and spot candidates\u2019 behaviour patterns is a win win situation for both recruiter and candidate; recruiters have more time to focus on reaching the most suitable candidates, and job seekers don\u2019t find themselves inundated with follow up calls.\n\nGoogle have already introduced Cloud Jobs API, which works to improve the recruitment process by matching \u201cjob seeker preferences with relevant job listings based on sophisticated classifications and relational models\u201d, and we can only expect to see more companies and recruitment agencies following suit.\n\nIt\u2019s likely that recruiters will find themselves relying on AI more and more over the coming years as the amount of depth and efficiency it can bring to the recruitment industry is phenomenal. But, it could eventually lead to jobs being replaced.\n\nIt can be argued that a human\u2019s ability to feel emotion and assess character is something that AI cannot compete with, so humans will always be needed as part of the recruitment process. A recruiter\u2019s time and effort can often be consumed by low-level admin aspects of their role, so if AI can take over these tasks it shouldn\u2019t be seen as a negative impact.\n\nSo collaborating AI and applicant tracking system such as RecruitBPM can offer maximum ROI on your recruiting process.", "sentiment": 0.1544879327022184},
{"link_title": "A simple physical interface for your Raspberry Pi", "url": "https://github.com/mikeflynn/sensehat-service-ui", "text": "Powered by a simple ini file and set as a service to run on boot, Sense Hat Service UI allows you to turn on and off the services you've configured without having to login to the Pi via SSH or VNC. The menu is controlled completely via the joystick and LEDs on the Raspberry Pi Sense Hat. Just add 5V!\n\nI thought it would be cool to set up a little computer I could carry with me that would have a variety of services on it. The services would range from development to network security, but I realized that if I had to VNC or SSH in every time to activate a set of services it wouldn't be very helpful (and leaving them all on all the time isn't secure).\n\nWouldn't it be cool if you used the joystick on the Sense Hat to manipulate a simple graphical menu on the Sense Hat LED array?\n\nThis is that.\n\nThe services in the menu are all configured with a simple ini file and an example is included in the repo. Each section is an application or service (with the exception of the optional section) and each section should have three options: , , and .\n\nThe and bash commands don't need to return anything other than standard exit, but the command should return a number that is greater than zero if the service is running ( works great for this. Ex: )\n\nJust create a config file, there's a demo ini file included, and run the script like this...\n\nOnce you have a config ini file, you can install the app as a system service by running\n\nIf you make changes to the config ini file, you'll need to restart the service to pick up those changes.\n\nJust run the install script again, but with the flag.\n\nThis terribly un-idiomatic Python (the Sense Hat API is in Python) has been hacked together by Mike Flynn. You can find him here: @thatmikeflynn or here: thatmikeflynn.com and tell him how Python is awesome and you love that whitespace matters and stuff.", "sentiment": 0.20073529411764704},
{"link_title": "How Silicon Valley is erasing your individuality", "url": "https://www.washingtonpost.com/outlook/how-silicon-valley-is-erasing-your-individuality/2017/09/08/a100010a-937c-11e7-aace-04b862b2b3f3_story.html", "text": "Until recently, it was easy to define our most widely known corporations. Any third-grader could describe their essence. Exxon sells gas; McDonald\u2019s makes hamburgers; Walmart is a place to buy stuff. This is no longer so. Today\u2019s ascendant monopolies aspire to encompass all of existence. Google derives from googol, a number (1 followed by 100 zeros) that mathematicians use as shorthand for unimaginably large quantities. Larry Page and Sergey Brin founded Google with the mission of organizing all knowledge, but that proved too narrow. They now aim to build driverless cars, manufacture phones and conquer death. Amazon, which once called itself \u201cthe everything store,\u201d now produces television shows, owns Whole Foods and powers the cloud. The architect of this firm, Jeff Bezos, even owns this newspaper.\n\nAlong with Facebook, Microsoft and Apple, these companies are in a race to become our \u201cpersonal assistant.\u201d They want to wake us in the morning, have their artificial intelligence software guide us through our days and never quite leave our sides. They aspire to become the repository for precious and private items, our calendars and contacts, our photos and documents. They intend for us to turn unthinkingly to them for information and entertainment while they catalogue our intentions and aversions. Google Glass and the Apple Watch prefigure the day when these companies implant their artificial intelligence in our bodies. Brin has mused, \u201cPerhaps in the future, we can attach a little version of Google that you just plug into your brain.\u201d\n\nMore than any previous coterie of corporations, the tech monopolies aspire to mold humanity into their desired image of it. They think they have the opportunity to complete the long merger between man and machine \u2014 to redirect the trajectory of human evolution. How do I know this? In annual addresses and town hall meetings, the founding fathers of these companies often make big, bold pronouncements about human nature \u2014 a view that they intend for the rest of us to adhere to. Page thinks the human body amounts to a basic piece of code: \u201cYour program algorithms aren\u2019t that complicated,\u201d he says. And if humans function like computers, why not hasten the day we become fully cyborg?\n\nTo take another grand theory, Facebook chief Mark Zuckerberg has exclaimed his desire to liberate humanity from phoniness, to end the dishonesty of secrets. \u201cThe days of you having a different image for your work friends or co-workers and for the other people you know are probably coming to an end pretty quickly,\u201d he has said. \u201cHaving two identities for yourself is an example of a lack of integrity.\u201d Of course, that\u2019s both an expression of idealism and an elaborate justification for Facebook\u2019s business model.\n\n[Tech\u2019s sexism doesn\u2019t stay in Silicon Valley. It\u2019s in the products you use.]\n\nThere\u2019s an oft-used shorthand for the technologist\u2019s view of the world. It is assumed that libertarianism dominates Silicon Valley, and that isn\u2019t wholly wrong. High-profile devotees of Ayn Rand can be found there. But if you listen hard to the titans of tech, it\u2019s clear that their worldview is something much closer to the opposite of a libertarian\u2019s veneration of the heroic, solitary individual. The big tech companies think we\u2019re fundamentally social beings, born to collective existence. They invest their faith in the network, the wisdom of crowds, collaboration. They harbor a deep desire for the atomistic world to be made whole. (\u201cFacebook stands for bringing us closer together and building a global community,\u201d Zuckerberg wrote in one of his many manifestos.) By stitching the world together, they can cure its ills.\n\nRhetorically, the tech companies gesture toward individuality \u2014 to the empowerment of the \u201cuser\u201d \u2014 but their worldview rolls over it. Even the ubiquitous invocation of users is telling: a passive, bureaucratic description of us. The big tech companies (the Europeans have lumped them together as GAFA: Google, Apple, Facebook, Amazon) are shredding the principles that protect individuality. Their devices and sites have collapsed privacy; they disrespect the value of authorship, with their hostility toward intellectual property. In the realm of economics, they justify monopoly by suggesting that competition merely distracts from the important problems like erasing language barriers and building artificial brains. Companies should \u201ctranscend the daily brute struggle for survival,\u201d as Facebook investor Peter Thiel has put it.\n\nWhen it comes to the most central tenet of individualism \u2014 free will \u2014 the tech companies have a different way. They hope to automate the choices, both large and small, we make as we float through the day. It\u2019s their algorithms that suggest the news we read, the goods we buy, the paths we travel, the friends we invite into our circles.\n\nIt\u2019s hard not to marvel at these companies and their inventions, which often make life infinitely easier. But we\u2019ve spent too long marveling. The time has arrived to consider the consequences of these monopolies, to reassert our role in determining the human path. Once we cross certain thresholds \u2014 once we remake institutions such as media and publishing, once we abandon privacy \u2014 there\u2019s no turning back, no restoring our lost individuality.\n\nOver the generations, we\u2019ve been through revolutions like this before. Many years ago, we delighted in the wonders of TV dinners and the other newfangled foods that suddenly filled our kitchens: slices of cheese encased in plastic, oozing pizzas that emerged from a crust of ice, bags of crunchy tater tots. In the history of man, these seemed like breakthrough innovations. Time-consuming tasks \u2014 shopping for ingredients, tediously preparing a recipe and tackling a trail of pots and pans \u2014 were suddenly and miraculously consigned to history.\n\nThe revolution in cuisine wasn\u2019t just enthralling. It was transformational. New products embedded themselves deeply in everyday life, so much so that it took decades for us to understand the price we paid for their convenience, efficiency and abundance. Processed foods were feats of engineering, all right \u2014 but they were engineered to make us fat. Their delectable taste required massive quantities of sodium and sizable stockpiles of sugar, which happened to reset our palates and made it harder to sate hunger. It took vast quantities of meat and corn to fabricate these dishes, and a spike in demand remade American agriculture at a terrible environmental cost. A whole new system of industrial farming emerged, with penny-conscious conglomerates cramming chickens into feces-covered pens and stuffing them full of antibiotics. By the time we came to understand the consequences of our revised patterns of consumption, the damage had been done to our waistlines, longevity, souls and planet.\n\n[Most of my medical colleagues are women. The Google guy gets them wrong.]\n\nSomething like the midcentury food revolution is now reordering the production and consumption of knowledge. Our intellectual habits are being scrambled by the dominant firms. Giant tech companies have become the most powerful gatekeepers the world has ever known. Google helps us sort the Internet, by providing a sense of hierarchy to information; Facebook uses its algorithms and its intricate understanding of our social circles to filter the news we encounter; Amazon bestrides book publishing with its overwhelming hold on that market.\n\nSuch dominance endows these companies with the ability to remake the markets they control. As with the food giants, the big tech companies have given rise to a new science that aims to construct products that pander to their consumers. Unlike the market research and television ratings of the past, the tech companies have a bottomless collection of data, acquired as they track our travels across the Web, storing every shard about our habits in the hope that they may prove useful. They have compiled an intimate portrait of the psyche of each user \u2014 a portrait that they hope to exploit to seduce us into a compulsive spree of binge clicking and watching. And it works: On average, each Facebook user spends one-sixteenth of their day on the site.\n\nIn the realm of knowledge, monopoly and conformism are inseparable perils. The danger is that these firms will inadvertently use their dominance to squash diversity of opinion and taste. Concentration is followed by homogenization. As news media outlets have come to depend heavily on Facebook and Google for traffic \u2014 and therefore revenue \u2014 they have rushed to produce articles that will flourish on those platforms. This leads to a duplication of the news like never before, with scores of sites across the Internet piling onto the same daily outrage. It\u2019s why a picture of a mysteriously colored dress generated endless articles, why seemingly every site recaps \u201cGame of Thrones.\u201d Each contribution to the genre adds little, except clicks. Old media had a pack mentality, too, but the Internet promised something much different. And the prevalence of so much data makes the temptation to pander even greater.\n\nThis is true of politics. Our era is defined by polarization, warring ideological gangs that yield no ground. Division, however, isn\u2019t the root cause of our unworkable system. There are many causes, but a primary problem is conformism. Facebook has nurtured two hive minds, each residing in an informational ecosystem that yields head-nodding agreement and penalizes dissenting views. This is the phenomenon that the entrepreneur and author Eli Pariser famously termed the \u201cFilter Bubble\u201d \u2014 how Facebook mines our data to keep giving us the news and information we crave, creating a feedback loop that pushes us deeper and deeper into our own amen corners.\n\nAs the 2016 presidential election so graphically illustrated, a hive mind is an intellectually incapacitated one, with diminishing ability to tell fact from fiction, with an unshakable bias toward party line. The Russians understood this, which is why they invested so successfully in spreading dubious agitprop via Facebook. And it\u2019s why a raft of companies sprouted \u2014 Occupy Democrats, the Angry Patriot, Being Liberal \u2014 to get rich off the Filter Bubble and to exploit our susceptibility to the lowest-quality news, if you can call it that.\n\n[Uber\u2019s algorithms could spot crimes in progress. But do we want them to?]\n\nFacebook represents a dangerous deviation in media history. Once upon a time, elites proudly viewed themselves as gatekeepers. They could be sycophantic to power and snobbish, but they also felt duty-bound to elevate the standards of society and readers. Executives of Silicon Valley regard gatekeeping as the stodgy enemy of innovation \u2014 they see themselves as more neutral, scientific and responsive to the market than the elites they replaced \u2014 a perspective that obscures their own power and responsibilities. So instead of shaping public opinion, they exploit the public\u2019s worst tendencies, its tribalism and paranoia.\n\nDuring this century, we largely have treated Silicon Valley as a force beyond our control. A broad consensus held that lead-footed government could never keep pace with the dynamism of technology. By the time government acted against a tech monopoly, a kid in a garage would have already concocted some innovation to upend the market. Or, as Google\u2019s Eric Schmidt, put it, \u201cCompetition is one click away.\u201d A nostrum that suggested that the very structure of the Internet defied our historic concern for monopoly.\n\nAs individuals, we have similarly accepted the omnipresence of the big tech companies as a fait accompli. We\u2019ve enjoyed their free products and next-day delivery with only a nagging sense that we may be surrendering something important. Such blitheness can no longer be sustained. Privacy won\u2019t survive the present trajectory of technology \u2014 and with the sense of being perpetually watched, humans will behave more cautiously, less subversively. Our ideas about the competitive marketplace are at risk. With a decreasing prospect of toppling the giants, entrepreneurs won\u2019t bother to risk starting new firms, a primary source of jobs and innovation. And the proliferation of falsehoods and conspiracies through social media, the dissipation of our common basis for fact, is creating conditions ripe for authoritarianism. Over time, the long merger of man and machine has worked out pretty well for man. But we\u2019re drifting into a new era, when that merger threatens the individual. We\u2019re drifting toward monopoly, conformism, their machines. Perhaps it\u2019s time we steer our course.\n\nRead more from Outlook and follow our updates on Facebook and Twitter.", "sentiment": 0.08142720965568648},
{"link_title": "Proposed Consent Agreements: Uber Technologies, Inc", "url": "https://www.regulations.gov/document?D=FTC-2017-0063-0001", "text": "Your web browser must have JavaScript enabled in order for Regulations.gov to display correctly.", "sentiment": 0.0},
{"link_title": "Univariate Distribution Relationship Chart", "url": "http://www.math.wm.edu/~leemis/chart/UDR/UDR.html", "text": "", "sentiment": 0.0},
{"link_title": "Long-Lost Avro Arrow Model Found at Bottom of Lake Ontario", "url": "https://www.thestar.com/news/gta/2017/09/08/long-lost-avro-arrow-model-found-at-bottom-of-lake-ontario.html", "text": "\u201cWell, we found one,\u201d John Burzynski leader of the Raise the Arrow expedition told a news conference Friday morning before unveiling sonar images of a long-lost object that was a part of Canada\u2019s most significant aviation program. Burzynski confirmed that the expedition\u2019s engineers have located one of nine models of the Avro Arrow that have been sitting at the bottom of Lake Ontario since they were launched in test flights between 1954 and 1957. The Arrow was a fighter jet developed in the 1950s that was lauded as a groundbreaking technological achievement before the program\u2019s controversial cancellation by the Diefenbaker government in 1959. The Arrow\u2019s story, Burzynski said, was one of \u201cthe realization of dreams,\u201d as well as the \u201cbitter taste of defeat,\u201d when the program was cancelled and the only existing planes destroyed. Canadians were stunned when then-prime minister John Diefenbaker announced the cancellation, the reasons for which were never clear, but likely had to do with costs.\n\nThe Raise the Arrow expedition, Burzynski said, was not only about finding something that was lost. It was about the people who worked on the plane, and all the Canadians who held memories of the Arrow dear.\n\nThe expedition spent a total of 12 days since the end of July searching the lake. The model, which remains on the floor of the lake, is about three metres long and two metres wide. Images show orange paint, a hallmark of the treasured Canadian technology, still intact and peeking through the zebra mussels that almost entirely cover its surface. \u201cI think being able to showcase using cutting edge Canadian technology \u2014being our sonar systems and underwater vehicles \u2014 to actually find and resurrect cutting edge Canadian technology\u2026 I think it\u2019s an amazing example of what we can do as Canadians looking back at our history,\u201d said David Shea, vice-president of engineering for Kraken Sonar.\n\nShea remembers being fascinated by the Arrow as a child after reading his older brother\u2019s history books on the aircraft. \u201cI remember going through this book and looking at these jet fighters and I didn\u2019t understand why they didn\u2019t exist anymore,\u201d he said. \u201cEvery since then, growing up and going into engineering, I\u2019ve been fascinated with the fact that Canada had such a cutting edge technology and we were world leaders at one point in time.\u201d\n\nThe Avro Arrow program, Shea said, is unparalleled in the ability it had to inspire Canadian engineers. He hopes that the country is beginning to gain back some prestige in the field of science and technology \u2014 particularly as the advanced sonar technologies he uses proved successful in finding one Arrow model. The discovery of the model is the biggest Arrow-related event since a full-sized replica of the plane was unveiled in 2006.\n\nShea\u2019s looking forward to going back out onto the water to find the other eight right away. An archaeological team led by Scarlett Janusas will now get to work on recovering the model. She said the team hopes to send divers down before the end of the season.\n\nThe object will likely be retrieved next spring, at which point more information about its place in Arrow history is expected to come to light. Once all the models are removed from Lake Ontario, they will be housed at the Canada Aviation and Space museum in Ottawa and the National Air Force Museum of Canada in Trenton.", "sentiment": 0.08193400167084375},
{"link_title": "My Time with Google's Cellular Service Was Mostly a Disaster", "url": "http://gizmodo.com/my-weeks-with-googles-cellular-service-were-mostly-a-di-1724915476", "text": "I glimpsed the future before it collapsed into bullshit. \n\n\n\nI wanted Project Fi, Google\u2019s new and experimental cellular phone service, to be amazing. The idea sounded so superior to the status quo that it just had to be destined for greatness. But like the Star Wars prequels and most food with black olives, it let me down.\n\nA brief refresher: Project Fi is Google\u2019s attempt to provide the be-all-end-all for your smartphone needs. It\u2019s a mobile virtual network operator, or MVNO, that switches between T-Mobile, Sprint, and Wi-Fi intelligently so that you have the best service between the three. You pay 20 bucks for talk and text and an additional $10 per gigabyte of data, and it\u2019s all prepaid.\n\nDon\u2019t use all your gigs in one month? Get a refund! Go over? Pay extra on your next bill! But it\u2019s always just $10 for 1GB... that\u2019s it, and the simplicity is beautiful. The only catch is you have to use the hand-exhausting Nexus 6 smartphone to participate, but it makes sense: this is a pure Google experience, from hardware to software to carrier.\n\nA pure Google experience that started out wonderfully, and quickly became a nightmare.\n\nWhen you sign up for Project Fi, Google sends along a little care package\u2014a Nexus 6 case, an external battery, and a headphone splitter\u2014along with your Project Fi nano-SIM. Pop in the SIM, press a few buttons in the Google Fi app, and you\u2019re ready to go. Seriously, that\u2019s it. I just signed up for a phone service in 5 minutes using an app. This is the future. Yes, yes, .\n\nI started with Project Fi on a Friday, and things were going smoothly. I did notice that when I was tied up with T-Mobile, things were occasionally a little slow. Service fluttered between HSPA+ and LTE, but generally I didn\u2019t notice a problem. During the honeymoon phase, I even called our reviews editor Sean Hollister with a progress report of sorts. \u201cIt\u2019s great.\u201d I recall saying. \u201cThis is the way cell service should be.\u201d\n\nThree weeks later I was speaking about Project Fi with another Gizmodean outside our New York city office. \u201cIt\u2019s terrible.\u201d I said. \u201cI\u2019m so happy to be done with it.\u201d\n\nAlmost as soon as I got off that cheery convo with Sean, I stared down at my phone\u2014no service. \u201cWait, what?\u201d At the time, I happened to be sitting on the roof of my friend\u2019s apartment in Brooklyn playing Magic: The Gathering and drinking Corona. I hadn\u2019t moved more than a few feet during the entire conversation, so what the heck could have happened?\n\nParticularly when the whole appeal of Project Fi is that the service is supposed to switch when it encounters a problem on one network, or when it sees you\u2019re getting poor service. I had major doubts that both T-Mobile and Sprint were having major outages at the same time. I reset my phone, and got this message\u2014a message I would become oh-so-very familiar with:\n\nNothing worked until I actually physically ejected and re-inserted my SIM card, and even when it did finally pop back onto the network, it was at sub-LTE speeds. Over the next few days, I was able to eke by a piss poor cellular lifestyle with the same song and dance. Lose network. Eject SIM. Insert SIM. Power cycle phone. Have service for a few hours. Repeat.\n\n\n\nAnd remember, all this from a service where Google controls everything: the phone, the app, the network, and even the SIM.\n\nThen, things went from bad to worse. By the fourth day, my usual tricks weren\u2019t getting the job done. I started get weird, cryptic SIM card error messages like so:\n\nI was perpetually tied to wifi with none of the service I was paying $70 a month for. I couldn\u2019t take it. Something had to be done.\n\nThe truly torturous thing about my Project Fi experience wasn\u2019t the fact that I had no data, phone, or text\u2014though that definitely wasn\u2019t fun. It was that the whole thing felt like a glimmer of greatness tarnished by a service that just sucks.\n\nThe fact is, Project Fi has some of the best customer service I\u2019ve ever dealt with just by virtue of how easy they are to reach. Inside the Fi app, you can select what method you wish to be contacted by\u2014phone, text, or email\u2014and if you choose phone, real flesh-and-blood humans will call you back. No automated chat programs, \u201cdial one for\u201d whatever, or the always uncanny \u201cSorry, I didn\u2019t hear that\u201d computer-generated response. The app will even display how long it\u2019ll take for them to get back to you.\n\nThe wait times were usually 1 minute for phone or text and 1 hour for email, so I went the phone route. The first rep walked me through the prerequisite troubleshooting tips (the stuff I\u2019d been doing for the past few days) and then told me that Sprint service was down in NYC and that I simply needed to stay on wifi until it could get fixed. \u201cOk,\u201d I grumbled in response. Are you kidding me?\n\nThe next day, things were still terrible, so I called again (over wifi). This customer rep was much better, offering credit for my trouble and escalating my problem to technicians. Unfortunately, my wifi fluttered, and the call dropped dead. The representative couldn\u2019t return my call because, well, I didn\u2019t have a working phone. This crippled game of back-and-forth phone tag went on for a couple days until I resorted to email.\n\nFrom there spawned a 21-response long email chain, lasting 15 days, that solved nothing and when summarized, went like this:\n\nMe: So...my phone still doesn\u2019t work. Can you fix it?\n\nRep: We have identified the problem and there is a fix in progress.\n\nMe: (two days later): So...yeah. Still doesn\u2019t work.\n\nRep: We have identified the problem and there is a fix in progress.\n\nMe: Like when?\n\nRep: We know this is frustrating and there is a fix in progress.\n\nAnd on and on and on.\n\nDuring this whole conversation, I never once let on that I was a tech journalist or that I was writing a review. I didn\u2019t want to be flagged for special treatment. I was a paying customer (Google did not provide access for this review) and I just wanted Project Fi to run its course. The half-reason I was continually given was that I was improperly on the network and unable to access Project Fi. At first, Google blamed my Google phone. Then it blamed the system itself. Every email ended with the familiar response that a fix was coming shortly.\n\nThis whole period last almost two weeks. In that time, I\u2019d received an important text from a friend visiting from Denver...7 hours late. I had Slack messages from work, yelling for my thoughts on a breaking piece of news that I was oblivious to because I didn\u2019t have a working handset. Whenever I went out with friends, I was a child on an overprotective parent leash\u2014if we became separated, I was doomed.\n\nAfter nearly 3 weeks, I couldn\u2019t take it anymore. I marched down to a T-Mobile store a block away. It felt like I\u2019d come crawling back to my old carrier. Even in our goodbye, Project Fi showed that it was better in some ways than your average cellular service. I was able to hop into the app, tell Google I was leaving, and that was pretty much it. It even gave me a code to share with T-Mobile to resume my old plan. Since it was all prepaid, there was no contract to break and no early termination fees. I was just gone.\n\nAnd that\u2019s what\u2019s still so compelling about Project Fi. The idea that owning a phone could be way less of a hassle. A service that cuts out the middleman. A service that (theoretically) has the combined coverage map of two of the US\u2019 largest carriers. I hope they fix it. Or that I\u2019m an outlier, one of 1,000 or 10,000 for whom the service didn\u2019t work. Maybe you\u2019re enjoying Project Fi right now without a care in the world. Lucky you, I\u2019m jealous.\n\nBut this was my experience, and it just didn\u2019t work. I paid real money for Project Fi. I gave it a go, and it was unreliable as hell. And when I switched back to T-Mobile, everything went smoothly from the moment I popped that T-Mobile SIM back into my Nexus 6.\n\nOh, and after I finally had a working phone again, I found one more email from Project Fi waiting for me. They wanted to let me know a fix was in progress.\n\nEditor\u2019s Note: Darren might be done with Project Fi, but this won\u2019t be our last test. How well does Project Fi work when, you know, it\u2019s actually working? Stay tuned.\n\n", "sentiment": 0.06916227613902029},
{"link_title": "Text-only CNN", "url": "http://lite.cnn.io/", "text": "", "sentiment": 0.0},
{"link_title": "Tesla extends range of vehicles for free to help owners evacuate hurricane", "url": "https://electrek.co/2017/09/09/tesla-extends-range-vehicles-for-free-in-florida-escape-hurricane-irma/", "text": "Millions of people are currently affected by the evacuation of Florida as Hurricane Irma starts reaching the state and creates some difficult traffic situation when escaping north. There are reports of traffic jams and gas stations running out gas.\n\nThere are a lot Tesla owners in Florida and they are also escaping north using the Supercharger network.\n\nNow Tesla has even facilitated travels for some of them as the automaker remotely unlocked the full battery pack capacity of Model S/X 60/60D vehicles with 75 kWh battery packs.\n\nThat\u2019s due to Tesla using an unforeseen feature of their over-the-air software update system.\n\nTesla used to offer the option to buy a Model S or Model X with a 75 kWh battery pack software-locked at a capacity of 60 kWh. The option would result in a less expensive vehicle with a shorter range, but the option to pay to remotely enable the longer range at a later stage.\n\nSome of those owners reported this morning having more range than usual in their vehicles.\n\nA Tesla Model S 60 owner in Florida reached out to us with almost 40 more miles than in his usual full charge and a new \u201975\u2019 badge in his car software.\n\nWhile he didn\u2019t ask for it nor knew why it changed, Tesla had temporarily unlocked the remaining 15 kWh of the car\u2019s software-limited battery pack option to facilitate the owner\u2019s evacuation.\n\nWe reached to Tesla and a representative confirmed that the company has put in place the emergency measure to temporarily extend the range of the vehicles of Tesla owners in the path of Hurricane Irma.\n\nThe company says that a Tesla owner in a mandatory evacuation zone required another ~30 more miles of range to optimize his evacuation route in the traffic and they reached out to Tesla who agreed to a temporary access to the full 75 kWh of energy in the battery pack, an upgrade that has cost between $4,500 and $9,000 depending on the model and time of upgrade.\n\nConsidering the 15 kWh (30 to 40 additional miles) could also be useful to other owners affected by Irma, Tesla decided to also temporarily unlock other vehicles with the same software-lock battery packs in the region.\n\nTesla\u2019s Supercharger network is fairly extensive in Florida and most owners should be able to get by even with a Model S 60 (the shortest range option), but sometimes that 30 more miles of range can make a big difference.\n\nMost Supercharger stations in Florida are still online:\n\nThe station in the Florida Keys is offline and one in Myers is currently listed as \u201creduced service\u201d:\n\nOtherwise, the other Superchargers are reportedly online and accessible on the way north. Stay safe out there everyone.", "sentiment": 0.1217092803030303},
{"link_title": "Breaking the X86 Instruction Set \u2013 Finding hidden instructions [video]", "url": "https://www.youtube.com/watch?v=KrksBdWcZgQ?", "text": "", "sentiment": 0.0},
{"link_title": "Potentiam, the New Face of Online Music and Music Contents", "url": "https://medium.com/potentiam/potentiam-the-new-face-of-online-music-and-music-contents-90321cd36c01", "text": "Yes! With Potentiam, you\u2019ve truly got the power! The Potentiam network was created to find the much needed balance in the music and content distribution sub-industry. It is the new music distribution network created by veterans in the industry and powered by the Blockchain network. Potentiam is poised to put back the power to earn in the hands of content producers and their fans.\n\nPotentiam was borne out of the desire to proffer solution to the myriad of challenges faced by music artist, content producers and all those who strive to make music possible. Potentiam was designed and is promoted by the same music veterans that founded Britznbeatz in 2016. The goal of Potentiam is to give opportunities to artists and fans to collaborate by creating a decentralized autonomous system powered by the Blockchain network.\n\nPotentiam is a decentralized and incentivized collaborative music network dedicated to the development and exposure of music creatives and their audience. Their mission is to put the power back in the hands of the creators and their fans by providing a platform that creates direct and dynamic interactions.\n\nThe benefits of Potentiam for the budding up and coming artist is enormous. The opportunity for growth and getting heard becomes more real with Crowdsale. Crowdsale is supported on the Potentiam network, and it allows fans and supporters as a community, to collaborate and raise financial support for their favorite artist thus eliminating the corporate bottlenecks associated with breaking into the market for new, talented and budding artist. Potentiam supports the growth of the emerging artist by giving them the opportunity and a platform to showcase their talent and seek funding.\n\nAdvertising music on Potentiam is easy. Potentiam creates reaches, break down barriers and surmount hurdles to bring great content to the user and also give the artist the opportunity to create a fan base that is loyal and rewarding. Potentiam provides a collaborative network that allows artist and fans connect and share music. Even when you don\u2019t create music, Potentiam provides an opportunity for high value users to earn from their consistency through monetization of their feeds, It rewards your participation in the network.\n\nPotentiam network is beneficial for all. It connects and provides a stable platform for interaction and collaboration, Potentiam promotes a more collaborative approach to music distribution that is a far cry from the cutthroat competitive scenarios of traditional distribution process. Potentiam is rewarding and fuels the motivation to succeed by providing a trusted platform that connects disparate users with like minds, goals, and aspirations to share and conduct transactions.\n\nThe world today as we know it is fast evolving. Technology is the bane of every business and has become a major part of our lives. As we embrace technology, there is a need to stay safe in the online space, Potentiam has the mechanism in place to protect users and to control fraudulent activities that may occur on the platform. Registration is swift, fast and free.\n\nPotentiam as a network is the reality of millions of people. Potentiam is the Future.", "sentiment": 0.1890649551066218},
{"link_title": "Tesla flips a switch to increase the range of some cars in Florida", "url": "https://techcrunch.com/2017/09/09/tesla-flips-a-switch-to-increase-the-range-of-some-cars-in-florida-to-help-people-evacuate/?ncid=rss&utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+Techcrunch+%28TechCrunch%29", "text": "\n\n Tesla has pushed an over-the-air update to some of its vehicles in Florida that lets those cars go just a liiiittle bit further, thus helping their owners get that much further away from the devastation of Hurricane Irma.\n\nWondering how that\u2019s even possible?\n\nUp until a few months ago, Tesla sold a 60kWh version of its Model S and Model X vehicles \u2014 but the battery in those cars was actually rated at 75kWh. The thinking: Tesla could offer a more affordable 60kWh version to those who didn\u2019t need the full range of the 75kWh battery \u2014 but to keep things simple, they\u2019d just use the same 75kWh battery and lock it on the software side. If 60kWh buyers found they needed more range and wanted to upgrade later, they could\u2026 or if Tesla wanted to suddenly bestow owners with some extra range in case of an emergency, they could.\n\nAs first noticed by Tesla owners on Reddit, the company has pushed a \u201ctemporary update\u201d to vehicles within the evacuation zones that bumps the 60kWh models up to 75kWh.\n\nOn the road, according to Elektrek, this battery bump works out to about 30 miles of additional range on a full charge. If that little bit of range helps even one person avoid injury or get their loved ones out safely, I\u2019d say its worth whatever work this required.\n\nAlas, the upgrade won\u2019t stick around forever \u2014 Tesla generally charges at least $5,000 for the permanent equivalent. Members on the Tesla Motors Club fan forum report that the temporary update will be reversed on September 16th.", "sentiment": 0.10965909090909091},
{"link_title": "Mastercard Internet Gateway Service: Hashing Design Flaw", "url": "http://tinyhack.com/2017/09/05/mastercard-internet-gateway-service-hashing-design-flaw/", "text": "Last year I found a design error in the MD5 version of the hashing method used by Mastercard Internet Gateway Service. The flaw allows modification of transaction amount. They have awarded me with a bounty for reporting it. This year, they have switched to HMAC-SHA256, but this one also has a flaw (and no response from MasterCard).\n\nIf you just want to know what the bug is, just skip to the Flaw part.\n\nWhen you pay on a website, the website owner usually just connects their system to an intermediate payment gateway (you will be forwarded to another website). This payment gateway then connects to several payments system available in a country. For credit card payment, many gateways will connect to another gateway (one of them is MIGS) which works with many banks to provide 3DSecure service.\n\nThe payment flow is usually like this if you use MIGS:\n\nNotice that instead of communicating directly between servers, communications are done via user\u2019s browser, but everything is signed. In theory, if the signing process and verification process is correct then everything will be fine. Unfortunately, this is not always the case.\n\nThis bug is extremely simple. The hashing method used is:\n\nBut it was not vulnerable to hash length extension attack (some checks were done to prevent this). The data is created like this: for every query parameter that starts with vpc_, sort it, then concatenate the values only, without delimiter. For example, if we have this data:\n\nGet the values, and concatenate it:\n\nNote that if I change the parameters:\n\nGet the values, and concatenate it:\n\nThe MD5 value is still the same. So basically, when the data is being sent to MIGS, we can just insert additional parameter after the amount to eat the last digits, or to the front to eat the first digits, the amount will be slashed, and you can pay a 2000 USD MacBook with 2 USD.\n\nIntermediate gateways and merchant can work around this bug by always checking that the amount returned by MIGS is indeed the same as the amount requested.\n\nMasterCard rewarded me with 8500 USD for this bug.\n\nThe new HMAC-SHA256 has a flaw that can be exploited if we can inject invalid values to intermediate payment gateways. I have tested that at least one payment gateway (Fusion Payments) have this bug. I was rewarded 500 USD from Fusion Payments. It may affect other Payment gateways that connect to MIGS.\n\nIn the new version, they have added delimiters (&) between fields, added field names and not just values, and used HMAC-SHA256. For the same data above, the hashed data is:\n\nWe can\u2019t shift anything, everything should be fine. But what happens if a value contains & or = or other special characters?\n\nReading this documentation, it says that:\n\nThe \u201cNOT\u201d is my emphasis. It means that if we have these fields:\n\nIt will be hashed as:\n\nAnd if we have this (amount contains the & and =)\n\nIt will be hashed as:\n\nThe same as before. Still not really a problem at this point.\n\nOf course, I thought that may be the documentation is wrong, may be it should be encoded. But I have checked the behavior of the MIGS server, and the behavior is as documented. May be they don\u2019t want to deal with different encodings (such as + instead of %20).\n\nThere doesn\u2019t seem to be any problem with that, any invalid values will be checked by MIGS and will cause an error (for example invalid amount above will be rejected).\n\nBut I noticed that in several payment gateways, instead of validating inputs on their server side, they just sign everything it and give it to MIGS. It\u2019s much easier to do just JavaScript checking on the client side, sign the data on the server side, and let MIGS decide whether the card number is correct or not, or should the CVV be 3 or 4 digits, is the expiration date correct, etc. The logic is: MIGS will recheck the inputs, and will do it better.\n\nOn Fusion Payments, I found out that it is exactly what happened: they allow any characters of any length to be sent for the CVV (only checked in JavaScript), they will sign the request and send it to MIGS.\n\nTo exploit this we need to construct a string which will be a valid request, and also a valid MIGS server response. We don\u2019t need to contact MIGS server at all, we are forcing the client to sign a valid data for themselves.\n\nA basic request looks like this:\n\nand a basic response from the server will look like this:\n\nIn the Fusion Payment\u2019s case, the exploit is done by injecting vpc_CardSecurityCode (CVV)\n\nThe client/payment gateway will generate the correct hash for this string\n\nNow we can post this data back to the client itself (without ever going to MIGS server), but we change it slightly so that the client will read the correct variables (most client will only check for , and ):\n\nIt can be said that this is a MIGS client bug, but the hashing method chosen by MasterCard allows this to happen, had the value been encoded, this bug will not be possible.\n\nMasterCard did not respond to this bug in the HMAC-SHA256. When reporting I have CC-ed it to several persons that handled the previous bug. None of the emails bounced. Not even a \u201cwe are checking this\u201d email from them. They also have my Facebook in case they need to contact me (this is from the interaction about the MD5 bug).\n\n\n\nSome people are sneaky and will try to deny that they have received a bug report, so now when reporting a bug, I put it in a password protected post (that is why you can see several password-protected posts in this blog). So far at least 3 views from MasterCard IP address (3 views that enter the password). They have to type in a password to read the report, so it is impossible for them to accidentally click it without reading it. I have nagged them every week for a reply.\n\nMy expectation was that they would try to warn everyone connecting to their system to check and filter for injections.\n\nAs an extra note: even though payment gateways handle money, they are not as secure as people think. During my pentests I found several flaws in the design of the payment protocol on several intermediate gateways. Unfortunately, I can\u2019t go into detail on this one(when I say \u201cpentests\u201d, it means something under NDA).\n\nI also found flaws in the implementation. For example Hash Length Extension Attack, XML signature verification error, etc. One of the simplest bugs that I found is in Fusion Payments. The first bug that I found was: they didn\u2019t even check the signature from MIGS. That means we can just alter the data returned by MIGS and mark the transaction as successful. This just means changing a single character from F (false) to 0 (success).\n\nSo basically we can just enter any credit card number, got a failed response from MIGS, change it, and suddenly payment is successful. This is a 20 million USD company, and I got 400 USD for this bug. This is not the first payment gateway that had this flaw, during my pentest I found this exact bug in another payment gateway. Despite the relatively low amount of bounty, Fusion Payments is currently the only payment gateway that I contacted that is very clear in their bug bounty program, and is very quick in responding my emails and fixing their bugs.\n\nPayment gateways are not as secure as you think. With the relatively low bounty (and in several cases that I have reported: 0 USD), I am wondering how many people already exploited bugs in payment gateways.", "sentiment": 0.06673477633477636},
{"link_title": "Fixing MacOS Stupid DNS Problems with VPNs and Split DNS: Good Bye Resolv.conf", "url": "https://github.com/greenboxal/dns-heaven", "text": "This project fixes macOS DNS stack by enabling the usage of the native DNS stack through /etc/resolv.conf.\n\nSome programs like dig, nslookup and anything compiled with Go, doesn't make usage of macOS native name resolution stack. This makes some features like Split DNS not work with those programs.\n\nThis script downloads the latest version and install a LaunchAgent so dns-heaven is always running.\n\nJust download or build the binary and make sure it's running. Everything is automatic.\n\ndns-heaven is simple DNS proxy that mimics macOS name resolution stack by reading its configuration and proxyfying requests to the correct nameservers.\n\nFrom time to time, dns-heaven parses the output of , and tries to mimic macOS behaviour using the output as config.\n\nAlso, it forces to point to 127.0.0.1 as the native network manager will try to change it to the DNS servers configured on Network Settings or on DHCP.", "sentiment": 0.225},
{"link_title": "Introducing Pytorch for fast.ai", "url": "http://www.fast.ai/2017/09/08/introducing-pytorch-for-fastai/", "text": "The next fast.ai courses will be based nearly entirely on a new framework we have developed, built on Pytorch. Pytorch is a different kind of deep learning library (dynamic, rather than static), which has been adopted by many (if not most) of the researchers that we most respect, and in a recent Kaggle competition was used by nearly all of the top 10 finishers.\n\nWe have spent around a thousand hours this year working with Pytorch to get to this point, and we are very excited about what it is allowing us to do. We will be writing a number of articles in the coming weeks talking about each aspect of this. First, we will start with a quick summary of the background to, and implications of, this decision. Perhaps the best summary, however, is this snippet from the start of our first lesson:\n\nOur goal at fast.ai is for there to be nothing to teach. We believe that the fact that we currently require high school math, one year of coding experience, and seven weeks of study to become a world-class deep learning practitioner, is not an acceptable state of affairs (even although this is less prerequisites for any other course of a similar level). Everybody should be able to use deep learning to solve their problems with no more education than it takes to use a smart phone. Therefore, each year our main research goal is to be able to teach a wider range of deep learning applications, that run faster, and are more accurate, to people with less prerequisites.\n\nWe want our students to be able to solve their most challenging and important problems, to transform their industries and organisations, which we believe is the potential of deep learning. We are not just trying to teach people how to get existing jobs in the field \u2014 but to go far beyond that.\n\nTherefore, since we first ran our deep learning course, we have been constantly curating best practices, and benchmarking and developing many techniques, trialling them against Kaggle leaderboards and academic state-of-the-art results.\n\nAs we developed our second course, Cutting-Edge Deep Learning for Coders, we started to hit the limits of the libraries we had chosen: Keras and Tensorflow. For example, perhaps the most important technique in natural language processing today is the use of attentional models. We discovered that there was no effective implementation of attentional models for Keras at the time, and the Tensorflow implementations were not documented, rapidly changing, and unnecessarily complex. We ended up writing our own in Keras, which turned out to take a long time, and be very hard to debug. We then turned our attention to implementing dynamic teacher forcing, for which we could find no implementation in either Keras or Tensorflow, but is a critical technique for accurate neural translation models. Again, we tried to write our own, but this time we just weren\u2019t able to make anything work.\n\nAt that point the first pre-release of Pytorch had just been released. The promise of Pytorch was that it was built as a dynamic, rather than static computation graph, framework (more on this in a later post). Dynamic frameworks, it was claimed, would allow us to write regular Python code, and use regular python debugging, to develop our neural network logic. The claims, it turned out, were totally accurate. We had implemented attentional models and dynamic teacher forcing from scratch in Pytorch within a few hours of first using it.\n\nThe focus of our second course is to allow students to be able to read and implement recent research papers. This is important because the range of deep learning applications studied so far has been extremely limited, in a few areas that the academic community happens to be interested in. Therefore, solving many real-world problems with deep learning requires an understanding of the underlying techniques in depth, and the ability to implement customised versions of them appropriate for your particular problem, and data. Because Pytorch allowed us, and our students, to use all of the flexibility and capability of regular python code to build and train neural networks, we were able to tackle a much wider range of problems.\n\nAn additional benefit of Pytorch is that it allowed us to give our students a much more in-depth understanding of what was going on in each algorithm that we covered. With a static computation graph library like Tensorflow, once you have declaratively expressed your computation, you send it off to the GPU where it gets handled like a black box. But with a dynamic approach, you can fully dive into every level of the computation, and see exactly what is going on. We believe that the best way to learn deep learning is through coding and experiments, so the dynamic approach is exactly what we need for our students.\n\nMuch to our surprise, we also found that many models trained quite a lot faster on pytorch than they had on Tensorflow. This was quite against the prevailing wisdom, that said that static computation graphs should allow for more optimization to be done, which should have resulted in higher performance in Tensorflow. In practice, we\u2019re seeing some models are a bit faster, some a bit slower, and things change in this respect every month. The key issues seem to be that:\n\nUnfortunately, Pytorch was a long way from being a good option for part one of the course, which is designed to be accessible to people with no machine learning background. It did not have anything like the clear simple API of Keras for training models. Every project required dozens of lines of code just to implement the basics of training a neural network. Unlike Keras, where the defaults are thoughtfully chosen to be as useful as possible, Pytorch required everything to be specified in detail. However, we also realised that Keras could be even better. We noticed that we kept on making the same mistakes in Keras, such as failing to shuffle our data when we needed to, or vice versa. Also, many recent best practices were not being incorporated into Keras, particularly in the rapidly developing field of natural language processing. We wondered if we could build something that could be even better than Keras for rapidly training world-class deep learning models.\n\nAfter a lot of research and development it turned out that the answer was yes, we could (in our biased opinion). We built models that are faster, more accurate, and more complex than those using Keras, yet were written with much less code. We\u2019ve implemented recent papers that allow much more reliable training of more accurate models, across a number of fields.\n\nThe key was to create an OO class which encapsulated all of the important data choices (such as preprocessing, augmentation, test, training, and validation sets, multiclass versus single class classification versus regression, et cetera) along with the choice of model architecture. Once we did that, we were able to largely automatically figure out the best architecture, preprocessing, and training parameters for that model, for that data. Suddenly, we were dramatically more productive, and made far less errors, because everything that could be automated, was automated. But we also provided the ability to customise every stage, so we could easily experiment with different approaches.\n\nWith the increased productivity this enabled, we were able to try far more techniques, and in the process we discovered a number of current standard practices that are actually extremely poor approaches. For example, we found that the combination of batch normalisation (which nearly all modern CNN architectures use) and model pretraining and fine-tuning (which you should use in every project if possible) can result in a 500% decrease in accuracy using standard training approaches. (We will be discussing this issue in-depth in a future post.) The results of this research are being incorporated directly into our framework.\n\nThere will be a limited release for our in person students at USF first, at the end of October, and a public release towards the end of the year. (By which time we\u2019ll need to pick a name! Suggestions welcome\u2026) (If you want to join the in-person course, there\u2019s still room in the International Fellowship program.)\n\nIf it feels like new deep learning libraries are appearing at a rapid pace nowadays, then you need to be prepared for a much faster rate of change in the coming months and years. As more people enter the field, they will bring more skills and ideas, and try more things. You should assume that whatever specific libraries and software you learn today will be obsolete in a year or two. Just think about the number of changes of libraries and technology stacks that occur all the time in the world of web programming \u2014 and yet this is a much more mature and slow-growing area than deep learning. So we strongly believe that the focus in learning needs to be on understanding the underlying techniques and how to apply them in practice, and how to quickly build expertise in new tools and techniques as they are released.\n\nBy the end of the course, you\u2019ll understand nearly all of the code that\u2019s inside the framework, because each lesson we\u2019ll be digging a level deeper to understand exactly what\u2019s going on as we build and train our models. This means that you\u2019ll have learnt the most important best practices used in modern deep learning\u2014not just how to use them, but how they really work and are implemented. If you want to use those approaches in another framework, you\u2019ll have the knowledge you need to develop it if needed.\n\nTo help students learn new frameworks as they need them, we will be spending one lesson learning to use Tensorflow, MXNet, CNTK, and Keras. We will work with our students to port our framework to these other libraries, which will make for some great class projects.\n\nWe will also spend some time looking at how to productionize deep learning models. Unless you are working at Google-scale, your best approach will probably be to create a simple REST interface on top of your Pytorch model, running inference on the CPU. If you need to scale up to very high volume, you can export your model (as long as it does not use certain kinds of customisations) to Caffe2 or CNTK. If you need computation to be done on a mobile device, you can either export as mentioned above, or use an on-device library.\n\nWe still really like Keras. It\u2019s a great library and is far better for fairly simple models than anything that came before. It\u2019s very easy to move between Keras and our new framework, at least for the subset of tasks and architectures that Keras supports. Keras supports lots of backend libraries which means you can run Keras code in many places.\n\nIt has a unique (to our knowledge) approach to defining architectures where authors of custom layers are required to create a method which tells Keras what shape output it creates for a given input. This allows users to more easily create simple architectures because they almost never have to specify the number of input channels for a layer. For architectures like Densenet which concatenate layers it can make the code quite a bit simpler.\n\nOn the other hand, it tends to make it harder to customize models, especially during training. More importantly, the static computation graph on the backend, along with Keras\u2019 need for an extra phase, means that it\u2019s hard to customize a model\u2019s behaviour once it\u2019s built.\n\nWe expect to see our framework and how we teach Pytorch develop a lot as we teach the course and get feedback and ideas from our students. In past courses students have developed a lot of interesting projects, many of which have helped other students\u2014we expect that to continue. Given the accelerating progress in deep learning, it\u2019s quite possible that by this time next year, there will be very different hardware or software options that will make todays\u2019 technology quite obsolete. Although based on the quick adoption of new technologies we\u2019ve seen from the Pytorch developers, we suspect that they might stay ahead of the curve for a while at least\u2026\n\nIn our next post, we\u2019ll be talking more about some of the standout features of Pytorch, and dynamic libraries more generally.\n\nTo discuss this post at Hacker News, click here", "sentiment": 0.20260178686561664},
{"link_title": "China to Ban Sale of Fossil Fuel Cars in Electric Vehicle Push", "url": "https://www.bloomberg.com/amp/news/articles/2017-09-09/china-to-ban-sale-of-fossil-fuel-cars-in-electric-vehicle-push", "text": "China will set a deadline for automakers to end sales of fossil-fuel powered vehicles, a move aimed at pushing companies to speed efforts in developing electric vehicles for the world\u2019s biggest auto market.\n\nXin Guobin, the vice minister of industry and information technology, said the government is working with other regulators on a timetable to end production and sales. The move will have a profound impact on the environment and growth of China\u2019s auto industry, Xin said at an auto forum in Tianjin on Saturday.\n\nA ban on combustion-engine vehicles will help push both local and global automakers to shift toward electric vehicles, a carrot-and-stick approach that could boost sales of energy-efficient cars and trucks and reduce air pollution while serving the strategic goal of cutting oil imports. The government offers generous subsidies to makers of new-energy vehicles. It also plans to require automakers to earn enough credits or buy them from competitors with a surplus under a new cap-and-trade program for fuel economy and emissions.\n\nHonda Motor Co. will launch an electric car for the China market in 2018, China Chief Operating Officer Yasuhide Mizuno said at the same forum. The Japanese carmaker is developing the vehicle with Chinese joint ventures of Guangqi Honda Automobile Co. and Dongfeng Honda Automobile Co. and will create a new brand with them, he said.\n\nInternet entrepreneur William Li\u2019s Nio will start selling ES8, a sport-utility vehicle powered only with batteries, in mid-December. The startup is working with state-owned Anhui Jianghuai Automobile Group, which also is in a venture with Volkswagen AG to introduce an electric SUV next year.\n\nChina, seeking to meet its promise to cap its carbon emissions by 2030, is the latest country to unveil plans to phase out vehicles running on fossil fuels. The U.K. said in July it will ban sales of diesel- and gasoline-fueled cars by 2040, two weeks after France announced a similar plan to reduce air pollution and meet targets to keep global warming below 2 degrees Celsius (3.6 degrees Fahrenheit).", "sentiment": 0.01359180035650624},
{"link_title": "Promotional Colored Pencils Set", "url": "http://www.dataipencil.com/product/Promotional-Colored-Pencils-Set.html", "text": "", "sentiment": 0.0},
{"link_title": "Putin's Russia Embraces ICO as China Balks", "url": "https://www.bloomberg.com/news/articles/2017-09-08/russia-makes-u-turn-on-cryptocurrencies-after-backing-from-putin", "text": "Russia is drawing up rules about how to conduct initial coin offerings, breaking ranks with China after President Vladimir Putin signaled his approval for digital currencies.\n\nWhile China slapped a blanket ban on ICOs this month, the government in Moscow plans to regulate cryptocurrencies like securities rather than outlawing them, Finance Minister Anton Siluanov told reporters on Friday. That marks a full reversal from his ministry\u2019s proposal last year to punish people who use digital currencies with up to seven years in jail.\n\nAppetite for the new instruments has been growing ever since Putin met in June with the founder of the world\u2019s second-largest cryptocurrency after bitcoin and gave his blessing for Russia to develop blockchain, the technology underlying bitcoin. A consortium of lenders including Sberbank PJSC is now seeking to use the technology to cut costs, while a presidential aide last month announced plans for an ICO.\n\nBy contrast, China\u2019s central bank has ordered all fundraising efforts related to ICOs -- which have raised at least $1.25 billion globally so far -- halted immediately, a decision that may have an impact on investors who had participated in at least 65 of the projects by mid-July. Chinese regulators have also decided to close domestic trading cryptocurrency platforms, Caixin reported, citing unidentified people close to the nation\u2019s Internet financial risk prevention team.\n\n\u201cThe state certainly understands that cryptocurrencies are a reality, there is no point in prohibiting them,\u201d Siluanov told reporters in Moscow. \u201cIt is possible to regulate them, so the Finance Ministry will draw up a bill by the end of the year.\u201d\n\nThat reality wasn\u2019t always apparent in Russia. Before Putin\u2019s meeting with Vitalik Buterin, the Russian-Canadian founder of Ethereum, the legal status of cryptocurrencies was unclear.\n\nSince then, a company co-owned by the president\u2019s internet ombudsman, Dmitry Marinichev, has announced a plan to raise $100 million in an ICO to fund a domestic digital currency-mining operation. Herman Gref, Sberbank\u2019s Tesla-driving chief executive, has put the weight of Russia\u2019s biggest bank behind a modified ethereum protocol dubbed Masterchain to make interbank money transfers safer and faster.\n\nNot all Russian officials are believers, however. Bank of Russia Governor Elvira Nabiullina warned Friday at the same forum that there was \u201cgold fever\u201d surrounding digital currencies and said that they shouldn\u2019t be used as a surrogate for money.", "sentiment": -0.0018981018981018988},
{"link_title": "Dear Amazon, We Picked Your New Headquarters for You", "url": "https://www.nytimes.com/interactive/2017/09/09/upshot/where-should-amazon-new-headquarters-be.html?utm_content=bufferea91b&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer", "text": "The company announced this week that it was looking for a metropolitan area in North America with at least a million people, so we\u2019ve started with the map above. (With apologies to Canada, we\u2019ve set aside Toronto and several other large cities because they\u2019re not included in most of the data sets we\u2019ve used to determine which places meet Amazon\u2019s needs.)\n\nAreas with at least one million people where job growth is strong \u2026\n\n\u2026 and the right labor pool is large and growing \u2026\n\nIn these metro areas, more than one in eight workers is in an industry related to tech, science or professional services, according to the census. (That figure is one in five in Raleigh, N.C.; San Francisco; San Jose, Calif.; and Washington). And the segment of the labor pool that Amazon is particularly interested in \u2014 software programmers and designers \u2014 is growing rapidly, according to an analysis of tech jobs by the Brookings Institution. For its recruiting, Amazon also says it requires a strong university system nearby. All of these metro areas offer that, with colleges that include computer science degrees.\n\n\u2026 and the quality of life is high \u2026\n\n\u201cQuality of life\u201d in this context is primarily about two things \u2014 housing costs and amenities \u2014 and striking a balance between them. The Bay Area does a poor job of that, given that its high cost of living is now driving away even tech workers with six-figure salaries. New York loses out because of high housing costs, too, as measured by median rents in census data. Boston is relatively expensive, but we\u2019re keeping it in the running because its tech job pool is so good. Washington is also expensive, but more affordable Baltimore is a commuter rail line away. And Jeff Bezos, for one, has a nice quality of life in D.C. \u2014 he bought a $23 million home there this year.\n\nAs for the amenities, the winning region will also have the restaurants, outdoor recreation, cultural attractions and general cool of Amazon\u2019s first home, Seattle. Urban economists suggest that such amenities are important to explaining the allure of cities. We asked the economist David Albouy to rank these metro areas for us with an index he uses to measure how much people would be willing to sacrifice, in terms of housing costs and commutes, to live in desirable places. On that basis, we cut Charlotte, N.C., and Indianapolis, because they rank lower on the cultural edginess that attracts young, educated workers.\n\n\u2026 and workers can easily get around \u2014 and out of town \u2026\n\nAn Amazon priority is mass transit, and it has asked applicants to provide their traffic congestion rankings during peak commuting hours. These remaining metro areas are among the top 15 in the country in the share of workers who commute by transit, according to the American Community Survey. Gone are those with both weak transit and bad congestion rankings according to the company INRIX: Atlanta, Miami, Dallas and Austin.\n\nAmazon could easily sort through all the criteria above by itself (the company knows data, after all). But there are two questions it can\u2019t answer without information from potential host cities: What are they willing to give the company in incentives and tax breaks? And what real estate is on the table? This leads us to our winner:\n\n\u2026 and there is space and a willingness to pay to play.\n\nIt\u2019s hard to imagine where the Boston region would find the room for a company that will ultimately want up to eight million square feet of office space (the Pentagon, for comparison, has 6.6 million). Mayor Marty Walsh also said on Thursday that Boston is \u201cnot going to get into a bidding war with another city over something like this.\u201d And it\u2019s pretty clear that a bidding war is what Amazon wants.", "sentiment": 0.09756344230482161},
{"link_title": "Equifax security freeze PIN is just a timestamp", "url": "https://twitter.com/webster/status/906638411930497029", "text": "", "sentiment": 0.0},
{"link_title": "Two Centuries of Dinosaur Art Come Alive in This Gorgeous New Book", "url": "http://www.smithsonianmag.com/smart-news/new-book-beautiful-history-dinosaur-art-180964712/?no-ist", "text": "For most dinosaur nerds, it\u2019s not pictures of bone-white skulls or smashed fossils that got them hooked on paleontology. It\u2019s all those awesome paintings of T. rex ripping out the throats of iguanodons, pterodactyls gliding over prehistoric jungles and long-necked titanosaurs slurping up tons of vegetation.\n\nIt turns out there\u2019s a name for that genre of mind-blowing images: Paleoart. In Taschen\u2019s new book Paleoart: Visions of the Prehistoric Past, writer and art historian Zo\u00eb Lescaze explores the history of the art form, which began about 200 years ago and has developed into a critical part of the paleontological world.\n\nThe history was the brain chile of Lescaze and artist Walton Ford, who contributes a forward and whose paintings are often a weird, satirical take on 19th-century naturalist paintings. Lescaze spent almost four years traveling the United States and Europe tracking the history of paleoart, which was inadvertently first developed in 1830 by scientist Henry Thomas De la Beche, founder of the British Geological Survey. Beche\u2019s friend and neighbor, the fossil hunter Mary Anning, was making incredible finds including the first complete Plesiosaurus, but because of her sex, poverty and lack of education she received little recognition. To bring attention to Anning, Beche painted the watercolor \"Duria Antiquior\u2014A More Ancient Dorset,\" illustrating her finds. Prints of the image became a bestseller.\n\nThat popular painting set off the entire genre. At first, Lescaze explains, the works were largely confined to scientific texts. But in 1854, British naturalist and artist Benjamin Waterhouse Hawkins displayed life-sized sculptures of dinosaurs at the Crystal Palace in London, introducing dinosaurs to a mass audience. Americans, too, caught the dinosaur bug, and illustrations of extinct animals soon infiltrated the academic and popular press and became common at natural history museums.\n\nToday, such illustrations are carefully vetted and produced in an almost photo-realistic style. But in the first 150 years of paleoart, artists had much less information to work with, taking some interesting liberties with their subjects and often rendering them in the style of the day, whether that was neo-Impressionism, Art Nouveau or even Socialist Realism.\n\n\u201cPaleoart went from this sort of niche two-dimensional format to take on every conceivable form,\u201d Lescaze says. \u201cOne of the highlights of my research was going to Moscow and finding an enormous concave mosaic that towers several dozen feet above you that was just magnificent with hundreds of animals on this glazed ceramic. In the same museum you have a mural that is golden and pastel sort of like Monet\u2019s water lilies. So it went from small scale origins to these monumental statements and everything in between. That\u2019s what makes the genre so interesting to me.\"\n\nWe asked Lescaze to give us some more insight into the overlooked history of dino-art.\n\nWhere did you find all these incredible images?\n\nPaleoart is this sprawling genre that spans the UK, Europe and the United States. The research became this fascinating process of tracking down these more obscure works and unsung artists. There are so many works I found in university archives and natural history museums\u2014 oil paintings that were lodged between shelves of saber-toothed tiger skulls that were just beautiful pieces that had never been reproduce or only once in an obsolete science book. So it was a real pleasure to bring some of these artworks to light and perhaps introduce audiences to a genre they may not be familiar with.\n\nShould this stuff be in art museums, or are they just curiosities from paleontology's past?\n\nI think that they are extremely valuable and their value extends beyond their original scientific purposes. They occupy this nebulous niche between scientific illustration and fine art proper. They are not works of fine art, many of them are didactic and designed to relay information. Because they're images of things no human has ever seen, works of paleoart can get discarded in a way that I think images of hawks and herons would not. They are seen as being scientifically obsolete, so why keep them around?\n\nI came to appreciate works of paleoart as being able to tell us a lot about the time in which they were created, the political context and the cultural context. A dinosaur painted in Soviet Russia looks a lot different than one painted in occupied France or Gilded Age America. Because of that they\u2019re worth hanging onto, and if this book has any effect on natural history museums and other institutions on preserving outdated works of paleoart I\u2019d be thrilled.\n\nHas paleoart skewed our view of prehistoric creatures?\n\nI think in the beginnings of the genre in particular paleoart was really controversial. Some scientists didn\u2019t believe it should be made in a lot of situations. [For instance], Labyrinthodont, was a species that Benjamin Waterhouse Hawkins sculpted, and he sort of made it like a very wonky-looking frog. Shortly after that, more specimens were found and scientists revised their idea of what it looked like. But [Hawkins'] form kept getting repeated everywhere. [Leading American paleontologist] Othniel Charles Marsh was like, just look at that snafu, let\u2019s not do more of those.\n\nThese ideas are hard to eradicate once they\u2019ve lodged themselves in people\u2019s minds. It\u2019s interesting to consider that now. Scientists have had evidence for a while now that many dinosaurs had feathers. But the new Jurassic Park film comes out and none of them have feathers. People are married to the idea that dinosaurs have this crocodilian, leathery, scaly, reptile skin. This is the power of these images.\n\nDo you have a favorite paleoartist?\n\nYes! Konstantin Konstantinovich Flyorov, this Russian artist who I wasn\u2019t even a little bit aware of when I started this project. Despite being a scientist himself, working in Soviet-era Russia, he really played it fast and loose with the fossil evidence, adapting dinosaurs and prehistoric mammals to his own aesthetic purposes. He was obviously having so much fun through the sheer act of painting, and, of course, this is at a time when proper fine artists were under pretty strict scrutiny from the state, so he inadvertently had almost more room to play by painting within the scientific arena. You see these animals painted in shades of lilac and marigold and these big expressive brushstrokes. They're not depicted as literally scientific or particularly helpful in any educational way. They are just gorgeous paintings, and I think they\u2019re great.", "sentiment": 0.20653398417821495},
{"link_title": "Building a 40 Year Old Computer", "url": "https://www.youtube.com/watch?v=MJOjK746cYw", "text": "", "sentiment": 0.0},
{"link_title": "Spectra, a demoscene engine in rust", "url": "https://github.com/phaazon/spectra", "text": "spectra is a demoscene framework \u2013 or call it engine \u2013 which aims are to ease demoscene productions development. Those aims must be reached in terms of the demo itself but also in terms of tools written to make the demo. spectra features both sides of such a making process. The idea is to provide a robust library that can be used to build end-user binaries as well as demo tools for editing process for instance.\n\nEven though the main goals are to provide a strong codebase for demo making, the engine should be usable for other applications, like video game making. Interaction is not a primary focus, but it should enhance with time though.\n\nIf you target animation-related applications, you should find spectra a friend of yours.\n\nBecause spectra exists to help me writing demos, I don\u2019t usually accept PR on that project, since I want to be the only one working on my demo. However, as I love Open Source, if you really provide a useful patch or a very nice idea, I shall merge it. But hush\u2026 ;)", "sentiment": 0.11821428571428572},
{"link_title": "How a bullet turns into a beep", "url": "https://blogs.msdn.microsoft.com/oldnewthing/20070104-12/?p=28513", "text": "That last character is U+2022. Select that line with the mouse, right-click, and select Copy to copy it to the clipboard. Now go to a command prompt and paste it and hit Enter.\n\nYou'd expect a \u2022 to be printed, but instead you get a beep. What happened?\n\nHm, there's that beep again. How about this program:\n\nRun this program and it prints \"7\".\n\nBy now you should have figured out what's going on. In the OEM code page, the bullet character is being converted to a beep. But why is that?\n\nWhat you're seeing is in reverse. Michael Kaplan discussed a while ago. It determines whether certain characters should be treated as control characters or as printable characters when converting to Unicode. For example, it controls whether the ASCII bell character 0x07 should be converted to the Unicode bell character U+0007 or to the Unicode bullet U+2022. You need the flag to decide which way to go when converting to Unicode, but there is no corresponding ambiguity when converting from Unicode. When converting from Unicode, both U+0007 and U+2022 map to the ASCII bell character.\n\n\"But converting a bullet to 0x07 is clearly wrong. I mean, who expects a printable character to turn into a control character?\"\n\nWell, you're assuming that the code who does the conversion is going to interpret it as a control character. The code might treat it as a glyph character, like this:\n\nRun this program and you get a happy bullet in the corner of the window. The function does not interpret control characters as control characters; it interprets them as glyphs.\n\nThe function doesn't know what you're going to do with the string it produces. It converts the character and leaves you to decide what to do next. There doesn't appear to be a flag, so you're going to get glyph characters whether you like it or not.", "sentiment": 0.03363095238095239},
{"link_title": "The Teen Malware Career of Marcus Hutchins", "url": "https://www.itwire.com/security/79777-researchers-dismiss-bid-to-cast-aspersions-on-marcus-hutchins.html", "text": "Well-known British information security researcher Kevin Beaumont has dismissed an article by reporter Brian Krebs about Marcus Hutchins, the Briton who is awaiting trial in the US on charges of writing and distributing the Kronos banking malware, by pointing out that it has nothing to do with the case.\n\nIn a tweet, Beaumont said, \"Re the MalwareTech Krebs article \u2013 nothing was news to me and it has nothing to do with case (which is purely, per FBI, about Kronos).\"\n\nHe added, \"Also, the UK intelligence services vetted him \u2013 a year ago? I can't remember. Gosh, imagine if they knew he used online forums.\"\n\nKrebs wrote a long article about Hutchins, concluding: \"The clues suggest that Hutchins began developing and selling malware in his mid-teens \u2014 only to later develop a change of heart and earnestly endeavour to leave that part of his life squarely in the rearview mirror.\"\n\nIn what was obviously a sarcastic reference to Krebs' claims that Hutchins had many accounts on underground forums, Beaumont: \"I hear @briankrebs has lots of accounts on underground forums.\"\n\nAnother security researcher, who uses the pseudonym x0rz, said in a tweet: \"He (Hutchins) did have fun trying to learn when he was young, maybe he did resell malware \u2013 yet it has nothing to do with Kronos.\"\n\nA third researcher, Robin Jackson, had this to say: \"I respect @briankrebs but his exercise in \"doxing\" Marcus did nothing to show validity of the charges (by his own admission)...NOTHING.\"\n\nAnd yet another well-known infosec figure, Rob Graham, issued a long thread of tweets, starting out with this: \"1/ That @MalwareTechBlog is the 'wrong guy' was never the narrative. The narrative is that we stand behind members of our community.\"\n\nHe followed up with this: \"2/ People are innocent until proven guilty. We should not expel him as a pariah until he's been proven guilty.\"\n\nHutchins, who gained the attention of the world when he stopped the spread of the WannaCry ransomware by accident in May, was arrested by the FBI in Las Vegas on 2 August after he had boarded a plane to leave the US.\n\nHe had gone to the US to attend the annual DEFCON security conference.\n\nThe chargesheet against him said he had written and helped distribute Kronos along with an unnamed co-conspirator.\n\nHutchins was granted bail on 14 August after a court appearance during which he pleaded not guilty to all six charges levelled against him. He is set to face court again in Milwaukee in October.", "sentiment": 0.02969924812030074},
{"link_title": "Tesla remotely extends range of vehicles for free in Florida", "url": "https://electrek.co/2017/09/09/tesla-extends-range-vehicles-for-free-in-florida-escape-hurricane-irma/amp/", "text": "Millions of people are currently affected by the evacuation of Florida as Hurricane Irma starts reaching the state and creates some difficult traffic situation when escaping north. There are reports of traffic jams and gas stations running out gas.\n\nThere are a lot Tesla owners in Florida and they are also escaping north using the Supercharger network.\n\nNow Tesla has even facilitated travels for some of them as the automaker remotely unlocked the full battery pack capacity of Model S/X 60/60D vehicles with 75 kWh battery packs.\n\nThat\u2019s due to Tesla using an unforeseen feature of their over-the-air software update system.\n\nTesla used to offer the option to buy a Model S or Model X with a 75 kWh battery pack software-locked at a capacity of 60 kWh. The option would result in a less expensive vehicle with a shorter range, but the option to pay to remotely enable the longer range at a later stage.\n\nSome of those owners reported this morning having more range than usual in their vehicles.\n\nA Tesla Model S 60 owner in Florida reached out to us with almost 40 more miles than in his usual full charge and a new \u201975\u2019 badge in his car software.\n\nWhile he didn\u2019t ask for it nor knew why it changed, Tesla had temporarily unlocked the remaining 15 kWh of the car\u2019s software-limited battery pack option to facilitate the owner\u2019s evacuation.\n\nWe reached to Tesla and a representative confirmed that the company has put in place the emergency measure to temporarily extend the range of the vehicles of Tesla owners in the path of Hurricane Irma.\n\nThe company says that a Tesla owner in a mandatory evacuation zone required another ~30 more miles of range to optimize his evacuation route in the traffic and they reached out to Tesla who agreed to a temporary access to the full 75 kWh of energy in the battery pack, an upgrade that has cost between $4,500 and $9,000 depending on the model and time of upgrade.\n\nConsidering the 15 kWh (30 to 40 additional miles) could also be useful to other owners affected by Irma, Tesla decided to also temporarily unlock other vehicles with the same software-lock battery packs in the region.\n\nTesla\u2019s Supercharger network is fairly extensive in Florida and most owners should be able to get by even with a Model S 60 (the shortest range option), but sometimes that 30 more miles of range can make a big difference.\n\nMost Supercharger stations in Florida are still online:\n\nThe station in the Florida Keys is offline and one in Myers is currently listed as \u201creduced service\u201d:\n\nOtherwise, the other Superchargers are reportedly online and accessible on the way north. Stay safe out there everyone.", "sentiment": 0.1217092803030303},
{"link_title": "Basics of demoscene programming", "url": "https://www.youtube.com/watch?v=TbcZyAO6K7c", "text": "", "sentiment": 0.0},
{"link_title": "Tesla increases battery range for owners fleeing Hurricane Irma", "url": "http://www.teslarati.com/tesla-issues-software-update-help-owners-flee-hurricane-irmas-evacuation-zone/", "text": "Tesla has issued a surprise over the air software update to Model S/X 60(D) owners living within Florida\u2019s Hurricane Irma evacuation zone. Model S/X 60(D) vehicles with a 75 kWh battery pack that were previously software limited to 210 miles of driving range per single charge were automatically upgraded to 249 miles, and given temporary access to the full range capacity of the battery.\n\nThe update is seen as a gesture of good will by Tesla to help its drivers flee southern Florida as the state prepares for the most powerful Atlantic storm to hit its shores in nearly a decade.\n\nReports of Tesla Model S 60 owners living in the path of Hurricane Irma who received a sudden increase in battery range first surfaced on Reddit. \u201cI\u2019m in South Florida and I own a MS60. I just checked my app and look what it says. I\u2019ve never been higher that 215 miles. Did Tesla do this for evacuations?\u201d asked Redditor Ludachris9000.\n\nSimilar reports soon followed across the Tesla Motors Club, with one Tesla owner confirming that the battery range upgrade was issued to all Tesla owners with an upgradeable 60 kWh battery, and living in Hurricane Irma\u2019s evacuation zone in Florida, to 75 kWh. The update adds another 40 miles of driving range.\n\n\u201cI verified it was released to any 60s in the evacuation zones. I was told it will be removed on September 16.\u201d said CMPSL.\n\nTeslarati reached out to Tesla for confirmation after hearing wide reports of similar stories being reported by owners. A Tesla spokesperson confirms that an over-the-air software update was in fact issued to increase driving range for owners that may be impacted by Hurricane Irma.\n\nAs nearly 5.6 million people or more than one quarter of the state\u2019s population has been asked to evacuate ahead of Hurricane Irma making landfall, long lines at gas stations have put more stress on fuel supply than previous storms of the past, leaving many stations empty and drivers stranded. The hoarding of gasoline has prompted Governor Rick Scott to ask evacuees to take only as much gas as needed.\n\nTesla issues software update to help owners flee Hurricane Irma by increasing battery range\n\nInterested in solar? Get a solar cost estimate and find out how much a solar system would cost for your home or business.", "sentiment": 0.154421768707483},
{"link_title": "Academic excellence: Golden Germany", "url": "http://www.nature.com/nature/journal/v549/n7670/full/nj7670-119a.html", "text": "Berlin inspires. Less dominated by international finance and commerce than many other global cities, the vibrant German capital is a cultural laboratory that exerts a special fascination for hipsters, hedonists and holidaymakers. But Berlin's invigorating intellectual biosphere also attracts creative minds and scientists from around the world. John Dunlop, a group leader at the Max Planck Institute of Colloids and Interfaces in Potsdam, near Berlin, is one such researcher. The Australian biophysicist investigates parallels between living matter and synthetic structures \u2014 and he loves Berlin's special flair.\n\n\u201cThe atmosphere here reminds me of the Bauhaus approach to design and architecture a century ago,\u201d he says. \u201cThere is a lot of freedom to try new things without too much pressure to think about commercial exploitability.\u201d\n\nDunlop participates in an interdisciplinary laboratory, ambiguously called Image Knowledge Gestaltung (roughly translated as 'design'), that is funded by the German federal government as part of its Excellence Initiative. The collaboration \u2014 one of 43 'excellence clusters' across Germany \u2014 involves a mix of natural scientists, designers, cultural historians, media theorists and linguists from several universities and research institutes in the Berlin\u2013Brandenburg region. The idea is to identify principles that control how the design of material structures, ranging from office buildings to the shells of bizarre marine creatures, in turn control their inhabitants' operation and communication. This is interdisciplinary research with uncertain outcomes \u2014 but the results could be used at some point in robotics, teaching or health care.\n\n\u201cGermany is a really good place for such ventures,\u201d says Dunlop, who did a PhD in France before he came to Potsdam in 2005 as a postdoc. \u201cI can think of no other way of getting funding for a project where I could work together with so many people outside my own field.\u201d\n\nThe 4.6-billion (US$5.4-billion) Excellence Initiative, launched in 2005, has helped to attract at least 4,000 foreign scientists to Germany (see page 18). And more foreign talent is likely to arrive as the programme, which includes funding opportunities for numerous PhD students, postdocs and young group leaders, enters its next round in 2018. (All major political parties participating in the upcoming federal elections on 24 September have said that they want to continue the programme.)\n\nThe initiative has revolutionized Germany's academic landscape. It nullified a long-held paradigm that the country's more than 100 research universities (fewer than half of which are now involved in the excellence framework) are all equal in quality. The initiative, a competition for top-up funds from the government, aims to make German universities more competitive internationally. Its second round in 2012 produced 11 'elite' universities that each get an extra 12.5 million per year for their campus-wide institutional-improvement strategies. A further 28 universities received extra money for setting up promising excellence clusters \u2014 research hubs that bring together different groups within the university or the wider region \u2014 and international graduate schools.\n\nAccording to preliminary analysis by the DFG, Germany's main funding agency, universities involved in the programme have measurably improved their research output in key fields such as physics and chemistry.\n\nA 2015 analysis by Nature found that Germany's 14 top universities \u2014 the 'elite' of the Excellence Initiative \u2014 produced 35% of the country's total scientific output, up from 29% in 2002 (Nature 525, 168\u2013169; 2015). And of all articles published by German authors, one-quarter are now ranked in the top 10% worldwide by citation, compared to one-sixth 15 years ago. Some lesser-known research universities involved in the Excellence Initiative were rivalling the country's top institutions in their rate of improvement.\n\nAnd an international evaluation panel convened by the German government found last year that the excellence clusters are among the initiative's most tangible successes. The clusters, which each received up to 8 million per year in the 2012\u201317 programme period, should be the focus of future funding rounds, the panel recommended.\n\nThe government had decided in 2015 to continue the Excellence Initiative (now known as the Excellence Strategy) beyond 2017, dispelling fears that its gains might be unsustainable. From 2019 onwards, about 385 million per year will be available for 45\u201350 yet-to-be-selected clusters. By April, 63 universities had submitted 195 draft proposals (including those from all existing clusters), which are now under review. A shortlist will be announced on 28 September, and those selected will be invited to submit full proposals.\n\nAlthough the largest research universities, such as those in Munich and Heidelberg, have not climbed to top positions in international academic rankings, Germany as a whole is a big player in international science. Thanks to the government's strategic science policies \u2014 more than a decade of steadily rising public research budgets have created exceptionally good funding opportunities \u2014 it has gained in attractiveness to foreign researchers (see 'International flair'), including many from strong science nations such as Australia, Canada and the United Kingdom.\n\nKylie Luska, for example, had not planned to spend more than a couple of years in Germany when he took up a postdoc position at the RWTH Aachen University in 2012. The Canadian chemist specializes in processes involved in converting biomass into efficient biofuels. His supervisor at the University of Guelph in Canada told him about Germany's particular strength in chemistry and the country's ambitious green-energy goals. Aachen's excellence cluster for tailor-made fuel from biomass proved a perfect match for his own interests, he says. \u201cGathering experience in a country at the forefront of the global energy transition seemed a perfect opportunity to advance my career in green chemistry,\u201d Luska says.\n\nHis work on nanoparticles that convert biomass substrates into fuels earned him a group-leader position after four years, funded mostly by the Aachen excellence cluster. Employed on a fixed-term contract, he is not sure how his future will unfold. But he hopes that his stint in Aachen, where scientists and engineers are working to reconcile fundamental chemistry with technical aspects of combustion engines, will improve his prospects of finding a permanent position in Germany, Canada or elsewhere.\n\nThe future is also open for Rebecca Scott, an ecologist at the GEOMAR Hemholtz Centre for Ocean Research Kiel. In 2013, before she finished her PhD at Swansea University, UK, she won a 225,000, three-year grant from the 'Future Ocean' excellence cluster in Kiel for her research on the juvenile life stages of sea turtles. A follow-up 150,000 package allowed her to extend her postdoctoral stint at GEOMAR. Recently, she secured another 200,000 to add a further two years. With the centre's engineering department, she developed floating mock turtles and tiny acoustic tags, which stick to hatchlings' undersides.The team uses these to track the elusive drifting patterns of baby turtles in the ocean currents off Gabon in West Africa.\n\nInvolvement in the cluster gives her ample freedom for her expensive fieldwork and a level of funding security unusual at her early career stage. \u201cIt's a great advantage that I have face-to-face contact in Kiel with senior people on grant-review committees,\u201d she says. \u201cThese people understand my work and give me all the support I need.\u201d\n\nFor personal reasons, she still wanted to return to the United Kingdom \u2014 until the Brexit referendum last year, in which the nation voted to leave the European Union, prompted her to extend her stay in Germany for at least a couple more years. One reason, she says, is that a starting grant from the prestigious European Research Council (ERC), for which she plans to apply in the near future, might require her to be affiliated with a host institute in an EU country. \u201cFor now, Germany is a much safer place to do science than the UK, where nobody really has a clue what is going to happen,\u201d she says.\n\nThe DFG last year allocated a record 2.9 billion \u2014 not including extra money from the Excellence Initiative \u2014 to individual researchers and 298 collaborative research centres (Sonderforschungsbereichen). The success rate for individual grant proposals to the DFG \u2014 36.5% on average between 2013 and 2016 \u2014 is higher than the US or UK rates. Even so, increasing dependence on grant money is a concern for many young scientists\n\nSo is the lack of permanent positions in academia. Almost 30,000 doctoral titles are awarded in Germany every year, and the number is rising. A federal law (Wissenschaftszeitvertragsgesetz) limits the time that scientists can be employed on fixed-term contracts to 12 years. The federal science ministry promotes the introduction of US-style tenure-track programmes that would give young scientists more career security. A government-funded 1-billion programme launched this year aims to create 1,000 tenure-track positions over the next 15 years. But currently, these positions are still rare.\n\nZiad Hafed, a neuroscientist previously at the Salk Institute for Biological Sciences in La Jolla, California, was reluctant to accept a financially and scientifically attractive junior group leader position he was offered in 2009 by the Centre for Integrative Neuroscience in T\u00fcbingen, another excellence cluster. But the centre eventually agreed that he would be promoted to a faculty position after a successful evaluation of his group's progress. He passed that hurdle in 2015, and is now determined to stay in Germany.\n\nHafed, a citizen of Egypt and Canada who grew up in Bahrain, studies how the brain handles visual perception. \u201cI had never really thought about going to Europe,\u201d he says. \u201cAnd I wouldn't be here if it wasn't for the Excellence Initiative.\u201d\n\nBefore he took up his job in Germany, Hafed had convinced himself that research conditions in T\u00fcbingen, a hotspot for brain research at all fronts, would leave nothing to be desired. But issues cropped up. One is the \u201cshocking\u201d amount of paperwork associated with animal experiments, he says. Another is the level of public hostility to animal testing that in 2015 caused one prominent scientist in T\u00fcbingen to throw in the towel.\n\nIn Germany, says Hafed, brain researchers need to take a proactive approach to explaining their science to the wider public. \u201cI'm fairly happy as long as we're able to do our experiments,\u201d he says. \u201cBut I do realize that we must work hard to demonstrate that what we're doing is beneficial and justified.\u201d\n\nGermany was not at the top of Xiaoxiang Zhu's list in 2005 when she \u2014 then an undergraduate student in her native China \u2014 decided that it was time to gather international experience. But when she learnt from a Chinese cartographer in Germany that the Technical University of Munich (TUM) had launched an international master's course for Earth-oriented space science and technology, she opted for the Bavarian capital.\n\nIt was a good decision, she says. Dividing her time between the TUM and the German Aerospace Agency (DLR), Zhu is now an assistant professor in a rare tenure-track position. At a DLR centre outside Munich, she has access to global Earth-observation data sets, including high-quality data from a pair of German radar satellites. Her idea of using satellite imagery to make 3D maps of the world's major cities \u2014 and using social-media content to determine the function of urban infrastructures \u2014 secured her a 1.5-million ERC starting grant last year and made her a poster child for the TUM's efforts to attract foreign talent.\n\nForeign scientists are usually not expected to teach courses in German. Many, such as Zhu, are able to negotiate a light teaching load. But German-language skills and a certain instinct for cultural idiosyncrasies \u2014 deciding whom to address formally, and whom to offer the informal 'Du', requires some sensitivity, for example \u2014 are advantageous when dealing with university administrations, tax offices and in everyday life (see 'Settle in Deutschland').\n\n\u201cLanguage is not a barrier in the lab \u2014 but it helps to speak German when you are dealing with local ethics or regulatory authorities over research permits and the like,\u201d says Pierre-Yves Lozach, a French-born virologist with CellNetworks, an excellence cluster of cell-biology institutes in Heidelberg. Lozach took a career risk in 2013 when he quit his tenure-track position at the INRS-Institute Armand-Frappier in Laval, Canada, for a fixed-term group leader job at the University of Heidelberg, which had offered dual-career positions to him and his wife. Any newcomer to the country should take pains to learn German, he says, adding with a chuckle that he was a bit idle himself in that regard.\n\nBut language aside, Lozach sees no roadblocks to doing competitive science in Germany. \u201cIf I don't make it here, it will be my own fault,\u201d he says. Heidelberg, home also to the European Molecular Biology Laboratory and the German Cancer Research Center, is an unfailing source of talent and ideas. \u201cIf I can secure a permanent science job in the region,\u201d he says, \u201cI'm quite determined to stay.\u201d", "sentiment": 0.1293589698831634},
{"link_title": "These Are Not the Robots We Were Promised", "url": "https://www.nytimes.com/2017/09/09/opinion/sunday/household-robots-alexa-homepod.html", "text": "Still, if you were looking forward to having a Rosie scurrying around your abode, feather duster in hand, an Echo feels like a letdown. It just sits there.\n\nThere are good reasons that the domestic robot has taken such an uninspiring form. Some are technical. Visualizing a nimble, sure-footed android is easy, but building one is hard. As the Carnegie Mellon professor Illah Nourbakhsh explains in his book \u201cRobot Futures,\u201d it requires advances not only in artificial intelligence but also in the complex hardware systems required for movement, perception and dexterity.\n\nThe human nervous system is a marvel of physical control, able to sense and respond fluidly to an ever-changing environment. Achieving such agility with silicon and steel lies well beyond the reach of today\u2019s engineers. Even the most advanced of our current automatons still get flustered by mundane tasks like loading a dishwasher or dusting knickknacks.\n\nMeanwhile, thanks to gains in networking, language processing and miniaturization, it has become simple to manufacture small, cheap computers that can understand basic questions and commands, gather and synthesize information from online databanks and control other electronics. The technology industry has enormous incentives to promote such gadgets. Now that many of the biggest tech companies operate like media businesses, trafficking in information, they\u2019re in a race to create new products to charm and track consumers.\n\nSmart speakers provide a powerful complement to smartphones in this regard. Equipped with sensitive microphones, they serve as in-home listening devices \u2014 benign-seeming bugs \u2014 that greatly extend the companies\u2019 ability to monitor people\u2019s habits. Whenever you chat with a smart speaker, you\u2019re disclosing valuable information about your routines and proclivities.\n\nBeyond the technical challenges, there\u2019s a daunting psychological barrier to constructing and selling anthropomorphic machines. No one has figured out how to bridge what computer scientists term the \u201cuncanny valley\u201d \u2014 the wide gap we sense between ourselves and imitations of ourselves. Because we\u2019re such social beings, our minds are exquisitely sensitive to the expressions, gestures and manners of others. Any whiff of artificiality triggers revulsion. Humanoid robots seem creepy to us, and the more closely they\u2019re designed to mimic us, the creepier they are. That puts roboticists in a bind: The more perfect their creations, the less likely we\u2019ll want them in our homes. Lacking human characteristics, smart speakers avoid the uncanny valley altogether.\n\nAlthough they may not look like the robots we envisioned, smart speakers do have antecedents in our cultural fantasy life. The robot they most recall at the moment is HAL, the chattering eyeball in Stanley Kubrick\u2019s sci-fi classic \u201c2001: A Space Odyssey.\u201d But their current form \u2014 that of a stand-alone gadget \u2014 is not likely to be their ultimate form. They seem fated to shed their physical housing and turn into a sort of ambient digital companion. Alexa will come to resemble Samantha, the \u201cartificially intelligent operating system\u201d that beguiles the Joaquin Phoenix character in the movie \u201cHer.\u201d Through a network of speakers, microphones and sensors scattered around our homes, we\u2019ll be able to converse with our solicitous A.I. assistants wherever and whenever we like.\n\nMark Zuckerberg, the Facebook C.E.O., spent much of last year programming a prototype of such a virtual agent. In a video released in December, he gave a demo of the system. Walking around his Silicon Valley home, he conducted a running dialogue with his omnipresent chatbot, calling on it to supply him with a clean T-shirt and toast bread for his breakfast, play movies and music, and entertain his infant daughter. Hooked up to cameras with facial-recognition software, the digitized Jeeves also acted as a sentry for the Zuckerberg compound, screening visitors and unlocking the gate.\n\nWhether real or fictional, robots hold a mirror up to society. If Rosie and her kin embodied a 20th-century yearning for domestic order and familial bliss, smart speakers symbolize our own, more self-absorbed time.\n\nIt seems apt that as we come to live more of our lives virtually, through social networks and other simulations, our robots should take the form of disembodied avatars dedicated to keeping us comfortable in our media cocoons. Even as they spy on us, the devices offer sanctuary from the unruliness of reality, with all its frictions and strains. They place us in a virtual world meticulously arranged to suit our bents and biases, a world that understands us and shapes itself to our desires. Amazon\u2019s decision to draw on classical mythology in naming its smart speaker was a masterstroke. Every Narcissus deserves an Echo.", "sentiment": 0.1469312485759854},
{"link_title": "How to Run WordPress on a Kubernetes Cluster from Scratch", "url": "https://dotlayer.com/how-to-run-wordpress-in-a-kubernetes-cluster/", "text": "Learning about Kubernetes, in the beginning, would feel like drinking from a fire-hose. They have a number of various abstractions to solve various problems, all related to one essential goal \u2013 Running containerized applications across multiple hosts. If you are unfamiliar with what containers are, take a few minutes to read our post on Docker and containers in general.\n\nContainers are tied down to usually one physical machine or a VM (which again runs on a single physical machine). You can have multiple containers on one machine, but not the other way round. This is where Kubernetes comes into the picture. What Kubernetes tries to do, is allow us to aggregate multiple numbers of these VMs and physical machines and enable us to deploy applications (inside containers) that would seamlessly run over these multiple interconnected computers as if it were one system.\n\nThe best approach to learning anything new, like Kubernetes is to get a hands on experience by doing something productive with it. As such, we are going to go through the process of setting up a Kubernetes cluster from scratch and install a WordPress website on it.\n\nWe will be using Ubuntu 16.04, hosted on DigitalOcean for all of our running nodes.\n\nKubernetes architecture involves setting up one master node which would then delegate tasks and manage resources between various \u2018worker nodes\u2018. Accordingly, let\u2019s label one of our VPS master and another as node-1\n\nOnce the nodes are up and running, let\u2019s ssh into our master and configure it with Kubernetes. To begin with, we first update the package repository following which we install Docker\n\nNext, we would want to create an environment variable MASTER_IP which would store the Public IP address of the master node. (Replace the master.server.ip.address with the IP address of your desired master node)\n\nThis is followed by adding the necessary repositories to your master node. To do that run the following commands:\n\nNow that we have necessary modifications in place, let\u2019s update our system about the new repositories available, and then install all the desired packages\n\nAt this, point you may want to run the exact commands as above on your node-1 VPS. Make sure you haven\u2019t accidentally had your node\u2019s IP assigned to variable. The variable must always be containing the master node\u2019s IP, regardless of where it is declared.\n\nNow we pay attention to master node and perform a few actions that would help establish its authority and control over the worker node. We first initialize our Kubernetes master by running the following command, which would also prescribe networking private parameters :\n\nThe command would take a considerable amount to run, so you should wait patiently until you see something like this:\n\nThe last line of the output is very important, the token is what your worker nodes would need to establish a connection with the master. Save it on your local machine as a text file, for we will need it later on.\n\n You will notice that the output also asks us to start running as a regular user, so if you don\u2019t have a regular user, create one, set its password and give it sudo access.\n\nThen login as bob, the regular user by running:\n\nNow add the Kubernetes configuration in bob\u2019s home directory,\n\nLastly, we need to establish the networking between various nodes that will join this cluster in the future. To do this, we use yet another software defined networking layer, flannel:\n\nThe above commands would grab the .yaml files describing how to set up flannel on this cluster.\n\nThe node is made aware of the master by using the Token that was generated earlier, just after init command. Use the command that was generated in your terminal instead of copying the one below and run it on your target node as user.\n\nYou can keep adding as many nodes as you desire, using the Token. If your applications need more resources in the future, you can just add another node and the applications would start using it without any down time whatsoever. After this most of the commands would be run on the master node, as bob, the regular user.\n\nYou may want to check if everything is up and running. To do that there are a few handy commands that you may want to keep in the back of your head. These may take a few minutes to up to half an hour, so be patient.\n\nBoth of which would give you an output similar to this:\n\nYou may not see the entry with dashboard written in it, but others would work just fine. The dashboard is a web UI for easier monitoring and management of your Kubernetes cluster.\n\nOnce we have our cluster up and running, time to make it work for us. The first thing to know is that every node will talk to the master using which is a binary package that we installed earlier in the tutorial.\n\nWordPress would require a persistent volume (a volume which doesn\u2019t get destroyed over reboots or deletion of applications) this volume will be used by mysql for maintaining various databases to be used by WordPress. To do that, open a file persistent-volume.yaml using any terminal text editor that you prefer. Then enter the following contents in the file:\n\nAfter this is saved, in the same directory as this file, run the command:\n\nThis file consists instructions for creating two persistent volumes. To see if this has indeed worked, run:\n\nOnce the status shows that the volumes are available, let\u2019s move on to creating a secret which would be used to store your mysql password. Replace the field with your secure password.\n\nThe object is what holds the secret now, and we shall use it to deploy by first creating a file called\n\nThat file would kick in the database if you run:\n\nFinally, we are ready for our WordPress installation. Create a file:\n\nAfter creating the above file, run:\n\nThis is the final step and now your WordPress deployment should be up and running. To check the port forwarding run the command:\n\nYou may have to wait a few minutes as Kubectl gets all the necessary files and configuration set up, but eventually, you will see this:\n\nIf the external IP shows as or that is completely fine. The important portion is the port number that it is running on, in the above case it is so when you enter your node\u2019s IP address along with the port number, as shown below, you will be greeted with WordPress installation page.\n\nYou may think that most of the rigmarole that we went through is quite fuzzy and disorienting, and while it is true, it is largely because the above procedure is independent of your cloud provider. Most mainstream cloud providers have their custom solution for configuring the cluster which is largely automated.\n\nEven after that, if it seems too much to you, many professional DevOps would agree with that. There\u2019s nothing wrong with not liking overly complicated systems, especially if all you plan is to run a small blog or a portfolio holding website. Kubernetes is one of the many scalability options to choose from. Docker Swarm is yet another emerging technology. Most of them are in Beta, so was Kubernetes a few months back! So hopefully, these technologies will grow and become more user-friendly before your businesses grow to the point where you need to scale up.\n\nThe following references largely helped me get on my feet with Kubernetes, so giving credit where it is due, here\u2019s the necessary GitHub repository and further in-depth tutorials:\n\nWe hope you have enjoyed reading this post about how to install and run WordPress on a Kubernetes cluster. Feel free to reach out to our support crew in the case of any issues, we offer 24/7 WordPress support at a ridiculously low price. If you liked this article, then please subscribe to our mailing list for WordPress video tutorials. You can also find us on Twitter and Facebook.", "sentiment": 0.08594578792854654},
{"link_title": "Show HN: A Desktop Unicode Character Search and Insertion Tool", "url": "https://github.com/andrew-pa/ununi", "text": "Ununi is a utility for Windows that allows you to search for and insert Unicode characters.\n\nEither download a release build or build from source with . Running the executable will download the Unicode character reference, build the search index, and prompt to ask if you would like Ununi to start on startup/login for your user account.\n\nBy default Ununi is configured to open with the Alt+F1 hotkey. Typing will then search the Unicode standard for characters that match the query. Pressing Enter will copy the currently selected character to the window that was in the foreground when the hotkey was pressed. The arrow keys can be used to select a different character or move the cursor for the query text field. Pressing Escape will cancel the search and close the window, returning you to the previous foreground window.\n\nYou can configure the hotkey that Ununi uses and the colors by editing a configuration file (not there by default) in . After changing the configuration you must restart Ununi. A sample configuration with notes is given below, it gives the defaults that would be used if the file does not exist. Any key/table can be left out and the default will be used.\n\n[ ] the modifier key; can be one of: alt, ctrl, shift, win = the virtual key code of the second key. See MSDN for details: https://msdn.microsoft.com/en-us/library/windows/desktop/dd375731(v=vs.85).aspx = the colors used to draw the interface [ ] color of text and the box around the query = [ , , ] color of the cursor and character selection box. This is given an alpha value of 0.8 = [ , , ] = [ , , ]\n\nUnuni uses the clipboard to get characters into applications. This includes sending them the Ctrl-V paste shortcut. It does not yet restore the clipboard contents, all though that can be useful if you want to type the same character multiple times. Windows' Unicode support is not exactly fantastic so this seems to be the best way to go about it. Pressing Ctrl+Enter will send the character one UTF-16 codepoint at a time through WM_CHAR messages, which does work for some applications but is notably very janky.\n\nPressing Pause/Break while Ununi is open will kill the process. It can be restarted by rerunning the executable although it will again query if you want it to run on startup. Pressing NO does not yet actually change anything in this case. If you do not want it to show the message box or, passing the command line flag will disable it.\n\nThe query is feed directly into Tantivy, the query language is documented here.", "sentiment": 0.10166666666666666},
{"link_title": "Hurricane Irma is one of the strongest storms is history", "url": "https://www.nytimes.com/interactive/2017/09/09/us/hurricane-irma-records.html?smid=fb-nytimes&smtyp=cur", "text": "Here are all of the Atlantic hurricanes that reached Category 3 or stronger over the last 50 years. Hurricane Irma is highlighted in .\n\nNo other hurricane has matched the strength of Irma\u2019s winds so far east in the Atlantic.\n\nIrma was classified as a Category 5 hurricane for three consecutive days, longer than any other Atlantic hurricane.\n\nIrma has also broken records for accumulated cyclone energy, a measure that combines storm strength and duration. This metric is likely to increase during the weekend as the storm nears the United States.", "sentiment": -0.060416666666666674},
{"link_title": "The Original 1851 Reviews of Moby Dick", "url": "http://lithub.com/the-original-1851-reviews-of-moby-dick/", "text": "All that most maddens and torments; all that stirs up the lees of things; all truth with malice in it; all that cracks the sinews and cakes the brain; all the subtle demonisms of life and thought; all evil, to crazy Ahab, were visibly personified, and made practically assailable in Moby Dick.\n\n\u201cTo convey an adequate idea of a book of such various merits as that which the author of Typee and Omoo has here placed before the reading public, is impossible in the scope of a review. High philosophy, liberal feeling, abstruse metaphysics popularly phrased, soaring speculation, a style as many-coloured as the theme, yet always good, and often admirable; fertile fancy, ingenious construction, playful learning, and an unusual power of enchaining the interest, and rising to the verge of the sublime, without overpassing that narrow boundary which plunges the ambitious penman into the ridiculous; all these are possessed by Herman Melville, and exemplified in these volumes.\u201d\n\n\u201cThis is an ill-compounded mixture of romance and matter-of-fact. The idea of a connected and collected story has obviously visited and abandoned its writer again and again in the course of composition. The style of his tale is in places disfigured by mad (rather than bad) English; and its catastrophe is hastily, weakly, and obscurely managed \u2026 The result is, at all events, a most provoking book, \u2014 neither so utterly extravagant as to be entirely comfortable, nor so instructively complete as to take place among documents on the subject of the Great Fish, his capabilities, his home and his capture. Our author must be henceforth numbered in the company of the incorrigibles who occasionally tantalize us with indications of genius, while they constantly summon us to endure monstrosities, carelessnesses, and other such harassing manifestations of bad taste as daring or disordered ingenuity can devise\u2026\n\n We have little more to say in reprobation or in recommendation of this absurd book \u2026 Mr. Melville has to thank himself only if his horrors and his heroics are flung aside by the general reader, as so much trash belonging to the worst school of Bedlam literature \u2014 since he seems not so much unable to learn as disdainful of learning the craft of an artist.\u201d\n\n\u201cOf all the extraordinary books from the pen of Herman Melville this is out and out the most extraordinary. Who would have looked for philosophy in whales, or for poetry in blubber. Yet few books which professedly deal in metaphysics, or claim the parentage of the muses, contain as much true philosophy and as much genuine poetry as the tale of the Pequod\u2019s whaling expedition \u2026 To give anything like an outline of the narrative woven together from materials seemingly so uncouth, with a power of thought and force of diction suited to the huge dimensions of its subject, is wholly impossible \u2026 [Readers] must be prepared, however, to hear much on board that singularly-tenanted ship which grates upon civilized ears; some heathenish, and worse than heathenish talk is calculated to give even more serious offence. This feature of Herman Melville\u2019s new work we cannot but deeply regret. It is due to him to say that he has steered clear of much that was objectionable in some of his former tales; and it is all the greater pity, that he should have defaced his pages by occasional thrusts against revealed religion which add nothing to the interest of his story, and cannot but shock readers accustomed to a reverent treatment of whatever is associated with sacred subjects \u2026 [T]he artist has succeeded in investing objects apparently the most unattractive with an absorbing fascination. The flashes of truth, too, which sparkle on the surface of the foaming sea of thought through which the author pulls his readers in the wake of the whale-ship, \u2014 the profound reflections uttered by the actors in the wild watery chase in their own quaint forms of thought and speech, \u2014 and the graphic representations of human nature in the startling disguises under which it appears on the deck of the Pequod, \u2014 all these things combine to raise The Whale far beyond the level of an ordinary work of fiction. It is not a mere tale of adventures, but a whole philosophy of life, that it unfolds.\n\n\u201cNot only is there an immense amount of reliable information here before us; the dramatis personae \u2026 are all vivid sketches done in the author\u2019s best style. What they do, and how they look, is brought to one\u2019s perception with wondrous elaborateness of detail; and yet this minuteness does not spoil the broad outline of each. It is only when Mr. Melville puts words into the mouths of these living and moving beings, that his cunning fails him, and the illusion passes away \u2026 The rarely imagined character [Ahab] has been grievously spoiled, nay altogether ruined, by a vile overdaubing with a coat of book-learning and mysticism; there is no method in his madness; and we must needs pronounce the chief feature of the volume a perfect failure, and the work itself inartistic. There is nevertheless in it, as we have already hinted, abundant choice reading for those who can skip a page now and then, judiciously \u2026 Mr. Melville has crowded together in a few prefatory pages a large collection of brief and pithy extracts from authors innumerable, such as one might expect as headings for chapters. We do not like the innovation. It is having oil, mustard, vinegar, and pepper served up as a dish, in place of being scientifically administered sauce-wise.\u201d\n\n\u201cA new work by Herman Melville, entitled Moby Dick; or, the Whale, has just been issued by Harper and Brothers, which, in point of richness and variety of incident, originality of conception, and splendor of description, surpasses any of the former productions of this highly successful author \u2026 [T]he author has contrasted a romance, a tragedy, and a natural history, not without numerous gratuitous suggestions on psychology, ethics, and theology. Beneath the whole story, the subtle, imaginative reader may perhaps find a pregnant allegory, intended to illustrate the mystery of human life. Certain it is that the rapid, pointed hints which are often thrown out, with the keenness and velocity of a harpoon, penetrate deep into the heart of things, showing that the genius of the author for moral analysis is scarcely surpassed by his wizard power of description.\u201d\n\n\u201cThrice unlucky Herman Melville! \u2026 This is an odd book, professing to be a novel; wantonly eccentric; outrageously bombastic; in places charmingly and vividly descriptive. The author has read up laboriously to make a show of cetalogical learning \u2026 Herman Melville is wise in this sort of wisdom. He uses it as stuffing to fill out his skeleton story. Bad stuffing it makes, serving only to try the patience of his readers, and to tempt them to wish both him and his whales at the bottom of an unfathomable sea \u2026 Mr. Melville cannot do without savages so he makes half of his dramatis personae wild Indians, Malays, and other untamed humanities \u2026 What the author\u2019s original intention in spinning his preposterous yarn was, it is impossible to guess; evidently, when we compare the first and third volumes, it was never carried out \u2026 Having said so much that may be interpreted as a censure, it is right that we should add a word of praise where deserved. There are sketches of scenes at sea, of whaling adventures, storms, and ship-life, equal to any we have ever met with \u2026 Mr. Herman Melville has earned a deservedly high reputation for his performances in descriptive fiction. He has gathered his own materials, and travelled along fresh and untrodden literary paths, exhibiting powers of no common order, and great originality. The more careful, therefore, should he be to maintain the fame he so rapidly acquired, and not waste his strength on such purposeless and unequal doings as these rambling volumes about spermaceti whales.\u201d\n\n\u201cMr. Melville never writes naturally. His sentiment is forced, his wit is forced, and his enthusiasm is forced. And in his attempts to display to the utmost extent his powers of \u2018fine writing,\u2019 he has succeeded, we think, beyond his most sanguine expectations. The truth is, Mr. Melville has survived his reputation. If he had been contented with writing one or two books, he might have been famous, but his vanity has destroyed all his chances for immortality, or even of a good name with his own generation. For, in sober truth, Mr. Melville\u2019s vanity is immeasurable. He will either be first among the book-making tribe, or he will be nowhere. He will centre all attention upon himself, or he will abandon the field of literature at once. From this morbid self-esteem, coupled with a most unbounded love of notoriety, spring all Mr. Melville\u2019s efforts, all his rhetorical contortions, all his declamatory abuse of society, all his inflated sentiment, and all his insinuating licentiousness \u2026 We have no intention of quoting any passages just now from Moby Dick. The London journals, we understand, \u2018have bestowed upon the work many flattering notices,\u2019 and we should be loth to combat such high authority. But if there are any of our readers who wish to find examples of bad rhetoric, involved syntax, stilted sentiment and incoherent English, we will take the liberty of recommending to them this precious volume of Mr. Melville\u2019s.\u201d", "sentiment": 0.06745999780809908},
{"link_title": "Nordic.js \u2013 P2P, asynchronous JS, event-driven DBs, global connectivity", "url": "http://nordicjs.com/live", "text": "", "sentiment": 0.0},
{"link_title": "10 Best Apps to Manage Your Bar in 2017", "url": "https://www.bizimply.com/blog/10-apps-to-manage-your-bar-in-2017/", "text": "Seeing as it\u2019s 2017, we\u2019ve decided to update our list of apps to help you manage your bar and restaurant. There are always new apps entering the market, but we could be here all day if we went through all of the ones that we love. So we have chosen a nice mix of 10 which we think could be very beneficial to ensure the smooth running of your business.\n\nSay goodbye to chaos and confusion, Nowait allows you to manage reservations through the cloud. It gets rid of the dreaded queue that customers hate, by giving them real-time seating availability. Nowait also has a custom floormap to keep your staff up-to-date on table and waitlist status to help manage seating and server rotation.\n\nHere\u2019s what they say about it: \n\n Whether you own a mom-and-pop spot or a national chain, we have the tools and experience to help you run your business more efficiently and treat your guests to superior service. More than a technology solution, Nowait is a company of hospitality experts ready to partner with you.\n\nFounded in 2014, Bev Spot has an online tool that helps bar managers keep track of their inventory and spending. Learn how to manage inventory, ordering and invoicing to make your bar more profitable on this user-friendly platform.\n\nHere\u2019s what they say about it:\n\n Manage your bar online and access all of your bar\u2019s data in real time. Check out their great blog too, it is literally jam-packed with everything you need to run a successful bar.\n\nRockbot is a music platform which allows you to control the music that plays in your restaurant or bar. It allows you to engage with your guests by giving them the opportunity to pick what songs they want you to play by looking through your pre-approved library. It also lets you schedule your music for different times of the day to set the mood.\n\nHere\u2019s what they say about it: \n\n With over 14 million songs licensed for your business, Rockbot plays more of the artists that achieve your vibe. Pick from hundreds of stations designed for your business type or have our Curators perfect your sound.\n\nIt can be a struggle to keep on top of managing all of your social media accounts. That\u2019s where Hootsuite came up with the idea to have one platform where you can manage all of them in one place. It allows you to schedule posts and get the rest of your team involved by assigning them tasks.\n\nHere\u2019s what they say about it:\n\n Take your business to the next level on social media. If you\u2019re looking for a better, more streamlined way to manage all your social media accounts, you\u2019ve found it.\n\nNot only do Belly have a great name, they also have a great app. Everyone knows the struggle of holding onto loyalty cards. They either get lost or end up in the bin by accident along with receipts and general rubbish. That\u2019s where Belly comes in to save the day! The app allows your customers to store loyalty cards on their phone so no more wasting money on printing out cards!\n\nHere\u2019s what they say about it: \n\n Create and strengthen customer relationships with the world\u2019s best loyalty platform.\n\nNo one enjoys keeping track of inventory at the end of the day. It takes up so much time to measure your spirits and log them in, which is where Partender came up with the idea for their great app. All you have to do is tap where the liquor level is and then swipe onto the next bottle, so simple and easy to use!\n\nHere\u2019s what they say about it:\n\n ParTender allows you to build a virtual representation of your bar on your smartphone. Instead of manually writing down your inventory using cumbersome scales or expensive hardware, simply tap where the liquor level is in the bottle and swipe to the next bottle on your shelf. All your bar\u2019s inventory information is stored on the cloud. Just log in to see beautiful graphs of your bar\u2019s trends.\n\nNo one wants bad reviews to affect their business\u2019s reputation. That\u2019s where Review Trackers can help you stay on top of customer reviews, good or bad (hopefully mostly good). Their online tool alerts you of new customer reviews and feedback on one dashboard, making it very simple and easy to manage.\n\nHere\u2019s what they say about it:\n\n ReviewTrackers, an enterprise-level review monitoring and reputation management software platform, provides the technology and data that businesses need to manage, respond to, analyze, and collect online reviews and customer feedback across all major review sites.\n\nMixology\u2122 is great for those cocktail specials or to quickly try something new for your customers. They also have another app \u2018Mixologist\u2019 which is the same thing but with no advertisements and allows you to create your own recipes. Pretty cool, right?\n\nHere\u2019s what they say about it:\n\n Mixology\u2122 is the ultimate drink & cocktail recipe app. Browse and search through nearly 8,000 drink recipes and over 1,000 ingredients. Metric and Imperial (US) units can be chosen for all cocktail recipes with the click of one button on the \u201cMore\u201d tab.\n\nUncorkd gives you the opportunity to interact with your customers by giving them insights into your wine, cocktail, beer and liquor menu. This means that your customers can feel comfortable ordering drinks rather than being embarrassed by not knowing what certain drinks are (we\u2019ve all been there!).\n\nHere\u2019s what they say about it: \n\n Our company was created with a vision to make wine and alcoholic beverages accessible and understandable by consumers, removing the intimidation or pretentiousness often associated with wine, spirits, beer and cocktails. We accomplish this by providing innovative technology tools to restaurants to help them manage their programs and present drink information to customers that improves the dining experience.\n\nOk so I couldn\u2019t resist but in 2017 we can help you take control of your bar\u2019s labour costs and improve daily communication with your teams in multiple locations.\n\nBizimply allows you to quickly schedule your team across multiple locations, capture day to day information from each location, monitor attendance via our iPad app and prepare payroll in minutes, not hours. Simply sign-up on bizimply.com for your free trial today. Whatever you decide to do with your business, any one of these apps will make a difference.", "sentiment": 0.27087201906280856},
{"link_title": "Hilbert Curves", "url": "http://www.datagenetics.com/blog/march22013/index.html", "text": "", "sentiment": 0.0},
{"link_title": "The Bad Science Behind Campus Response to Sexual Assault", "url": "https://www.theatlantic.com/education/archive/2017/09/the-bad-science-behind-campus-response-to-sexual-assault/539211/?single_page=true", "text": "This is the second story in a three-part series examining how the rules governing sexual-assault adjudication have changed in recent years, and why some of those changes are problematic. Read the first installment here. As debate has begun over whether the rules governing sexual-assault adjudication have gone too far, one subject has received almost no attention, although it has become central to the way that many schools and many activists view sexual assault. In the last few years, the federal government has required that all institutions of higher education train staff on the effects of \u201cneurobiological change\u201d in victims of sexual assault, so that officials are able to conduct \u201ctrauma-informed\u201d investigations and adjudications. In meeting this federal demand, some schools have come to rely on the work of a small band of self-styled experts in the neurobiology of trauma who claim that sexual violations provoke a disabling, multifaceted physiological response. Being assaulted is traumatic, and no one should expect those who have been assaulted to have perfect recall or behave perfectly rationally, but this argument goes much further. It generally goes like this: People facing sexual assault become terrified, triggering a potent cascade of neurotransmitters and stress hormones.This chemical flood impairs the prefrontal cortex of the brain, impeding victims\u2019 capacity for rational thought, and interferes with their memory. They may have significant trouble recalling their assault or describing it coherently or chronologically. The fear of imminent death may further elicit an extended catatonic state known as \u201ctonic immobility,\u201d rendering them powerless to speak or move\u2014they feel \u201cfrozen.\u201d\n\nAs a result, those adjudicating sexual-assault allegations are told, the absence of verbal or physical resistance, the inability to recall crucial parts of an alleged assault, a changing story\u2014none of these factors should raise questions or doubt about a claim. Indeed, all of these behaviors can be considered evidence that an assault occurred. \u201cI don\u2019t think I\u2019ve seen a complaint in the past year that didn\u2019t use the word frozen somewhere.\u201d Rebecca Campbell, a professor of psychology at Michigan State University, has taught the science of trauma to law-enforcement officials and Title IX administrators. In 2015, she gave a keynote talk at the Association of Title IX Administrators\u2019 annual conference, about how the neurobiology of trauma could be used in their investigations. A highly influential presentation she gave in 2012 at the National Institute of Justice (NIJ), \u201cThe Neurobiology of Sexual Assault,\u201d has been used as part of some schools\u2019 Title IX\u2013personnel training. In her 2012 talk, Campbell acknowledged that she is not a neuroscientist, but rather is translating others\u2019 work. (Campbell\u2019s own scholarly work has involved community-based research into how contact with the legal and medical systems affect assault victims\u2019 psychological and physical health, and research into the use of rape kits.) She asserted that the damaged memory of a victim can be likened to \u201ctiny Post-it notes\u201d scattered randomly across \u201cthe world\u2019s messiest desk.\u201d For a sexual-assault victim to reconstruct what happened requires a sympathetic questioner who will give the victim the time and space to reassemble the Post-its in a coherent order. She assured her audience that the story that emerges will be a true account of the crime: \u201cWhat we know from the research is that the laying down of that memory is accurate and the recall of it is accurate.\u201d She briefly recognized that victims who consumed alcohol may have serious memory problems as a result\u2014\u201ctheir Post-it notes are just blank.\u201d Tonic immobility, she said, is a \u201cmammalian response that is in all of us,\u201d likely affecting close to 50 percent of sexual-assault victims. \u201cTheir body freezes on them,\u201d she said, and not just for a moment or two. The victims go into an extended state in which they can\u2019t speak or move, and hence cannot fend off an assailant.\n\nAs of 2014, Harvard Law\u2019s Title IX training for its disciplinary board included Campbell\u2019s PowerPoint slides. Janet Halley, a professor at Harvard Law School, wrote of the intended effect of the training on recipients: \u201cIt is 100% aimed to convince them to believe complainants, precisely when they seem unreliable and incoherent.\u201d All of these concepts are presented in information distributed to students on many campuses. The University of Michigan\u2019s Sexual Assault Prevention and Awareness Center has a webpage for students on the \u201cNeurobiology of Trauma\u201d based largely on Campbell\u2019s lecture. It explains that as a result of the \u201chormone soup\u201d provoked by an assault, \u201cthe survivor cannot move and is rendered immobile by the traumatic event\u201d and that \u201csurvivors may have trouble accurately remembering the assault.\u201d Bowdoin College\u2019s Title IX webpage on \u201cThe Neurobiology of Sexual Assault,\u201d also based on Campbell\u2019s lecture, tells students that \u201cthe flood of hormones can even, and often does, result in a complete shutdown of bodily function, a state referred to as \u2018tonic immobility,\u2019 but better described as paralysis\u201d and that victims \u201calso may exhibit fragmented memory recall due to the disorganized encoding that occurred during the incident.\u201d This information sends the message to young people that they are biologically programmed to become helpless during unwanted sexual encounters and to suffer mental impairment afterward. And it may inadvertently encourage them to view consensual late-night, alcohol-fueled encounters that might produce disjointed memories and some regret as something more sinister.\n\nJustin Dillon, a Washington, D.C., attorney who defends students across the country accused of Title IX violations, told me that a couple of years ago he had barely heard of this condition, but that its terminology has swiftly made its way into campus adjudications: \u201cI don\u2019t think I\u2019ve seen a complaint in the past year that didn\u2019t use the word frozen somewhere.\u201d Trauma, he says, is used to explain away all inconsistencies in some complainants\u2019 accounts that would otherwise seem to contradict their having been assaulted. Schools do not make public the training materials of those who investigate and adjudicate sexual assault. But through lawsuits, Dillon has obtained some examples, and he says the assertions of the \u201cneurobiology of trauma\u201d that infuse these materials make it almost impossible for the accused to mount a defense. When such assumptions are held by those sitting in judgment, he says, \u201cHow do you prove your innocence?\u201d \u201cThe brain is not a videotape machine. All of our memories are re-constructed.\u201d Various initiatives have spread ideas about this syndrome out into the collegiate world. In 2013, funding from the federal government established the National Center for Campus Public Safety, an educational resource for campus administrators that offers a \u201cTrauma-Informed Sexual Assault Investigation and Adjudication\u201d curriculum, and purveys the ideas popularized by Campbell and others. Last year, the University of Texas at Austin released a state-funded report, \u201cThe Blueprint for Campus Police: Responding to Sexual Assault,\u201d with the hope of it becoming a national model. It codified \u201cvictim-centered and trauma-informed\u201d investigations, asserting: \u201cTrauma victims often omit, exaggerate, or make up information when trying to make sense of what happened to them or to fill gaps in memory.\u201d And \u201cdue to the neurobiology of trauma, victims may suffer from a rape-induced paralysis called tonic immobility.\u201d In 2015, Illinois passed the Preventing Sexual Violence in Higher Education Act, which demanded that campus personnel receive \u201ctraining centered on the neurobiological impact of trauma\u201d; other states are considering similar legislation.\n\nI talked with Richard McNally, a psychology professor at Harvard and one of the country\u2019s leading experts on the effects of trauma on memory, about the assertions Campbell made in her presentation. He first said that because assaults do not occur within the laboratory, \u201cthere is no direct evidence\u201d of any precise or particular cascade of physiological effects during one, \u201cnor is there going to be.\u201d But there is plenty of evidence about how highly stressful experiences affect memory, and much of it directly contradicts Campbell. In his 2003 book, Remembering Trauma, McNally writes, \u201cNeuroscience research does not support [the] claim that high levels of stress hormones impair memory for traumatic experience.\u201d In fact, it\u2019s almost the opposite: \u201cExtreme stress enhances memory for the central aspects of an overwhelming emotional experience.\u201d There is likely an evolutionary reason for that, McNally said: \u201cIt makes sense for natural selection to favor the memory of trauma. If you remember life threatening situations, you\u2019re more likely to avoid them.\u201d Notably, survivors of recent horrific events\u2014the Aurora movie-theater massacre, the San Bernardino terror attack, the Orlando-nightclub mass murder\u2014have at trial or in interviews given narrative accounts of their ordeals that are chronological, coherent, detailed, and lucid. (In the years since McNally\u2019s book was published, some neuroscientific evaluations of military personnel have indicated that, in conditions of the most extreme stress, these hormones might prevent certain memories from being retained, causing gaps or errors in a person\u2019s recollection. But these findings are different from the assertion that traumatic memories are stored in infallible yet \u201cfragmented\u201d condition.)\n\nCampbell\u2019s claim that a sexual-assault victim\u2019s memory consists of completely accurate but disorganized fragments contradicts fundamental scientific knowledge of the nature of memory, McNally told me. \u201cThe brain is not a videotape machine,\u201d he said. \u201cAll of our memories are re-constructed. All of our memories are incomplete in that sense.\u201d Each time we recall an event, it is being reassembled, and sometimes changed by the very process of recall. All the experts I spoke with expressed concern about the malleability of memory, especially when people are being encouraged to clarify murky experiences. A 2015 study by the education insurance group United Educators, examining 305 claims of sexual assault at 104 schools, found that about 40 percent of students delayed reporting an assault (there are many valid reasons why they might do so, including fear of stigma). The average delay was 11 months, and \u201cin most cases, the victim labeled the incident a sexual assault only after talking with friends or attending prevention training.\u201d Elizabeth Loftus, a professor of psychology and social behavior at UC Irvine, has done pathbreaking work on memory manipulation. When I described to her what\u2019s now being taught to administrators and students, she said it sounded disturbingly like a return of \u201crecovered memory\u201d theory, with some neurobiology thrown in \u201cto give luster\u201d to the argument. During the recovered-memory scare of the 1980s and \u201990s, thousands of people were convinced by their therapist\u2014and by best-selling books on the subject\u2014that their problems were caused by repressed memories of childhood sexual abuse, often at the hands of their father. The theory was that the mind buried what had happened because it was so awful, but that the \u201cforgotten\u201d events nonetheless caused a lifetime of pain. Guided by therapists, victims were able to recover the memory and begin healing.\n\nAs it turned out, though, many therapists were implanting false memories in vulnerable people, resulting in baseless accusations that tore families apart. The frenzy eventually burned out when researchers, including McNally, discredited the underlying assumptions. \u201cThe notion that the mind protects itself by repressing or dissociating memories of trauma,\u201d he writes in Remembering Trauma, \u201cis a piece of psychiatric folklore devoid of convincing empirical support.\u201d Loftus also talked about the effects of drinking on memory. She said that with alcohol-induced memory fragmentation, attempts to reconstruct events are \u201cvery vulnerable to post-event suggestion.\u201d This can include someone, especially an authority figure, labeling a consensual act as rape. She said it\u2019s then easy for exaggerated, or even entirely false, memories to be created, ones that feel completely real. \u201cThis is extremely worrisome,\u201d she told me. \u201cThe universities are under enormous pressure to do something about sexual assault, and they sometimes fill these offices with people whose bias and agendas lead them to create victimhood.\u201d Tonic immobility has at least some scientific pedigree. The \u201cplaying dead\u201d mechanism of prey animals, particularly those that find themselves literally in the jaws of death, is well documented and likely adaptive; a predator might relax its grip if it believes the prey is dead, increasing the chance of escape. But research on the subject has focused primarily on small animals, notably poultry. (Factory-farm conditions can be frightening to chickens, and episodes of tonic immobility are thought to have deleterious effects on poultry production.)\n\nIn the mid-2000s, a few researchers began studying whether tonic immobility occurs in humans. A handful of surveys have asked people who said they had experienced a trauma whether they remembered, for example, feeling paralyzed or unable to call out. Some have said yes, and perhaps they did, but a few researchers have noted potential problems in these retrospective questionnaires, including leading or ambiguous language that might prompt certain responses. In the lab, some researchers have tried to find evidence of the response in people by measuring how much their body moved when they listened to scripts recounting real-life traumas they\u2019d faced, or by confronting them with images of a gun. But because it is ethically unacceptable to create a true sense of life-threatening terror, these experiments are inherently limited, and their findings are far from conclusive. To be sure, some people do freeze briefly in terrifying circumstances. But in her NIJ presentation, Campbell used the terms freeze and tonic immobility synonymously. These are two distinct phenomena, well documented in animals. Researchers describe a \u201cfreeze\u201d as a momentary state of high alert that occurs before deciding whether and how to act. Charles A. Morgan III, a forensic psychiatrist at the University of New Haven who studies stress response in military personnel, says there\u2019s ample evidence of this response on the part of people. But, he told me, \u201cit\u2019s not people acting like they\u2019re zombies\u201d; they generally return swiftly to responsiveness. Liz Phelps, a professor of psychology and neural science at NYU, concurs. She cited the Atlanta Olympic bombing as an example. A video of the event shows that when the bomb went off, many in the crowd froze. But it was \u201ca momentary freeze before people started running. It\u2019s such a subtle response in humans that it\u2019s very hard to study,\u201d she says.\n\nTonic immobility might occur in people, but science hasn\u2019t definitively shown this yet. Even if it does occur, we cannot say with any certainty that it affects nearly 50 percent of sexual-assault victims, as Campbell claims is likely in her presentation. Tonic immobility\u2019s frequent precursors in prey animals\u2014physical restraint combined with an imminent threat to life\u2014do describe some instances of rape. But the conditions that lead to many sexual-assault complaints on college campuses\u2014alcohol combined with miscommunication\u2014do not fit this template. I spoke with Campbell about all this last fall, and in our conversation, she said the goal of her work on neurobiology was to give law-enforcement officers a more nuanced understanding of how a sexual-trauma victim might behave. Its generalized use as a guide for campus investigations and adjudications\u2014and particularly to support the idea that no matter how a complainant behaves, she is almost certainly telling the truth\u2014was unintended, she said, and \u201cwould be an overreach.\u201d But whatever its intent, her National Institute of Justice talk on fragmented memory and tonic immobility is unsupported by prevailing scientific research and findings. Campbell acknowledged to me that her talk incorrectly conflated freeze and tonic immobility. \u201cThat is something in later presentations I\u2019ve learned to correct,\u201d she said. While her lecture conveys certainty and assurance, she told me that we don\u2019t know \u201cas much as we\u2019d like to\u201d about tonic immobility and that there are \u201cvery valid questions\u201d as to whether it occurs in humans.\n\nLawyers for accused students are starting to challenge the way such ideas about trauma may bias campus adjudications. In July, for example, a male student at the University of Oregon, who had been suspended after having been found to have sexually assaulted a female student with whom he\u2019d had a previous sexual relationship, filed a civil suit against the university and three university officials in federal court. According to the complaint, the female student provided inconsistent accounts of the alleged assault\u2014the male student says there was no sexual contact that night\u2014and both electronic evidence and witness statements contradicted her timeline of events. But, the complaint says, the Title IX officer investigating the case concluded that the female student\u2019s \u201cstories shifted because she suffered from trauma-induced memory problems.\u201d The male student appealed his suspension and submitted an expert report from a psychology professor arguing, the complaint says, that the Title IX officer\u2019s assertions about trauma and memory were \u201cnot scientifically supportable.\u201d (The suspension had been upheld by the school, but was later overturned by a state judge, who found that the school had conducted an unfair process, in part because the male student had not been allowed to rebut the Title IX officer\u2019s claims about memory in the original proceeding.) But one-off legal challenges are unlikely to eliminate the erroneous ideas being promulgated on many campuses. The spread of an inaccurate science of trauma is an object lesson in how good intentions can overtake critical thinking, to potentially harmful effect. Many rapes go unreported because the process of reporting them and seeking justice can be miserable for the victim. That the desire to lessen this misery has guided many reforms to campus adjudication is understandable and appropriate\u2014to a point. Campbell\u2019s 2012 lecture sought to persuade police investigators to give victims, in the immediate aftermath of a sexual assault, some space to collect themselves, and to conduct a first interview in a way that\u2019s neutral rather than hostile\u2014laudable, common-sense goals. But common-sense goals, when dressed up by policy makers and victims\u2019 advocates in the inaccurate science now widespread on campus, can be (and have been) easily expanded to serve the idea that virtually every action and behavior that might cast legitimate doubt on an assault should be routinely discounted\u2014and that no matter what precedes or follows an accusation of assault, the accused is always guilty. The result is not only a system in which some men are wrongly accused and wrongly punished. It is a system vulnerable to substantial backlash. University professors and administrators should understand this. And they, of all people, should identify and call out junk science.", "sentiment": 0.0708257242290856},
{"link_title": "YETI How it works", "url": "https://www.youtube.com/watch?v=zVPLX6LY5HM", "text": "", "sentiment": 0.0},
{"link_title": "Stone Soup is the great sales and marketing manual ever written", "url": "http://rexstjohn.com/stone-soup-greatest-business-book-ever-written/", "text": "I recently picked up a copy of Marcia Browns\u2019 1947 \u201cStone Soup.\u201d As an elementary school student, I didn\u2019t really appreciate Stone Soup. As an adult, I have to say that that Stone Soup is a shockingly accurate reproduction of the process truly great entrepreneurs and salespeople use to win.\n\nIn fact, I would go so far as to say that Stone Soup may be the greatest sales manual and business book ever written!\n\nSo lets get into it.\n\n\n\nThe story opens with three French-looking soldiers who have not eaten in several days, they are starving to death. For the sake of this article, lets think of these soldiers as entrepreneurs, sales people, technical evangelists or business development people.\n\nSeeing the soldiers coming from a long way off, the local villagers immediately recoil in terror. From past experience, \u201cSoldiers\u201d (ahem salespeople / recruiters) coming to town always means bad news.\n\nHaving had their opinions about oncoming Soldiers (ahem salespeople / recruiters) tainted from past experience, the villagers have no interest in making life easy for the starving wanderers.\n\nSo the peasants do exactly what your potential customers and investors are going to do if you telegraph that you are going to try to ask for money / sales. The villagers immediately go and hide every single morsel they have (and they have abundant morsels to hide).\n\nWhen the soldiers get into town, they are met by groaning peasants pretending to be starving, grasping their stomaches acting as though they have not eaten in months.\n\nThis behavior should be familiar to you if you have ever\u2026\n\nOh so many reasons! You will hear so many reasons from your investors and customers, believe me.\n\nSo the soldiers, especially the smug looking guy in the center, come up with a crafty plan.\n\nKnowing they have nothing to offer, can\u2019t use force, asking nicely is exhausted and are walking into the situation facing significant resistance which they can\u2019t overcome (as every salesperson, recruiter and entrepreneur does, every day of their lives)\u2026the soldiers decide to refactor their request.\n\nThis moment is the key to the entire story: The soldiers decide to restructure their \u201cselfish need\u201d as a \u201ccollective good,\u201d enlist their customers as partners, remove themselves from the sale and turn the direct sale into an indirect sale. What is about to follow is a sequence of events which cement the fact that this is the greatest manual on sales and business development ever committed to paper.\n\nSo the soldiers come back from their huddle. They immediately acknowledge that the peasants of the village don\u2019t have any food: \u201cWe realize that you don\u2019t have any food, so instead we are just going to have to make some stone soup!\u201d This is classic sales best practice: Never refute, never argue\u2026Acknowledge the customer\u2019s complaint and agree with them\u2026.but keep advancing the sale!\n\nInstead of getting stuck arguing about \u201cprice / availability of food\u201d with the farmers (and accusing them of being liars for pretending not to have any food), the soldiers ask for something else: An iron pot to make stone soup with. They keep the sale moving!\n\nAt this point in the book, I came to the conclusion that whoever wrote it must be one of the greatest sales geniuses of all time. There is simply no alternative explanation. If you have ever studied sales and marketing, you will instantly recognize which this request is:\n\nAnd with this request, the soldiers begin a \u201cCampaign of Yes\u201d with their customers\u2026\n\nAfter the farmers provide an empty Iron Pot, the soldiers proceed to ask the villagers for water to put in the pot, logs to burn and three stones (for soup!). All of these requests are \u201cFree and Easy\u201d to provide. With each request, the villagers become more and more involved in the process of creating the stone soup.\n\nThe ultimate persuasion pattern emerges: Yes to getting a pot, Yes to getting fire and kindling, Yes to getting water, Yes to getting three stones. After fulfilling these small requests, the farmers have been transformed from being passive actors into active participants in the sale\u2026the farmers are beginning to OWN the result of the sale by partnering with the soldiers to deliver it.\n\nAnd now the kill shot.\n\nIf you have ever expect to sell or attempted to persuade anyone to do anything\u2026you should hang a print-out of this image on your wall and look at it every single day. This sentence and phrasing is so good I could write a book about it. .\n\n\u201cAny soup needs salt and pepper\u2026\u201d said the soldier, masterfully. I will try my best to explain why this phrasing is so incredibly good:\n\nBy complying with this request for Salt and Pepper, the farmers show the soldiers that they are now in a mood to comply with a \u201creal\u201d request\u2026\n\nBAMN! This is \u201cThe Mythical Close\u201d itself. Seeing the farmers willing to comply for a request very close to food, they next observe that carrots would be an even better addition to the soup\u2026\n\nOnce again, the soldiers phrase the request indirectly. The farmers, now enlisted as partners in the sale immediately comply and go and fetch the carrots, not realizing that they have broken their cover (we have no food) of \u201cReasons.\u201d Soon, the farmers are agreeing to subsequent requests for more food and rushing back and forth to their larders fetching beef, barley, oats, salt and pepper, cabbage\u2026\n\nNotice that the soldier is not asking for carrots to feed himself, he is observing that carrots would make the soup better. By removing himself from the sale, the soldier puts the focus on improving the solution (The Stone Soup) and not lining his own pockets (something that produces resistance in customers).\n\nBefore you know it, the stone soup event has broken into a full on music festival / banquet. What started as a magic trick has become a sales bonanza for all. Everyone partakes in the event and eats to their fill \u2013 All because of a story about making soup out of stones. Everyone wins.\n\nBy restructuring their desire for food into a collective good, enlisting the help of the customers and removing themselves from the sale, the soldiers managed to achieve a result by offering nothing but an interesting story and a well-crafted series of statements into a feast.\n\nMy final closing thought is on Elon Musk, the greatest practitioner of the sales arts on earth. What does Elon Musk do? Why are his businesses so successful? Its because everything he does is positioned to help all of humanity.\n\nTesla cars to save the environment, SpaceX to save humanity by offering travel to Mars\u2026by focusing on selling the collective good, Elon Musk has amassed an infinite loyal customer fan base and enlists his customers to help him achieve a greater good: Saving humanity (while also becoming immensely rich himself).", "sentiment": 0.2624610260770974},
{"link_title": "Hurricane Irma is literally sucking the water away from shorelines", "url": "https://www.washingtonpost.com/news/capital-weather-gang/wp/2017/09/09/hurricane-irma-is-literally-sucking-the-water-away-from-shorelines/", "text": "As a meteorologist, there are things you learn in textbooks that you may never see in person. You know they happen theoretically, but the chances of seeing the most extraordinary weather phenomena are slim to none.\n\nThis is one of those things \u2014 a hurricane strong enough to change the shape of an ocean.\n\nTwitter user @Kaydi_K shared this video Saturday afternoon, and I knew right away that even though it looked as though it couldn\u2019t be possible, it was absolutely legit.\n\n\u201cI am in disbelief right now\u2026\u201d she wrote. \u201cThis is Long Island, Bahamas and the ocean water is missing!!!\u201d\n\nBasically, Hurricane Irma is so strong and its pressure is so low, it\u2019s sucking water from its surroundings into the core of the storm.\n\n[\u2018Never seen anything like it\u2019: Governor issues new warning as Florida sees first signs of Hurricane Irma\u2019s winds and rain]\n\nThe wind on Long Island in the Bahamas is from the southeast to the northwest on Saturday. On the northwest side of the island, it would be blowing the water away from the shoreline.\n\nIt also may be experiencing the effects of what I call the hurricane \u201cbulge.\u201d In the center of the storm, where there is extreme low pressure, water is drawn upward. Low pressure is basically a sucking mechanism \u2014 it sucks the air into it, and when it\u2019s really low, it can change the shape of the surface of the ocean. As the storm draws water toward the center, it gets pulled away from the surroundings.\n\nIn any case, this isn\u2019t the sign of a tsunami. The water will return to Long Island, and it probably won\u2019t rush back with any great force. It will probably be back by Sunday afternoon.", "sentiment": 0.09230248917748918},
{"link_title": "Essential A11 Phone Teardown", "url": "https://www.ifixit.com/Teardown/Essential+Phone+Teardown/96764", "text": "New Yorkers stand up for what they believe in. And we're asking you to stand up for repair.\n\nThis year, New York could be the first state in the nation to pass the Fair Repair Act, A8192 and S618. We have a chance to guarantee our right to repair electronic equipment\u2014like smartphones, computers, and even farm equipment. We have a chance to protect local repair jobs\u2014the corner mom-and-pop repair shops that keep getting squeezed out by manufacturers.\n\nIt\u2019s not going to be easy. Manufacturers are standing in the way. When your stuff breaks, they want to be the only people allowed to fix it. So far, they\u2019ve managed to stop Fair Repair legislation before your representatives get a chance to vote on it. We\u2019ve got to be louder than their lobbyists.\n\nThe Fair Repair Act, known as A8192 and S618, requires manufacturers to provide owners and independent repair businesses with fair access to service information, security updates, and replacement parts. Tell your state representative to support S618. Tell them you believe repair should be fair, affordable, and accessible. Stand up for your right to repair.", "sentiment": 0.3439055735930736},
{"link_title": "Breaking the x86 Instruction Set: how to find undocumented x86 instructions", "url": "https://www.youtube.com/watch?v=KrksBdWcZgQ", "text": "", "sentiment": 0.0},
{"link_title": "Equifax's tool to check if your data was released in the hack may be inaccurate", "url": "https://techcrunch.com/2017/09/08/psa-no-matter-what-you-write-equifax-may-tell-you-youve-been-impacted-by-the-hack/", "text": "Those hoping to find out if their Social Security number and other identifying info was stolen, along with a potential 143 million other American\u2019s data won\u2019t find answers from Equifax.\n\nIn what is an unconscionable move by the credit report company, the checker site, hosted by Equifax product TrustID, seems to be telling people at random they may have been affected by the data breach.\n\nI started noticing most people who\u2019d tested out the site in my Facebook and Twitter feeds had been given the message that they may have been part of the millions who\u2019s information was affected. It stood to reason that was likely, given the scope of the leak would affect possibly one out every two people I know in the country.\n\nHowever, I then decided to try it out for myself. First, I entered my real information\u2026and received the bad news.\n\n\u201cBased on the information provided, we believe that your personal information may have been impacted by this incident,\u201d the site said.\n\nI was then encouraged on the next line to continue my enrollment in TrustedID Premier. I was not aware I was enrolling in anything simply by giving my information. I had been instructed to add my last name and the last six digits of my Social Security number only to find out if I\u2019d been impacted.\n\nSo then I decided to test the system with a different last name and six random numbers. I used the more popular English spelling of my last name for this purpose, entering \u201cBurr\u201d instead of \u201cBuhr\u201d and entered six random numbers I don\u2019t even remember now.\n\nSure enough, this made-up person had also been impacted. I tried it over and over again and got the same message. The only time I did not get the message I\u2019d been impacted was when I entered \u201cElmo\u201d as the last name and \u201c123456\u201d as my Social Security number.\n\nSome of my colleagues also tried to fool the system and came up with different outcomes. Sometimes, after entering a made-up name, the site said they had been impacted. A few times it said they were not.\n\nOthers have tweeted they received different answers after entering the same information.\n\nThe assignment seems random. But, nevertheless, they were still asked to continue enrolling in TrustID.\n\nWhat this means is not only are none of the last names tied to your Social Security number, but there\u2019s no way to tell if you were really impacted.\n\nIt\u2019s clear Equifax\u2019s goal isn\u2019t to protect the consumer or bring them vital information. It\u2019s to get you to sign up for its revenue-generating product TrustID.\n\nEarlier it was revealed executives had sold stock in the company before going public with the leak. We also found TrustID\u2019s Terms of Service to be disturbing. The wording is such that anyone signing up for the product is barred from suing the company after.\n\nNew York attorney general Eric Schneiderman has hammered Equifax for using language meant to discourage arbitration and is asking Equifax for answers over the data breach. The company has stated since it would not bar consumers from joining breach-related lawsuits.\n\nNo doubt, those who sold company stock before publicly admitting the issues are going to face some legal trouble of their own as well.\n\nI\u2019ve since reached out to the company but so far for this story and inquiries I\u2019ve made in the last two days, I have yet to hear back.\n\nThese actions, and many others, are disgraceful, especially for a company of this size and responsibility and I truly hope Equifax feels the heat they are under for mishandling what is the largest data breach in the history of the U.S.", "sentiment": 0.011578282828282832},
{"link_title": "Windows networking kubernetes on par with linux", "url": "http://blog.kubernetes.io/2017/09/windows-networking-at-parity-with-linux.html", "text": "\"So what?\", you may ask. There are multiple application and infrastructure-related reasons why these platform improvements make a substantial difference in the lives of developers and operations teams wanting to run Kubernetes. Read on to learn more!\n\nThese improvements enable tightly-coupled communication between multiple Windows Server containers (without Hyper-V isolation) within a single \" Pod \". Think of Pods as the scheduling unit for the Kubernetes cluster, inside of which, one or more application containers are co-located and able to share storage and networking resources. All containers within a Pod shared the same IP address and port range and are able to communicate with each other using localhost. This enables applications to easily leverage \"helper\" programs for tasks such as monitoring, configuration updates, log management, and proxies. Another way to think of a Pod is as a compute host with the app containers representing processes.\n\nWe also simplified the network topology on Windows nodes in a Kubernetes cluster by reducing the number of endpoints required per container (or more generally, per pod) to one. Previously, Windows containers (pods) running in a Kubernetes cluster required two endpoints - one for external (internet) communication and a second for intra-cluster communication between between other nodes or pods in the cluster. This was due to the fact that external communication from containers attached to a host network with local scope (i.e. not publicly routable) required a NAT operation which could only be provided through the Windows NAT (WinNAT) component on the host. Intra-cluster communication required containers to be attached to a separate network with \"global\" (cluster-level) scope through a second endpoint. Recent platform improvements now enable NAT''ing to occur directly on a container endpoint which is implemented with the Microsoft Virtual Filtering Platform (VFP) Hyper-V switch extension. Now, both external and intra-cluster traffic can flow through a single endpoint.\n\nKubernetes worker nodes rely on the kube-proxy to load-balance ingress network traffic to Service IPs between pods in a cluster. Previous versions of Windows implemented the Kube-proxy's load-balancing through a user-space proxy. We recently added support for \"Proxy mode: iptables\" which is implemented using VFP in the Windows kernel so that any IP traffic can be load-balanced more efficiently by the Windows OS kernel. Users can also configure an external load balancer by specifying the externalIP parameter in a service definition. In addition to the aforementioned improvements, we have also added platform support for the following:\n\nIn addition to the platform improvements for Windows, the team submitted code (PRs) for CNI, kubelet, and kube-proxy with the goal of mainlining Windows support into the Kubernetes v1.8 release. These PRs remove previous work-arounds required on Windows for items such as user-mode proxy for internal load balancing, appending additional DNS suffixes to each Kube-DNS request, and a separate container endpoint for external (internet) connectivity.\n\nThese new platform features and work on kubelet and kube-proxy align with the CNI network model used by Kubernetes on Linux and simplify the deployment of a K8s cluster without additional configuration or custom (Azure) resource templates. To this end, we completed work on CNI network and IPAM plugins to create/remove endpoints and manage IP addresses. The CNI plugin works through kubelet to target the Windows Host Networking Service (HNS) APIs to create an 'l2bridge' network (analogous to macvlan on Linux) which is enforced by the VFP switch extension.\n\nThe 'l2bridge' network driver re-writes the MAC address of container network traffic on ingress and egress to use the container host's MAC address. This obviates the need for multiple MAC addresses (one per container running on the host) to be \"learned\" by the upstream network switch port to which the container host is connected. This preserves memory space in physical switch TCAM tables and relies on the Hyper-V virtual switch to do MAC address translation in the host to forward traffic to the correct container. IP addresses are managed by a default, Windows IPAM plug-in which requires that POD CIDR IPs be taken from the container host's network IP space.\n\nSoon after RTM, we will also introduce these improvements into the Azure Container Service (ACS) so that Windows worker nodes and the containers hosted are first-class, Azure VNet citizens. An Azure IPAM plugin for Windows CNI will enable these endpoints to directly attach to Azure VNets with network policies for Windows containers enforced the same way as VMs.", "sentiment": 0.06806604248464715},
{"link_title": "Facebook Wins, Democracy Loses", "url": "https://www.nytimes.com/2017/09/08/opinion/facebook-wins-democracy-loses.html", "text": "The service is popular among advertisers for its efficiency, effectiveness and responsiveness. Facebook gives rich and instant feedback to advertisers, allowing them to quickly tailor ads to improve outcomes or customize messages even more. There is nothing mysterious or untoward about the system itself, as long as it\u2019s being used for commerce instead of politics. What\u2019s alarming is that Facebook executives don\u2019t seem to grasp, or appreciate, the difference.\n\nA core principle in political advertising is transparency \u2014 political ads are supposed to be easily visible to everyone, and everyone is supposed to understand that they are political ads, and where they come from. And it\u2019s expensive to run even one version of an ad in traditional outlets, let alone a dozen different versions. Moreover, in the case of federal campaigns in the United States, the 2002 McCain-Feingold campaign-finance act requires candidates to state they approve of an ad and thus take responsibility for its content.\n\nNone of that transparency matters to Facebook. Ads on the site meant for, say, 20- to 30-year-old home-owning Latino men in Northern Virginia would not be viewed by anyone else, and would run only briefly before vanishing. The potential for abuse is vast. An ad could falsely accuse a candidate of the worst malfeasance a day before Election Day, and the victim would have no way of even knowing it happened. Ads could stoke ethnic hatred and no one could prepare or respond before serious harm occurs.\n\nUnfortunately, the range of potential responses to this problem is limited. The First Amendment grants broad protections to publishers like Facebook. Diplomacy, even the harsh kind, has failed to dissuade Russia from meddling. And it\u2019s even less likely to under the current administration.\n\nDaniel Kreiss, a communication scholar at the University of North Carolina, proposes that sites such as Facebook, Twitter and YouTube maintain a repository of campaign ads so that regulators, scholars, journalists and the public can examine and expose them. But the companies have no impetus to concur and coordinate. And Congress is unlikely to reform a system that campaigns are just learning to master.\n\nFacebook has no incentive to change its ways. The money is too great. The issue is too nebulous to alienate more than a few Facebook users. The more that Facebook saturates our lives, families and communities, the harder it is to live without it.\n\nFacebook has pledged to install better filtering systems using artificial intelligence and machine-learning to flag accounts that are run by automated \u201cbots\u201d or violate the site\u2019s terms of service. But these are just new versions of the technologies that have caused the problem in the first place. And there would be no accountability beyond Facebook\u2019s word. The fact remains that in the arms race to keep propaganda flowing, human beings review troublesome accounts only long after the damage has been done.\n\nOur best hopes sit in Brussels and London. European regulators have been watching Facebook and Google for years. They have taken strong actions against both companies for violating European consumer data protection standards and business competition laws. The British government is investigating the role Facebook and its use of citizens\u2019 data played in the 2016 Brexit referendum and 2017 national elections.\n\nWe are in the midst of a worldwide, internet-based assault on democracy. Scholars at the Oxford Internet Institute have tracked armies of volunteers and bots as they move propaganda across Facebook and Twitter in efforts to undermine trust in democracy or to elect their preferred candidates in the Philippines, India, France, the Netherlands, Britain and elsewhere. We now know that agents in Russia are exploiting the powerful Facebook advertising system directly.\n\nIn the 21st-century social media information war, faith in democracy is the first casualty.", "sentiment": 0.0539860005585812},
{"link_title": "The Doctor Is In. Co-Pay? $40,000", "url": "https://mobile.nytimes.com/2017/06/03/business/economy/high-end-medical-care.html?referer=&pagewanted=all", "text": "SAN FRANCISCO \u2014 When John Battelle\u2019s teenage son broke his leg at a suburban soccer game, naturally the first call his parents made was to 911. The second was to Dr. Jordan Shlain, the concierge doctor here who treats Mr. Battelle and his family.\n\n\u201cThey\u2019re taking him to a local hospital,\u201d Mr. Battelle\u2019s wife, Michelle, told Dr. Shlain as the boy rode in an ambulance to a nearby emergency room in Marin County. \u201cNo, they\u2019re not,\u201d Dr. Shlain instructed them. \u201cYou don\u2019t want that leg set by an E.R. doc at a local medical center. You want it set by the head of orthopedics at a hospital in the city.\u201d\n\nWithin minutes, the ambulance was on the Golden Gate Bridge, bound for California Pacific Medical Center, one of San Francisco\u2019s top hospitals. Dr. Shlain was there to meet them when they arrived, and the boy was seen almost immediately by an orthopedist with decades of experience.\n\nFor Mr. Battelle, a veteran media entrepreneur, the experience convinced him that the annual fee he pays to have Dr. Shlain on call is worth it, despite his guilt over what he admits is very special treatment.\n\n\u201cI feel badly that I have the means to jump the line,\u201d he said. \u201cBut when you have kids, you jump the line. You just do. If you have the money, would you not spend it for that?\u201d\n\nIncreasingly, it is a question being asked in hospitals and doctor\u2019s offices, especially in wealthier enclaves in places like Los Angeles, Seattle, San Francisco and New York. And just as a virtual velvet rope has risen between the wealthiest Americans and everyone else on airplanes, cruise ships and amusement parks, widening inequality is also transforming how health care is delivered.\n\nMoney has always made a big difference in the medical world: fancier rooms at hospitals, better food and access to the latest treatments and technology. Concierge practices, where patients pay several thousand dollars a year so they can quickly reach their primary care doctor, with guaranteed same-day appointments, have been around for decades.\n\nBut these aren\u2019t the concierge doctors you\u2019ve heard about \u2014 and that\u2019s intentional.\n\nDr. Shlain\u2019s Private Medical group does not advertise and has virtually no presence on the web, and new patients come strictly by word of mouth. But with annual fees that range from $40,000 to $80,000 per family (more than 10 times what conventional concierge practices charge), the suite of services goes far beyond 24-hour access or a Nespresso machine in the waiting room.\n\nIndeed, as many Americans struggle to pay for health care \u2014 or even, with the future of the Affordable Care Act in question on Capitol Hill, face a loss of coverage \u2014 this corner of what some doctors call the medical-industrial complex is booming: boutique doctors and high-end hospital wards.\n\n\u201cIt\u2019s more like a family office for medicine,\u201d Dr. Shlain said, referring to how very wealthy families can hire a team of financial professionals to manage their fortunes and assure the transmission of wealth from generation to generation.\n\nOnly in this case, they are managing health, on behalf of clients more than equipped to pay out of pocket \u2014 those for whom, as Dr. Shlain put it, \u201cthis is cheaper than the annual gardener\u2019s bill at your mansion.\u201d\n\nThere are rewards for the physicians themselves, of course. A successful internist in New York or San Francisco might earn $200,000 to $300,000 per year, according to Dr. Shlain, but Private Medical pays $500,000 to $700,000 annually for the right practitioner.\n\nFor patients, a limit of no more than 50 families per doctor eliminates the rushed questions and assembly-line pace of even the best primary care practices. House calls are an option for busy patients, and doctors will meet clients at their workplace or the airport if they are pressed for time.\n\nIn the event of an uncommon diagnosis, Private Medical will locate the top specialists nationally, secure appointments with them immediately and accompany the patient on the visit, even if it is on the opposite coast.\n\nMeanwhile, for virtually everyone else, the typical wait to see a doctor is getting longer.\n\nA survey released in March by Merritt Hawkins, a Dallas medical consulting and recruiting firm, found it takes 29 days on average to secure an appointment with a family care physician, up from 19.5 days in 2014. For some specialties, the delays are similarly long, with a 32-day wait to see a dermatologist, and a 21-day delay at the typical cardiologist\u2019s office.\n\nAnd some patients are willing to pay a lot to avoid that. MD Squared, an elite practice that charges couples up to $25,000 a year, opened a Silicon Valley office in 2013 and within months had a waiting list to join.\n\n\u201cYou have no idea how much money there is here,\u201d said Dr. Harlan Matles, who specializes in internal medicine and joined MD Squared after working at Stanford, where he treated 20 to 25 patients a day and barely had time to talk to them. \u201cDoctors are poor here by comparison.\u201d\n\nNowhere is the velvet rope in health care rising faster than here in Northern California, where newfound tech wealth, abundant medical talent and a plethora of health-conscious patients have created a medical system that has more in common with a luxury hotel than with the local clinic.\n\nIn fact, before founding Private Medical, Dr. Shlain, 50, worked as the on-call doctor at the Mandarin Oriental hotel here, an experience he said taught him about how to provide five-star service as well as good medical care.\n\nPrivate Medical started 15 years ago with a single location in San Francisco, and has since opened practices in nearby Menlo Park, in 2011, and Los Angeles, in 2015. Dr. Shlain is now eyeing an expansion into New York, Seattle and Santa Monica, Calif.\n\nThe annual fee covers the cost of visits, all tests and procedures in the office, house calls and just about anything else other than hospitalization, as well as personalized annual health plans and detailed quarterly goals for each patient.\n\nAlthough Private Medical provides its patients with doctors\u2019 cellphone numbers and same-day appointments, like more conventional concierge practices do, Dr. Shlain does not like the term \u201cconcierge care.\u201d\n\n\u201cWhen I\u2019m at a country club or a party and people ask me what I do, I say I\u2019m an asset manager,\u201d Dr. Shlain explained. \u201cWhen they ask what asset, I point to their body.\u201d\n\n\u201cWe organize health care for the entire family,\u201d he said, sitting in his hip-but-not-too-fancy office in a nondescript building in upscale Presidio Heights. Dr. Shlain and his team will coordinate treatment for grandparents in a nursing home and care for their middle-aged children, as well as provide adolescent or pediatric medicine for the grandchildren.\n\nFor example, when a teenage patient with a history of depression or anxiety moves across the country to Boston for college, Private Medical will line up a top psychiatrist near the school beforehand so a local professional is on call in case there is a recurrence. Or if a middle-aged patient is found to have cancer, Dr. Shlain can secure an appointment in days, not weeks or months, with a specialist at MD Anderson Cancer Center in Houston or Memorial Sloan Kettering Cancer Center in New York.\n\n\u201cIt\u2019s not because we pay them,\u201d he added. \u201cIt\u2019s because we have relationships with doctors all over the country.\u201d\n\nAs with the ever more rarefied tiers of frequent-flier programs or V.I.P. floors at hotels, the appeal of MD Squared and Private Medical is about intangibles like time, access and personal attention.\n\n\u201cI am able to give the time and energy each patient deserves,\u201d said Dr. Matles, the MD Squared physician in Menlo Park. \u201cI wish I could have offered this to everyone in my old practice, but it just wasn\u2019t feasible.\u201d\n\nSo in addition to providing immediate access to specialists, concierge doctors also come in handy when otherwise wealthy, powerful people find themselves flummoxed by a health care system that is opaque to outsiders.\n\n\u201cIf you need to go to Mass General, we can get you in,\u201d Dr. Matles said. \u201cWe are connected. I don\u2019t know if I can get you to the front of the line, but I can make it smoother. Doctors like to help other doctors.\u201d\n\nBut for all their confidence about the advantages of their particular brand of concierge medicine, these physicians are quick to admit they struggle with the ethical issues of providing elite treatment for a wealthy few, even as tens of millions of American struggle to afford basic care.\n\nDr. Shlain founded a software start-up, HealthLoop, that aims to \u201cdemocratize\u201d his boutique approach by allowing patients to communicate directly with their doctors through daily digital checklists and texts.\n\nHe sees no reason that the medical world should not respond to consumer demand like any other player in the service economy. \u201cWhenever I bump into a bleeding-heart liberal, which I am, I mention that schools, housing and food are all tiered systems,\u201d he said. \u201cBut health care is an island of socialism in a system of tiered capitalism? Tell me how that works.\u201d\n\nDr. Howard Maron, who founded MD Squared, is similarly candid about the new reality of ultra-elite medical care.\n\n\u201cIn my old waiting room in Seattle, the C.E.O. of a company might be sitting next to a custodian from that company,\u201d he recalled. \u201cWhile I admired that egalitarian aspect of medicine, it started to appear somewhat odd. Why would people who have all their other affairs in order \u2014 legal, financial, even groundskeepers \u2014 settle for a 15-minute slot?\u201d\n\nIt\u2019s a fair question \u2014 but the new approach does not sit so well with veteran practitioners like Dr. Henry Jones III, one of Silicon Valley\u2019s original concierge doctors at the Palo Alto Medical Foundation\u2019s Encina Practice. He charges $370 a month, a fraction of what newer entrants in the area like MD Squared and Private Medical do. \u201cIt\u2019s priced so the average person in this ZIP code can afford it,\u201d he said.\n\nA third-generation doctor from Boston, Dr. Jones offers a version of concierge medicine that is a way of providing more personalized service \u2014 the way doctors did when he graduated from medical school more than four decades ago \u2014 rather than delivering a different standard of care.\n\n\u201cEncina is like a Unesco World Heritage site \u2014 we practice medicine the way it has been traditionally practiced,\u201d he said. \u201cJust because you\u2019re an Encina patient doesn\u2019t mean you can go to the front of the line, unless you need to because of your case.\u201d\n\nNot far from Dr. Jones\u2019s office in Palo Alto, the new wing of Stanford\u2019s hospital is taking shape. Designed by the star architect Rafael Vi\u00f1oly, it will feature a rooftop garden and a glass-paneled atrium topped with a 65-foot dome. And unlike the old wing, all of the new building\u2019s 368 rooms will be single occupancy, a crucial amenity for hospitals competing to attract elite patients from across the United States and overseas.\n\nStanford raised a significant portion of the project\u2019s $2 billion cost by cultivating wealthy patients \u2014 a funding model used by university hospitals around the country, which is especially effective among the millionaires and billionaires of Silicon Valley.\n\nNot to be outdone, Lenox Hill Hospital in New York recently hired a veteran of Louis Vuitton and Nordstrom, Joe Leggio, to create an atmosphere that would remind V.I.P. patients of visiting a luxury boutique or hotel, not a hospital. \u201cThis is something that patients asked for, and we want to go from three-star service to five-star service,\u201d said Mr. Leggio, the hospital\u2019s director of patient and customer experience.\n\nIn its maternity ward, the Park Avenue Suite costs $2,400 per night, twice what a deluxe suite at the Carlyle Hotel down the street commands, but that\u2019s not a problem for well-heeled new parents. Beyonc\u00e9 and Jay Z welcomed their baby, Blue Ivy, into the world at Lenox Hill, as did Chelsea Clinton and her husband, and Simon Cowell and his girlfriend.\n\nWith a separate sitting room for family members, a kitchenette and a full wardrobe closet, the suite overlooking Park Avenue is a world away from the semiprivate experience upstairs at the hospital, where families share an old-fashioned room divided by a curtain. Slightly less exalted but still private rooms in Lenox Hill\u2019s maternity ward range from $630 to $1,700 per night.\n\nAs the stream of celebrity couples suggests, there is plenty of demand for these upscale options, crowding out traditional maternity wards. Lenox Hill is replacing some of its shared maternity rooms with private rooms, a far more profitable offering for hospitals since patients pay for them out of pocket, not through insurance plans that can bargain down rates.\n\nHospital executives argue that giving the well heeled extra attention is a way of keeping the lights on and providing care for ordinary middle- and even upper-middle-class patients, as reimbursements from private insurers and the federal government shrink. \u201cI need to succeed to pay for the children we are bringing in from all over the world and treat for free,\u201d said Dr. Angelo Acquista, a veteran pulmonologist who leads Lenox Hill\u2019s executive health and international outreach programs.\n\nThen there are the red blankets that some big Stanford benefactors receive when they check in as patients. For doctors and nurses, it is a quiet sign of these donors\u2019 special status, which is also noted in their medical records.\n\n\u201cYou don\u2019t get better care,\u201d Dr. Jones said. \u201cBut maybe the dean comes by, and if it\u2019s done well, it\u2019s done invisibly. It\u2019s an acknowledgment of a contribution to the organization.\u201d\n\nRex Chiu, an internist with Private Medical in Menlo Park, spent more than a decade as a doctor on Stanford\u2019s faculty. \u201cI loved my time at Stanford, but I was spending less and less time with patients,\u201d he said. \u201cFifteen or 20 minutes a year with each patient isn\u2019t enough.\u201d\n\n\u201cWe all say we should get the same care, but I got sick and tired of waiting for that to happen,\u201d he added. \u201cI decided to go for quality, not quantity.\u201d\n\nBesides more money, the calmer pace of high-end concierge medicine is also a major selling point for physicians \u2014 Dr. Matles said he never made it to an event at his children\u2019s school until he joined MD Squared. But for Dr. Sarah Greene, it wasn\u2019t really the money or the lifestyle that led her to Private Medical.\n\n\u201cI really have time to think about my patients when they\u2019re not in front of me,\u201d said Dr. Greene, a pediatrician who joined the company\u2019s Los Angeles practice in October. \u201cI may spend a morning researching and emailing specialists for one patient. Before, I had to see 10 patients in a morning, and could never spend that kind of time on one case.\u201d\n\nGetting in the door as a new hire isn\u2019t easy. When it comes to credentials like college, medical school and residency, Dr. Shlain said, \u201cat least two out of the three need to be Ivy League, or Ivy League-esque.\u201d\n\nIn many ways, today\u2019s elite concierge physician provides the same service as the family doctor did a half-century ago for millions of Americans, except that it is reserved for the tiny sliver of the population who can pay tens of thousands of dollars annually for it.\n\n\u201cI didn\u2019t know this level of care was possible,\u201d said Trevor Traina, a serial entrepreneur here who is a patient of Dr. Shlain\u2019s. \u201cI have a better relationship with my veterinarian than the doctors I went to in the past.\u201d\n\nWhat about everyone else? Mr. Traina doesn\u2019t see much future for the conventional family doctor, except for patients who go the concierge route.\n\n\u201cThe traditional model of having a good internist is dying,\u201d said Mr. Traina, a scion of a prominent family here that arrived with the California Gold Rush. \u201cEven the 25-year-olds at my company either have some form of concierge doc, or they\u2019ll just go to an H.M.O. or a walk-in clinic. No one here has a regular doctor anymore.\u201d", "sentiment": 0.11786916786916789},
{"link_title": "Report: Billy McFarland Used Fyre Investments to Fund Other Bad Business", "url": "https://www.spin.com/2017/09/billy-mcfarland-fyre-festival-credit-card-magnises-tickets/", "text": "Before he started promoting Fyre Festival, and before that misadventure resulted in a wire fraud charge and a forced bankruptcy, Billy McFarland founded another dubious startup targeting millennial music fans. Magnises, a supposed \u201cblack card\u201d entertainment concierge, said it provided subscribers certain perks, such as discounted tickets to popular events. Its users complained of tickets and reservations that were abruptly canceled, or materialized only at the last minute.\n\nMagnises is a separate company from Fyre, and technically, it still exists. But new credit card documents reviewed by Vice News suggest financial entanglements between McFarland\u2019s ventures. According to the documents, McFarland charged more than $1.1 million in tickets to a Fyre Media AmEx in order to re-sell them to Magnises members. Tickets were purchased from Ticketmaster or resellers like StubHub, often at a steep markup, then re-sold at a loss; a former Magnises employee estimated the company lost as much as $1,200 on each of its discounted Hamilton tickets. Often, McFarland appears to have delayed buying tickets until the day of the show, leaving employees scrambling to issue them to members in time.\n\nSeveral other high-level Fyre employees had company-issued cards, including McFarland\u2019s partner Ja Rule. Those cards had a $100,000 credit limit, and their charges appear to be related to Fyre business, Vice reports. Meanwhile, McFarland has reportedly failed to pay the full $1.1 million balance on the card he allegedly used to charge tickets for Magnises. Former Fyre Media employee David Low, a signatory on McFarland\u2019s card, said he\u2019s being held responsible for more than $200,000. Read Vice\u2019s full report here.", "sentiment": 0.055153180153180145},
{"link_title": "A Requiem for Florida, the Paradise That Should Never Have Been", "url": "http://www.politico.com/magazine/story/2017/09/08/hurricane-irma-florida-215586", "text": "ORLANDO, Fla.\u2014The first Americans to spend much time in South Florida were the U.S. Army men who chased the Seminole Indians around the peninsula in the 1830s. And they hated it. Today, their letters read like Yelp reviews of an arsenic caf\u00e9, denouncing the region as a \u201chideous,\u201d \u201cloathsome,\u201d \u201cdiabolical,\u201d \u201cGod-abandoned\u201d mosquito refuge.\n\n\u201cFlorida is certainly the poorest country that ever two people quarreled for,\u201d one Army surgeon wrote. \u201cIt was the most dreary and pandemonium-like region I ever visited, nothing but barren wastes.\u201d An officer summarized it as \u201cswampy, low, excessively hot, sickly and repulsive in all its features.\u201d The future president Zachary Taylor, who commanded U.S. troops there for two years, groused that he wouldn\u2019t trade a square foot of Michigan or Ohio for a square mile of Florida. The consensus among the soldiers was that the U.S. should just leave the area to the Indians and the mosquitoes; as one general put it, \u201cI could not wish them all a worse place.\u201d Or as one lieutenant complained: \u201cMillions of money has been expended to gain this most barren, swampy, and good-for-nothing peninsula.\u201d\n\nToday, Florida\u2019s southern thumb has been transformed into a subtropical paradise for millions of residents and tourists, a sprawling megalopolis dangling into the Gulf Stream that could sustain hundreds of billions of dollars in damage if Hurricane Irma makes a direct hit. So it\u2019s easy to forget that South Florida was once America\u2019s last frontier, generally dismissed as an uninhabitable and undesirable wasteland, almost completely unsettled well after the West was won. \u201cHow far, far out of the world it seems,\u201d Iza Hardy wrote in an 1887 book called Oranges and Alligators: Sketches of South Florida. And Hardy ventured only as far south as Orlando, which is actually central Florida, nearly 250 miles north of Miami. Back then, only about 300 hardy pioneers lived in modern-day South Florida. Miami wasn\u2019t even incorporated as a city until 1896. And even then an early visitor declared that if he owned Miami and hell, he would rent out Miami and live in hell.\n\nThere was really just one reason South Florida remained so unpleasant and so empty for so long: water. The region was simply too soggy and swampy for development. Its low-lying flatlands were too vulnerable to storms and floods. As a colorful governor with the colorful name of Napoleon Bonaparte Broward put it: \u201cWater is the common enemy of the people of Florida.\u201d So in the 20th century, Florida declared war on its common enemy, vowing to subdue Mother Nature, eventually making vast swaths of floodplains safe for the president to build golf courses and Vanilla Ice to flip houses and my kids to grow up in the sunshine. Water control\u2014even more than air conditioning or bug spray or Social Security\u2014enabled the spectacular growth of South Florida. It\u2019s a pretty awesome place to live, now that so much of its swamp has been drained, much better than Boston or Brooklyn in the winter, and, for the obvious economic and political reasons, much better than Havana or Caracas all year long.\n\nBut Mother Nature still gets her say. Water control has ravaged the globally beloved Everglades and the rest of the South Florida ecosystem in ways that imperil our way of life as well as the local flora and fauna. And sometimes, as we\u2019re about to be reminded, water can\u2019t be controlled. Hurricanes routinely tore through South Florida even before hundreds of gleaming skyscrapers and thousands of red-roof subdivisions sprouted in their path. Our collective willingness not to dwell on that ugly inevitability has also enabled the region\u2019s spectacular growth.\n\nI was thinking about all this on Thursday while evacuating my family from our home in Miami to my mother-in-law\u2019s home near Orlando, which, incidentally, one Seminole War veteran called \u201cby far the poorest and most miserable region I ever beheld.\u201d Our house is about 17 feet above sea level, which is practically Everest in South Florida terms, but we were still in a mandatory evacuation zone, because nothing in this part of the world is safe from a killer like Irma. Over the last century, we\u2019ve built a weird but remarkable civilization down here in a weird and unsustainable way. This weekend, history\u2019s bill might come due.\n\n\n\nMore than a half-century before the Pilgrims landed at Plymouth Rock, a Spanish adventurer named Pedro Menendez de Aviles landed in North Florida, and began preparing for battle with French Lutherans who coveted the same territory. Then a hurricane destroyed the French fleet on the open seas. Menendez took this as a sign from God, and gleefully slaughtered the rest of the \u201cevil and detestable Protestants\u201d in an inlet he proudly named Matanzas, Spanish for \u201cmassacre.\u201d He went on to create St. Augustine, America\u2019s oldest permanent settlement, an enduring reminder that Florida\u2019s history was forged by storms as well as blood.\n\nMenendez dreamed of colonizing the whole peninsula, but he made no progress in the backwaters of southern Florida; as his nephew reported to the king in 1570, the entire region was \u201cliable to overflow, and of no use.\u201d And it stayed that way for the next few centuries. That\u2019s because it was dominated by the Everglades, an inhospitable expanse of impenetrable sawgrass marshland, described in an 1845 Treasury Department report as \u201csuitable only for the haunt of noxious vermin, or the resort of pestilential reptiles.\u201d White men avoided it, because they viewed wetlands as wastelands. As late as 1897, five years after the historian Frederick Jackson Turner declared the closing of the Western frontier, an explorer named Hugh Willoughby embarked on a Lewis-and-Clark-style journey of discovery through the Everglades in a dugout canoe. \u201cIt may seem strange, in our days of Arctic and African exploration for the public to learn that in our very midst, in one of our Atlantic coast states, we have a tract of land 130 miles long and 70 miles wide that is as much unknown to the white man as the heart of Africa,\u201d Willoughby wrote.\n\nBut white men began to realize that South Florida had real potential if they could figure out how to drain its \u201cmonstrous\u201d swamp. Governor Broward vowed to dig a few canals and create an instant \u201cEmpire of the Everglades,\u201d a winter garden that would grow food for the world and cities larger than Chicago. Swindlers sold swampland to suckers, turning Florida real estate into a land-by-the-gallon punchline. Pioneers flocked to long-forgotten marshy boom towns with names like Utopia and Hope City and Gladesview, buying lots that looked great in the dry season only to find that they still flooded regularly during the rainy season.\n\nMeanwhile, the Standard Oil baron Henry Flagler built a railroad down the east coast, luring tourists to beachfront towns like Palm Beach, Fort Lauderdale and Miami, setting the stage for a wild 1920s land bubble that rivaled the 17th century Dutch tulip craze. Motor-mouthed \u201cbinder boys\u201d in knickers known as \u201cacreage trousers\u201d mobbed the streets of Miami, harassing pedestrians to buy and sell lots that often changed hands three times a day. One entrepreneur bought and resold a contract for a $10,000 profit on a stroll down Flagler Street. The New York Times started a stand-alone Florida real estate section. \u201cNobody in Florida thinks of anything else in these days when the peninsula is jammed with visitors from end to end and side to side,\u201d the Times reported. The insanity was immortalized by the Marx Brothers movie Cocoanuts, with Groucho capturing Florida\u2019s sleazy new land ethic: \u201cYou can even get stucco! Oh boy, can you get stucco.\u201d\n\nPretty soon, South Florida got stucco. In 1926, a few weeks after the Miami Herald urged its readers not to worry about hurricanes because \u201cthere is more risk to life from venturing across a busy street,\u201d a Category 4 storm flattened Miami, killing 400 and abruptly ending the coastal boom.Then in 1928, another Category 4 storm blasted Lake Okeechobee through its flimsy dike, killing 2,500 and abruptly ending the Everglades boom. It was the second-deadliest natural disaster in U.S. history, and afterward Florida\u2019s attorney general testified before Congress that much of the southern half of his state might be unsuited to human habitation: \u201cI\u2019ve heard it advocated that what the people ought to do is build a wall down there and keep the military there to keep people from coming in.\u201d\n\nNeedless to say, nobody built a wall. But America finally did get serious about draining the swamp. The Army Corps of Engineers, the shock troops in the nation\u2019s war on Mother Nature, built the most elaborate water management system of its day, 2,000 miles of levees and canals along with pumps so powerful some of the engines would have to be cannibalized from nuclear submarines. The engineers aimed to seize control of just about every drop of water that falls on South Florida, whisking it out to sea to prevent flooding in the flatlands. They made it possible for Americans to farm 400,000 acres of sugar fields in the northern Everglades, to visit Disney World at the headwaters of the Everglades, to drive on the Palmetto and Sawgrass Expressways where palmettos and sawgrass used to be. They made South Florida safe for a long boom that has occasionally paused but has never really stopped, bringing 8 million people to the Everglades watershed, pushing the state\u2019s population from 27th in the nation before World War II to third in the nation today.\n\nBut they made South Florida safe only most of the time, not all of the time. Now the Big One might be coming, with millions more people and structures in harm\u2019s way than there were in 1926 or 1928. And Mother Nature looks pissed.\n\n\n\nLast year, Florida\u2019s \u201cTreasure Coast,\u201d about 100 miles north of Miami, made national news when its sparkling estuary was shrouded in toxic glop that looked like guacamole and smelled like a sewer. This was an economic as well as environmental disaster, shredding the fishing and tourism industries around the town of Stuart. And it\u2019s not a huge stretch to think of it as the latest damage created by the 1928 hurricane. Water managers don\u2019t want Lake Okeechobee\u2019s dike to fail again now that there\u2019s a civilization behind it, so they routinely blast filthy water from the lake into the fragile estuaries to the east and west. Sometimes, glop happens.\n\nThe problem, like most problems in South Florida, is a water problem. Half the Everglades has been drained or paved for agriculture and development, so in the rainy season, water managers have to dump excess water into estuaries and what\u2019s left of the Everglades. Then it\u2019s no longer available in the dry season, which is why South Florida now faces structural droughts that create wildfires in the Everglades and endanger the region\u2019s drinking water, which happens to sit underneath the Everglades. Meanwhile, the Everglades itself\u2014once reviled as a vile backwater, now revered as an ecological treasure\u2014has all kinds of problems of its own, including 69 endangered species. In 2000, Congress approved the largest environmental restoration project in history to try to resuscitate the Everglades, an unprecedented effort to fix South Florida\u2019s water problems for people and farms as well as nature. But 17 years later, virtually no progress has been made. It\u2019s a real mess.\n\nBut the fundamental issue is that South Florida is an artificial civilization, engineered and air-conditioned to insulate its residents and tourists from the realities of its natural landscape. We call animal control when alligators wander into our backyards, and it doesn\u2019t occur to us that we\u2019ve wandered into the alligators\u2019 backyard. Most residents of suburban communities carved out of Everglades swampland\u2014Weston, Wellington, Miami Springs, Miami Lakes\u2014are blissfully oblivious to the intricate water diversion strategies that their government officials use to keep them dry every day. Most South Floridians don\u2019t think much about climate change, either, even though it\u2019s creating more intense storms, even though the rising seas around Miami Beach now flood low-lying neighborhoods on sunny days during high tide. People tend not to think too much about existential threats to the places they live. They just live.\n\nAnd they keep coming. Twenty-five years ago, Hurricane Andrew ripped through Miami\u2019s southern exurbs, but the homes destroyed were quickly replaced, and most of us who live here now weren\u2019t here then. So we weren\u2019t really ready for Irma, even though at some level we knew it was possible. It\u2019s conceivable that Irma will finally shut down our insatiable growth machine, but I wouldn\u2019t bet on that. Our inclination towards collective amnesia is just too strong.\n\nThe thing is, it\u2019s really nice here, except when it isn\u2019t. Those Seminole War soldiers would be stunned to see how this worthless hellscape of swarming mosquitoes and sodden marshes has become a high-priced dreamscape of swimming pools and merengue and plastic surgery and Mar-a-Lago. It probably isn\u2019t sustainable. But until it gets wiped out\u2014and maybe even after\u2014there\u2019s still going to be a market for paradise. Most of us came here to escape reality, not to deal with it.", "sentiment": 0.0609695165945166},
{"link_title": "Multi-stage builds can help you save gigabytes in Docker images", "url": "https://blog.onebar.io/reducing-docker-image-size-using-multi-stage-build-4ec8ee111aae", "text": "This is a short article that may help you significantly reduce your docker images size. It is based on my experience optimizing the front-end docker image for OneBar. I have spent less than 20 minutes and shrank our biggest image from 1.52Gb to 117Mb.\n\nThis is the before the optimization:\n\nIt follows a very simple approach: take a pre-built Nginx container, copy project code into it, install Node-js dependencies and , build the Ember-js project, copy custom configs and finally serve static files from the folder. Very simple, but in the current implementation several issues make the final image extremely large and the whole build time-consuming. Nginx image from Docker Hub doesn\u2019t come with Node-js pre-installed. It\u2019s also missing a few heavy dependencies needed to complete node modules installation and ember build: git and python. More importantly, we only care about the whole Node-js stack only during the build time. At the runtime, we just serve pre-built static files using Nginx and do not use Node-js at all. It turns out, however, that all this unused stuff accounts for about 1.4Gb in the resulting image. There are several solutions for this problem and build stages, introduced in Docker 17.05, seems to be the most elegant one.\n\nHere\u2019s how the looks after I refactored it to use multi-stage build.\n\nThe build is now split into two stages. First, build the Ember-js project inside the Node-js image. Second, derive a new image from one and just copy the build artifacts from the first stage into it. Only things starting from a second statement will make it into the final image and the whole Node-js part will be abandoned. Note that image already comes with and pre-installed and that saves us a few lines of a and a few seconds of a build-time.\n\nObviously, this process could be emulated using an external script, orchestrating a build process that involves multiple images. But now it\u2019s natively supported by Docker, and to me, it seems like the most elegant and least hacky way of doing things.\n\nThis simple trick saved us 1.4Gb of the Docker image size. This will result in a lot of time saved while pushing and pulling images between hosts and the registry. Also, the is now better organized, looks cleaner and, if nothing changes in the , the build should be way faster than before.", "sentiment": 0.09906778827233373},
{"link_title": "Show HN: Pool of elements works as queue", "url": "https://github.com/Jimmy02020/queue-pool", "text": "simple FIFO implementation with push and shift prototypes like array, with ability to auto-adjust length.\n\nIf you handle a group of incoming streams of chunks, process them in a pool, there is a need to release them in same order they get in.\n\nan array contains size of each element in the pool. push and adjust the size. It accepts callback as second argument. In case you made several calls passing the allowed number set in . It auto over elements and then the new element. Using this method you guarantee that you are not passing the number of elements you set.\n\nThis project is licensed under the MIT License\n\nFor border view you can take a look of as well.", "sentiment": 0.02727272727272727},
{"link_title": "The Great American Janitor Test", "url": "https://medium.com/the-new-york-times/the-great-american-janitor-test-567bebb1db68", "text": "The New York Times has locked this story exclusively for members. But with a free Medium account, you can access this post, plus two other exclusive stories this month.\n\nAlready have an account? Sign in.", "sentiment": 0.13712121212121212},
{"link_title": "Messenger.com Now 50% Converted to Reason", "url": "https://reasonml.github.io/community/blog/#messengercom-now-50-converted-to-reason", "text": "Messenger.com is the web version of Facebook Messenger; we also share code with facebook.com's inbox view and chat tabs. For over a year, the Reason team has been working directly on Messenger in order to integrate Reason + BuckleScript into the codebases. As of a while ago, we've reached 50% Reason code coverage!\n\nWe believe in iterating on/alongside product teams in order to create the best infra. The product teams' and open source folks' feedback has changed our strategy a few times, for the better. As of today, Reason and BuckleScript are also deployed on a WhatsApp internal tool, Instagram Web (small scale), plus some critical Ads internal tools. We'll be working closely with these teams over the next year.\n\nWe've successfully onboarded regular JavaScript folks to Reason; in the most extreme case, an intern with no JS knowledge was able to ship ReasonReact code in production (and made 0 bug while doing so). Give your own team a try! =)\n\nA big thanks to all these teams' members, to Hongbo (BuckleScript author) and to the OCaml community (really, 50% Reason means 50% OCaml. We're nothing without you); and of course, to all of you folks in the community for being with us all this time. The best is yet to come.\n\nSee you soon!\n\nSometimes when I'm busy working, some random colleague/Discord member would ping me and tell me \"Yo Cheng Lou why are Reason's errors so bad? Why can't you be more like Elm Cheng Lou? Why? Look at how great Elm's errors are Cheng Lou, look.\"\n\nIn reality I'm pretty darn ashamed of our error messages; here we are, a type system with two decades of solid research and implementation, but sometimes presented to the end users as if it's something that'd get in their way.\n\nNo more! We've heard you loud and clear, and delivered you much improved error messages! A few things we did:\n\nThe last point is a tradeoff; errors end up taking more space. Seeing that you'd usually focus on a single error rather than trying to get an overview of all errors, we've deemed this tradeoff worthwhile, especially in the context of a big amount of build output. Considering the new warning format:\n\nHere's the same warning, old version, buried among other outputs:\n\nAt Messenger, we've seen people ship warnings to production not because they didn't want to fix them, but because they've missed them! It's not rocket science. Leave some negative space here and there. Color things appropriately. Voil\u00e0!\n\nThe new errors can be turned on by adding to your bsconfig.json, like so. They're also available for bsb-native. True to our stack's spirit, they're fast, simple to configure, and solid.\n\nOne more thing: we're vertically integrated common pitfalls of ReasonReact into these messages too, when applicable.\n\nThis is just the first of many iterations to come! Got a message you'd like to see explained better? File an issue here!\n\nBased on popular feedback, we've now improved our online Try section. Highlights:\n\nThe new documentation website is built by our community member Jared (make sure to check his Reason blog posts too!). The new site keeps most of the same content from the old one, while providing a better structure to navigate through them. You'll notice \"Suggest an edit\" links all over the place. Take a look around!\n\nVery exciting release! Short version: ReasonReact now has its own documentation site here. Accompanying this is the new BuckleScript release. Both are non-breaking.\n\nNow that the community is taking off, keeping folks up-to-date through Discord and other existing channels became less ideal. We're starting a blog post section for this reason. In the spirit of the community, these posts will stay short and concise.\n\nWe've moved unused first-party projects from GitHub/reasonml to GitHub/reasonml-old. Old URLs are redirected, so no breakage here.\n\nWe've cleaned up the Reason codebase. Editor integrations moved out to their dedicated repos. Updated instructions are still here. Other Reason repo cleanups are still ongoing.\n\nSome discord rooms got merged together. Fewer rooms, more focused discussions.\n\nAs you can see: this documentation site got a few rearrangements too. In general, if you'd like to contribute to docs, please ping us on Discord!", "sentiment": 0.1837496617965368},
{"link_title": "Show HN: Learn and practice flashcards in a different way", "url": "https://oigovoz.com?invite=HN", "text": "Whether you are in a gym, walking on a trail, on a beach, or driving a car, Oigovoz offers a way to learn anything with minimal effort using the skills given by nature. Our service will help you practice foreign languages, prepare for tests or jobs, improve cognitive abilities.", "sentiment": -0.041666666666666664},
{"link_title": "Science debate: Should we embrace an enhanced future?", "url": "http://www.bbc.com/news/science-environment-41161425", "text": "To write this article, I drank a beverage containing an effective cognitive enhancer - something that helps me - almost daily - to focus.\n\nIn my pocket, I carry an extension of my memory, on which I made notes for this very story.\n\nCaffeine and smart phones might not strike most people as human enhancements, but in changing how we use our bodies and brains, they are exactly that. They improve our subjective wellbeing and facilitate our meeting day-to-day life goals.\n\nThe rather more futuristic-sounding concept of transhumanism - the idea that every human should have the right to enhance themselves beyond the so-called \"norm\" through science and technology - was the subject under scrutiny at a debate this week at the British Science Association Festival in Brighton.\n\nThe big question being posed: do we all have the right to enhance our bodies as technology and pharmaceuticals will allow, or is that immoral? As the probably over-used term has it, would that be \"playing God\"? And who gets to decide?\n\nFirst we should be frank about what is a correction - a medical fix - and what is an enhancement.\n\nEyeglasses and contact lenses would be seen by most people as falling in the correction category, restoring sight to normal levels. However, although everyone may strive for 20-20 vision and invest in the technology to get it, such acute sight is far from normal and in many cases more enhancement than correction.\n\nThere is no easy line to draw between medicine and enhancement, because the very notion of \"normal\" is not easy to define. And as science and technology moves on, we have a new and ever-changing normal.\n\nRitalin a central nervous system stimulating drug is used by people unable to maintain normal levels of concentration. It is often prescribed for those who suffer from attention-deficit hyperactivity disorder (ADHD).\n\nHowever, it is also in now widespread use as a \"smart drug\" - to enhance the concentration of people without ADHD. This form of enhancement, often by students battling to hit tight deadlines, has proven very controversial.\n\nCritics of such smart drugs argue that they give users an unfair advantage. But others points out that exams have never been a level playing field - some people can afford the luxury of personal tutors, while others have to work in full-time jobs to support themselves in higher education.\n\nMany people find the idea of any such cognitive enhancement deeply troubling, but what if, as Rebecca Roache, a philosopher at Royal Holloway, University of London argued at the festival debate, \"this might be the fastest way to find a cure for cancer?\"\n\nThat, she pointed out would benefit everyone.\n\nSo is it then immoral not to attempt to upgrade people for the sake of improving society? And who and what should we be upgrading?\n\nSarah Chan from the University of Edinburgh argued that research into enhancement should be directed towards helping those who are worst off in society first.\n\nIf not, as Florence Okoye of the Natural History Museum noted, enhancement could magnify already existing societal inequalities.\n\nThis then opens up the question of whether society's goal should be to make upgrades available for everyone.\n\nMost humans are now enhanced to be resistant to many infectious diseases. Vaccination is human enhancement. Apart from \"anti-vaxxers\" - as those who lobby against childhood inoculations are often dubbed - most of us are content to participate. And society as a whole benefits from being free of those diseases.\n\nSo what if we took that a pharmaceutical step further. What if, as well as vaccines against polio, mumps, measles, rubella and TB, everyone also \"upgraded\" by taking drugs to modify their behaviour? Calming beta-blocker drugs could reduce aggression - perhaps even helping to diffuse racial tension. Or what if we were all prescribed the hormone oxytocin, a substance known to enhance social and family bonds - to just help us all just get along a little better.\n\nWould society function better with these chemical tweaks? And might those who opt out become pariahs for not helping to build a better world - for not wanting to be \"vaccinated\" against anti-social behaviours?\n\nAnd what if such chemical upgrades could not be made available to everyone, because of cost or scarcity? Should they be available to no one? An enhanced sense of smell might be useful for a career in wine tasting but not perhaps in rubbish disposal.\n\nA case in point is military research - an arm of which is already an ongoing transhumanism experiment.\n\nMany soldiers on the battlefield routinely take pharmaceuticals as cognitive enhancers to reduce the need to sleep and increase the ability to operate under stress. High tech exoskeletons, increasing strength and endurance, are no longer the realms of science fiction and could soon be in routine military use.\n\nThe US military, always a research leader, has recently tested electrical brain stimulation, which was shown to improve increase multitasking skills and performance in people using flight simulators.\n\nBut like GPS and the Internet, many initially military breakthroughs will become available to wider society, leaving us to charter new self-enhancing waters.\n\nSome ethicists have argued that there is an urgent need to enhance human moral decision-making - biomedically, if necessary. And Dr Roache, during the debate, was quick to suggest that most people might approve of morally enhanced politicians.\n\nPublic perception of new technology though is often cold to begin with.\n\nBut just a few decades ago treatments like IVF were seen by many as morally wrong. IVF is now largely considered routine and socially acceptable. So might we one day accept gene therapy on cells in early-stage embryos, which would produce heritable genetic changes for future generations?\n\nAs technology moves inexorably forward, we are being faced with new questions about how enhanced we want ourselves and everyone else to be.\n\nIn answering those questions, perhaps we need to recognise how enhanced we already are.\n\nDr Alexander Lees is a British Science Association media fellow and lecturer at Manchester Metropolitan University @Alexander_Lees", "sentiment": 0.13882367632367637},
{"link_title": "Http4k \u2013 a funtional HTTP toolkit for Kotlin", "url": "https://github.com/http4k/http4k", "text": "http4k is an HTTP toolkit written in Kotlin that enables the serving and consuming of HTTP services in a functional and consistent way.\n\nIt consists of a core library providing a base HTTP implementation + a number of abstractions for various functionalities (such as servers, clients, templating etc) that are provided as optional add-on libraries.\n\nThe principles of the toolkit are:\n\nThis quick example is designed to convey the simplicity & features of http4k. See also the quickstart for the simplest possible starting point.\n\nTo install, add these dependencies to your Gradle file:\n\nThis \"Hello World\" style example demonstrates how to serve and consume HTTP services with dynamic routing:\n\norg.http4k.client.OkHttp org.http4k.core.Filter org.http4k.core.HttpHandler org.http4k.core.Method.GET org.http4k.core.Request org.http4k.core.Response org.http4k.core.Status.Companion.OK org.http4k.core.then org.http4k.filter.CachingFilters org.http4k.routing.bind org.http4k.routing.path org.http4k.routing.routes org.http4k.server.Jetty org.http4k.server.asServer ( < >) { we can bind HttpHandlers (which are just functions from Request -> Response) to paths/methods to create a Route, then combine many Routes together to make another HttpHandler HttpHandler routes( bind to { _ Request Response( ) body( ) }, bind to { req Request ? req path( ) Response( ) body( ) } ) call the handler in-memory without spinning up a server Response app(Request( , )) println(inMemoryResponse) this is a Filter - it performs pre/post processing on a request or response Filter { next HttpHandler { request Request System currentTimeMillis() next(request) System currentTimeMillis() start println( ) response } } we can \"stack\" filters to create reusable units, and then apply them to an HttpHandler CachingFilters Response NoCache() then(timingFilter) HttpHandler compositeFilter then(app) only 1 LOC to mount an app and start it in a container filteredApp asServer(Jetty( )) start() HttpHandler OkHttp() Response client(Request( , )) println(networkResponse) }\n\nThis project exists thanks to all the people who contribute.\n\nIf you use http4k in your project or enterprise and would like to support ongoing development, please consider becoming a backer or a sponsor. Sponsor logos will show up here with a link to your website.", "sentiment": 0.07179487179487179},
{"link_title": "Shadow of the Colossus Was Unfinished, and Better Because of It", "url": "http://kotaku.com/shadow-of-the-colossus-was-unfinished-and-better-becau-1796405108", "text": "Shadow Of The Colossus was broken, unpolished, and incomplete, and the upcoming remake shouldn\u2019t try to fix that.\n\nTechnically, Team Ico did complete Shadow Of The Colossus. It shipped in 2005. You could put the disc in a PS2 and experience what would become one of the most iconic adventures in gaming. But a much more sprawling vision for its world, and the colossi that would inhabit it, was never realized. Thanks to technological constrains and limited time, large swaths of the game remained hidden away unfinished, or cut out entirely.\n\nAfter Sony revealed at this E3 that the game would be making the jump to PS4 in 2018, the company\u2019s President, Shuhei Yoshida, confirmed to Famitsu that it would be more than just a remaster. Specifically, he noted that it might include updated controls, and that designer Fumito Ueda was not much involved in the new project. So if Bluepoint, the development company behind the game\u2019s PS3 HD port, will potentially be updating not just the look but also the controls, why not go even further and try to partially restore the game to what Team Ico first envisioned it might be?\n\n\n\nShadow of the Colossus as it exists today only has sixteen colossi. Some of them are more memorable than others, but all have been killed millions of times. The game even includes time trials to reward players for finding unique and innovative ways to go about doing it. One player has even managed to complete all of them on hard in under an hour.\n\nThere used to be more though back when Team Ico first started working on the game. Not surprisingly, given what we know about him now and the ungodly amount of time Last Guardian spent in development, Ueda had planned for up to 48 different hulking bosses to be roaming about the game\u2019s map. Based on interviews, concept art, and the game\u2019s code combed over by ravenous fans, we know that some of these colossi looked similar to ones that ended up making the final cut, while others, like this giant spider, were quite distinct.\n\nOne of the game\u2019s more famous players, who goes by Nomad, compiled a thorough investigation of the matter on his blog several years ago. Part of a large group of colossi hunters searching desperately to unearth every last available secret in the game, Nomad and others helped demonstrate not just that this stuff exists, but what was so compelling about the underlying game to make people go looking in the first place.\n\nShadow of the Colossus has all the markers of a cult classic but it\u2019s not one. Roger Ebert talked about it, sparking a decade-long collective migraine about the relationship of video games to art. Adam Sandler and Don Cheadle played it in a major motion picture. And it\u2019s not hard to see why. The game\u2019s appeal transcends gaming and is immediately obvious, with an ambiance that\u2019s both contemplative and intimate by virtue of how open and hands off the world is. People like wandering across a valley toward a mountain only to realize that mountain is alive and has just noticed them.\n\nYou don\u2019t have to beat Shadow of the Colossus or even be good at it to be taken in by the mysteries embedded in its geography. What are these crumbling ruins? What kind of previous civilization do they hint at? And why do some of them move and try to kill me? Aided by the game\u2019s penchant for silence and its minimal exposition, as well as an ending whose twist is just the right mix of irony and ambiguity, its easy to let these lines of questioning take over, and be driven forth by them into the unknown. Fortunately for the millions of us who originally fell under that spell, the maze was never finished, and so we as a result never had to be finished with it.\n\nThe original Shadow of the Colossus is rife with glitches. Some appeared the result of tight (by Ueda\u2019s standards) production schedules and a challenging QA process, while others almost seemed intentional, as if the game\u2019s creators were gesturing toward another level of mysteries below the game\u2019s surface. Birds you could hop a flight on, a lake that didn\u2019t obey the normal laws of physics, and a secret garden atop the game\u2019s central tower players spent years trying to ascend\u2014they all helped extend the life of the game and resisted pat interpretations of its meaning and the designers\u2019 intents. Who creates a door that can\u2019t be opened and then taunts players with it?\n\nThe best ruins in Shadow of the Colossus are the ones left there by the artists themselves and which players have spent over a decade struggling with. A remake as beautiful looking as the one Sony showed at E3 can enhance the details on every tree and replace a hellish third-person camera with a control scheme less rage-inducing, but it\u2019s not likely to find a way to recapture the magic of playing a game filled with mysteries it never intended to create in the first place.", "sentiment": 0.03914035311094135},
{"link_title": "Visualizing Entropy in Binary Files", "url": "https://teaearlgraycold.me/visualizing-entropy-in-binary-files/", "text": "A lot of the data we store is redundant. We tell our computers to keep record of the same information over and over. Compression algorithms are an attempt to avoid that waste.\n\nUsing Python's zlib package we can see how a human-readable pattern is turned into a noisy string of bytes:\n\nSome compressing algorithms use domain-specific knowledge to reduce the number of bytes needed to store something. We need to look at data on a byte level in order to remove repetition in a way agnostic to the type of file being compressed. However, the ability to witness the compression usually comes down to nothing more than seeing a file's size decrease. You don't normally get to see the repetitiousness of the uncompressed file - or the noise of the compressed file.\n\nBut if you naively interpret the bytes of a file as pixels in an image, you can get a feel for what's happening to the file when it's compressed.\n\nHere is an excerpt from a 2-channel WAV file as it looks when converted into an image (with 1 bit of audio data per pixel).\n\nIt's actually possible to see the two seperate channels of the WAV file. Some areas of this excerpt look noisy, but there's a lot of repetition. For example, the black bar in the middle on the image.\n\nAfter compressing the WAV file as an MP3 it's apparent that something has changed\n\nThis pretty much looks like static. The information entropy in this file is much higher per byte on disk.\n\nThe same phenomena can be noticed in image files. Here's an uncompressed BMP image.\n\nInterpretting the image as raw bytes yields this visualization (cropped,\n\n 24 bits of image data per pixel).\n\nAnd when compressed as a PNG image\u2026\n\nAgain, this image looks like pure noise (with exception to the header and footer data). Interestingly, this image has significantly smaller dimensions than that of the original BMP image. The image size is visually representative of the file's size.\n\nThe images in this post were rendered with a tool called binimage which I wrote.", "sentiment": 0.06462148962148963},
{"link_title": "Leaked iOS 11 Golden Master", "url": "https://www.macrumors.com/2017/09/09/ios-11-golden-master-leaked/", "text": "The golden master version of iOS 11 appears to have leaked this evening, shedding some early light on products and features Apple plans to announce next Tuesday. The golden master software seems to have been sent to multiple sites including MacRumors and 9to5Mac.In the Apple Watch app, there's an image of what may be the third-generation Apple Watch with a cellular connection. The device has the same general design as the current Apple Watch, featuring a Space Gray body and matching band, but it appears to have a bright red Digital Crown. Code in the update suggests the Apple Watch will be able to make calls over LTE, sharing a phone number with the iPhone. The Apple Watch will be added to an existing phone plan, and there are hints that carriers will offer pricing promotions.As noted by 9to5Mac , there are references to Face ID, which may be the name of the new facial recognition system in the iPhone 8, and there are images that feature the design of the iPhone 8 with slim bezels and a notch at the top of the device for the camera. The display of the device may feature True Tone support much like the iPad Pro, a feature that has been previously rumored.Two iPhone 8 screenshots buried within the update give us a look at the new navigation bar on the iPhone 8, which will replace the Home button with a set of gestures. Pulling upwards on the bar brings up the Home screen and a longer pull brings up the App Switcher. There are also hints that the elongated sleep/sleep wake button (now called side button) will activate Siri and bring up Apple Pay cards and passes.There appears to be a new \"Portrait Lighting\" feature that may work with the flash on the device, offering up Contour Light, Natural Light, Stage Light, Stage Light Mono, and Studio Light, and there are new video recording options:- 1080p HD at 240 fps 480 MB with 1080p HD at 240 fps- 4K at 24 fps (Footer) 270 MB with 4K at 24 fps (film style) (HEVC Footer) 135 MB with 4K at 24 fps (film style)- 4K at 60 fps (Footer) 450 MB with 4K at 60 fps (higher resolution, smoother) (HEVC Footer) 400 MB with 4K at 60 fps (higher resolution, smoother)There are hints of animated emoji for iMessage and references to AirPods 1,2, which may be a revised version of the wire-free headphones Apple first introduced last year.Last but not least, there are a selection of colorful new wallpapers that are sure to look impressive on an OLED display. There are stills of the earth and moon, several floral images, a selection of retro-style rainbow wallpapers, and one wallpaper that's plain black.With the iOS 11 firmware released early, it's likely there will be additional discoveries about the unreleased iPhone 8 and the third-generation Apple Watch over the weekend. Apple will officially unveil the new devices at its Tuesday event set to be held at 10:00 a.m. Pacific Time.", "sentiment": 0.2080292723149867},
{"link_title": "Connect Watch: first AsteroidsOS powered smartwatch", "url": "http://connect-watch.com/en/", "text": "CONNECT WATCH reinvents the meaning of a connected watch user experience by guaranteeing you total freedom of use while completely respecting your privacy.\n\nPowered by AsteroidOS, a whole world of free, open source opportunity is at your grasp. Simple and intuitive, organizing your everyday life has never been easier.", "sentiment": 0.07142857142857142},
{"link_title": "The Incredible Growth of Python Language", "url": "http://stacktrender.com/post/st/the-incredible-growth-of-python-stack-overflow-blog", "text": "", "sentiment": 0.0},
{"link_title": "Camembert crackdown China bans soft cheese imports", "url": "http://money.cnn.com/2017/09/08/news/china-soft-cheese-import-ban/index.html", "text": "Suppliers in the country are no longer able to import a range of famous French cheeses such as brie, camembert, and roquefort, according to companies involved in the industry.\n\n\"The entire Chinese market for soft cheeses is now closed,\" said William Fingleton, a spokesman for the Delegation of the European Union to China.\n\n\"This effectively means that China is banning famous and traditional European cheeses that have been safely imported and consumed in China for decades. There is no good reason for the ban, because China considers the same cheese safe if produced in China,\" he said.\n\nThe measures affect products containing bacteria found in the rind of brie and camembert, as well as bacteria used to produce blue cheese.\n\n\"Right now, we can only sell hard cheese like cheddar, comt\u00e9 and manchego,\" said Vincent Marion, CEO of Shanghai-based Cheese Republic. His business, which sells imported cheeses online, warned customers of the ban in a message shared Thursday on Chinese messaging platform WeChat.\n\nA spokeswoman for Shanghai Fuzhen Commerce, a foreign food importer, said it received notice of the ban from government regulators in June.\n\nBut Marion said several of Cheese Republic's suppliers only started flagging the problem two weeks ago, warning him that Chinese officials were preventing soft cheese from entering the country.\n\n\"So we can sell only the stock which was imported before\" the ban took effect, he said. Soft cheeses make up roughly half of his company's product range.\n\nRelated: China will import American rice for the first time\n\n\"The European cheese industry is extremely concerned by this ban,\" said the EU's Fingleton. \"We are concerned that potentially many other types of cheeses may be affected in the future in the same way.\"\n\nChina has cracked down on cheese imports in the past.\n\nIn 2014, it temporarily banned imports of British cheese after food inspectors found hygiene standards at a U.K. dairy farm were inadequate. And it banned Italian mozzarella in 2008 after Italy ordered a recall of the product because it was possibly tainted with a cancer-causing dioxin.\n\nEvery now and then, Chinese authorities will block one type of cheese for a short period of time, but this time is different, according to Marion.\n\n\"It's very surprising that they decide to block so many,\" he said.\n\nChina's food watchdog, the General Administration of Quality Supervision, Inspection and Quarantine, didn't immediately respond to a request for comment.", "sentiment": 0.08413650793650794},
{"link_title": "Vigil, the eternal morally vigilant programming language", "url": "https://github.com/munificent/vigil", "text": "Vigil is a very safe programming language, and an entry in the January 2013 PLT Games competition.\n\nMany programming languages claim to take testing, contracts and safety seriously, but only Vigil is truly vigilant about not allowing code that fails to pass programmatic specifications.\n\nVigil is very similar to Python with the minor proviso that you must provide a function which will be automatically called for you.\n\nInfinitely more important than mere syntax and semantics are its addition of supreme moral vigilance. This is similar to contracts, but less legal and more medieval.\n\nOften, a function will require that parameters have certain desirable properties. A function in Vigil can state what it requires by using :\n\nIf a caller fails to provide valid arguments, it is wrong and must be punished.\n\nIf a good caller meets its obligations, the onus is thus on you to fulfill your end of the bargain. You can state the oaths that you promise to uphold using :\n\nIf a function fails to uphold what it has sworn to do, it is wrong and must be punished.\n\nIt goes without saying that any function that throws an exception which isn't caught is wrong and must be punished.\n\nThis is where Vigil sets itself apart from weaker languages that lack the courage of their convictions. When a Vigil program is executed, Vigil itself will monitor all oaths (implorations and swears) that have been made. If an oath is broken, the offending function (the caller in the case of and the callee in the case of ) will be duly punished.\n\nSimple: it will be deleted from your source code.\n\nThe only way to ensure your program meets its requirements to absolutely forbid code that fails to do so. With Vigil, it will do this for you automatically. After enough runs, Vigil promises that all remaining code meets its oaths.\n\nVigil is a command-line executable. Pass it the path to a file to run:\n\nThe \"example\" directory has some to get you started.\n\nNo, wanting to keep code that demonstrably has bugs according to its own specifications is crazy. What good could it possibly serve? It is corrupted and must be cleansed from your codebase.\n\nVigil will do this for you automatically.\n\nIt would seem that those functions appear to be corrupted as well. Run Vigil again and it will take care of that for you. Several invocations may be required to fully excise all bugs from your code.", "sentiment": -0.011344537815126055},
{"link_title": "Planetary disasters: It could happen one night (2013)", "url": "http://www.nature.com/news/planetary-disasters-it-could-happen-one-night-1.12174", "text": "One hundred thousand years ago, a massive chunk of the Mauna Loa volcano cracked away from Hawaii and slid into the sea, launching a wave that rose as high as the Eiffel tower up the slopes of a nearby island. That mega-tsunami was not an isolated incident: the past 40,000 years have seen at least ten gigantic landslides of more than 100 cubic kilometres in the North Atlantic ocean alone, each capable of producing waves tens to hundreds of metres high. Another is bound to happen sometime \u2014 although whether it will strike tomorrow or 10,000 years from now is anyone's guess.\n\nThis week, the World Economic Forum published its 2013 global risks report, which includes a section, produced in collaboration with Nature, on X factors: low-probability, high-impact risks resulting mainly from human activity (see go.nature.com/outhzr). But the natural world holds unpredictable threats as well. The geologic record is peppered with evidence of rare, monstrous disasters, ranging from asteroid impacts to supervolcanoes to \u03b3-ray bursts. Nature looks into some of the life-shattering events that Earth and the broader Universe could throw our way.\n\nEarth is now in the middle of a flare-up of supervolcanic activity1. Over the past 13.5 million years, no fewer than 19 giant eruptions have each spewed more than 1,000 cubic kilometres of rock \u2014 enough to coat an entire continent in a few centimetres of ash and push the planet into 'nuclear winter'. One of the most recent such eruptions, of Toba in Indonesia 74,000 years ago, was such a catastrophic event that some scientists have blamed it for starting the last ice age and slashing the human population to about 10,000 people. One estimate1 suggests that there is a 1% chance of a super-eruption in the next 460\u20137,200 years.\n\nThe four youngest, most active supervolcanic systems in the world are Toba, Campi Flegrei in Italy, Yellowstone in the northwestern United States and Taupo in New Zealand. All four systems are being monitored for groundswell and seismic swarms \u2014 clusters of small earthquakes that can signal moving magma \u2014 and all occasionally show these warning signs. But no one knows whether the result of each flare-up will be a small squirt of steam or \u2014 much more hazardous \u2014 a mega-eruption of lava. \u201cIf something were brewing, we would get warning hours, days and months ahead,\u201d says Shan de Silva, a volcanologist at Oregon State University in Corvallis. \u201cBut how big it's going to be, we don't have a handle on.\u201d\n\nTo help answer these questions, scientists are now drilling into the heart of one of the top contenders for the next blow-up: the Campi Flegrei caldera, a crater that is 13 kilometres wide and includes the city of Naples. Since 1969, the ground at Campi Flegrei has bulged upwards by as much as 3.5 metres, and researchers are eager to find out whether the culprit is underground steam or a pool of magma. Previous bouts of volcanic activity in the caldera came after the ground surface had swelled up by several metres or more2, and researchers think that major activity could occur within the next few decades or centuries. To investigate the risk, scientists at Campi Flegrei plan to drill more than 3 kilometres into the crater, despite concerns from some researchers that the drilling could trigger earthquakes or an explosion.\n\nOne goal is to look at the magma pool beneath the crater: the shallower and more molten it is, the greater the chances of a super-eruption. Characterizing such pools through seismic studies is hard, and the range of error is huge. \u201cWe really are groping in the dark,\u201d says de Silva. Scientists estimate that 10\u201330% of the magma under Yellowstone, for example, is liquid \u2014 shy of the 50% thought to be needed for super-eruption. But pockets of molten magma in the chamber could still cause eruptions several-fold larger than the 1980 blast from Mount St Helens in Washington state, warns Jacob Lowenstern, head of the Yellowstone Volcano Observatory for the US Geological Survey in Menlo Park, California.\n\nThe effort to drill into Campi Flegrei and measure features such as temperature and rock permeability should help researchers to interpret seismic-imaging studies of magma pools, says Lowenstern. \u201cIf we want to be able to successfully image Earth, we occasionally need to make a few strategic incisions into the patient,\u201d he says. As for the dangers of drilling, Lowenstern is convinced that the project will have minimal impact. \u201cIt's like a pinprick on an elephant,\u201d he says. The Campi Flegrei team finished an initial 500-metre test well in December 2012 without incident. And seismologists safely drilled a hole of similar size into the Long Valley caldera in California \u2014 a supervolcano site that erupted 760,000 years ago and holds the same killer potential as Yellowstone.\n\nUntil more is learned about these systems, societies must accept that the threat of a super-eruption is real, yet remote. Lowenstern says that although the chances of one happening this year are tiny, \u201cit is theoretically possible\u201d.\n\nAlthough viruses and bacteria grab more attention, fungi are the planet's biggest killers. Of all the pathogens being tracked, fungi have caused more than 70% of the recorded global and regional extinctions3, and now threaten amphibians, bats and bees. The Irish potato famine in the 1840s showed just how devastating such pathogens can be. Phytophthora infestans (an organism similar to, and often grouped with, fungi) wiped out as much as three-quarters of the potato crop in Ireland and led to the death of one million people.\n\nPotato blight is still a threat: 13_A2, a highly aggressive strain of P. infestans, is now rampant in Europe and North Africa. Across the globe, Phytophthora causes some US$6.7 billion in annual damages, according to a 2009 estimate4. Sarah Gurr, a plant pathologist at the University of Oxford, UK, estimates that the worst theoretical potato infestation would deprive 1.3 billion people of food each year. Other major staple crops face similar threats, such as rice blast (Magnaporthe oryzae), corn smut (Ustilago maydis), soya bean rust (Phakopsora pachyrhizi) and wheat stem rust (Puccinia graminis). The stem-rust superstrain Ug99 has in recent years slashed yields in parts of Africa by as much as 80%.\n\nIf all five crop staples were hit with fungal outbreaks at the same time, more than 60% of the world's population could go hungry, says Gurr. \u201cThat's apocalyptic\u201d, but unlikely, she says \u2014 \u201cmore of a James Bond movie\u201d. David Hughes, a zoologist at Pennsylvania State University in University Park, adds that terrorists could use fungi to wreak havoc by targeting economically important crops. In the 1980s, for example, a possibly deliberate infection wiped out cacao crops in northern Brazil, changing the country's demographics and ecology as people moved from unproductive farms to the cities and cleared more rainforest. \u201cIf you wanted to destabilize the world, you could easily introduce rubber blight into southeast Asia,\u201d he says, which would trigger a chain reaction of economic and political effects.\n\nModern agriculture has exacerbated societies' vulnerability by encouraging farmers to plant the same strains of high-yield crops, limiting the variety of resistance genes among the plants, says Gurr. \u201cWe've skewed the arms race in favour of the pathogen,\u201d she says. \u201cThat's why we're on the brink of disaster.\u201d\n\nResearchers estimate that there are 1.5 million to 5 million species of fungi in the world, but only 100,000 have been identified. Reports of new types of fungal infection in plants and animals have risen nearly tenfold since 1995 (ref. 3). Gurr suggests that climate change might be a culprit.\n\nHumans have cause for concern as well. In the past decade, a tropical fungus called Cryptococcus gattii has adapted to thrive in cooler climes and invaded the forests of North America's Pacific Northwest. By 2010, it had infected some 280 people, dozens of whom died. Although fungi are not spread as easily from person to person as viruses, for example, and anti-fungal agents can effectively tackle most infections, there are still reasons to worry. Fungi continue to evolve, and once they are established in an ecosystem, they can be almost impossible to wipe out.\n\nGiven these trends, experts say that fungi have not received enough attention from researchers and governments. \u201cI'd be very surprised if an abrupt fungal infection killed a large swathe of people. But it's not impossible,\u201d says Matthew Fisher, an emerging-disease researcher at Imperial College London. \u201cComplacency is not a recommended course of action.\u201d\n\nThe heavens hold plenty of threats. The Sun occasionally launches outsize solar flares, which fry electricity grids by generating intense currents in wires. The most recent solar megastorm, in 1859, sparked fires in telegraph offices; today, a similarly sized storm would knock out satellites and shut down power grids for months or longer. That could cause trillions of dollars in economic damage.\n\nA solar flare some 20 times larger than that may have hit Earth in 774, according to Adrian Melott, a cosmologist at the University of Kansas in Lawrence, and Brian Thomas, an astrophysicist at Washburn University in Topeka, Kansas. \u201cThat's not an extinction event,\u201d says Melott, \u201cbut for a technological civilization, it could kill hundreds of millions of people and set us back 150 years.\u201d Fortunately, there are ways to mitigate this worst-case scenario should it occur: engineers can protect the grid with fail-safes or by turning off the power in the face of an incoming blast.\n\nNext up the scale of disaster magnitude is a large comet or asteroid strike. Sixty-five million years ago, an asteroid 10 kilometres wide hit Earth and triggered the end-Cretaceous mass extinction; 2-kilometre rocks, thought to be capable of causing extinctions on a smaller scale, smack the planet once or twice every million years. Astronomers are hard at work tallying and tracking asteroids in Earth's vicinity, and scientists are investigating ways to divert any real threats that might materialize.\n\nA far rarer danger \u2014 and one that could not be avoided \u2014 is the blast of radiation from a nearby \u03b3-ray burst. Perhaps the most frightening of these celestial explosions is the 'short-hard' \u03b3-ray burst, caused by the violent merger of two black holes, two neutron stars or a combination. If one such blast were directed at Earth from within 200 parsecs away (less than 1% of the distance across the Milky Way), it would zap the globe with enough high-energy photons to wipe out 30% of the atmosphere's protective ozone layer for nearly a decade5. That sort of event \u2014 expected once every 300 million years or so \u2014 would double the amount of ultraviolet (UV) light reaching the ground and scorch phytoplankton, which make up the base of the ocean's food web.\n\nAstronomers have no way of knowing whether such a rare event is imminent. Neutron stars are small and dark, so there is no catalogue of those within striking distance. \u201cWe wouldn't see it coming,\u201d says Thomas. In as-yet-unpublished work, he estimates that such an event could cause a 60% increase in UV damage to crops, with up to 60% reduction in crop yields.\n\nFrom a distance of about 2,000 parsecs, 'long-soft' \u03b3-ray bursts \u2014 which result from the collapse of massive stars \u2014 could also cause extinctions. But these events are rarer than short-hard bursts, and easier to spot in advance because they come from larger, brighter stars. The two-star system WR 104 is some 2,500 parsecs away from Earth, and is far enough along in its life cycle that it is expected to explode some time in the next few hundred thousand years \u2014 although the beam from the burst is unlikely to hit Earth.\n\nIt is possible that a \u03b3-ray blast has hit the planet before. Melott, Thomas and their colleagues have suggested that the mass extinction at the end of the Ordovician period, 440 million years ago, could have been triggered by a \u03b3-ray blast that wiped out some species through UV exposure and killed off others by creating a sunlight-blocking haze of nitrogen dioxide6. This would explain why some species went extinct before the globe cooled during that period, and it fits the extinction pattern, which shows that among marine organisms, the greatest toll was on plankton and other life in the upper part of the ocean.\n\nThomas says that none of these potential disasters is keeping him up at night. He does, however, \u201chave some canned food in the basement\u201d \u2014 a prudent backup in the event of any disaster.\n\nEight thousand years ago, sediments covering an underwater area the size of Scotland slipped from their moorings off the west coast of Norway and raced along the sea floor. The Storegga slide triggered a tsunami that ran at least 20 metres up the nearby Shetland Islands, and probably wiped out some coastal tribes as it clobbered shores around northern Europe. The scar it left on the ocean floor stretches nearly 300 kilometres. \u201cIt's absolutely enormous, and I'm not using the word 'enormous' lightly,\u201d says Peter Talling, a sedimentologist at the University of Southampton, UK, who is leading a project to assess the country's risk of similar slides.\n\nThe United Kingdom is not the only country concerned about giant submarine landslides. \u201cThere are definitely areas that have potential,\u201d says Uri ten Brink, a geophysicist at the US Geological Survey in Woods Hole, Massachusetts, who conducted a 2008 study of possible sources of tsunamis on the US east coast, where some nuclear power plants are within striking distance of such waves. \u201cThere are far larger piles of sediment around today than Storegga ever was,\u201d ten Brink says, including deposits along the coast of southern Alaska and off the Amazon, Niger and Nile river deltas. Smaller slides are more probable and can still have a huge local impact \u2014 and they often strike without warning. In 1998, a relatively small (magnitude-7) earthquake triggered an underwater slide that launched a 15-metre-high tsunami into Papua New Guinea, killing 2,200 people.\n\nResearchers say that it is hard to quantify the threat of marine slides, particularly the giant ones. \u201cThere is so little information about events that happen so rarely,\u201d says ten Brink. \u201cWe just have to learn as much as we can.\u201d", "sentiment": 0.056631888910214034},
{"link_title": "Vim Within Emacs: An Anecdotal Guide", "url": "http://cachestocaches.com/2016/12/vim-within-emacs-anecdotal-guide/", "text": "Who this guide is for: If you've been using Emacs for a while, you should probably try the Vim way of doing thingsThe reverse of this is probably true as well, and if you use Vim I encourage you to give Emacs a try, but that's not the purpose of this guide. . Too many resources online push people in the direction of giving Spacemacs a try; most such guides are designed to encourage Vim users to try out Emacs and have a habit of alienating established Emacs users such as myself. During my efforts to embrace Evil, Emacs' Vim emulator, I hit a few road blocks and decided I would put this article together to help others through them. Whether you'd like to try out evil-mode or you're simply curious to see how the other half do development, keep reading.\n\nThe reverse of this is probably true as well, and if you use Vim I encourage you to give Emacs a try, but that's not the purpose of this guide.\n\nMy first thought in conducting this experiment was just to open my mind and learn something new. In addition, my Emacs pinky occasionally bothers me, and I wanted to try a more ergonomic way to appreciate the editing tools I use on a daily basis. This is a log of my experience, and some code for setting up your files. My verdict after roughly a week:\n\nI really love the way everything came together; the modal system of editing has started to change the way I think about composing text and code. The macro system and a couple of other niceties I'll talk about below blew me away, and I've quickly integrated them into how I work. Finally, Evil is quite popular, so many of the packages I use on a regular basis, like magit and org-mode have extensions to add more vim-like default keybindings.\n\nTo get started, I added to my configuration files, and made it a point to do all of my work for (at least) a few days within the new environment. Full disclosure: I never really got into using Vim with any regularity. At this point, I understand how very basic movement works, relying on the keys to move around the cursor, however that's really the extent of my knowledge within Vim. This means that I have just taken an editor I've loved to work with for nearly two years and crippled my ability to interact with it. How delightfully evil (*cough*).\n\nIn order to use my \"new\" editor, I followed a vim tutorial. It wasn't particularly long, so I'm sure there's a lot I still need to knowI won't cover basic movement or the \"modal\" editing scheme in this guide. If you're serious about following along, I'd recommend you at least try them out before proceeding. . In addition, I found it useful to have a reference card at hand, like this one, so that whenever I found myself forgetting some of the functions I could quickly look it up.\n\nI won't cover basic movement or the \"modal\" editing scheme in this guide. If you're serious about following along, I'd recommend you at least try them out before proceeding.\n\nOne of my primary annoyances is having to reach for the key every time I want to leave insert mode. A colleague of mine recommended remapping the caps-lock key to ESC so it was easier to accessTo do this, I used Karabiner, since I'm on macOS. . This made a huge difference is ease of use, and allowed me to truly embrace the idea of leaving the home row as little as possible.\n\nTo do this, I used Karabiner, since I'm on macOS.\n\nOkay, deleting is a bit of a pain; I still don't quite have the hang of deleting en masse using plus movement, though I'm starting to get better. I haven't yet quite embraced the \"modal\" way of editing, but I keep discovering little shortcut features for navigating that I'm really enjoying. For instance, being able to move to the beginning/end of sentences using and is very convenient while writing.\n\nOne thing which I've found rather infuriating is the lack of support for people who want to transition from using 'vanilla' Emacs to Evil. Following Vim tutorials is only half of the story. Sure they teach you the basic movement commands and some interesting ways of editing commands, but how do I split the window into different buffers and are the commands for horizontal and vertical window splitting. and then how do I switch between them will switch between open windows, which is a really nice feature! ? In addition, how do I open and edit different files at once, and how can I make all of these features play nicely with Helm, an Emacs package? Clearly, it was time for me to edit my configuration files.\n\nand are the commands for horizontal and vertical window splitting.\n\nwill switch between open windows, which is a really nice feature!\n\nI use Helm, Org, and Magit a lotIf you're unfamiliar with these three, I've included a brief description of each with their respective sections below. . Really a lot. They're the tools I've pointed to in the past whenever anyone asks me if they should choose Emacs over Vim, so it was especially important that I get these tools to work properly. Fortunately, with Evil being quite popular and the Spacemacs distribution including a bunch of solutions for smaller issues, getting everything to work in tandem wasn't particularly difficult.\n\nIf you're unfamiliar with these three, I've included a brief description of each with their respective sections below.\n\nHelm is a tool that provides a clean graphical interface for narrowing long lists of commands, files or pretty much anything. Helm overhauls the Emacs interface so that when one is looking for a file, for instance, a list of all files in the current directory appears, can be narrowed using fuzzy search, and each file or folder can be easily opened and exploredI also use the fantastic helm-ag extension, which lets me search my files using the Silver Searcher tool within a buffer. .\n\nI also use the fantastic helm-ag extension, which lets me search my files using the Silver Searcher tool within a buffer.\n\nBefore enabling Evil, I used Helm do navigate all of my files and open buffers, however, the built-in commands don't open Helm. To fix this, I added some key bindings, so that whenever I type or , Helm will open as desired:\n\nUse Helm for Files and Buffers\n\nWhen Evil is enabled, one issue I discovered was that an ugly, blank rectangle would appear at the beginning of any line which was currently selected within Helm. This code, taken from the Spacemacs source provides the solution:\n\nFinally, the movement keys within Helm are still Emacs derivative, and rely on and for cycling through options rather than the more vim-like . Fortunately, Spacemacs again provides a solution:\n\nI've also added an additional binding for to quit Helm, which comes from this StackOverflow question. Note that, to work, these bindings may need to be called from the . With all of these changes implemented, Helm is nicely integrated into my new workflow:\n\nMagit is the most popular Emacs wrapper for Git, the version control system. It's ease of use is phenomenal, and I can't imagine doing my job without it. Getting Vim-like keybindings in Magit was relatively simple, and required only installing the evil-magit package from MELPA, the Emacs package manager, and to it from my init file. Once I installed the package, I could use the standard keys can be used to navigate between entriesThere are a few more changes as well, though none are particularly surprising. See the full list here: evil-magit. and everything felt whole again.\n\nThere are a few more changes as well, though none are particularly surprising. See the full list here: evil-magit.\n\nJust about everything worked the way I would have wanted, except that the window that pops up when making a commit would begin in Vim's command mode, rather than allowing me to immediately insert text (which is its primary function). Furthermore, I wanted to avoid using the Emacs to accept the commit, so I defined some new bindings of my own to apply in that mode.\n\nMy complete configuration can be found on my GitHub.\n\nI use Org for my note-taking, clocking my time, keeping appointments, and organizing my projects. I could discuss it's features for days, but instead, I'll just link to an article I wrote on how I use it to manage much of my life: My Workflow with Org-Agenda.\n\nUnfortunately, unlike Magit and Helm, Org didn't have any out-of-the-box packages which fully satisfied me. Fortunately, I very much enjoy putting together a clean set of key bindingsDon't judge; everyone's got their hobbies. . Editing files is easy enough, since it opens in the normal mode, however still relies on the old Emacs bindings and required some changes; For brevity, I'll omit all of the bindings here, but you can view my additions on GitHub.\n\nToday's the real test: how can I write code using the newer setup. So far, it's not so bad. Right off the bat, it's clear that (for undo) is my new best friend. Ensuring that I can properly undo changes whenever I make a mistake (which happens quite frequently) is important.\n\nSome of the features, like indenting with then a movement command, weren't exactly what I expected, but how these work and I can see how I'll get used to them over time. I'm starting to really embrace the \"modal\" way of doing things, like using the different \"insert\" commands to put the cursor precisely where I want it before entering new text. The navigation is really \"snappy\" too; within Emacs, I'd always get roughly where I wanted to go within a document, but all of the different commands within Vim are much more precise and feel overwhelmingly more natural for moving around quickly (that is, once I'm accustomed to using them). The command, which switches between corresponding parentheses and brackets within my code is a very nice feature. So too is the commandThe \"live\" search and replace functionality is gorgeous: , which allows for regular-expression-based search and replace inline. One of the nicest features of all (include a separate image for the interface in the sidebar) is the \"live\" search and replace feature, which shows the matched search and replace candidates in real time.\n\nThe \"live\" search and replace functionality is gorgeous:\n\nFinally, the most pleasant surprises was Vim's macro system, which, because of the sophistication of the movement commands, allows one to record very complex and versatile functions in real time and bind them to a \"register\" (essentially a key on the keyboard). This is best shown via an example:\n\nI've been editing code for a little while now and I have to say that I'm really beginning to enjoy it and my new workflow is here to stay. Certainly, there are times during which I have to stop and consult a guide or miss some of my old way of doing things, however I'm likely going to keep the new bindings for a little while longer. There's a lot I didn't cover here, but a bit of doing is all one needs to really learn.", "sentiment": 0.1292171717171717},
{"link_title": "Rust application that converts C++ libraries to single self-contained headers", "url": "https://github.com/SuperV1234/unosolo", "text": "This is my first Rust project, mainly created to start getting used to the language. My intention is to improve as I get better with Rust and the final goal is being able to successfully use it on popular libraries.\n\nI also do not encourage people to create single-header libraries and use those in their projects: they're mainly useful when dealing with very complicated build systems or when experimenting on an online compiler that doesn't allow users to easily import multiple files.\n\nContributions and code reviews are welcome!\n\nGiven a set of paths containing the C++ library's header files and a \"top-level include\" file where the graph traversal will start from, outputs a self-contained single-header version of the library to . Here's the auto-generated help:\n\nis currently able to transform , my latest C++17 header-only library, to a single-header version. In fact, I've used to add two badges to 's README that allow users to try the library either on wandbox or on godbolt. This idea was taken from Michael Park's excellent variant implementation: .\n\nThe command used to transform was:\n\nSince 0.1.1, supports multiple library include paths and \"absolute directives\". My library, which depends on , can be transformed to a single header as follows:\n\nA single-header version of can be created using as follows:\n\nI haven't tested it very thorougly, but I compiled the example on 's README without any hiccups.", "sentiment": 0.325187969924812},
{"link_title": "Firejail and Chromium", "url": "http://blog.jdevelop.com/software/linux/2017/09/09/firejail.html", "text": "People tend to think that if they\u2019re using Linux - it makes them invulnerable. Whilst this is true in most of the cases ( the number of viruses, lockers and trojans targeting Linux-based operating systems is infinitesimal if compared to Windows hell ) - still there are some vulnerabilities that could lead to the personal data leaks.\n\nSome people with level of paranoia above average tend to isolate every app into a container, like Docker - and there are plenty of dockerized versions of Google chrome. This approach makes things pretty much secure - its unlikely that some malicious website will breach through the container and get access to the host OS environment. However it is not very handy since it starts the container, connects it to the network and induces various other overheads that are not truly necessary.\n\nThere is another, more lightweight approach to achieve the pretty comfortable level of safety - Firejail. Those who are curious - could find all the details about how that works on the project\u2019s website. I will focus on how to use this stuff.\n\nFirst of all, firejail comes with the pretty good profile for Chromium browser, and you most likely don\u2019t want to tweak it up. So the first attempt to move Chromium into a Firejail sandbox would be to write a simple shell-script, or even create an alias - that will wrap the execution of chromium into the call to firejail.\n\nThis works pretty well - it will start Chromium browser within the firejail sandbox, albeit there is the problem - you won\u2019t be able to open the URL in a browser tab anymore. Second invocation of this script will lead to start of another instance of Chromium, with some conflicts in profiles. The reason is - this script will start another sandbox, which doesn\u2019t know if there\u2019s another instance running. Luckily, firejail allows to attach to another container. So the monified version of the launch script\n\nwill do the trick.\n\nNow using this script it is possible to open an URL from terminal, or integrate with Termite to open links on mouseclick.", "sentiment": 0.14766335814722908},
{"link_title": "Headspace \u2013 Meditation made simple", "url": "https://www.headspace.com/science/meditation-benefits", "text": "A study evaluating the benefits of an in-person mindfulness-based relationship enhancement program suggests that mindfulness enhances couples\u2019 levels of relationship satisfaction, autonomy, closeness and acceptance of each other, while reducing relationship distress.3 In fact, three months after participating in the study, couples were still experiencing these improvements.", "sentiment": -0.125},
{"link_title": "On fexprs and defmacro (2011)", "url": "https://www.brinckerhoff.org/scraps/joe-marshall-on-FEXPRS-and-DEFMACRO.txt", "text": "", "sentiment": 0.0},
{"link_title": "QBE: Embeddable Compiler Back End", "url": "https://c9x.me/compile/", "text": "QBE aims to be a pure C embeddable backend that provides 70% of the performance of advanced compilers in 10% of the code. Its small size serves both its aspirations of correctness and our ability to understand, fix, and improve it. It also serves its users by providing trivial integration and great flexibility.\n\nQBE is known to compile and pass its test suite on the following x64 operating systems.\n\nCompiling QBE requires GNU Make and a C99 compiler. The HTML documentation is generated from regular text files by an OCaml program. Thanks to Ori we also have a continuous build setup.\n\nQBE is in constant change. It is a young project and I still have many ideas to try. The core of it should now work pretty reliably for simple language experiments. To palliate the youth and lack of users, complex parts of the register allocator and the ABI implementation have been thoroughly fuzz tested.\n\nSome smartness is already baked in at this time.\n\nYou might encounter some friction because of these points.\n\nThe intermediate language used as input to QBE is spoken by all the compilation passes and can be dumped anytime in the compiler pipeline using command line options. The example program below can call the C function out of the box, with no special syntax. C compatibility is baked in. Similarly C can call any function compiled with QBE.\n\nToss the above in a file, then run . You should get a binary executable file ready to rock.\n\nTo report a problem with QBE or ask a question: rot13(ohtf@p9k.zr).", "sentiment": 0.11386554621848739},
{"link_title": "Peruvian Amazon Company", "url": "https://en.wikipedia.org/wiki/Peruvian_Amazon_Company", "text": "The Peruvian Amazon Company, also called the Anglo-Peruvian Amazon Rubber Co,[1] was a rubber boom company that operated in Peru in the late 1800s and early 1900s. Based in Iquitos, it became notorious for the ill treatment of its indigenous workers in the Amazon Basin, whom its field forces treated as virtual slaves. The company's practices were exposed in 1913 by the investigative report of British consul-general Roger Casement and an article and book by journalist W.E. Hardenburg.\n\nThe Arana Brothers company, who had sought capital in London, were fused with the PAC in 1907.[1] Julio C\u00e9sar Arana ran the company in Peru.[2] Its British Board of Directors included Sir J. Lister Kaye.[3]\n\nThe company operated in the area of the Putumayo River,[2] a river that flows from the Andes to join the Amazon River deep in the tropical jungle. This area was contested at the time among Peru, Colombia, Ecuador, and it was inhabited by numerous indigenous people.[1]\n\nThe company participated in abuses and criminal actions against laborers in the area, with its overseers using force and even killing to suppress the workers. An investigative report by Roger Casement, British consul, exposed the abuses, embarrassing British members of the company's board. They put pressure on Arana to improve operations. A movement grew to stop the abuse and eventually led to the end of the company. The Anti-Slavery and Aborigines Protection Society was one of the activist groups working to stop the abuses.[3]\n\nW.E. Hardenburg wrote a scathing article in the British magazine Truth. The British government in 1910 sent the consul-general Roger Casement to investigate. His report also denounced the operations of the PAC. A 1912 book by Hardenburg, which contained edited extracts of Casement's report, was described by its editor as \"perhaps the most terrible page in the whole history of commercialism.\"[4] A Select Committee of the House of Commons published a paper on the investigations in 1913. The British Board of Directors was considered not criminally liable under the Slave Trade Acts.[1] However, the Parliament and others moved to tighten up anti-slavery laws. World War I interrupted this work.[1]\n\nAmong the findings by the various investigatory parties were widespread debt bondage, slavery, torture, mutilation, and many other crimes in the Amazonian Rubber industry, with the Putumayo area being but one example. Religious leaders such as Manuel Polit, Bishop of Cuenca in Ecuador, denounced these activities and worked to reform the system.[1] Organizations such as the Sociedad Pro-Indigena also worked to improve conditions for indigenous workers. The area governments also attempted to implement measures to control the abuses, but it was difficult in the large, sparsely populated countryside, which had few road connections to major cities.[1]\n\nArana was appointed liquidator in September 1911,[5][6] and the company was forced into closure by a judge in 1913.[2] After the company was closed, the receiver stated that the shareholders would receive nothing, and the creditors would receive small amounts. He blamed the downfall on the British directors,[7] the Chairman of whom was known as J. Russell Gubbins.[5]", "sentiment": -0.0832792207792208},
{"link_title": "Eight 500 Startup's Making a Major Impact on the Retail Industry \u2013 Bizimply", "url": "https://www.bizimply.com/blog/eight-500-startups-making-a-major-impact-on-the-retail-industry/", "text": "We are very proud at Bizimply to be a part of the 500 Startup community and have really appreciated their continuous support. For readers who don\u2019t already know, 500 Startups is an early-stage venture fund and seed accelerator founded by Dave McClure and Christine Tsai in 2010.\n\nFor this blog post, we decided to take a look at our fellow 500 Startup companies who help retail organisations to solve issues that they tackle on a daily basis. As you can imagine, there are a lot of companies that are in the 500 startup community so it was quite difficult to narrow it down to just eight. It took us a while but we got there in the end! Here are our favourites, we hope you enjoy reading!\n\nStorefront provides pop-up shops to help retailers to build their brand, create awareness and increase sales. They have pop-up shops available throughout the world so all you need to do is choose a location, find a space that meets your needs and then launch it with the help of Storefront. Don\u2019t worry if you find it hard to find the right space, Storefront will provide you with a personal concierge to assist you!\n\nFounded in 2013, StoreHub is an iPad POS system which helps you to run your business at your fingertips. Their POS system gives you valuable insights into your best selling products through real-time reports and tells you which items are underperforming to free up your cashflow. Another great feature it has is automatic low stock alerts to ensure you never run out of your top sellers.\n\nBrandtrack is a company which allows you to customise the music you play in your stores to match your brand characteristics and customer profile. They have a team of expert music professionals who select the songs that best apply to your preferred in-store emotional experience. All you have to do is download and install the app, play the music and control it! You can even include advertisements in your playlists to promote products.\n\nFounded by Sean Rucker and John Coombs, Rover has a platform which features a full suite of tools allowing you to drive mobile engagement and monetization using location. Rover makes it easy to manage all of your beacons, geofences and configurations. With features like beacon grouping, fast search, bulk editing and smart tagging, it\u2019s easy to manage your location infrastructure at scale. With their app, they make it easy to send location-triggered, scheduled and instant messages so you can target your customers at the right time and place.\n\nFounded by Devon Wright and Chris Gilpin, TurnStyle is a Wi-Fi marketing platform that allows brands and marketers to connect with their customers, resulting in increased sales, loyalty and insights into consumer behaviour. The customer just needs to connect to your Wifi hotspot and is then redirected to your log-in page where they can join via their social media profile, phone number or email. Retailers can then send real-time messages, rewards and coupons directly to customer\u2019s phones without them needing to download an app.\n\nWiser gives retailers up-to-date information on pricing to help you to sell the right product, at the right time, at the right price. You can monitor your competitors\u2019 prices so that you can always stay one step ahead and offer your customers better deals. It allows you to make better decisions faster by visualising the impact of pricing changes and quickly gain insight into changes in key metrics like revenue, conversion rate, profitability, and more.\n\nfulfil.io allows your customers to shop for your products from any platform, whether they wish to shop in store, on Amazon, eBay or wherever the case may be. You as a retailer can then manage all of these orders from one platform, making packaging and shipping a seamless process. Once an order comes through, fulfil.io will automatically update the inventory levels on every channel.\n\nFounded by Kate Hiscox, Marco Sylvestre and Xin Huang, Venzee is a company which enables suppliers and manufacturers to update their product information and then immediately notify you, as the retailer of the changes they have made. They provide a quality scoring which encourages vendors to improve the product information they send you. They also have system friendly imports for vendors to adapt their information to meet the retailers\u2019 requirements.\n\nAs we said before, Bizimply is also part of the 500 Startup community. If you\u2019d like to learn a bit more about us just click the link below! As always, thanks for reading! \ud83d\ude42", "sentiment": 0.28558531746031746},
{"link_title": "Lyft to unleash self-driving cars on Bay Area roads", "url": "http://www.siliconvalley.com/2017/09/07/lyft-unleash-self-driving-cars-bay-area-roads/", "text": "SAN FRANCISCO \u2014 Self-driving Lyfts are coming to the Bay Area, promising to give local residents a first-hand look at the technology that\u2019s poised to dramatically change the nature of transportation.\n\nLyft will launch a pilot fleet of self-driving cars to pick up local passengers, the San Francisco-based ride-hailing company said Thursday, though it didn\u2019t reveal an exact start date. The announcement comes as Lyft is accelerating its efforts to dominate the self-driving car market and compete with the likes of Uber, Google and Tesla \u2014 Lyft recently opened a new autonomous vehicle hub in Palo Alto and earlier this summer announced plans to bring self-driving cars to Boston.\n\nNow Lyft plans to unleash autonomous vehicles on busy Bay Area roads, where they will have to interact with local drivers, bicyclists and pedestrians, representing a crucial test of the technology. Lyft has teamed up with another startup \u2014 Drive.ai \u2014 which already has obtained the necessary permits from state regulators to test self-driving cars on public roads. The partnership is a signal that Lyft learned its lesson from the failure of competitor Uber\u2019s self-driving car launch in San Francisco last year.\n\nMountain View-based Drive.ai, born from Stanford University\u2019s Artificial Intelligence Lab, has been testing its self-driving cars on California roads for more than a year.\n\n\u201cThe majority of the world hasn\u2019t seen autonomous vehicles,\u201d said Drive.ai co-founder and president Carol Reiley. \u201cFor them to have a chance to actually ride in an AV car is just so exciting. We really want to start making this a reality and start bringing this to real customers \u2014 this isn\u2019t just a demo.\u201d\n\nLyft customers will be able to opt-in to the pilot program, and when they use the Lyft app to summon a ride, they might be matched with an autonomous car free of charge. A safety driver will be behind the wheel, ready to take control if needed, Reiley said.\n\nThe cars, which will belong to Drive.ai \u2014 not Lyft \u2014 will be four-door sedans outfitted with Drive.ai\u2019s self-driving sensors and software. Neither Reiley nor Lyft would say where in the Bay Area the cars will be available.\n\nBecause Drive.ai already has the California permit, Lyft may avoid repeating the fiasco that was Uber\u2019s San Francisco autonomous vehicle launch. Uber rolled out a fleet of self-driving cars in December, but the ride-hailing giant refused to apply for the $150 testing permit. After a contentious, week-long stand-off, the California Department of Motor Vehicles pulled the Uber cars\u2019 registrations and forced them off the road.\n\nUber finally backed down in March and agreed to apply for the permit, which the DMV granted. Now Uber\u2019s cars are back on the streets of San Francisco \u2014 but not picking up passengers. The high-profile fight helped cement Uber\u2019s reputation as a bull-headed rule-breaker.\n\nLyft doesn\u2019t plan to apply for a permit either, but by teaming up with a partner that already has one, Lyft is signaling yet again that it\u2019s the \u201cgood guy\u201d in the ride-hailing field. Even prior to Thursday\u2019s announcement, Lyft has taken advantage of a recent string of scandals plaguing Uber \u2014 including allegations of sexual harassment and other inappropriate behavior at the office, a lawsuit accusing Uber of stealing rival Waymo\u2019s self-driving car technology, and a criminal probe into Uber\u2019s use of a software tool to evade law enforcement stings. Meanwhile, Lyft is touting its feel-good policies \u2014 such as a program that lets passengers round-up their fares and donate that money to charity. And Lyft is growing quickly. The smaller ride-hailing company last week announced its platform is live across 40 states.\n\nBut Uber has a head start in the self-driving car market. Uber has been picking up passengers in autonomous vehicles in Pittsburgh since 2016, and earlier this year launched a similar pilot in Arizona. Lyft has yet to launch its own self-driving fleet, though in June the startup announced plans to bring autonomous cars to Boston via a partnership with nuTonomy. A Lyft spokeswoman on Wednesday said the company remains on track to launch the Boston pilot in the coming months.\n\nThese pilot programs bring Lyft one step closer to reaching its ultimate goal of a driverless future. In 2016, Lyft President John Zimmer predicted that autonomous vehicles will account for the majority of Lyft rides within five years. And getting real people to test the technology plays a major role in facilitating its wide-spread deployment, said Bryant Walker Smith, a Stanford Law School researcher and self-driving car expert.\n\n\u201cIt\u2019s a very important step, to invite the public into these vehicles,\u201d he said. \u201cPeople tend to feel more favorable about automated driving once they\u2019ve experienced these types of demonstrations.\u201d", "sentiment": 0.14402230639730643},
{"link_title": "Geneticists pan paper that claims to predict a person's face from their DNA", "url": "http://www.nature.com/news/geneticists-pan-paper-that-claims-to-predict-a-person-s-face-from-their-dna-1.22580", "text": "A storm of criticism has rained down a paper by genome-sequencing pioneer Craig Venter that claims to predict people\u2019s physical traits from their DNA. Reviewers and even a co-author of the paper say that it overstates the ability to use a person\u2019s genes to identify the individual, which could raise unnecessary fears about genetic privacy.\n\nIn the paper1, published on 5 September in the Proceedings of the National Academy of Sciences (PNAS), Venter and colleagues at his company Human Longevity, Inc. (HLI), based in San Diego, California, sequenced the whole genomes of 1,061 people of varying ages and ethnic backgrounds. Using the genetic data, along with high-quality 3D photographs of the participants\u2019 faces, the researchers used an artificial intelligence approach to find small differences in DNA sequences, called SNPs, associated with facial features such as cheekbone height. The team also searched for SNPs that correlated with factors including a person\u2019s height, weight, age, vocal characteristics and skin colour.\n\nThe approach correctly identified an individual out of a group of ten people randomly selected from HLI\u2019s database 74% of the time. The findings, according to the paper, suggest that law-enforcement agencies, scientists and others who handle human genomes should protect the data carefully to prevent people from being identified by their DNA alone. \u201cA core belief from the HLI researchers is that there is now no such thing as true deidentification and full privacy in publicly accessible databases,\u201d HLI said in a statement.\n\nBut other geneticists, having studied the paper, say that in their opinion, the claim is vastly overblown. \u201cI don't think this paper raises those risks, because they haven\u2019t demonstrated any ability to individuate this person from DNA,\u201d says Mark Shriver, an anthropologist at Pennsylvania State University in University Park. In a randomly selected group of ten people \u2014 especially one chosen from a data set as small and diverse as HLI\u2019s \u2014 knowing age, sex and race alone rules out most of the individuals, he says.\n\nTo demonstrate this, computational biologist Yaniv Erlich of Columbia University in New York City looked at the age, sex and ethnicity data from HLI\u2019s paper. In a study2 published 6 September on the preprint server bioRxiv, he calculated that knowing only those three traits was sufficient to identify an individual out of a group of ten people in the HLI data set 75% of the time. Erlich contends that there was no need to know anything about the people\u2019s genomes. Furthermore, he says, HLI\u2019s reconstructions of facial structure from SNPs are not highly specific \u2014 they tend to look as much like an individual as anyone of that person\u2019s sex and race.\n\nBefore it was published in PNAS, the paper had been submitted to Science, says Shriver who reviewed the paper for that journal. He says that HLI\u2019s actual data are sound, and he is impressed with the group\u2019s novel method of determining age by sequencing the ends of chromosomes, which shorten over time. But he says that the paper doesn\u2019t demonstrate that individuals can be identified by their DNA, as it claims to. \u201cI think it totally misrepresents what they did and what they found,\u201d he says.\n\nHLI said that its paper states that using multiple parameters, of which a person's face is only one, to identify someone is possible based on work with over a thousand genomes. \u201cIt heralds that prediction will become increasingly precise,\u201d says Heather Kowalski, an HLI spokesperson. HLI stated that it stands by its methodology and acknowledged that the sample set was small. The company added that \u201cthe HLI team is working on rebuttal to criticisms by Yaniv in BioRxiv [sic]\u201d.\n\nShriver says that he and Erlich pointed out their concerns to the study authors in their reviews of the paper for Science. Both Shriver and Erlich say that the journal ultimately rejected the paper. (Science does not comment on unpublished studies.) The paper was then submitted to PNAS under an option that allows a member of the US National Academies of Science, Engineering, and Medicine, such as Venter, to choose the reviewers. Two of them are information-privacy experts and the remaining reviewer is a bioethicist.\n\nPNAS confirmed that Venter chose all three reviewers for the study. HLI declined to comment on the PNAS review process for the paper.\n\nJason Piper, a computational biologist and a paper co-author who now works at Apple in Singapore, agrees that the paper misrepresents the findings that he and the other co-authors produced. Piper adds that his contract with the company waived his right to approve the manuscript before it was submitted, allowing HLI to present his data however it saw fit. HLI responded to that by confirming that \u201cauthors were given an opportunity to review and comment on the paper\u201d.\n\nPiper has since criticized the paper heavily on Twitter and says that, in his opinion, HLI has a potential conflict of interest in encouraging restricted access to DNA databases. HLI, a for-profit company, is attempting to build the world\u2019s largest database of human genetic information.\n\n\u201cI think genetic privacy is very important, but the approach being taken is the wrong one,\u201d Piper says. \u201cIn order to get more information out of the genome, people have to share.\u201d A more useful approach, he says, would be to find a way to make genomic data public without allowing individuals to be identified.\n\nIn response to criticisms of the paper, the company responded with a statement saying that \u201cHLI stands by the protection of genome data and the promotion of modern solutions for data exchange\u201d. They added that the paper was intended to spur discussion about how to share genetic information while protecting a person's privacy.\n\nStill, Erlich is concerned that Venter\u2019s stature gives the paper extra weight in the eyes of policymakers, who may become overly concerned about DNA privacy. \u201cNew rules and regulations are based on papers like that,\u201d he says. \u201cIt\u2019s important when we deal with privacy risks to get the facts right.\u201d", "sentiment": 0.0606812545093795},
{"link_title": "The better way to learn a new programming language", "url": "https://codewithoutrules.com/2017/09/09/learn-a-new-programming-language/", "text": "Have you ever failed to learn a new programming language in your spare time? You pick a small project to implement, get a few functions written\u2026 and then you run out of time and motivation. So you give up, at least until the next time you give it a try.\n\nThere\u2019s a better way to learn new programming languages, a method that I\u2019ve applied multiple times. Where starting a side project often ends in failure and little knowledge gained, this method starts with success. For example, the last time I did this was with Ruby: I started by publishing a whole new Ruby Gem, and getting a bug fix accepted into the Sinatra framework.\n\nIn this post I will:\n\nCreating a new software project from scratch in your spare time is a tempting way to learn a new language. You get to build something new: building stuff is fun. You get to pick your language: you have the freedom to choose.\n\nUnfortunately, learning via a side project is a difficult way to learn a new language. When you\u2019re learning a new programming language you need to learn:\n\nThis is a huge amount of knowledge, and you\u2019re doing so with multiple handicaps:\n\nLearning on your own: You have to figure out everything on your own.\n\nBlank slate: You\u2019re starting from scratch. Quite often there\u2019s no scaffolding to help you, no good starting point to get you going.\n\nSimultaneous learning: You are trying to learn everything in the list above at the same time.\n\nLimited time: You\u2019re doing this in your spare time, so you may have only limited amounts of spare time to apply to the task.\n\nLack of motivation: If you care about the side project\u2019s success, you probably will be motivated to switch back to a language you know. If you just care about learning the language, you\u2019ll be less motivated to do all the boring work to make the side project succeed.\n\nVague goals: \u201cLearning a language\u201d is an open-ended task, since there\u2019s always more to learn. How will you know you\u2019ve achieved something?\n\nPersonally I have very limited free time: I can\u2019t start a new side project in a language I already know, let alone a new one. But I do occasionally learn a new language.\n\nRather than learning new languages at home, I use a better method: learning a language by solving problems at my job.\n\nFor example, I know very little Ruby, and when I started learning it I knew even less. One day, however, I joined a company that was publishing a SDK in multiple languages, one of which was Ruby.\n\nMy first task involving Ruby was integrating the SDK with popular Ruby HTTP clients and servers. Which is to say, I started learning a new language with a specific goal, motivation, and time to learn at work. Much better than a personal side project!\n\nI started by learning one thing, not multiple things simultaneously: which 3rd party HTTP libraries were popular. Once I\u2019d found the popular HTTP clients and servers, my next task was implementing the SDK integration. One integration was with Sinatra, a popular Ruby HTTP server framework.\n\nAs a coding task this was pretty simple:\n\nI learned just enough to implement the middleware: 40 lines of trivial code.\n\nNext I needed to package the middleware as a gem, Ruby\u2019s packaging format. Once again, I was only working on a single task, a well-documented task with many examples. And I had motivation, specific goals, examples to build off of, and the time to do it.\n\nAt this point I\u2019d learned: a tiny bit of syntax and semantics, some 3rd party libraries, packaging, and a little bit of the toolchain.\n\nShortly after creating our SDK integration I discovered a bug in Sinatra: Sinatra middleware was only initialized after the first request. So I tracked down the bug in Sinatra\u2026 which gave me an opportunity to learn more of the language\u2019s syntax, semantics, and idioms by reading a real-world existing code base. And, of course, the all-important skill of knowing how to add debug print statements to the code.\n\nReading code is a lot easier than writing code. And since Sinatra was a pre-existing code base, I could rely on pre-existing tests as examples when I wrote a test for my patch. I didn\u2019t need to figure out how to structure a large project, or every edge case of the syntax that wasn\u2019t relevant to the bug. I had a specific goal, and I learned just enough to reach it.\n\nAt the end of the process above I still couldn\u2019t start a Ruby project from scratch, or write more than tiny amounts of Ruby. And I haven\u2019t done much with it since. But I do know enough to deal with packaging, and if I ever started writing Ruby again I\u2019d start with a lot more knowledge of the toolchain, to the point where I\u2019d be able to focus purely on syntax and semantics.\n\nBut I\u2019ve used a similar method to learn other languages to a much greater extent: I learned C++ by joining a company that used it, and I became a pretty fluent C++ programmer for a while.\n\nHow should you learn a new programming language? As in my story above, the best way to do so is at work, and ideally by joining an existing project.\n\nThe easiest way to learn a new language is to join an existing project or company that uses a language you don\u2019t know. None of the problems you would have with a side project apply:\n\nLacking an existing project to join, look out for opportunities where there\u2019s a strong motivation for your project to add a new language. Some examples:\n\nStarting a new project is not quite as easy a learning experience, unfortunately. But you\u2019re still starting with specific goals in mind, and with time at work to learn the language.\n\nMake sure to limit yourself to only learning one thing at a time. In my example above I sequentially learned about: which 3rd party libraries existed, the API for one library, writing miniscule amounts of trivial integration code, packaging, and then how to read a lot more syntax and semantics. If you\u2019re doing this with co-workers you can split up tasks: you do the packaging while your co-worker builds the first prototype, and then you can teach each other what you\u2019ve learned.\n\nMore broadly, your job is a wonderful place to learn. Every task you do at work involves skills, skills you can practice and improve. You can get better at debugging, or notice a repetitive task and automate it, or learn how to write better bug reports. Perhaps you could figure out what needs changing so you can get make changes done faster (processes? architecture? APIs?). Maybe you can figure out how to test your code better to reduce the number of bugs you ship. And if that\u2019s not enough, Julia Evans has even more ideas.\n\nIn all these cases you\u2019ll have motivation, specific goals, time, and often an existing code base to build off of. And best of all, you\u2019ll be able to learn while you\u2019re getting paid.", "sentiment": 0.1096602350800112},
{"link_title": "Dear Amazon, We Picked Your New Headquarters for You", "url": "https://www.nytimes.com/interactive/2017/09/09/upshot/where-should-amazon-new-headquarters-be.html?_r=0", "text": "The company announced this week that it was looking for a metropolitan area in North America with at least a million people, so we\u2019ve started with the map above. (With apologies to Canada, we\u2019ve set aside Toronto and several other large cities because they\u2019re not included in most of the data sets we\u2019ve used to determine which places meet Amazon\u2019s needs.)\n\nAreas with at least one million people where job growth is strong \u2026\n\n\u2026 and the right labor pool is large and growing \u2026\n\nIn these metro areas, more than one in eight workers is in an industry related to tech, science or professional services, according to the census. (That figure is one in five in Raleigh, N.C.; San Francisco; San Jose, Calif.; and Washington). And the segment of the labor pool that Amazon is particularly interested in \u2014 software programmers and designers \u2014 is growing rapidly, according to an analysis of tech jobs by the Brookings Institution. For its recruiting, Amazon also says it requires a strong university system nearby. All of these metro areas offer that, with colleges that include computer science degrees.\n\n\u2026 and the quality of life is high \u2026\n\n\u201cQuality of life\u201d in this context is primarily about two things \u2014 housing costs and amenities \u2014 and striking a balance between them. The Bay Area does a poor job of that, given that its high cost of living is now driving away even tech workers with six-figure salaries. New York loses out because of high housing costs, too, as measured by median rents in census data. Boston is relatively expensive, but we\u2019re keeping it in the running because its tech job pool is so good. Washington is also expensive, but more affordable Baltimore is a commuter rail line away. And Jeff Bezos, for one, has a nice quality of life in D.C. \u2014 he bought a $23 million home there this year.\n\nAs for the amenities, the winning region will also have the restaurants, outdoor recreation, cultural attractions and general cool of Amazon\u2019s first home, Seattle. Urban economists suggest that such amenities are important to explaining the allure of cities. We asked the economist David Albouy to rank these metro areas for us with an index he uses to measure how much people would be willing to sacrifice, in terms of housing costs and commutes, to live in desirable places. On that basis, we cut Charlotte, N.C., and Indianapolis, because they rank lower on the cultural edginess that attracts young, educated workers.\n\n\u2026 and workers can easily get around \u2014 and out of town \u2026\n\nAn Amazon priority is mass transit, and it has asked applicants to provide their traffic congestion rankings during peak commuting hours. These remaining metro areas are among the top 15 in the country in the share of workers who commute by transit, according to the American Community Survey. Gone are those with both weak transit and bad congestion rankings according to the company INRIX: Atlanta, Miami, Dallas and Austin.\n\nAmazon could easily sort through all the criteria above by itself (the company knows data, after all). But there are two questions it can\u2019t answer without information from potential host cities: What are they willing to give the company in incentives and tax breaks? And what real estate is on the table? This leads us to our winner:\n\n\u2026 and there is space and a willingness to pay to play.\n\nIt\u2019s hard to imagine where the Boston region would find the room for a company that will ultimately want up to eight million square feet of office space (the Pentagon, for comparison, has 6.6 million). Mayor Marty Walsh also said on Thursday that Boston is \u201cnot going to get into a bidding war with another city over something like this.\u201d And it\u2019s pretty clear that a bidding war is what Amazon wants.", "sentiment": 0.09756344230482161},
{"link_title": "Should the world eat more like the Cantonese?", "url": "http://www.bbc.com/future/story/20170612-should-the-world-eat-more-like-the-cantonese", "text": "Sitting down in this small Hong Kong restaurant, I assume that the white chest of drawers behind me are filled with tea leaves, herbs, and fungi. So I\u2019m rather perturbed when my guide Cecilia Leung tells me that they are not filled with dried plant life \u2013 but live snakes.\n\nIf the owner were here, Cecilia says, he would happily bring one out for me to inspect. Indeed, the \u201csnake king\u2019s\u201d talents are so famous that he is sometimes called out in the middle of the night to capture and relocate venemous specimens blocking public rights of way.\n\nLuckily for me, the only serpent I see is skinned, sliced, and served up in a thick, gravy-like broth with pork, chicken, mushrooms, and lemon grass. The snake meat itself is greyish with a slightly pink blush \u2013 and what appear to be the imprint of its scales still marking its delicate surface.\n\nThe rich fumes of the broth are undoubtedly appetising, but I let my spoon hover nervously over the edge of the bowl \u2013 not quite sure whether I dare to dig in. Intellectually, I know that snake meat is really no different from eating fish, or squid, or chicken. And yet my brain can\u2019t quite seem to communicate that message to my mouth or to my stomach.\n\nI\u2019m in Hong Kong on a reporting assignment \u2013 and part of my mission is to understand the territory\u2019s rich culinary history. But as a science journalist, I\u2019m also interested in what experiences can tell me about the psychology of eating, including the somewhat arbitrary mental barriers that determine what we will and won\u2019t eat \u2013 and the surprising benefits of overcoming those boundaries.\n\nAs I hesitate, Cecilia tells me that about half the people she brings on this tour are willing to overcome their aversion and eat the snake. Will I be one of them?\n\nI first meet Cecilia and her sister Silvana at a bakery and caf\u00e9 in the working-class district of Sham Shui Po. Together, they run Hong Kong Foodie Tours, and over breakfast they offer me a crash course in the history and culture of Hong Kong cuisine. \u201cI hope you\u2019ve got a big appetite,\u201d Silvana asks me, before ordering me a huge pineapple bun, made from a sweet, airy dough and covered in a cracked yellow topping. It is served with a mug of steaming Hong Kong-style milk tea, made by repeatedly drawing water through a huge \u201cstocking\u201d filled with tea leaves, which helps release the tannins and the caffeine for a \u2018smoother\u2019 flavour, and mixed evaporated milk.\n\nThe Hong Kong government recently included the meal as part of the territory\u2019s \u201cintangible cultural heritage\u201d, and it clearly betrays the influence of British colonialism and its status as one of the world\u2019s most important ports. Along similar lines, the caf\u00e9\u2019s menu also includes a \u201cmacaroni soup\u201d and \u201cSwiss wings\u201d \u2013 chicken coated in a sticky brown liquid that looks suspiciously like chocolate, but which is really a sweet soy sauce. \u201cBecause it was a colonial city, we borrowed and adapted a lot of our foods from cultures,\u201d Cecilia tells me. It is for the same reason that the locals will happily cover their traditional dim sum in Worcester sauce.\n\nWe next stop for a bowl of wonton noodle soup, topped with shrimp roe. Many of the wonton restaurants came via mass migration from mainland China many years ago, Cecilia and Silvana tell me \u2013 but like those Western dishes, the locals have now made it their own. \u201cThese people, when they came here, they had no skills, they were not educated \u2013 but they could make food for a living,\u201d explains Silvana. For lunch, we taste the city\u2019s famous roast meats, dripping with a glaze that turns my rice gold in the bowl \u2013 with a few detours into the markets and dried seafood shops selling shark fins and fish bladders eaten at important banquets.\n\nMany of the local foods are thought to have medicinal value, and along the way I stop off at a stall to buy a cup of kuding, or \u201cbitter nail tea\u201d. According to traditionalists, it is meant to disperse excess fire in the body, soothing the digestion, and sharpening the mind. I found its bitterness to be surprisingly invigorating \u2013 like a square of dark chocolate \u2013 though it is not always a hit with Cecilia\u2019s tour groups. \u201cYou\u2019re the first foreigner who\u2019s ever told me that it\u2019s refreshing!\u201d she tells me.\n\nMy first real test comes a little later with a bowl of a black jelly called guilinggao, made from ground-up turtle shells, which is meant to reduce acne and improve circulation. I had been too nervous to try it on my first trip to Hong Kong, but like my cup of kuding, it is pleasingly bitter, and slides soothingly across the tongue and down the throat. As I leave, I notice an Uber Eats sign at their entrance, offering home delivery \u2013 a sign of the ways that even the most traditional foods are being incorporated into the modern lifestyle.\n\nBy the early afternoon, I\u2019m already beginning to feel that I\u2019ve at least caught a glimpse of the amazing fusion of culinary influences that have shaped Hong Kong\u2019s diet today. And it seems to serve them well: with its emphasis on balance and moderation, the territory has one of the highest life expectancies in the world.\n\nBut I\u2019m intrigued to hear how new expats, unused to living in such a rich melting pot of cultures, adapt to the choice. Do they all learn to take advantage of the foods on offer? Cecilia says that most people tend to fall to the two extremes: she even knows one couple where the wife will eat everything, and the husband will try nothing new.\n\nLooking into the scientific literature, I found that psychologists describe those two \u201cfood personalities\u201d as neophilia and neophobia. Studies have shown that neophobics exhibit significant signs of stress when facing unfamiliar foods, including increased pulse, fast breathing, and increased skin conductance from sweating.\n\nBy influencing the variety of our diet, that fear of new foods may damage our overall health and wellbeing. Neophobics are more likely to be overweight, for instance, perhaps because they tend to opt for blander, more calorific foods. They also tend to show deficits in key nutrients, including proteins, monounsaturated fats, and minerals like magnesium.\n\nSuch preferences may partly be down to our genes. Food neophobes tend to be more sensitive to a particular chemical, phenylthiocarbamide, that gives many foods their bitter tastes (and which, in nature, may have been a way of judging a plant\u2019s toxicity). Such variation may explain why I find the kuding, guilinggao and stir-fried bitter melon to be pleasantly refreshing, while other visitors claim that the flavours are over-powering to the point of being revolting.\n\nBut our food personalities may also be determined by a range of psychological factors. One study by Laith Al Shawaf, then at the University of Texas in Austin, found that food neophobic people seem to be more fearful of diseases and pests in general \u2013 suggesting it may be part of an overall heightened disgust response. An evolved fear of infection may be the reason that we are particularly suspicious of new types of meat, given that they may carry the greatest risk of food poisoning, he says.\n\nIntriguingly, Al Shawaf found that food neophobia also seemed to correlate with self-reported disgust in other domains \u2013 including sex \u2013 and it even appeared to be related to the kinds of relationships they preferred. \u201cThose who are more oriented toward short-term mating and casual sex tend to be more food neophilic than those who are more inclined toward monogamy and committed mating,\u201d says Al Shawaf, who is now based at Bilkent University in Turkey. Al Shawaf therefore wonders if a tendency to try new foods may also be a way of demonstrating the fact that we have a robust immune system, capable of dealing with the potential pathogens that may come from eating new foods.\n\nWhatever our current food personality, he thinks we can all learn to overcome our disgust response to some degree, through increased exposure to new tastes and textures. \u201cThere\u2019s no reason to expect food neophobia to be set in stone,\u201d he says. \u201cIt can change across the lifespan, from context to context, and as a function of mood and physiological state.\u201d One study, from Phan Hong at the University of Wisconsin, found that adopting a deliberately attentive, mindful approach (akin to meditation) is best \u2013 apparently, the act of calmly observing and savouring the unfamiliar experience helps us to override the initial aversion.\n\nI experience this myself, once we reach the snake soup restaurant at Tai Po Market in the New Territories. In England, we have been conditioned to fear snakes, and the sight of its scaly flesh swimming in a swampy, viscous broth, would have once been hard for me to stomach. But after my day\u2019s whistle-stop tour through Hong Kong\u2019s culinary highlights, I decide to take my first spoonful. Far from being a trial to eat, the soup felt like a perfect, warming comfort food, with the snake itself offering a subtle fishy flavour, offset by the citric bite of the lemongrass. The locals believe it can improve circulation and fend off illness.\n\nIt\u2019s our last stop before our final destination \u2013 a restaurant on the top floor of the Tai Po wet market. Cecilia tells me that fresh, lightly seasoned seafood is the pride of the region, and so we order a gently steamed fish in black bean sauce, deep fried squid balls, and glutinous rice with a fresh crab. The art is in the precision of the steaming, Cecilia tells me. \u201cIf you leave it for just one minute too long it becomes a bit too tough.\u201d Since the Cantonese word for \u201cfish\u201d sounds similar to the word for \u201cleftovers\u201d, the dish is often eaten at New Year\u2019s celebrations to signify abundance and prosperity, she tells me.\n\nBy the time we\u2019re finished, I\u2019m burning to explore more of Hong Kong\u2019s hotspots on my own. Cecilia, who is a keen traveller herself, compares the process to learning a second language; it can take a while to adjust your mind to the new way of thinking, but once you have begun to absorb its vocabulary, there\u2019s really no better way of getting to know a culture. \u201cIt\u2019s really getting to the soul of who we are.\u201d\n\nDavid Robson is a freelance writer. He is @d_a_robson on Twitter.\n\n\n\n Join 800,000+ Future fans by liking us on Facebook, or follow us on Twitter.\n\nIf you liked this story, sign up for the weekly bbc.com features newsletter, called \u201cIf You Only Read 6 Things This Week\u201d. A handpicked selection of stories from BBC Future, Earth, Culture, Capital, and Travel, delivered to your inbox every Friday.", "sentiment": 0.1343855218855219},
{"link_title": "Show HN: Mif_gen Matlab utility for Altera mems init", "url": "http://fpgasite.blogspot.com/2017/09/mifgen-matlab-utility.html", "text": "In this tutorial we will see how to design and test a VHDL component. We will start with a very simple block and gradually add features to it. We will also simulate it and test its output with Matlab. Over the process we will see: How to start with a simple block and gradually add features and improvementsHow to add a test bench (simulation)Adding parameters to a VHDL componentSaving the component data output to files (from simulation)Importing the files to Matlab in order to:Verify the results, andAnalyze the results (in this case, using FFT).The tutorial comprises three chapters, and it is divided into three entries of this blog. Make sure that you haven't missed to visit part 2 and part 3 of the tutorial!For this tutorial it is assumed that you already have basic knowledge of the VHDL language and know how to use simulation tools (We will use the Xilinx's Vivado built in simulator, but you can easily adapt the tutorial to other tools you may be familiar with).Chapter 1 - \u2026", "sentiment": 0.16904761904761906},
{"link_title": "100% typesafe ORM for golang (object-first) based on GORM", "url": "https://github.com/jirfag/go-queryset", "text": "100% type-safe ORM for Go (Golang) with code generation and MySQL, PostgreSQL, Sqlite3, SQL Server support. GORM under the hood.\n\nImagine you have model in your file:\n\nNow transform it by adding comments for query set generation:\n\nAnd you will get file in the same directory (and package) as .\n\nIn this autogenerated file you will find a lot of autogenerated typesafe methods like these:\n\nNow you can use this queryset for creating/reading/updating/deleting. Let's take a lot at these operations.\n\nYou can not embed into your model (e.g. if you don't need field), but you must use to properly work. Don't worry if you don't use GORM yet, it's easy to create :\n\nIf you already use another ORM or raw , you can reuse your object (to reuse connections pool):\n\nUnder the hood method just calls .\n\nIt's the most powerful feature of query set. Let's execute some queries:\n\nIt generates this SQL request for MySQL:\n\nfiltering is added by GORM (soft-delete), to disable it use .\n\nIn this example we will define custom method on generated for later reuse in multiple functions:\n\nGoqueryset generates DB names for struct fields into variable. In this example we used .\n\nAnd this code generates next SQL:\n\nSometimes we don't have model object or we are updating multiple rows in DB. For these cases there is another typesafe interface:\n\nGolang >= 1.7 is required. Tested on go 1.7, 1.8, 1.9 versions by Travis CI\n\nI like GORM: it's the best ORM for golang, it has fantastic documentation, but as a Golang developers team lead I can point out some troubles with it:\n\nI didn't see any ORM that properly handles code duplication. GORM is the best with support, but even it's far from ideal. E.g. we have GORM and next typical code:\n\nAt one moment PM asks us to implement new function, returning list of users registered today AND sorted by rating. Copy-paste way is to add to . But it leads to typical copy-paste troubles: when we change rating calculation logics (e.g. to ) we must change it in two places.\n\nHow to solve it? First idea is to make reusable functions:\n\nWe can use GORM Scopes to improve how it looks:\n\nLooks nice, but we loosed ability to parametrize our reusable GORM queries (scopes): they must have only one argument of type . It means that we must move out from them (let's say we get it from user). If we need to implement query - we can't do it.\n\nNow compare it with go-queryset solution:\n\nUserQuerySet is an autogenerated struct with a lot of typesafe methods. We can define any methods on it because it's in the same package ( ) { qs. (minMarks). () } () { qs. ( ()) } now we can parametrize it minRatingMarks = ( ) ([] , ) { []User ( ()). (minRatingMarks). (limit). (&users) err != { , err } users, } ( ) ([] , ) { []User ( ()). (). (limit). (&users) err != { , err } users, } ( ) ([] , ) { []User ( ()). (). (minRatingMarks). (limit). (&users) err != { , err } users, }", "sentiment": 0.1622237762237762},
{"link_title": "Equifax's Insurance Is Likely Inadequate for Breach", "url": "https://www.bloomberg.com/news/articles/2017-09-09/equifax-s-insurance-said-likely-to-be-inadequate-against-breach", "text": "Beazley is said to be among insurers on hook for some of costs\n\nCompany has said its policy may not be adequate for all risks\n\nEquifax Inc.\u2019s insurance against cyber breaches is likely inadequate to cover the credit-reporting company\u2019s costs tied to one of the biggest hacks in history, according to people familiar with the coverage.\n\nThe company holds a policy that would probably cover about $100 million to $150 million, with costs shared by carriers in the London market and elsewhere, said the people, who asked not to be identified discussing a private contract. Though Equifax\u2019s eventual expense may not be known for years, it could be multiples higher than the insurance payout, given what the company has disclosed and the costs at hacking victims like Yahoo and Target Corp., they said.\n\n\u201cEquifax carries cybersecurity, crime, general-liability and other lines of insurance, and we have begun discussions with our carriers regarding the incident,\u201d a spokesperson said by email Saturday, without commenting further.\n\nThe company has offered free credit-monitoring to victims after reporting Thursday that a breach affected 143 million people, revealing Social Security numbers, drivers license data and birth dates. The Atlanta-based company now faces multiple state and federal investigations, and a proposed multibillion-dollar class action lawsuit was filed against Equifax. In its annual report, the company addressed the limits of its insurance protection tied to cyber risks.\n\n\u201cOur property and business interruption insurance may not be adequate to compensate us for all losses or failures that may occur,\u201d Equifax said in the filing. \u201cAlso, our third-party insurance coverage will vary from time to time in both type and amount depending on availability, cost and our decisions with respect to risk retention.\u201d\n\nTo read more on Social Security numbers at risk, read here.\n\nEquifax dropped 14 percent in New York trading Friday. The company is one of the three major bureaus that maintain databases of consumers\u2019 credit status, payment history and address information. The same banks that furnish much of the bureaus\u2019 credit data also use it to make lending decisions.\n\nBeazley Plc, which has been expanding offerings to protect clients against cyber risks, is the lead insurer for Equifax, according to two people familiar with the contract. A representative for the London-based insurer declined to comment.", "sentiment": 0.15827352472089315},
{"link_title": "Mosquitoes, Medicine and Mold: Texas Battles Post-Harvey Health Issues", "url": "https://www.nbcnews.com/storyline/hurricane-harvey/mosquitoes-medicine-mold-texas-battles-post-harvey-health-issues-n799951", "text": "Dr. Carrie de Moor has a nasty cough, and she\u2019s not sure if it's allergies or one of the common respiratory infections that have been spreading since Hurricane Harvey hit southeast Texas late last month.\n\nShe's been sleeping in a trailer adjacent to her free-standing emergency room and urgent care clinic in Rockport, Texas, which was devastated by Harvey\u2019s winds and flood waters. The clinic had only been open for two weeks when Harvey hit.\n\nDe Moor is home in Dallas now for a few days with her children but will soon head back to the clinic, which is overwhelmed by people crowding in for stitches, tetanus shots, ear infections and skin rashes.\n\n\"We were seeing numbers outpacing anything we were prepared to take care of,\" said de Moor, an ER physician who is CEO of Code 3 ER and Urgent Care.\n\nPhysician volunteers have been cramming into the trailer and sleeping on the clinic floors as they tend to as many as 90 patients a day.\n\nRelated: Doctor Wades to Get to Cancer Patient\n\nLara Hamilton describes a similar scene at Christ Clinic in Katy, west of Houston.\n\n\"We typically see about 200 patients a week. In the first week we saw triple that,\" said Hamilton, a registered nurse who is executive director for the clinic. Volunteer doctors from as far afield as Vermont, Minnesota and Oregon have brought cots and are camped out in the clinic and at other nearby facilities, offering their services as needed.\n\n\"They're showering at the YMCA,\" Hamilton said.\n\nNo one single entity has authority over health matters in Texas, which has a history of turning away federal funding and oversight and which has a limited Medicaid program. County health authorities like the one in Harris County take care of issues like mosquitoes, clean water, inspecting restaurants and some vaccinations, but free and charity clinics like Christ Clinic and for-profit operations like Code 3 struggle in the best of times to fill in the gaps.\n\nNow, post-Harvey, they are overwhelmed.\n\nSo far, there's no big epidemic to cope with. The Harris County Health Department had to squelch rumors that plague was being spread by flood waters. Plague is carried by fleas, not in water.\n\nBut there are plenty of other messes left behind by Hurricane Harvey\u2019s floodwaters.\n\nThey include masses of mosquitoes, respiratory infections and a dramatic worsening of the day-to-day ills that people could cope with in normal times, but that get out of control in a crisis.\n\n\"We are seeing people who have just been eaten up by mosquito bites,\" Hamilton said. \"Typically, people won\u2019t go to the doctor for mosquito bites.\" But the combination of standing water, a lack of electricity and the need to work outside means a lot of exposure.\n\n\"People are working outside all day long, cleaning up their homes,\u201d said Hamilton. \"The doors are standing open because they are carrying debris in and out.\"\n\nAerial spraying for mosquitoes started this week, said Chris Van Deusen, spokesman for the Texas Department of State Health Services. They\u2019re bringing in heavy ordnance for the job, including an Air Force reserve wing from Youngstown, Ohio flying specially equipped C-130 aircraft.\n\n\"As the floodwaters recede, mosquito numbers are going to start going up,\" said Dr. Peter Hotez, dean for the National School of Tropical Medicine at Houston's Baylor College of Medicine.\n\n\"There\u2019s just debris everywhere. It\u2019s like Aedes aegypti heaven.\"\n\nAedes aegypti are the mosquitoes that carry viruses such as dengue and Zika virus. Stagnating water in ditches, bayous and flooded fields will breed other mosquitoes that spread West Nile virus.\n\nNo one\u2019s sure how much worse things will get, but Dr. Umair Shah, who heads the Harris County health department, is sure it will get bad.\n\nHe says teams are trying to replace mosquito traps pulled out hurriedly ahead of Harvey\u2019s floods. \"It's good we removed them,\" he said. They would have been washed away, but now it will take time to get back up and running to monitor where disease-causing mosquitoes are breeding.\n\nPatients are also beginning to come in with more urgent problems as they run out of insulin or blood pressure medications, said Jody Hopkins, CEO of the Texas Association of Charitable Clinics.\n\n\"For some of our clinics who supply insulin, a lot of them lost it because they lost refrigeration,\" Hopkins said.\n\nAnd people are starting to show up with depression, anxiety and post-traumatic stress disorder.\n\n\"I think mental health is going to be huge,\" Hopkins said. \"A patient came and said, 'I worked at Dollar General and the store is wiped out. I am out of a job.' That has a huge impact,\" she said.\n\nHamilton said Christ Clinic has already seen a big uptick in visits from people needing help from the licensed clinical social workers and psychologists helping out at the facility.\n\n\"Typically, a mental health provider will see eight to nine patients a day,\" she said. \"We saw 32 mental health patients in one day this week.\"\n\nHarris County\u2019s Shah said he was struck by the experience endured by an evacuee staying in a hotel. \"Every time the air conditioning came on at the hotel, she heard the water dripping and she started having flashbacks to the rain and the rising water. That was true post-traumatic stress,\" Shah said.\n\nAuthorities are cutting through red tape as quickly as they can. The state medical board is expediting licenses for out-of-state doctors to practice and schools have loosened immunization requirements.\n\n\"Last night a waiver came through for mold remediators,\u201d said the health department\u2019s Van Deusen.\n\nAs long as they register with the state health department, mold contractors can work without a license for as long as the disaster declaration is in effect.", "sentiment": 0.02428955453149002},
{"link_title": "The $1tn question: how far can the new iPhone 8 take Apple?", "url": "https://www.theguardian.com/business/2017/sep/09/apple-iphone-8-one-trillion-dollar-question", "text": "Apple\u2019s stock market value is heading towards a new milestone and its latest product launch on 12 September could push the tech giant closer to becoming the first ever $1tn (\u00a3760bn) company.\n\nAt the end of last week, the company\u2019s market capitalisation hovered around $830bn, continuing a 10-year run that has generally headed upwards since a low of $69bn in January 2009, during the financial crisis. Tuesday\u2019s event, with the iPhone 8 the star attraction, will strive to meet investors\u2019 \u2013 and customers\u2019 \u2013 vaulting expectations.\n\nBut what will Apple tempt users with to justify Wall Street\u2019s faith in its future profits? An Apple spokesman declined to discuss what will be revealed at the event in the company\u2019s $5bn, spaceship-shaped Cupertino headquarters. However, although Apple is always tight-lipped, this year leaks from its suppliers, and from the company itself (through details embedded in a software update) have told us much about what\u2019s coming.\n\nThe smartphone market is more competitive than ever, with sophisticated devices available for much less than the rumoured \u00a3900 cost of the iPhone 8. Most rivals are swallowing losses by cutting prices to win sales but Apple is heading upmarket to protect the iPhone, which is crucial to its success.\n\nThree new models are expected: two updating its present 7 and 7 Plus models (probably called the 7S and 7S Plus), and one entirely new \u2013 the iPhone 8. Internally known as \u201cD22\u201d, its screen will unlock via facial recognition, potentially replacing the fingerprint unlock system used since 2013. The screen will also cover more of the front, allowing the display to go right to the edges. And the screen will use a technology bought from Samsung \u2013 called Amoled, or active matrix organic light-emitting diode \u2013 which gives brighter colours. It could also mean the new phone will have a longer battery life because it does not need to be backlit, unlike the LCD screens Apple uses presently.\n\nBut none of these technological tweaks are cheap \u2013 hence the \u00a3900 price tag, compared to the \u00a3719 starting price of the larger iPhone 7 Plus.\n\nThe new phone will be a tricky sell, says Jan Dawson, who runs US-based tech consultancy Jackdaw Research. \u201cIt has to get the balance just right, giving people a compelling upgrade in the successors to the iPhone 7 and 7 Plus, while also offering up a higher tier,\u201d he explains. \u201cIt has to do that without alienating people who can\u2019t afford or justify spending the higher price for the new device, but don\u2019t want to settle for second best.\u201d\n\nThe last time Apple had a \u201csecond best\u201d phone, the plastic iPhone 5C in 2013, its sales were slower than expected, while demand for the top-end 5S outstripped supply. Apple needs to avoid that happening again, says Dawson: \u201cIt has to supply the new premium phone in high enough numbers so that if there\u2019s a big demand shift from the standard models to the new one, it doesn\u2019t end up depressing overall sales while there are supply constraints.\u201d\n\nApple seems confident. For the current quarter, it has forecast revenues of $49bn-$52bn, which would represent growth of between 4% and 11% from a year ago, and bring its performance back to 2015 levels. Dawson expects that iPhone sales will grow year-on-year in the October-December and January-March quarters: \u201cMuch of the timing of that growth will depend on the supply constraints.\u201d\n\nA few years ago, \u201cwearables\u201d \u2013 the market sector dominated by digital watches and Fitbits \u2013 were seen as the next technology hit. But the first Apple Watch, released in April 2015, underwhelmed many reviewers.\n\nNone the less, early adopters liked it; the research company IDC reckons 28.8m had sold by the end of July this year. Though Apple doesn\u2019t release unit sales or revenues, it is definitely the world\u2019s most popular smartwatch, while Google\u2019s rival Android Wear business has failed to take off.\n\nNow Apple is readying a version that can use 4G phone networks. It means those who have bought an Apple Watch for fitness reasons \u2013 the watch\u2019s biggest customer base \u2013 can stream music or podcasts while they run and work out, as well as making FaceTime video or audio calls, getting map directions, and receiving and replying to messages. According to Bloomberg, the 4G version will be on sale at the four US mobile carriers, and possibly through European networks too.\n\nApple\u2019s wearables strategy doesn\u2019t stop at the Watch: its wireless in-ear AirPods headphones, which have been in short supply since their launch last year, have delighted those who managed to get their hands on them. With supply improving, they could be a Christmas hit.\n\nWith the smartphone market now well established, the home is the new battlefield for the big tech companies. A few years ago most people expected that Microsoft would be a serious contender because its Xbox console was installed in millions of living rooms.\n\nBut instead Amazon has taken a lead, having sold an estimated 15 million of its voice-controlled Echo and Dot devices, which can provide weather, news and traffic reports and play music, as well as controlling digitally connected lights and similar devices around the home. Google joined in last year with its Google Home device. Now Apple is pitching in with HomePod, a high-quality music speaker controlled by its Siri voice assistant. As you might guess, it\u2019s pricey, with a reported cost of around \u00a3349 in the UK.\n\nAlso expected is an update to Apple TV, the company\u2019s set-top box, to allow it to stream higher definition pictures. On its own, that might not sound much. But the company has big ambitions in the US market, where millions of homes are abandoning expensive monthly cable-TV contracts and opting for cheaper services such as Netflix. Apple always wants to get ahead of those broader digital trends. Now it aims to become an alternative TV service, offering a la carte programming if you buy its hardware.\n\nHowever, TV networks won\u2019t license their programmes cheaply because they want to retain the viewers who in turn watch the adverts that provide their revenues. So Apple is having to make its own. Eddy Cue, the executive behind this drive, is well-armed for the fight. As well as hiring TV and film executives, he has bought the rights to James Corden\u2019s Carpool Karaoke and has a $1bn warchest for producing original content. Although that\u2019s a long way from Netflix\u2019s $6bn annual spending, or Amazon\u2019s estimated $4.5bn, Apple is ambitious.\n\nA new iPhone means a new version of Apple\u2019s iOS software, which will update about 500m existing devices as well as running on the new products. With iOS 11, iPhone and iPads will be able to run \u201caugmented reality\u201d (AR) apps, which can overlay Star Wars spaceships, or map directions, or geolocated information, on to a live camera view on the screen. AR apps are forecast to spark a new app boom; some of them will struggle, but it only takes one success to validate the entire field. And Apple will have an advantage over Android, where AR will only work on a few million devices by the end of the year.\n\nFor the past seven quarters, and 12 of the past 19, the fastest-growing part of Apple has been its \u201cservices\u201d. Most recently generating $7.2bn \u2013 more than either iPad or Mac sales \u2013 it includes Apple Music fees, the 30% cut of payments and subscriptions on millions of apps in the App Store, and payment for iCloud storage (where only the first 5GB is free).\n\nChief executive Tim Cook has repeatedly pointed out to analysts that, on its own, the services segment generates enough revenue to enter the Fortune 100 list of America\u2019s biggest companies. As the iPhone user base grows, and people buy more apps, the services segment keeps growing. And if the Apple TV ploy succeeds, it could really take off. What Wall Street really likes about services, though, is its promise of recurring revenue, separate from hardware sales. If Apple grows its user base, the Services segment grows too; thus it also provides an indicator of the overall health of the company\u2019s ecosystem of hardware, software, users and developers. Suddenly, a $1tn valuation looks attainable \u2013 and sustainable.\n\nWhat else is Apple working on? The company\u2019s car project, code-named \u201cProject Titan\u201d, appears to have been scaled down and the firm will no longer build self-driving vehicles: according to reports, it will concentrate on the software behind autonomous cars instead. More likely to arrive sooner are \u201cAR glasses\u201d for putting augmented reality right in front of you. In March the Financial Times reported that Apple is working on such glasses; what\u2019s unclear, as ever, is the timescale. Months? Years? We can\u2019t be sure until Tim Cook shows it off on stage.", "sentiment": 0.10862362385089658},
{"link_title": "Open Source ML Platform", "url": "http://studio.ml/", "text": "For authentication, you must set up an email/password authentication or a Google account. See the for more details. For now, get started as a guest by uncommenting \"guest:true\" in studio/default_config.yaml. At this point, you can run a machine learning experiment using Studio. In the helloworld folder, you can find basic examples of experiments to run. Let's try training a Keras model for mnist:\n\nNow you can visualize your experiment by navigating to the web interface. Open the web visualizer:\n\nYou can see the results of your job at http://127.0.0.1:5000. Here you can examine or delete any current or past experiments. Run studio {ui|run} --help for a full list of ui / runner options\n\nClick on your experiment to view experiment artifacts such as the output and workspace, experiment information such as the status and creation time, and the tail of the experiment log.", "sentiment": 0.075},
{"link_title": "Deep learning models and active learning techniques at US Open", "url": "https://developer.ibm.com/dwblog/2017/cognitive-highlights-usopen/?cm_mmc=dw-_-social1709-_-hackernews-_-usopen", "text": "For two weeks in the summer, the center of the sporting world is the US Open. Millions of tennis fans follow along hoping to feel close to the matches, wherever they are in the world. They want scores, they want player and tournament information and they want highlights. Capturing all the action at the event\u2014with matches on up to 17 courts simultaneously, and up to six matches per court per day\u2014is a monumental video production effort. This year, one of the innovations IBM is bringing to the US Open to aid in that effort is Cognitive Highlights, a solution that ensures fans can see all the tournament\u2019s best moments.\n\nSo far at this year\u2019s tournament, the Cognitive Highlights system has clipped over 25,000 points from over 300 hours of coverage of men and women\u2019s singles matches, often happening simultaneously. The system uses deep learning models and \u201cself-supervised\u201d active learning techniques to recognize which of these points are significant and understand what makes a good highlight. Using Watson APIs, it understands the importance of certain scores\u2014like a point that clinches a set\u2014and uses visual and audio cues to create ratings for each point. The output is a one-of-a-kind system for ranking exciting moments and auto-curating a highlights package.\n\nFor visual cues, technicians trained Watson using Visual Recognition to identify when a player is performing actions that typically mark an exciting moment in the match\u2014celebrating, waving to the crowd, fist pumps, etc. The system also uses facial recognition to read the emotional reactions of the players.\n\nVisual Recognition also helps Watson divide each match into individual points, as it reads the camera placement and zoom which create the scene at the start of each point and knows that\u2019s the place to begin the clip.\n\nFor sound classification, the team worked with MIT to develop a deep neural network called \u201cSoundNet\u201d for environmental sound analysis like crowd noise. Refining this system to adapt to the various environments (different crowd sizes and makeups) helps monitor crowd excitement over the course of a match.\n\nFinally, the system has a sophisticated analytic model that analyzes the statistics that correlate with important moments in a match. Not all winners are equal in impact and the model helps keep focus on true turning points and defining moments.\n\nThe data from the visual, audio and statistical cues are combined and clips are scored on Overall Excitement. These clips are then used by the USTA\u2019s editorial team, who have been spared the burden of watching hours of simultaneous video streams, and can focus on getting the right story told. Highlight packages are made quickly, tailored around specific players and sent out to the world of expectant fans through app notifications, social media, on player bios pages on the US Open digital platforms and elsewhere.\n\nThere\u2019s a lot more we can do with Watson Media platform. Wherever there is video being recorded, context can be understood. These cognitive solutions can help monitor, categorize and derive meaning and insight quickly. There are potential applications of this solution for broadcasters, media, entertainment, security, education, industry, retail\u2014and more. As the use of video continues to increase across all sectors, so will the applications of this technology, which inspires us to keep refining and making these solutions more powerful and more helpful to the people that depend on them.", "sentiment": 0.1744565217391304},
{"link_title": "POLYBIUS \u2013 The Video Game That Doesn't Exist", "url": "https://www.youtube.com/watch?v=_7X6Yeydgyg", "text": "", "sentiment": 0.0},
{"link_title": "xtermcontrol: change colors, title, font and geometry of a running xterm", "url": "http://www.thrysoee.dk/xtermcontrol/", "text": "Xtermcontrol enables dynamic control of xterm properties. It makes it easy to change colors, title, font and geometry of a running xterm, as well as to report the current settings of these properties. Window manipulations de-/iconify, raise/lower, maximize/restore and reset are also supported.\n\nTo complete the feature set; xtermcontrol lets advanced users issue any xterm control sequence of their choosing.\n\nPackages are known to be available for Debian, Ubunto, Gentoo, Arch Linux, NetBSD, FreeBSD, MacPorts and via their respective Package Management Systems. Thanks to the maintainers of these packages!\n\nIt's also possible to install with Homebrew on OS X as:\n\nThe default configuration file is sourced if xtermcontrol is run without options or if specifically requested, e.g. .\n\nEach line in a configuration file is either a comment or contains an attribute. Attributes consist of a keyword and an associated value:\n\nWhitespace is ignored in attributes unless within a quoted value. The character is taken to begin a comment. Each and all remaining characters on that line is ignored.\n\nFont names like are very cumbersome to write, so it is convenient to make use of aliases, e.g. or , if present in fonts.alias files of the font directories.\n\nBasically this means that colors are specified by name or rgb value, e.g. , or . Colors are typically reported by the xterm in a device-dependent numerical form, e.g. .\n\nNote that old syntax rgb values should always be quoted to avoid being interpreted as the beginning of a comment by the shell.\n\nThe secret behind xtermcontrol is xterm control sequences. All the possible (there are a plethora of them) control sequences are documented in ctlseqs.ms, found in the xterm distribution. This file is troff formated, so in order to make sense of it, a text file could be generated as follows:\n\nBoth ctlseqs.ms and ctlseqs.txt are distributed with xtermcontrol, and can be found in the docs directory.\n\nThe OS X Terminal.app emulates an extended set of the VTxxx series commands, closely resembling dtterm. Therefore the following subset of xtermcontrol options works as expected:\n\nIt can be very useful to use xterm colors to indicate which is your current user ID. As an example, configure the of the login user to:\n\nNow the red foreground color works as a reminder of the privileges when logged into the root account.\n\nTo change back to current ID preferences after a session, include the following code snippet in the shell initialization file of the effected accounts.\n\nThis will work equally well when logging into remote accounts. Of course xtermcontrol must be installed on the remote servers in order for it to work.\n\nFor further examples of xtermcontrol capabilities view the in the distribution tarball. Issue 'make test' to execute it.\n\nIf read/write permissions on the tty's are changed so that special group membership is required to be able to write to the pseudo terminal, the easiest workaround is to install xtermcontrol setuid root.\n\nXterm has three resources, allowWindowOps, allowTitleOps, and allowFontOps, that enables or disables special operations which xtermcontrol relies on. If any of these resources are set (or defaults) to false xtermcontrol may hang. The resources corresponds to xtermcontrol options as:\n\nAll three resources can usually be enabled for the current xterm session via a menu; ctrl+rightclick and look for menu item names like 'Allow Window Ops'. To set these resource values persistently you can add the following to either your local file, or to a system-wide resource file like :", "sentiment": 0.04363756613756614},
{"link_title": "What Can Deep Neural Networks Teach Us About Human Thought?", "url": "https://www.quora.com/What-can-deep-neural-networks-teach-us-about-human-thought?share=1", "text": "It\u2019s hard to study Deep Learning without being struck by the parallels with both the way individual humans think, and the way groups of humans think collectively. Indeed it seems likely that Deep Learning will have profound impacts on philosophy, similarly to how the Scientific Revolution led to the Enlightenment.\n\nThe key insights that have jumped out to me are:\n\nI'm sure there are a bunch of other connections too.\n\nMore generally I think it's interesting to think of artificial neural networks as a transfer learning opportunity for understanding human thought - artificial neural networks are easier to study than human thought and it's likely that the representations of thought we create while studying them will help us understand thought more broadly - even if human thought and artificial thought are not entirely the same.", "sentiment": -0.0008680555555555478},
{"link_title": "Does AWS Lambda keep its serverless marketing promise of continuous scaling?", "url": "https://read.acloud.guru/does-aws-lambda-keep-its-serverless-marketing-promise-of-continuous-scaling-e990114bb379", "text": "Does AWS Lambda keep its serverless marketing promise of continuous scaling? OpsGenie\u2019s migration to serverless hit a major bump in the road \u2014 we\u2019ve exceeded AWS Lambda\u2019s concurrent execution limit\n\nFor invocations that are not stream-based, AWS uses the following formula to calculate concurrent Lambda execution counts:\n\nThe throttle differs based on whether your Lambda function is processing events from a stream-based event source versus a source that is not stream-based. The AWS Lambda Developer Guide provides much more detailed information on concurrent execution limits.\n\nAWS defines concurrency as the number of executions your function code are occurring at any given time. The service applies throttling of your functions if a certain concurrent execution count is exceed.\n\nInstead of implementing microservices, we shared the rationale for moving OpsGenie to a serverless architecture. We found several advantages with using AWS Lambda instead of building dockerized applications on top of AWS ECS \u2014 or deploying them to a PaaS platform like Heroku.\n\nWe recently shared some insight on why OpsGenie is switching to serverless technologies , and our engineers continue to blog about the journey with new lessons from the experience.\n\nOpsGenie has been heavily investing in serverless technologies. We are using AWS Lambda in application development, synthetic monitoring of production systems, cross-region replication of database systems, customization needs of customers, and more.\n\nDoes AWS Lambda really deliver on the promises of continuous scaling?\n\nDuring this period of time, we had a concurrent execution limit of 1000.\n\nWhen our engineering team used CloudWatch metrics to look deeper, we realized that an operational task that sends AWS ALB logs to our Graylog server started to suffer latencies \u2014 which caused throttles of the AWS Lambda service.\n\nI\u2019m not here to argue about whether the calculation by AWS to throttle our requests was right or wrong. I\u2019m here to rant \u2014 I mean, discuss \u2014 about some of the common issues that are challenging our journey to serverless.\n\nWith 200 events per second and a 2.8 second execution duration, on average, it seemed unlikely that we had reached the limit that throttled our requests.\n\nLatencies. Yeah \u2014 it seems so obvious \u2026 now!\n\nHere\u2019s the thing \u2014 any function can suffer latency because of network or performance problems in third party systems. Any public API endpoints can receive massive loads, or an application function can experience DynamoDB throttles and retry their database calls a couple of times.\n\nEventually, any back-pressure happening in any part of your infrastructure can make your whole system go down.\n\nMany everyday situations can cause back-pressure, but AWS Lambda\u2019s concurrent execution limit model makes it almost impossible to deal with it in a quick and convenient manner. As a workaround, you have to buffer loads in a queue or stream \u2014 and then call your Lambda functions with the buffered events.\n\nAt first, this issue seemed to almost destroy the value of all event-based Lambda integrations with services like API Gateway, S3, and SNS. To keep moving forward, our engineering team had to figure out what could be done for short-term workaround.\n\nOur immediate response was obvious \u2014 request a limit increase. It turns out that you don\u2019t receive limit increases immediately \u2014 you have to wait a week or so!\n\nFor next steps, our team focused on buffering the load. There are two basic options for buffering loads in the AWS serverless world: Kinesis and SQS.\n\nThe cost of using Kinesis to buffer loads\n\nKinesis is Amazon\u2019s fully managed and scalable streaming data solution. You can use it to capture and send any kind of streaming data such as application logs or metrics, website clicks, sensor data, player-game interaction \u2014 and you consume and analyze the data as it arrives.\n\nThe Kinesis scalability model sits on top of sharding, which is the base throughput unit of a Kinesis stream. One shard supports 1000 put requests of 1MB input data per second. If you need more more throughput, you can dynamically add or remove shards via Kinesis API.\n\nFor stream-based sources like Kinesis, the number of shards per stream is the unit of concurrency. If your stream has 10 active shards, there will be at most 10 Lambda function invocations running concurrently.\n\nThis approach protects you against concurrent execution limit threats \u2014 but it comes with a cost. In order to increase the throughput, you\u2019ll need to increase the number of shards. If your logic is not appropriate for batch processing, a very large number of shards is required.\n\nThe one thing about AWS Kinesis service I hate most is its pricing model. AWS forces you to pay $0.015 per shard per hour \u2014 even if you don\u2019t send or consume any data!\n\nAnd unlike most other streaming solutions, Kinesis does not support topic subscriptions. You\u2019ll have to create a Kinesis stream for every Lambda function that you create! In a common local development and test environment model with 50 engineers, creating that many streams could be catastrophic.\n\nThe cost of AWS Kinesis might immediately disqualify it as a viable option for many organizations.\n\nUsing Simple Queue Service is still a (better) workaround\n\nSQS is Amazon\u2019s messaging queue service, Similar to Kinesis, it is fully managed and highly scalable. You can use it as messaging middleware to decouple your services and applications.\n\nSQS is a very simple, lightweight, and cost-effective solution \u2014 which I happen to like very much (in case you hadn\u2019t already guessed!).\n\nThe problem with SQS, however, is that AWS does not offer a native integration with Lambda. You have to provide your own solution for polling queues and submitting messages to relevant functions \u2014 then you must manually delete the messages you processed.\n\nA key issue with these workarounds is that they unnecessarily increase the complexity of the infrastructure you have to manage. So, it made me wonder \u2026 what kind of services should AWS provide so development teams and organization can design and deliver better solutions?\n\nAWS \u2014 make it so\n\nAt a time when microservices and service-oriented architectures are gaining popularity\u2014 and when developing decoupling systems by applying separation of concerns pattern is inevitable \u2014 an account-based concurrency limit is unacceptable!\n\nIf AWS plans to keep its serverless marketing promise of continuous scaling \u2014 developers need a much better model for consuming Lambda functions.\n\nAWS already uses this approach for their DynamoDB service. A developer can create tables with different read and write capacities, create threshold metrics at CloudWatch, and get notified about load spikes so that you can immediately provision your capacity according to the load.\n\nThe engineering team at OpsGenie has already made these feature requests to AWS \u2014 and we don\u2019t expect they\u2019ll be implemented exactly as we propose. We just hope that AWS will deliver a solution that addresses our concerns \u2014 and quickly.\n\nEven after that rant \u2014 I mean discussion \u2014 I truly believe that serverless is the future of computing and application development. OpsGenie is enjoying the journey and our overall experience (so far).\n\nOur team is well aware that one of the costs of being an early adopter includes dealing with immaturity of services \u2014 like the lack of ideal tools and frameworks for local development, testing, and deployment.\n\nI fully expect that we\u2019ll face many more issues as we continue to evolve our serverless solution alongside AWS. I\u2019m confident we\u2019ll meet those challenges, continue to learn, and share our insights with the engineering community.\n\nOpsGenie is really enjoying being a part of the serverless community, learning from others, and contributing to it\u2019s growth and maturity. We hope that sharing about our insights will help others with their journey.\n\nLooking forward to hearing about your experiences in the comments below!", "sentiment": 0.13402701383583734},
{"link_title": "The next Apple Watch will have LTE cell service", "url": "https://techcrunch.com/2017/09/09/the-next-apple-watch-will-have-lte-cell-service/", "text": "Last night an iOS 11 GM download link made its way onto Reddit, which 9to5Mac discovered and has been digging through.\n\nYou can check out the full list of discoveries here, but one big thing that stood out is that the next Apple Watch (or at least one version of it) will finally have LTE cell service, meaning it doesn\u2019t need to be tethered to your phone at all times. The leaked screenshot above shows a LTE signal strength indicator in the top left corner of the watch.\n\nThe benefits of a cell connection are clear \u2013 you could stream music or take a phone call on a run without needing to keep your phone in your pocket.\n\nInterestingly, 9to5Mac says that your LTE Apple Watch will share the same phone number as your existing iPhone. This means that you\u2019ll be able to take calls on either device, and that plans may be less expensive than traditional data-only devices like an iPad.Traditionally carriers assign new phone numbers to devices even if they are only being used for data, like an iPad \u2013 so this is a new arrangement for carriers.\n\nLastly, a leaked setup screen from 9to5Mac suggests that the new LTE watch will have a red crown, to differentiate it from the older versions.\n\nTwo years ago, when the original Apple Watch launched, Tim Cook was spotted in an Apple Store sporting a version with a red crown. It\u2019s not clear if this means Cook was testing a version of the LTE watch in the wild, or the red dot meant something different at the time.", "sentiment": 0.06841692789968651},
{"link_title": "An Inductive Functional Programming System for Haskell Programmers", "url": "http://nautilus.cs.miyazaki-u.ac.jp/~skata/MagicHaskeller.html", "text": "Please install the 2014 or 2015 version. (2015 is slightly better.)\n\n\n\n \n\n (Ignore which just denotes the prompt.)\n\nOn Windows + Haskell Platform 2014, you may need to use \n\n \n\n instead. The point is to force to use the already-installed version of the network package.\n\nYou may need to unregister some package temporarily, and restart installation again.\n\n In my case, I had to unregister the package, so\n\n \n\n", "sentiment": 0.5},
{"link_title": "Deliberation increases the wisdom of crowds", "url": "https://arxiv.org/abs/1703.00045", "text": "", "sentiment": 0.0},
{"link_title": "This is how your world could end", "url": "https://www.theguardian.com/environment/2017/sep/09/this-is-how-your-world-could-end-climate-change-global-warming", "text": "Many of us share some dim apprehension that the world is flying out of control, that the centre cannot hold. Raging wildfires, once-in-1,000-years storms and lethal heatwaves have become fixtures of the evening news \u2013 and all this after the planet has warmed by less than 1C above preindustrial temperatures. But here\u2019s where it gets really scary.\n\nIf humanity burns through all its fossil fuel reserves, there is the potential to warm the planet by as much as 18C and raise sea levels by hundreds of feet. This is a warming spike of an even greater magnitude than that so far measured for the end-Permian mass extinction. If the worst-case scenarios come to pass, today\u2019s modestly menacing ocean-climate system will seem quaint. Even warming to one-fourth of that amount would create a planet that would have nothing to do with the one on which humans evolved or on which civilisation has been built. The last time it was 4C warmer there was no ice at either pole and sea level was 80 metres higher than it is today.\n\nI met University of New Hampshire paleoclimatologist Matthew Huber at a diner near his campus in Durham, New Hampshire. Huber has spent a sizable portion of his research career studying the hothouse of the early mammals and he thinks that in the coming centuries we might be heading back to the Eocene climate of 50 million years ago, when there were Alaskan palm trees and alligators splashed in the Arctic Circle.\n\n\u201cThe modern world will be much more of a killing field,\u201d he said. \u201cHabitat fragmentation today will make it much more difficult to migrate. But if we limit it below 10C of warming, at least you don\u2019t have widespread heat death.\u201d\n\nIn 2010, Huber and his co-author, Steven Sherwood, published one of the most ominous science papers in recent memory, An Adaptability Limit to Climate Change Due to Heat Stress.\n\n\u201cLizards will be fine, birds will be fine,\u201d Huber said, noting that life has thrived in hotter climates than even the most catastrophic projections for anthropogenic global warming. This is one reason to suspect that the collapse of civilisation might come long before we reach a proper biological mass extinction. Life has endured conditions that would be unthinkable for a highly networked global society partitioned by political borders. Of course we\u2019re understandably concerned about the fate of civilisation and Huber says that, mass extinction or not, it\u2019s our tenuous reliance on an ageing and inadequate infrastructure, perhaps, most ominously, on power grids, coupled with the limits of human physiology that may well bring down our world.\n\nIn 1977, when power went out for only one summer day in New York, swaths of the city devolved into something like Hobbes\u2019s man in a state of nature. Riots swept across the city, thousands of businesses were destroyed by looters and arsonists lit more than 1,000 fires.\n\nIn 2012, when the monsoon failed in India (as it\u2019s expected to do in a warmer world), 670 million people \u2013 that is, 10% of the global population \u2013 lost access to power when the grid was crippled by unusually high demand from farmers struggling to irrigate their fields, while the high temperatures sent many Indians seeking kilowatt-chugging air-conditioning.\n\n\u201cThe problem is that humans can\u2019t even handle a hot week today without the power grid failing on a regular basis,\u201d he said, noting that the ageing patchwork power grid in the United States is built with components that are allowed to languish for more than a century before being replaced. \u201cWhat makes people think it\u2019s going to be any better when the average summer temperature will be what, today, is the hottest week of the year in a five-year period and the hottest temperatures will be in the range that no one has ever experienced before in the United States? That\u2019s 2050.\u201d\n\nBy 2050, according to a 2014 MIT study, there will also be five billion people living in water-stressed areas.\n\n\u201cThirty to 50 years from now, more or less, the water wars are going to start,\u201d Huber said.\n\nIn their book Dire Predictions, Penn State\u2019s Lee Kump and Michael Mann describe just one local example of how drought, sea level rise and overpopulation may combine to pop the rivets of civilisation:\n\n\u201cIncreasingly severe drought in West Africa will generate a mass migration from the highly populous interior of Nigeria to its coastal mega-city, Lagos. Already threatened by rising sea levels, Lagos will be unable to accommodate this massive influx of people. Squabbling over the dwindling oil reserves in the Niger river delta, combined with potential for state corruption, will add to the factors contributing to massive social unrest.\u201d\n\n\u201cMassive social unrest\u201d here being a rather bloodless phrase masking the utter chaos coming to a country already riven by corruption and religious violence.\n\n\u201cIt\u2019s sort of the nightmare scenario,\u201d said Huber. \u201cNone of the economists is modelling what happens to a country\u2019s GDP if 10% of the population is refugees sitting in refugee camps. But look at the real world. What happens if one person who was doing labour in China has to move to Kazakhstan, where they aren\u2019t working? In an economic model, they\u2019d be immediately put to work. But in the real world, they\u2019d just sit there and get pissed. If people don\u2019t have economic hope and they\u2019re displaced, they tend to get mad and blow things up. It\u2019s the kind of world in which the major institutions, including nations as a whole, have their existence threatened by mass migration. That\u2019s where I see things heading by mid-century.\u201d\n\nAnd it doesn\u2019t get any better after 2050. But forecasts about the disintegration of society are social and political speculations and have nothing to do with mass extinctions. Huber is more interested in the hard limits of biology. He wants to know when humans themselves will actually start to disintegrate. His 2010 paper on the subject was inspired by a chance meeting with a colleague.\n\n\u201cI presented a paper at a conference about how hot tropical temperatures were in the geological past and [University of New South Wales climate scientist] Steve Sherwood was in the audience. He heard my talk and he started asking himself the very basic question: \u2018How hot and humid can it get before things start dying?\u2019 It was literally just an order of magnitude kind of question. I guess he thought about it and realised that he didn\u2019t know the answer and wasn\u2019t sure anyone else did either\u2026 Our paper really wasn\u2019t motivated by the future climate per se, because when we started we didn\u2019t know if there was any kind of realistic future climate state that would fall within this habitability limit. When we started, it was just like, \u2018We don\u2019t know. Maybe you have to go to, like, 50C global mean temperature.\u2019 Then we ran a whole set of model results and it was rather alarming to us.\u201d\n\nSherwood and Huber calculated their temperature thresholds using the so-called wet-bulb temperature, which basically measures how much you can cool off at a given temperature. If humidity is high, for instance, things like sweat and wind are less effective at cooling you down and the wet-bulb temperature accounts for this.\n\n\u201cIf you take a meteorology class, the wet-bulb temperature is calculated by basically taking a glass thermometer, putting it in a tight wet sock and swinging it around your head,\u201d he said. \u201cSo when you assume that this temperature limit applies to a human, you\u2019re really kind of imagining a gale force wind, blowing on a naked human being, who\u2019s doused in water, and there\u2019s no sunlight, and they\u2019re immobile and actually not doing anything other than basal metabolism.\u201d\n\nToday, the most common maximums for wet-bulb temperatures around the world are 26C to 27C. Wet-bulb temperatures of 35C or higher are lethal to humanity. Above this limit, it is impossible for humans to dissipate the heat they generate indefinitely and they die of overheating in a matter of hours, no matter how hard they try to cool off.\n\n\u201cSo we were trying to get across the point that physiology and adaptation and these other things will have nothing to do with this limit. It\u2019s the easy-bake oven limit,\u201d he said. \u201cYou cook yourself, very slowly.\u201d\n\nWhat that means is that this limit is likely far too generous for human survivability.\n\n\u201cWhen you do real modelling, you hit a limit much sooner, because human beings aren\u2019t wet socks,\u201d he said. According to Huber and Sherwood\u2019s modelling, 7C of warming would begin to render large parts of the globe lethally hot to mammals. Continue warming past that and truly huge swaths of the planet currently inhabited by humans would exceed 35C wet-bulb temperatures and would have to be abandoned. Otherwise, the people who live there would be literally cooked to death.\n\n\u201cPeople are always like, \u2018Oh, well, can\u2019t we adapt?\u2019 and you can, up to a point,\u201d he said. \u201cIt\u2019s just after that point that I\u2019m talking about.\u201d\n\nAlready in today\u2019s world, heated less than 1C above preindustrial times, heatwaves have assumed a new deadly demeanour. In 2003, two hot weeks killed 30,000 people in Europe. It was called a once-in-500-year event. It happened again three years later (497 years ahead of schedule). In 2010, a heatwave killed 15,000 people in Russia. In 2015, nearly 700 people died in Karachi alone from a heatwave that struck Pakistan while many were fasting for Ramadan. But these tragic episodes are barely a shade of what\u2019s projected.\n\n\u201cIn the near term \u2013 2050 or 2070 \u2013 the US Midwest is going to be one of the hardest hit,\u201d said Huber. \u201cThere\u2019s a plume of warm, moist air that heads up through the central interior of the US during just the right season and, man, is it hot and sticky. You just add a couple of degrees and it gets really hot and sticky. These are thresholds, right? These aren\u2019t just like smooth functions. It gets above a certain number and you hurt yourself very badly.\u201d\n\nChina, Brazil, and Africa face similarly infernal forecasts, while the already sweltering Middle East has what Huber calls \u201cexistential problems\u201d. The first flickers of this slow-motion catastrophe might be familiar to Europeans struggling to accommodate the tens of thousands of refugees at their borders: the collapse and mass migration of Syrian society came after a punishing four-year drought. Still others have noted that the Hajj, which brings two million religious pilgrims to Mecca each year, will be a physically impossible religious obligation to fulfil due to the limits of heat stress in the region in just a few decades.\n\nBut for the very worst-case emissions scenarios, heatwaves would not merely be a public health crisis or a \u201cthreat multiplier\u201d, as the Pentagon calls global warming. Humanity would have to abandon most of the Earth it now inhabits. In their paper, Huber and Sherwood write: \u201cIf warmings of 10C were really to occur in the next three centuries, the area of land likely rendered uninhabitable by heat stress would dwarf that affected by rising sea level.\u201d\n\nHuber said: \u201cIf you ask any schoolchild, \u2018What were mammals doing in the age of the dinosaurs?\u2019 they\u2019d say they were living underground and coming out at night. Why? Well, heat stress is a very simple explanation. Interestingly, birds have a higher set-point temperature \u2013 ours is 37C, birds\u2019 is more like 41C. So I actually think that\u2019s a very deep evolutionary relic right there. Because that wet-bulb temperature was probably maxing out around 41C in the Cretaceous, not 37C.\u201d\n\nBack at the diner in New Hampshire, Huber told me about his \u201cfavourite story\u201d: the US Army\u2019s real-life parable of the so-called Motivated Point Man. In 1996, a platoon of light infantry spent days in the Puerto Rican jungle acclimatising to stifling heat and humidity, cautiously monitoring their water intake before simulating a night-time raid. The platoon included \u201csome of the most fit and motivated soldiers in the battalion\u201d. When the evening of the raid came, the platoon leader began leading his troops through the jungle, machete-ing a path through the brush. Before long, he was felled by fatigue and delegated his leadership to an underling. When the second private failed to advance the platoon quickly enough, the platoon leader demanded to lead again. But soon he found himself hyperthermic and unable to walk. His soldiers had to douse him in cold water and supply him with intravenous infusions. Eventually, four soldiers had to carry him. Before long, the extra demands vitiated the entire platoon, all of whom began to fall prey to heat stress. The exercise had to be called off before it became a massacre.\n\n\u201cSo I look at that as, if it\u2019s night-time and acclimatised, fit people can just disintegrate into a pool of useless people on stretchers. That\u2019s what I see happening to society, to cultures,\u201d Huber said. \u201cIf you want to know how mass extinctions happen, that\u2019s how. So when people talk about the Pleistocene megafauna extinctions and Clovis people, sometimes they act like it\u2019s a mystery how these things happen. But it happens in exactly the same way. You have something tearing apart the strongest members, the weaker ones try to fill in the gaps, they\u2019re really not strong enough to take it and the whole thing collapses.\n\n\u201cYou want to know how societies collapse?\u201d Huber said. \u201cThat\u2019s how.\u201d\n\nExtracted from Ends of the World by Peter Brannen is published by Oneworld Publications (\u00a318.99). To order a copy for \u00a316.14 go to guardianbookshop.com or call 0330 333 6846. Free UK p&p over \u00a310, online orders only. Phone orders min p&p of \u00a31.99\n\n1.0C higher\n\nAs warming causes ice sheets to melt, krill populations suffer, threatening the penguins\u2019 main food source.\n\n1.6C higher\n\nAbout half of wooded tundra is lost, putting pressure on its inhabitants such as moose, lynx and brown bears.\n\n2.2C higher \n\nAt warming just over the +2C limit agreed in Paris 25% of large mammals in Africa are extinct.\n\n2.6C higher\n\nMajor loss of tropical rainforests and the species that depend on them for habitat, including orangutans, sloths and jaguars.\n\nMore than 4C higher\n\nAt these temperatures, up to 70% of species would be extinct, coral reefs would be dead and deserts would expand across the globe.", "sentiment": 0.07437250562250566},
{"link_title": "France, Germany, Italy, Spain seek to base taxes on digital giants' revenues", "url": "http://www.reuters.com/article/us-eu-tax-digital/france-germany-italy-spain-seek-tax-on-digital-giants-revenues-idUSKCN1BK0HX", "text": "FILE PHOTO The logo of the web service Amazon is pictured in this June 8, 2017 illustration photo. REUTERS/Carlos Jasso/Illustration\n\nPARIS (Reuters) - France, Germany, Italy and Spain want digital multinationals like Amazon and Google to be taxed in Europe based on their revenues, rather than only profits as now, their finance ministers said in a joint letter.\n\nFrance is leading a push to clamp down on the taxation of such companies, but has found support from other countries also frustrated at the low tax they receive under current international rules.\n\nCurrently such companies are often taxed on profits booked by subsidiaries in low-tax countries like Ireland even though the revenue originated from other EU countries.\n\n\u201cWe should no longer accept that these companies do business in Europe while paying minimal amounts of tax to our treasuries,\u201d the four ministers wrote in a letter seen by Reuters.\n\nThe letter, signed by French Finance Minister Bruno Le Maire, Wolfgang Schaeuble of Germany, Pier-Carlo Padoan of Italy and Luis de Guindos, was addressed to the EU\u2019s Estonian presidency with the bloc\u2019s executive Commission in copy.\n\nThey urged the Commission to come up with a solution creating an \u201cequalization tax\u201d on turnover that would bring taxation to the level of corporate tax in the country where the revenue was earned.\n\n\u201cThe amounts raised would aim to reflect some of what these companies should be paying in terms of corporate tax,\u201d the ministers said in the letter, first reported on by the Financial Times.\n\nLe Maire, Schaeuble, Padoan and de Guindos of Spain said they wanted to present the issue to other EU counterparts at a Sept. 15-16 meeting in Tallinn.\n\nThe EU\u2019s current Estonian presidency has scheduled a discussion at the meeting about the concept of \u201cpermanent establishment\u201d, with the aim of making it possible to tax firms where they create value, not only where they have their tax residence.\n\nFrance has stepped up pressure for EU tax rules after facing legal setbacks trying to obtain payments for taxes on activities in the country.\n\nA French court ruled in July French court ruled that Google, now part of Alphabet Inc, was not liable to pay 1.1 billion euros ($1.3 billion) in back taxes because it had no \u201cpermanent establishment\u201d in France and ran its operations there from Ireland.", "sentiment": -0.030761316872427988},
{"link_title": "Double-Check Those \u201cBest Seller\u201d Tags on Amazon", "url": "http://lifehacker.com/double-check-those-best-seller-tags-on-amazon-1798669238", "text": "Buying on Amazon is increasingly an exercise in trust. While the company has taken steps to mitigate the spread ofsponsored product reviews and fake ratings, illegitimate sellers are still finding ways to scam consumers. The latest trick? Gaming Amazon\u2019s popularity ranking system and taking over the \u201cBest Seller\u201d tag\u2014arguably a pretty important marker when it comes to buying something with which you may be unfamiliar.\n\nPeter Koch, professional Amazon reseller, described the method by which hackers and untrustworthy product sellers could game Amazon\u2019s ranking system, boosting products with little to no reviews to the top of certain popular search categories.\n\nAmazon\u2019s algorithm measures not only what\u2019s purchased, but what is viewed by users. Shady sellers hire contract workers who boost the rankings of particular products by searching for them. Searches, time spent on a product page (known as engagement), and adding products to wish lists all help boost a product\u2019s ranking, whether or not it has actually been purchased. Sellers will also slightly miscategorize products to avoid competition, though it\u2019s harder to be found when you\u2019re in the wrong section.\n\nTo eliminate competition, sellers will file false copyright claims with Amazon, knocking their competitor\u2019s products off the market until the issue is resolved, and filing complaints to further dismantle the reputation of a seller with a legitimate product.\n\nAll this adds up to some products receiving a \u201cBest Seller\u201d tag that clearly are not the cream of the crop. As a rule, you should always double-check the brand name of your product, and who is handling the selling and shipping. You\u2019re more likely to be purchasing a legitimate \u201cBest Seller\u201d when purchasing products shipped and sold by Amazon. Your next best bet is a product sold by a third-party seller with the shipment order fulfilled by Amazon. It also pays to be skeptical of reviews, which don\u2019t always equal a quality product, and use review analytics services like Fakespot to determine their legitimacy.\n\nExtremely Unethical Tricks Amazon Sellers Are Using To Rank Fast | Seller At Heart", "sentiment": 0.10611471861471863},
{"link_title": "How Personalities Determine Investment Styles", "url": "http://news.morningstar.com/articlenet/article.aspx?id=824222", "text": "On my desk is a paper that purports to show how investment preferences are correlated with personality. This being 2017 rather than the Reagan years, the paper doesn\u2019t explain that rock-climbers like stocks, while librarians favor bonds. Arguments that physical risk-takers prefer equities have come and gone (and have largely been discredited.) This effort, instead, involves investment styles, and bears a title that only a professor could love: Personality Traits and Portfolio Tilts Toward Value and Size\n\nAll right, two professors\u2014the authors being the duo of Andrew Conlin and Jouko Miettunen, from Finland\u2019s University of Oulu. (A cold place that; the average January high is -8 degrees Celsius, which doesn\u2019t sound a whole lot better when expressed in Fahrenheit.)\n\nFinland, it turns out, takes a rather different view of privacy than does the United States. The authors not only were able to determine \u201cthe number of stocks held, the number of shares owned of each stock, and the value of each position\u201d for each person in their study, by viewing the records of the Finnish Central Securities Depository, but they were also able to access those investors\u2019 psychological-testing records, which are contained in the Northern Finland Birth Cohort 1966 data set. For investment researchers, at least, Finland is heaven on this earth.\n\nThe psychological profile gives the results from a test called the Temperament and Character Inventory (TCI) , which, the authors state, \u201chas been used extensively in the fields of medicine and psychiatry.\u201d The TCI test measures four personality traits, from which the authors select two: novelty-seeking and reward-dependence.\n\n\u201cNovelty-seeking measures the degree to which one exhibits active behavior in response to stimuli and actively seeks pleasure and reward when none is currently on offer.\u201d\n\nRestated into plainer English, people who score highly on novelty-seeking are easily bored. In contrast, those with patience receive low scores.\n\n\u201cReward-dependence measures the degree to which one is emotional and responsive to social stimuli.\u201d\n\nPeople who score highly on reward-dependence care greatly about what others think. Their actions are guided by the social cues that they receive. Conversely, those with low scores are indifferent to public reactions. Frankly, they couldn\u2019t give a damn.\n\nThe authors then tinker with the sub-scores for the two traits to measure related traits. Their conclusions:\n\nIn this context, extravagant means the propensity to spend/splurge, impulsive means the willingness to make decisions based on incomplete information, sentimentality means the tendency to be affected by emotional stimuli, and sociability means the propensity to join groups and feel attached to others.\n\nDo you see yourself in there?\n\nMy take: The first two items make sense. Extravagance and impulsiveness aren\u2019t quite the words that I would use in describing growth-stock fans, based on my conversations with (quite literally) hundreds of growth-style portfolio managers over the years, but they correlate with my general impressions. Those are aspects of a broader personality trait.\n\nThe second two items \u2026 hmmm. They may well be correct. Far be it for me to critique the output of TCI questionnaires, given that I have never taken a psychology class and learned about the existence of the TCI only yesterday\u2014but sentimentality and sociability don\u2019t fit into my growth/value framework. (And I don\u2019t have a framework for large/small preferences; if those investors who purchase large companies differ from those who like smaller firms, I have not noticed that distinction.)\n\nIn my experience, growth-stock managers are optimists. They believe that the U.S. is the greatest country in which to live, that now is the greatest of all times, and that their companies will do great things. They pay higher stock-price multiples for their purchases than do other investors, in exchange for getting something better. So, they must have faith; if they do not, they would cease to be growth-stock owners.\n\nThis faith leads to trust. Much more than value-stock managers, growth-style managers rely on what corporate executives tell them. (Frequently, when interviewing growth managers, I would ask them to discuss why they bought a stock, and they would respond by repeating back to me the company\u2019s public-relations message.) Whether that makes them sociable or sentimental as those terms are used by the Personality Traits paper, I do not know, but it does make them extroverts.\n\nAnd yes, I would agree that growth-stock managers tend to be extravagant and impulsive. The first occurs by definition\u2014they do indeed spend more! The second also follows quite directly from the premise. Growth-stock owners invest in the future; they pay high prices now, to receive a high payoff later. They purchase hopes and dreams. They have no choice but to be impulsive, in the sense of making decisions based on incomplete information, because that is what buying the future entails. The task is not for those who require precision.\n\nIt would be a stretch to call value managers pessimists, in the attempt to make the contrast between them and growth-stock buyers symmetrical. Pessimists don\u2019t buy stocks, for the most part. Rather, value-stock buyers are skeptics. They do believe that happy days will come again, thereby boosting the prices of their holdings, but they dampen their expectations. This is not necessarily the best of all possible worlds, and their companies are not necessarily great. Pretty good is enough.\n\nValue managers invest looking backward, not forward. They do not know what the future will bring to their companies, but they do understand how similar investments have fared in the past. They are market historians, and are far, far likelier than growth-stock managers to follow the academic literature. (The next finance professor I meet who is a growth-stock investor will be my first.) Value investors are data-driven.\n\nThis means that they don\u2019t place much faith in corporate executives. Why would they? If a company has been well managed, and operates in an attractive industry, it almost certainly will cost more than a value manager is willing to pay. Value investors hold businesses that have broken their promises, or which are dragged down by occupying an unprofitable sector. Perhaps those companies\u2019 executives can offer useful guidance\u2014but best not to take too much from them. Trust history\u2019s odds instead.\n\nThe authors conclude by suggesting that their observed personality-based preferences can help to explain why value-style and smaller-company stocks have enjoyed higher returns, over time. Perhaps the existing asset-pricing models should be revised to accommodate these traits. It may even be possible to accomplish that task while retaining the assumption of rationality.\n\nMy conclusion is less theoretical. People are going to do what they wish to do. For most investors, the link between personality and investment style is immaterial\u2014they will own a handful of broadly diversified funds, and thus their portfolios will not strongly tilt toward a particular style. Those of us who are more interested and involved as investors, however, will likely end up following our preferences.\n\nI know that I have. My portfolio shades toward value. If you regularly read this column, your portfolio probably does, too. I would be surprised if I had many growth-stock investors as readers. We might do well in marriage, to the extent that opposites do indeed attract, but not so much with investment matters.\n\nNote: After reading this article, Mr. Conlin responded that perhaps he would test to see if my anecdotal impressions are correct, that growth managers are more trusting and value managers more skeptical. I hope that he does; it's good to have views, but it's even better knowing if those views are correct. Particularly when somebody else is doing the work of checking.\n\nJohn Rekenthaler has been researching the fund industry since 1988. He is now a columnist for Morningstar.com and a member of Morningstar's investment research department. John is quick to point out that while Morningstar typically agrees with the views of the Rekenthaler Report, his views are his own.", "sentiment": 0.17385332921047209},
{"link_title": "NYT thinks Amazon should choose Denver for their new HQ", "url": "https://www.nytimes.com/interactive/2017/09/09/upshot/where-should-amazon-new-headquarters-be.html?mcubz=1", "text": "The company announced this week that it was looking for a metropolitan area in North America with at least a million people, so we\u2019ve started with the map above. (With apologies to Canada, we\u2019ve set aside Toronto and several other large cities because they\u2019re not included in most of the data sets we\u2019ve used to determine which places meet Amazon\u2019s needs.)\n\nAreas with at least one million people where job growth is strong \u2026\n\n\u2026 and the right labor pool is large and growing \u2026\n\nIn these metro areas, more than one in eight workers is in an industry related to tech, science or professional services, according to the census. (That figure is one in five in Raleigh, N.C.; San Francisco; San Jose, Calif.; and Washington). And the segment of the labor pool that Amazon is particularly interested in \u2014 software programmers and designers \u2014 is growing rapidly, according to an analysis of tech jobs by the Brookings Institution. For its recruiting, Amazon also says it requires a strong university system nearby. All of these metro areas offer that, with colleges that include computer science degrees.\n\n\u2026 and the quality of life is high \u2026\n\n\u201cQuality of life\u201d in this context is primarily about two things \u2014 housing costs and amenities \u2014 and striking a balance between them. The Bay Area does a poor job of that, given that its high cost of living is now driving away even tech workers with six-figure salaries. New York loses out because of high housing costs, too, as measured by median rents in census data. Boston is relatively expensive, but we\u2019re keeping it in the running because its tech job pool is so good. Washington is also expensive, but more affordable Baltimore is a commuter rail line away. And Jeff Bezos, for one, has a nice quality of life in D.C. \u2014 he bought a $23 million home there this year.\n\nAs for the amenities, the winning region will also have the restaurants, outdoor recreation, cultural attractions and general cool of Amazon\u2019s first home, Seattle. Urban economists suggest that such amenities are important to explaining the allure of cities. We asked the economist David Albouy to rank these metro areas for us with an index he uses to measure how much people would be willing to sacrifice, in terms of housing costs and commutes, to live in desirable places. On that basis, we cut Charlotte, N.C., and Indianapolis, because they rank lower on the cultural edginess that attracts young, educated workers.\n\n\u2026 and workers can easily get around \u2014 and out of town \u2026\n\nAn Amazon priority is mass transit, and it has asked applicants to provide their traffic congestion rankings during peak commuting hours. These remaining metro areas are among the top 15 in the country in the share of workers who commute by transit, according to the American Community Survey. Gone are those with both weak transit and bad congestion rankings according to the company INRIX: Atlanta, Miami, Dallas and Austin.\n\nAmazon could easily sort through all the criteria above by itself (the company knows data, after all). But there are two questions it can\u2019t answer without information from potential host cities: What are they willing to give the company in incentives and tax breaks? And what real estate is on the table? This leads us to our winner:\n\n\u2026 and there is space and a willingness to pay to play.\n\nIt\u2019s hard to imagine where the Boston region would find the room for a company that will ultimately want up to eight million square feet of office space (the Pentagon, for comparison, has 6.6 million). Mayor Marty Walsh also said on Thursday that Boston is \u201cnot going to get into a bidding war with another city over something like this.\u201d And it\u2019s pretty clear that a bidding war is what Amazon wants.", "sentiment": 0.09756344230482161},
{"link_title": "Tracking the Evolution of the IoT Concept Across Different Application Domains", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5492403/", "text": "The Internet of Things (IoT) has been in the spotlight for the past decade. It is regarded as one of the disruptive technologies of this century [1] and so far, has caught the attention of society, industry and academy as a way of technologically enhancing day to day activities, the creation of new business models, products and services, and as a broad source of research topics and ideas. Several alliances, institutions, enterprises and even governments have understood its importance and identified the potential benefits that can be obtained from the IoT, leading them to undertake strategic projects and initiatives aiming to develop this field and profit from it [2,3,4,5,6,7,8,9]. Though the first idea of IoT emerged no more than two decades ago, the technologies that shape and support it have been in development for many years. As its name suggests, one of the core technologies is the Internet itself, which has its origins in the ARPANET project, started in 1969 with the objectives of developing techniques and obtaining experience on interconnecting computers, improving and increasing computer research productivity through research sharing, and to permit the linking of specialized computers to the many general purpose computer centers in the U.S. Defense Department and in the private and public sectors [10]. Nowadays, the Internet is a global system of networks that interconnect computers using the standard Internet protocol suite. It has significant impact on the world as it can serve billions of users worldwide. Millions of private, public, academic, business, and government networks, of local to global scope, all contribute to the formation of the Internet [11]. Another fundamental technology for the IoT is the embedded computer system. This term was first used in 1974 and describes a computer that is physically incorporated into a larger system whose primary function is not data processing, and integral to such a system from a design, procurement and operations viewpoint [12]. These systems are implemented by using devices like microcontrollers and single board computers (SBC), and have recently gained popularity with affordable and easy to use prototyping platforms as Arduino, Raspberry Pi or Lego Mindstorms. At the early 1990s, Mark Weiser proposed the concept of ubiquitous computing, later regarded as \u201cpervasive\u201d, where the main idea was for computers to be present and invisible in everything [13]. The backbone of the ubiquitous computing paradigm relies on the advances in embedded computing technologies and deploying a ubiquitous network on a scale of hundreds of computers per room. The concept might resemble that of the actual IoT, but at that time, Weiser stated that the main challenge was the design of operating systems that allow software to fully exploit the capabilities of networks, as software systems barely took any advantage of them [13]. By mid-1990s, sensor nodes started developing as several technologies like wireless communications and digital electronics presented important advances. These are tiny modules capable of sensing data, which is then processed and transmitted over a network. Large numbers of sensor nodes allow for the implementation of sensor networks and have applications in several areas [14]. Things in the IoT share some of the characteristics and purpose of sensor nodes. However, despite all the existing work and experience related to these and other areas such as nanotechnology, big data, identification, localization and cloud technologies, there is still no consensus on the definition of the Internet of Things concept, as pointed out by several researchers and evidenced by the number of ideas and conceptions around the topic that can be found in research literature, magazines and websites of alliances, organizations and industry interested on development of the IoT. Since the term was used for the first time in 1999, its scope has widened and different definitions have been proposed, varying according to changes and creation of new technologies or the addition of old ones that have found a place within the IoT. Moreover, new terms have appeared as the IoT has extended into several domains or depending on the intended use of the technology. Taking as a starting point this diversity of views, a systematic literature review (SLR) on the evolution of the concept of the Internet of Things was conducted. It was intended to understand the way the concept has changed and to assess the influential factors that triggered such changes. Certainly, there exist reviews, surveys and overviews that have been performed on the IoT, reflecting a considerable amount of work and analysis in order to obtain important results, but they usually try to cover a comprehensive view of the state of the art in the IoT, its enabling technologies, application areas, research opportunities and common problems. It is of our interest to focus specifically on the IoT concept and the way it is understood across different application domains, to provide a detailed report that serves as a reference for future research on the area. The paper is structured as follows: in Section 2 the methodology for conducting the SLR is described, along with its quantitative results. The definitions and visions of the IoT found with the SLR are presented in Section 3. Next, Section 4 deals with the definitions and properties of things within the IoT. In Section 5, several related concepts and names are briefly mentioned. Results from previous sections are discussed in Section 6, and, finally, the conclusions of the study are presented in Section 7.\n\nA systematic literature review (SLR) is a means of evaluating and interpreting all available research relevant to a specific research question, topic area, or phenomenon of interest [15]. Our topic of interest is the concept of the Internet of Things, the aggregated efforts to define it, the way its definition has evolved and the technologies or factors driving such evolution. Different definitions are found in both scientific and non-scientific publications and forums and, as solutions and applications are developed in several application domains, pertaining names tend to be used. This variety of definitions and names used to refer to the same concept, reflect the existing lack of agreement on a common IoT definition. Therefore, we are conducting this SLR aiming to identify, analyze, understand, and report on the efforts that have been made to define the IoT, providing a solid background for new and current research activities in this field. This section depicts the steps undertaken in this SLR, providing the reasons for each decision and summarizing the obtained results. Two research questions to drive the SLR were formulated as follows: RQ1\u2014How has the definition of the Internet of Things evolved? RQ2\u2014What are the names given to the IoT and how is it understood across different application domains? The first question is oriented to find different definitions for the IoT and to understand how this definition has changed and what the drivers for those changes are. Realizing the IoT as a general designation for a technology, we want to identify the concepts that have been proposed in the different areas using it. So, a second question was formulated, in order to first identify such areas and then the way the IoT is referred and understood in each of them. One restriction to these questions is that we are interested in independent points of view, either coming from academic research groups, alliances or standardization agencies, avoiding any bias towards specific definitions, technologies and/or proprietary solutions from major industries or vendors. Two main sources were selected to carry out the SLR. Both are recognized for listing and indexing publications from well renowned journals and major conferences: Even though these sources would provide sufficient results for the SLR, four more were included as a way to provide a comprehensive and wider search. Most of their publications are indexed in the main sources, but we wanted to avoid missing important papers due to non-indexed recent creation journals or indexing delays. One example of the former is IEEE Internet of Things Journal, that was first published in 2014 and not all of its papers were found while performing searches in the main sources. The secondary sources, sorted alphabetically, are: To answer RQ1, two search strings were structured as follows: ST1\u2014Documents containing the phrase \u201cThe Internet of Things\u201d in the title. These documents were considered as highly probable to contain a definition of IoT. For this query, the following terms were avoided: \u201ccall for papers\u201d, \u201ceditor\u2019s note\u201d, \u201cguest editorial\u201d, \u201cspecial issue\u201d and \u201ctheme issue\u201d, because of preliminary search results did not contribute to our review. ST2\u2014The second search string allowed to expand the search and obtain a comprehensive set of publications about IoT. The phrase \u201cInternet of Things\u201d or its acronym \u201cIoT\u201d in the title of the documents was used. Additionally, the search was narrowed by requiring any of the words \u201cdefinition\u201d, \u201cconcept\u201d, \u201cevolution\u201d, \u201cvision\u201d, \u201cstory\u201d or \u201capproach\u201d to be present on the title, abstract or keywords. Each of the search strings was tailored for the source search engines, and used conforming to what each of them allowed. For instance, among the selected sources, only ACM Digital Library, Elsevier\u2019s Science Direct, Scopus and IEEE Explore support refining search by abstract and keywords. The approach on RQ2 was different, as there was no previous indication on the number of application domains and/or possible names given to the IoT in each of them. Documents returned from the execution of ST1 and ST2 were selected, that clearly indicated a different name for the IoT. Additionally, the analysis of the literature provided a significant number of domains and names that are listed in Section 5. The studies selection stage of the SLR was executed following the flowchart shown in . The quantitative results of executing both search strings are shown in . Main sources are listed first and sorted alphabetically. Classification is made accordingly to the most common names used for types of documents in the different sources. Totals for each source and type of document are indicated in italics in the rightmost column and bottom row respectively for each search string. Flowchart for the search stage of the SLR. Results of executing ST1 and ST2 in each of the selected sources. A first selection of studies was based on the publication title, and classified in two disjoint sets: published in journals, magazines and reviews; and published in conference proceedings and book chapters. This classification was carried out because it is expected that the formers have passed through a more exhaustive reviewing process and results would be more suitable to the SLR\u2019s goals. This first step was made iterating through all the sources for every search string, merging results from the different search engines and eliminating duplicates ( ). Results from different search strings were then merged, keeping them classified and new duplicates found were eliminated ( ). The final selection of primary studies was performed in two steps. The first step involved reading the abstract and keywords, selecting those papers with a clear indication on providing a definition for the IoT or discuss its definition, and excluding those related to its architecture, reference models, related technologies, or applications in specific areas, like automotive, health or fashion. These documents were excluded from the SLR, but kept as separate sets as further stages on our planned research deal with such topics, and they could also provide an answer to RQ2. Even though some terms in the search strings had the purpose of excluding some types of publications, we still found at this step several editorials, technology news, market reports, essays, and project descriptions. These documents were discarded, along with a few others that could not be obtained in full text. Several documents did not provide enough information in the abstract or keywords to decide on either including or excluding them as primary studies. For these cases, a second step was performed, skimming through each document to get a clearer insight on its purpose. At the end, a set of two disjoint lists of documents {PJ, PC} was selected and its elements were used as primary studies for our SLR ( ), where PJ are the primary studies found in journals, magazines and reviews, and PC the ones found in conference proceedings and book chapters. A total of 75 studies were selected for revision, 36 in the PJ set and 39 in PC. Studies in PJ were published from 2002 to 2016, while those in PC were published from 2010 to 2016. Each study was revised to extract the following data: Definition of IoT, either by the authors or adherence to previously published definitions. Technologies that support the definition given or used. Application domains of the IoT, as well as the names used in each domain to refer to the IoT, if any. Additional literature to be included in the review. After fully reviewing the studies, it was found that 29 of the studies in PJ and 22 in PC deal with the concept of the IoT at some extent. Several studies coincide on the lack of a common or unified definition, as the technology itself is still on a maturing stage or it is interpreted in accordance to specific needs, interests, or technical bias of a given group [16,17,18,19,20,21,22,23,24,25,26]. In addition, as expressed by Borgia, the meaning of the term continuously evolves because technology and the ideas behind it change themselves over time [18]. Some of the characteristics of the studies and journals are: The number of citations in the set of selected studies had a mean of 43.081 with a median of 3. Trimming studies with 0 citations makes a mean of 62.5 with a median of 6. In the set of discarded studies, means for each of the previous cases are 6.49 and 9.42, while medians are 1 and 2. Selected studies with at least 1 citation had a mean of 10.92 citations per year, while the discarded studies were cited 1.87 times per year. The average number of years since publication is 4.20 for all the selected studies, and 4.08 for the discarded ones. The average Journal Citations Report (JCR) impact factor (IF) for journals where selected studies were published was 2.055, most of them being in the second quartile of the JCR. The Scimago Journal Ranking (SJR) that Scopus uses to rank journals, had a mean of 0.81 for journals in the set of selected studies, with most of them being in the first quartile of this ranking. IF and SJR means for journals in the set of discarded studies resulted almost identical to means in the set of accepted studies. The following section shows the different ideas, concepts, visions and definitions of the IoT found while analyzing the results of the primary studies from the SLR, as well as gray literature identified during the analysis. They are listed in chronological order, grouping some of the results that share some perspectives, and pointing the most relevant facts. A discussion is included afterwards.\n\nThe notion of IoT has evolved since it was conceived in the late 1990s. Joy initially proposed it as part of his six Webs taxonomy ( ) in a speech at the World Economic Forum in Davos (Switzerland), which was later replicated as a lecture in several technological and academic forums [27]. The sixth of such Webs is referred as device to device (D2D) and defined as an internet of sensors distributed across a mesh network, setting up urban systems for maximum efficiency [28]. Ashton first coined the phrase Internet of Things in 1999 as the title of a presentation where he linked the use of RFID in Procter & Gamble\u2019s supply chain to the Internet. He described a vision where computers would be capable of gathering data without human help and render it into useful information, which would be possible with technologies like sensors and Radio Frequency Identification (RFID) that enable computers to observe, identify and understand the world [29]. Sarma et al. describe a world where every electronic device is interconnected and every object, electronic or not, is electronically tagged with information related to it [30]. Such tags would allow obtaining the information in a remote and contactless fashion, setting the objects as nodes in an internetworked physical world, analogous to the Internet and regarded as a new \u201cInternet of Things.\u201d A key element for this architecture was the Electronic Product Code (EPC) as a means to identify all physical objects and link them to the network [31]. Here, the network was understood as a seamless, ubiquitous and inexpensive system that would automatically link physical objects to the global Internet, adopting standards in cooperation with governing bodies, commercial consortiums and industry groups. The earliest document returned by our searches where the phrase \u201cInternet of Things\u201d appeared as part of the title was written by Schoenberger and published in Forbes Magazine in 2002. Quoting Ashton, the IoT was deemed as a standardized way for computers to understand the real world, portraying RFID-based applications for inventory management and customer experience improvement [32]. A more comprehensive proposal was the so-called Internet-0 (I0) [33]. The potential uses and benefits of connecting everyday objects to a data network were exemplified in a series of exhibits enhanced with embedded computers and sensors. The intention with I0 was not to replace the existing Internet, but provide a compatible layer below it where connected devices depend on existing routers, gateways and name servers. Here, the original Internet idea of linking computer networks into a seamless whole was considered feasible to be extended to networks of all types of devices, a concept known as interdevice internetworking [33]. One of the earliest contributions to defining and understanding the IoT was the International telecommunication Union (ITU) Internet of Things report, published in 2005. They prospect devices and all kinds of things becoming active users of the Internet on behalf of humans, with most of the traffic flowing between them, and a number of active connections that could be measured in terms of tens or hundreds of billions. Connecting inanimate objects and things to communication networks, in addition to the deployment of higher-speed mobile networks that provide an always-on connectivity, would fulfill the vision of a truly ubiquitous network, \u201canytime, anywhere, by anyone and anything\u201d [34]. The ITU portrays the IoT as a virtual world mapping the real world, where everything in our physical environment has its own identity in virtual cyberspace, thus enabling communication and interaction between people and things, and between things. This vision is based on the application of key technological enablers that would account for an expanded Internet, able to detect and monitor changes in the physical status of connected things in real-time [34]. Building on the previous ideas, the concept starts changing from what can get connected to the network, to what can be done with the things that are connected to the network. Haller et al. state that the role of the Internet of Things is to bridge the gap between the physical world and its representation in information systems [35]. In accordance, they define the IoT as \u201cA world where physical objects are seamlessly integrated into the information network, and where the physical objects can become active participants in business processes. Services are available to interact with these smart objects over the Internet, query their state and any information associated with them, taking into account security and privacy issues\u201d. Based on the belief of continuous and steady advances in microelectronics, communications and information technology, Mattern and Fl\u00f6rkemeier see the Internet extending into the real world embracing everyday objects [36]. Physical items connect to the virtual world where they are controlled remotely and can act as physical access points to Internet services. The gap between the virtual and physical world is bridged by several technical developments taken together. Such developments provide the objects with capabilities like communication, cooperation, addressability, identification, sensing, actuation, embedded information processing, localization, and novel user interfaces, which contribute to an evolution of the IoT paradigm that started with the remote identification of objects to a system where smart objects actually communicate with users, Internet services and even among each other. More than defining the IoT, they describe the features of the objects that get connected in the IoT, a characteristic commonly found in the reviewed papers, which is discussed in Section 4. Two definitions that are commonly adopted by researchers or used as a starting point for their own definitions are the results of European Commission promoted initiatives. The first of these concepts was determined within the CASAGRAS project as [37]: \u201cA global network infrastructure, linking physical and virtual objects through the exploitation of data capture and communication capabilities. This infrastructure includes existing and evolving Internet and network developments. It will offer specific object-identification, sensor and connection capability as the basis for the development of independent cooperative services and applications. These will be characterized by a high degree of autonomous data capture, event transfer, network connectivity and interoperability.\u201d Then, the European Commission, through the Cluster of European Research Projects on the Internet of Things (CERP-IoT), defines the IoT as \u201ca dynamic global network infrastructure with self-configuring capabilities based on standard and interoperable communication protocols where physical and virtual things have identities, physical attributes, and virtual personalities and use intelligent interfaces, and are seamlessly integrated into the information network\u201d [38]. Atzori et al. discussed several interpretations and definitions of the IoT [16]. They state the basic idea of this concept is the pervasive presence around us of a variety of things or objects\u2014such as RFID tags, sensors, actuators, mobile phones, etc., which, through unique addressing schemes, are able to interact with each other and cooperate with their neighbors to reach common goals [39]. In their analysis, the IoT paradigm is depicted as the result of the convergence of three main visions: Things-oriented, Internet-oriented, and Semantic-oriented\u2014see [16]. Three main visions of the IoT. Adapted from [ 16 ]. Under this paradigm, three fundamental building blocks of the IoT concept can be inferred: the way objects or things get connected to the network, the capabilities of interaction and communications provided by the latter, and the use and interpretation of the information provided by things. As the authors in [16] suggest, the IoT has the potential to add a new dimension to the process of interaction between people and machines, by enabling communications with and among smart objects, thus leading to the vision of \u2018\u2018anytime, anywhere, any media, anything\u201d communications. Huang and Li analyzed the semantic meaning of the phrase Internet of Things to understand the properties of IoT [40]. They see it as a network for globally sharing information about things, which they refer to as products. After reflecting on both the literal and intrinsic meaning, they propose a semantic meaning for IoT as a global system for sharing product information among interconnected products. A similar approach is offered by Zhang et al., defining the IoT as a network that uses sensing and localization devices attached to things, or articles, connected to the Internet for information exchange and communication, to achieve intelligent recognition, location, tracking, monitoring and management functions [41]. They understand it as an extension and expansion of the existing Internet, which remains as the core and foundation of IoT, with articles that make use of clients for Internet services. According to Ma, the IoT can enable the interconnection and integration of the physical world and the cyberspace; representing the trend of future networking, while leading the third wave of the IT industry revolution [42]. Comparing IoT with Wireless Sensor Networks (WSNs), Internet, ubiquitous networks and further analysis, the author defines IoT as a network that interconnects ordinary physical objects with identifiable addresses, providing intelligent services. The IoT is based on traditional information carriers like the Internet or telecommunication networks. Furthermore, the instrumentation of ordinary objects, their interconnection through autonomic terminals, and providing intelligent pervasive services are listed as important characteristics of the IoT. There are visions where the IoT is perceived as an extension to the existing Internet. This is supported by the fact that, in the IoT, not only subjects but also objects will be connected and enabled to exchange and share information. In addition, the development of the IoT is considered as one of the manifestations of the concept of ubiquitous computing [43]. Coetzee and Eksteen see the IoT as part of the Future Internet [44]. They describe a vision where objects become part of the Internet: where every object is uniquely identified, and accessible to the network, its position and status known, where services and intelligence are added to this expanded Internet, fusing the digital and physical world, ultimately impacting on our professional, personal and social environments. The driver for the IoT is an expansion of the Internet through the inclusion of physical objects combined with an ability to provide smarter services to the environment as more data becomes available. 3.3. A shift Towards Data and Services (Starting in 2012) In most recent definitions and visions, the role of things as data producers and the network as enabler of new services are highlighted. In their work, Miorandi et al. see the IoT as an extension of the existing Internet and the Web into the physical realm [45]. As they state, the IoT uses the Internet as a global platform for letting machines and smart objects communicate, dialogue, compute, and coordinate. These objects act as providers and/or consumers of data related to the physical world, giving the IoT a focus on data and information rather than on point-to-point communications. They define the IoT from a system-level perspective, as a highly dynamic and radically distributed networked system, composed of a very large number of smart objects producing and consuming information. With a different perspective, Mayordomo et al. envisage the IoT as a sort of evolution of the Internet to reach the physical everyday objects. They define it as a new dynamic network of networks, where every daily object can communicate to each other [46]. Lee et al. see the Internet evolving from a network of interconnected computers to a network of interconnected objects, bolstered by the combination of several technologies including embedded microcontrollers, sensors, actuators, network interfaces, and the greater Internet [47]. They recognize three principal viewpoints from where different interpretations of the IoT emerge: Internet or network oriented; Objects or things oriented; and complex distributed systems. In the opinion of Lee et al., these diverging views have brought many attempts to define IoT that it results advisable to move forward with a common understanding in a global perspective. Gubbi et al. discuss that although the definition of \u201cThings\u201d has changed as technology evolved, making it more inclusive in order to cover a wide range of applications, the main goal of making a computer sense information without the aid of human intervention has remained the same [48]. This represents an evolution of the current Internet into a network of interconnected objects, able to harvest information from the environment, interact with the physical world and use existing Internet services for information transfer, analytics, applications and communications. Building on previous works in the field of ubiquitous computing, they provide a user-centric definition for the IoT in the context of smart environments, as the \u201cinterconnection of sensing and actuating devices providing the ability to share information across platforms through a unified framework, developing a common operating picture for enabling innovative applications\u201d [48]. Though the definition might seem too general, the authors claim its purpose is to allow development and deployment of long-lasting applications using the available state of the art protocols at any given point in time, enabled by seamless ubiquitous sensing, data analytics and information representation with cloud computing at the center of a conceptual IoT framework. In June 2012, the ITU approved recommendation ITU-T Y.2060, which provides an overview of the IoT and has the objective of highlighting it for future standardization. As part of the recommendation, they define the IoT as \u201ca global infrastructure for the Information Society, enabling advanced services by interconnecting (physical and virtual) things based on, existing and evolving, interoperable information and communication technologies\u201d, understanding it as a far-reaching vision with technological and societal implications [2]. This definition has since been adopted by several researchers, as it is supported by a comprehensive reference model updated from their 2005 report [34]. Another aspect that gets considered in IoT conceptualizations is that of intelligence. Mzahm et al. introduce it through the concept of Agents of Things, understanding the IoT as the enabler for objects or things to connect and communicate with other objects in the world via the Internet [49]. However, they find some deficiencies as things lack the ability to reason on their environments and make intelligent decisions and actions to achieve their objectives. Accordingly, they infer the system as a whole may be represented as being intelligent but not individual things. This interpretation is shared by Chen et al., who describe the IoT as an intelligent network which connects all things to the Internet for exchanging information and communicating through the information sensing devices in accordance with agreed protocols [19]. Such protocols need not to be reinvented. As suggested by Aggarwal et al., if the objects are uniquely addressable and connected to the Internet, then the information about them can flow through the same protocol that connects our computers to the Internet [50]. Some visions of the IoT highlight the business impact of the IoT as a provider of new services. Alam et al. see the IoT evolving from the field of Machine to Machine (M2M) communication, adding value to services and applications for businesses relying on the M2M value chain, taking advantage of the network infrastructure and computing capabilities to improve operational and business processes and enhance customer experience [51]. As the authors describe it, \u201cthe IoT to be seen tomorrow is a concept that moves beyond the basic connectivity and technological innovations and merges the gap to the envisioned use cases in order to bring the needed functionality and business values. This includes a focus on platforms and how solutions are delivered with horizontal platforms that are able to support a multitude of vertical solutions\u201d. Chaouchi et al. describe the IoT as a provider of new services to networked and connected objects, which in turn will provide services to persons. They imply the Internet model must be adapted to support the connectivity and traffic transport of the new services based upon the connected objects [52]. For Skar\u017eauskien\u0117 and Kalinauskas, the main idea of the IoT is circling around a connected network or networks in which things and other sensor-based objects communicate with each other. The ability to integrate small, energetically efficient and cheap sensor based objects into clouds of networks would cause the new business and service models and would expand beyond the human-machine interaction approach which is most frequently used in Internet-based environments [53]. Even though it has been over a decade of evolution for the IoT, in recent publications the IoT is still considered to be in its early stages. As Borgia points out, IoT refers to an emerging paradigm consisting of a continuum of uniquely addressable things communicating one another to form a worldwide dynamic network [18]. Chen states that in the future, digital sensing, communication, and processing capabilities will be ubiquitously embedded into everyday objects, turning them into the IoT. In this new paradigm, smart devices will collect data, relay the information or context to each another, and process the information collaboratively using cloud computing and similar technologies. Finally, either humans will be prompted to take action, or the machines themselves will act automatically [54]. As a long-term vision, the trend of IoT is the fusion of sensing and Internet, where all the networked things are flexible, smart, and autonomous enough to provide required services [55]. In 2015, the results of a study aimed to define the IoT were published at the IEEE IoT Initiative Web portal. Authors proposed two definitions depending on size and complexity of systems. For low complexity systems, they define the IoT as a network that connects uniquely identifiable things to the Internet, where the things have sensing/actuation and potential programmability capabilities and, through the exploitation of unique identification and sensing, information about the thing can be collected and its state can be changed from anywhere, anytime, by anything. For global distributed systems, where a large number of things can be interconnected to deliver a complex service and support an execution of a complex process, they propose the following definition of IoT [56]: \u201cInternet of Things envisions a self-configuring, adaptive, complex network that interconnects \u2019things\u2019 to the Internet using standard communication protocols. The interconnected things have physical or virtual representation in the digital world, sensing/actuation capability, a programmability feature and are uniquely identifiable. The representation contains information including the thing\u2019s identity, status, location or any other business, social or privately relevant information. The things offer services, with or without human intervention, through the exploitation of unique identification, data capture and communication, and actuation capability. The service is exploited through the use of intelligent interfaces and is made available anywhere, anytime, and for anything taking security into consideration.\u201d This definition encompasses many of the previously found definitions, but we see it more as a comprehensive description of the IoT than a definition. However, viewing the IoT according to the complexity and size of the systems could actually lead to a better understanding and ease implementation of solutions at different scales. Some recent definitions revolve around four basic actions: identifying, sensing, networking and processing. The use of technology required for those actions is evolving in terms of the number and kinds of devices as well as the interconnection of these devices across the Internet. The IoT proposes to attach this technology to everyday devices and making them online, even if they were not initially designed with this capability in mind [22]. Hurlburt describes a model for the IoT involving sensing, thinking, and acting, which usually occur iteratively in that order. He compares sensing to the human five primary senses, thinking to the way the human brain processes information, and acting to interaction, which represents a difference to the concept of autonomy proposed by others [24]. Despite proposing a definition mostly around the interconnection of objects, Pintus et al. support the need for interaction between people and things, which they present as a humanized view of the IoT [57]. Finally, a pair of definitions were found that, as others before, take a data approach, but stress on what is done with data and how it is transformed into knowledge via data mining techniques. Dorsemaine et al. define the IoT as a group of infrastructures interconnecting connected objects and allowing their management, data mining and the access to the data they generate [23]. They identify a connected object as a device equipped with sensors and/or actuators carrying out a specific function and being able to communicate with other equipment. Authors back their definition with a four-layer architecture. On their part, Qin et al. focus their study of the Internet of Things from a data perspective, considering that data is processed differently in the Internet of Things and traditional Internet [11]. In the Internet of Computers, both main data producers and consumers are human beings. However, in the Internet of Things, the main actors become things, which means things are the majority of data producers and consumers. Therefore, computers will be able to learn and gain information and knowledge to solve real world problems directly with the data fed from things. As a goal, computers enabled by the Internet of Things technologies will be able to sense and react to the real world for humans.\n\nAn imperative component of the IoT are the objects that get connected to it, or simply things. Characterizing them helps understanding the capabilities and possibilities of the IoT and that's why many researchers have made an effort in describing and defining things as a means to express their conception of the IoT. As pointed by Coetzee and Eksteen, the definition of things in the IoT vision is very wide and includes a variety of physical elements, like personal objects we carry around such as smart phones, tablets and digital cameras. It also includes elements in our environments as well as things fitted with tags which become connected via a gateway device [44]. In this section, several definitions, descriptions and properties of things, as found in the analysis phase of the SLR, are presented. Within the context of the Internet of Things, a thing is defined as a real/physical or digital/virtual entity that exists and moves in time and space and that can be identified [43]. This definition reminds of that of the spime, presented by Bruce Sterling in his book \u201cShaping Things\u201d. He refers to spimes as manufactured objects whose informational support is so overwhelmingly extensive and rich that they are regarded as material instantiations of an immaterial system. These are digital objects that can be tracked through space and time and contain the data history related to the specific object they represent. Therefore, a key element to the spime is identity, i.e. a spime must be a thing with a name [58]. A more commonly used name is the smart object. Kopetz considers smart objects the building block of the IoT and describes them as everyday physical things that are enhanced by a small electronic device to provide local intelligence and connectivity to the cyberspace established by the Internet [59]. Aggarwal et al. see smart objects as examples of the spime, describing them as tiny computers which have sensors or actuators, and a communication device [50]. Smart objects are also defined by their characteristics, as objects that [45,60]: Have a physical embodiment and a set of associated physical features. Are associated to at least one name and one address. Can sense and store measurements made by sensor transducers associated with them. Have a minimal set of communication functionalities that allow them to make their identification, sensor measurements, and other attributes available to external entities such as other smart objects or systems. May possess means to trigger actions having an effect on the physical reality. Possess some basic computing capabilities which can be used to make decisions about themselves and their interactions with external entities. Usually, things are referred to just as objects. An object in the IoT is regarded to as any machine, device, application, computer, virtual or physical object involved in a communication that could connect to the Internet, and could have the ability to create, request, consume, forward or have access to digital information [61]. There are similar concepts often mentioned in literature like smart parts, smart items or intelligent products. Several properties of things are found in the literature, often with different names but implying the same meaning. In this section, the properties that better describe things in the IoT are listed. This property dates to the initial visions of the IoT where each object was meant to include an RFID tag allowing it to be uniquely identified. In the ITU\u2019s 2005 report, they mention RFID provides the means for location-specific item identification that is fundamental to thing-to-thing communication, implying that tagging virtually every object on earth with an RFID tag would become feasible, and deeming RFID as a key enabler of the IoT [34]. With a more general perspective, S\u00e1nchez L\u00f3pez et al. consider automatic identification technologies as fundamental to the realization of the IoT because they enable things to be linked with their virtual identity on the Internet [60]. Miorandi et al. mention three pillars on which the IoT is built, the first of them being the ability of smart objects to be identifiable [45]. Additionally, Borgia suggests this identification should be based on assigned numbers, names, or addresses [18]. Miorandi et al. say things are associated to at least one name and one address, where the name is a human-readable description of the object and can be used for reasoning purposes, while the address is a machine-readable string that can be used to communicate to the object [45]. Moreover, the vision of the IoT, where billions of smart objects can communicate via the Internet, requires a well-thought-out naming architecture to be able to identify a smart object and to establish an access path to the object [59]. As huge amounts of objects get connected to the IoT, and provided they can be uniquely identified, individual objects will be tracked, its condition and location communicated in real time to a higher-level service [44]. Chaouchi et al. present a classification of objects, adapted from the CASAGRAS project, where they consider aspects of movability, with objects that are inherently mobile and need to be tracked, while others maintain a fixed position and this property is not needed [52]. The way things get connected to the IoT, either wired or wireless, provides a lead to where they might stand in this classification and their need to be tracked. However, as implied by Speed, tracking a thing not only refers to being aware of its physical location, but also to its individual history, since manufacturing to the end of its lifetime [62]. This way, a single thing can provide useful information that wouldn\u2019t be available otherwise. This property refers to the ability of things to collect data from the environment. The ITU names things equipped with sensors as \u201cfeeling things\u201d and consider sensors complement human senses [34]. The use of sensors as a key element of the IoT was introduced in the first visions proposed in the late 1990s [28,29], though the earliest implementations of the IoT focused mainly on identifying, locating and tracking objects. With sensors, things can become aware of their characteristics, context and situation [47]. This is, a thing not only provides information about its environment, but also about its status. By means of actuators, things can influence their environment [44]. This actuation can be based on sensed data and controlled remotely via the Internet [18,63] and is fundamental for automation of processes. The property of processing data and executing commands is frequently mentioned as intelligence. The ITU describes \u201cthinking things\u201d in reference to materials and things that get labeled as smart [34]. S\u00e1nchez L\u00f3pez et al. describe it as embedded processing for local intelligence and autonomy [60]. In [63], Xue et al. give the denomination of robots to things that perform intelligent computing and execute agents. On their part, Razzaque et al. imply that as the processing capabilities of things improve, they can become not only providers of data but also of services [64]. Additional properties are mentioned in literature, though we don\u2019t consider them to be critical to objects in the IoT. However, an interesting feature is proposed by Jazayeri et al., for IoT devices to easily plug and play, each IoT device needs to be self-describable and self-contained in order to communicate with other objects or services, so they can describe and advertise themselves and their capabilities [65]. They also imply a need for interoperability, as communication protocols and data encoding for current IoT devices are usually proprietary and different from each other. By taking the aforementioned properties as capabilities of things, they can be abstracted as devices in overlapping sets that possess one or more of such capabilities. In the context of the IoT, a thing is considered a physical object with attached, embedded or built-in electronic devices with Internet connecting capabilities. Considering as a requirement being connected to the Internet, things can be categorized as follows, where at least one of the first four capabilities is required to be part of the IoT: Identification capability (IC): Things that can be uniquely and unmistakably identified, either by an electronic tag, hard-coded serial number or printed label that is read by another object. Localization capability (LC): Things that know their precise physical location in the world by their own means, e.g. using embedded geolocation devices, and can communicate it to other things and services. Sensing capability (SC): Things equipped with sensors to obtain data from their actual state or the environment. They may or not include temporary storage capabilities or make use of cloud-based storage services. Actuation capability (AC): Things equipped with actuating devices that can be remotely controlled to modify the environment. Processing capability (PC): Things that can process information obtained by themselves or received via the Internet. Connected devices with processing capabilities but none of the previous, are considered as part of the Internet of Computers (IoC), but not the IoT. presents an Euler diagram with the relationships between the capabilities of Internet connected devices. Devices with any of the capabilities in the shaded area of the diagram are considered part of the IoT. The diagram shows that any combination of the first four capabilities is possible, but having processing capabilities in things that can only be identified or located is not seen as an added value, as these types of things only share information about their characteristics or physical location, without creating or transforming data from their environment or received from the Internet. From this, four subsets that result of interest in the IoT are identified and shown in . Capabilities of Internet connected devices as sets. The shaded area represents the set of Things in the IoT. The subsets are described as follows: Trackable Objects (TO): Mobile things that can be uniquely identified and are aware of their physical location. Data Objects (DO): Things producing data either from sensors or their current properties or state. Interactive Objects (IO): Things that allow an interaction with the environment where they are immersed, either by measuring environmental variables, modifying the environment or both. In the figure, the darker area represents the latter type of things. Smart Objects (SO): Interactive things that can apply some degree of processing to data obtained or received and act accordingly. Objects in the darker area have the five capabilities and are the most comprehensive devices in the IoT.\n\n5. Other Names Given to the IoT As the IoT extends into several application contexts, so does the way it is named and understood. This section presents the names given to the IoT in such contexts, as found in the reviewed literature, accompanied by a brief description. Some of these names represent complete new concepts on their own, while others are mere specializations of the IoT. The Web of Things (WoT) is a concept described in [66] as making things web-present by embedding web-servers in them or by hosting their web-presence within a web server. The name was first used as part of Sun Microsystem\u2019s project JXTA [67], defining a set of protocols for building applications and deploying them on a virtual network. More recently, Guinard and Trifa proposed an architecture for making devices an integral part of the Web by using HTTP as an application layer [68]. In this context, the term Web of Objects is also used [52] as well as Physical Web [69]. It is important to note that all of the Web-based visions consider naming services of things as an important feature. Extending from the IoT and the WoT, the notion of Wisdom Web of Things (W2T) represents a holistic intelligence methodology for realizing the harmonious symbiosis of humans, computers, and things in the hyper world. This concept relies on different abstractions of intelligence and the creation of knowledge from data. The word \u201cwisdom\u201d implies that each thing in the WoT can be aware of both itself and others to provide the right service for the right object at a right time and context [70]. A frequently used name is the Future Internet of Things (FIoT), based on the gradual development and incorporation of several innovative techniques into the IoT. Among these techniques, how to extract data from sensing and transfer it into knowledge is usually found [21]. The term Future Internet is often used to make reference to the future conditions and applications that will be available through the Internet. In [49], The Agents of Things (AoT) concept is proposed to mitigate the effect of the loT deficiencies and limitations in terms of intelligence. The core idea of AoT is that everything in this concept should have an internal reasoning and intelligence capability, enabling the things to interact directly with other things in the same or different system types. Building on the idea of adding intelligence to the IoT, the concept of Cognitive Internet of Things (CIoT) is proposed. A CIoT is an IoT with cognitive capability which is integrated to promote performance and achieve intelligence. Given that there is a business process flow corresponding to a given application, CIoT comprehends current business types and network conditions, analyzes the perceived information based on the prior knowledge, makes intelligent decisions, and performs adaptive and control actions, aiming to maximize network performance and meet the application requirements [71]. A different approach is that of the Social Internet of Things (SIoT), which is based on a sort of social relationship among objects, analogous to what happens for human beings. The SIoT represents an innovative paradigm of interaction among objects, where the basic idea is the definition of a \u201csocial network of intelligent objects\u201d [72]. The human perspective of the social aspect of the IoT is represented in the Internet of People (IoP), envisaged as people becoming part of ubiquitous intelligent networks having the potential to seamlessly connect, interact and exchange information about themselves and their social context and environment [73]. Integrating the notions of the IoT, IoP and SIoT allows to present the idea of a Humanized Internet of Things (HIoT), which enables interactions between communicating entities: smart things and people, the physical world and the digital one [57]. The interaction with the environment is also important for any type of system, and that is represented in the Green IoT. It is defined as the energy efficient procedures, either in hardware or software, adopted by IoT to facilitate reducing the greenhouse effect of existing applications and services or to reduce the impact of greenhouse effect of IoT itself [74]. A concept that is used along the IoT is Cyber-Physical Systems (CPS). CPS can be considered a confluence of embedded systems, real-time systems, distributed sensor systems and controls [75]. They are integrations of computation with physical processes by means of embedded computers and networks that monitor and control the physical processes, usually with feedback loops where physical processes affect computations and vice versa [76]. CPS are physical systems designed with the electronic devices for communications, sensing, and controlling as a part of them and hence invisible to the user, providing a sense of immediacy; in the IoT, these devices may be embedded into existing physical systems to get them connected. Hence, some authors identify the IoT as a subset of CPS [18,77]. One of the most popular designations is the Industrial Internet of Things (IIoT) or just Industrial Internet. This is a form of IoT where the devices, or things, are objects in manufacturing plants, dispatch centers, process control industries, etc. [78]. This name, proposed by General Electric, is mostly used in North America, while European initiatives usually rely on the German designation of Industry 4.0, referring to the fourth industrial revolution and often understood as the application of the generic concept of CPS to industrial production systems (cyberphysical production systems) [79]. Several other fields are adopting the IoT and specific interpretations are being proposed. The idea in each case is to take advantage of the sensing, actuating, communications, data processing, identification, and interaction capabilities of objects pertaining to each application domain. Some of the most commonly found in the literature are the Internet of Vehicles [80], Health Internet of Things [81,82], Internet of Personal Health (IoPH) [83], Internet of m-health Things (m-IoT) [84], Internet of Agriculture [85], and Agriculture Internet of Things [86,87].\n\nTwo clear trends were identified in the literature: some authors describe the IoT as an extension to the existing Internet, while others present it as an evolution of the Internet. We find it is important to make a distinction, as the first idea represents that new technology, devices, applications and services are being added and made available through the Internet, while the second implies a progressive change that would end in a replacement of the existing technology. These diverging views clearly represent some of the reasons why this SLR was conducted, and exemplify the lack of consensus on defining the IoT expressed by several authors [16,17,18,19,20,21,22,23,24,25,26]. What can be inferred from the literature is that the Internet of Computers (IoC) and the Internet of Things (IoT) are complementary entities in an ecosystem of Internet-connected devices, providing data and services for each other. Therefore, even though in terms of devices the IoC and IoT can be seen as two disjoint sets in a universe of Internet reachable devices, in terms of data and software, devices in the IoC function as means for accessing those in the IoT for configuration, modification, and data storage, processing and visualization. displays examples of devices and applications that can be found in each of these sets, sharing different types of data, like binary strings containing commands or raw data, documents in application-specific formats, or images. By means of their networking capabilities, elements in both sets can communicate to each other and take advantage of their specific characteristics and features, and the services they provide. Definitions of the IoT tend to be centered on distinct aspects. The initial notions of Ashton and Joy made clear references to the capabilities of things to sense data from the environment. However, for most of the first decade the focus of the definitions was on networking issues and RFID-based services. Later, desired characteristics of communications were integrated into the concept, and the view on services shifted towards business processes, new business models and customer experience, describing services as independent, cooperative, intelligent or advanced. The work of Atzori et al. [16] is the first to explicitly define the IoT in terms of the networks requirements for connecting things, their capabilities for communications and interaction, and the use and interpretation of data. More recent definitions, while still mentioning network and communications characteristics, are more oriented to describe the capabilities and properties of things and the importance of harvesting and processing data that can be turned into knowledge for the improvement of business processes. shows where definitions and visions of the IoT were centered in the selected studies (Only the last name of the first author is shown). Orientation of the definitions and visions of the IoT. In , the most relevant concepts and notions that were used by authors in explaining their visions of IoT are listed. Terms like identification, location, tracking, and specific technologies like RFID were taken off the list as we consider they don\u2019t provide additional information to what has been said before. Also, most of the authors make use of specific applications or whole application fields as to exemplify and describe the potential impact of the IoT. The last three columns, shadowed in light gray, correspond to publications where the authors describe the characteristics and properties of things instead of a definition or vision of the IoT. Some of the publications shown in do not appear in , as the selected concepts are not used by their authors. Concepts and notions used by authors in presenting their visions of IoT. The three most recurring concepts are sensors, intelligence, and actuators. While sensing capabilities have been considered since the first interpretations of the IoT, it is not the case with intelligence and actuators. Intelligence in the IoT is used in several contexts, but its common understanding is the ability to process data, which in its most common use refers to data processing. Concepts like security, privacy, interoperability, and the need for standard communication protocols are often used in presenting the visions, but they are seldom included in the final definitions. The execution of data mining processes is seen as essential for the IoT in recent definitions, as well as the use of cloud technologies for providing services and storing data. This can accelerate knowledge acquisition via information processes and, with the aid of data analytics and visualization techniques improve the outcomes of both business and social processes mediated by things in the IoT, which also will have important benefits for industry, an actor portrayed as one of the main participants in the future development of IoT. As explained in most of the descriptions of the IoT, including those specific to different application domains, a huge number of things will be or have already been deployed, generating data that should be treated with technologies and algorithms for big data, which is also mentioned by some authors. For all connected things, two important factors are for end users to be able to easily find and use them. As we are used to find a website by its name, finding a thing by its name may appear as an obvious choice, and so it is proposed by several authors. On the other side, not many of them seem to pay special attention to the final user means of interaction with things, but those who are concerned with describing things and their properties emphasize the importance of designing proper user interfaces.\n\nThe Internet of Things is the confluence of several technologies that allow providing Internet-based services and applications supported by electronic devices attached to physical things for acquiring data and controlling processes. As a general understanding, this phrase might well describe the IoT, but the variety of definitions and visions found in literature prove defining a complex technology or range of technologies is also a complex process. However, though these definitions might appear as diverging, they are usually presented around five elements, occasionally including more than one of them: networking, services, communications, data and things. Earlier definitions were more commonly centered on networking aspects, while the most recent tend to be more comprehensive, as expected when a technology is in a maturing stage and its scope begins widening and more capabilities and possibilities are discovered. Conducting a SLR on the concept of the IoT allowed us to obtain a clearer insight on this technology and what can be achieved through it. Observing the way these definitions have evolved and how different concepts, technologies and ideas have been incorporated as the IoT develops suggests that a correct description and characterization of the things at the end-points of the IoT should be one of the first goals towards a final definition. Latest efforts are paying more attention to things and what things can do as part of new services, applications and business models inspired by the IoT. Several visions and definitions of the Internet of Things were found and analyzed. They can be categorized in different ways, as to how the technology is understood, which part of the IoT spectrum is the definition biased to, or the technologies that are mentioned as fundamental part of specific visions. These definitions are extended and complemented with specific terms of the different application domains of the IoT, often with designations that researchers adapt to each domain. Just as important is how data gets immersed in the whole concept. Enormous amounts of different data will be gathered from a myriad of sources, and a lot more can be inferred from what is directly obtained by sensors. One of the main goals of the IoT should be the creation of new data and obtaining data that couldn\u2019t be obtained otherwise. So, advanced and novel data mining, big data, and data analytics techniques and algorithms will be required to treat this data and shouldn\u2019t be excluded from a definition of the IoT.", "sentiment": 0.10228925977276908},
{"link_title": "VR64, Virtual Reality Goggles for the Commodore 64", "url": "http://64jim64.blogspot.com/2017/09/vr64-virtual-reality-goggles-for.html", "text": "The VR64 project is my attempt to make VR goggles for the commodore 64.\n\nThe project started with my 12 year old daughter's science fair. She studied VR goggles. We were able to make our own \"Google Cardboard\" style goggles, 3D images, and even a 3D movie. Of course we used our smart phones as screens and video cameras.\n\nI kept thinking, why can't this be done for our beloved commodore 64? So I built the VR64 using three components, a $10 plastic VR goggle, a $26 LCD and a cheap power transformer (plus lots of glue gun fun!).\n\nI split the screen into two sections, one for the left eye and one for the right. Each section is 19 columns by 25 rows, and the center two rows are not used. Each eye, has 152X200 pixels in high resolution and only 76X200 in multi-color mode!\n\nAfter experimenting, I found that objects at the same location in both eyes looked like a \"normal\" distance. If the objects both moved towards the center, through the VR64 it would appear in the same location but closer to the user! I found that I could move objects up to 10 pixels towards the center each (20 pixels closer to each other) and maintain the 3D effect. At a movement of about 13 pixels, my eyes got confused.\n\nAt first I was also moving the objects away from the center, but images with objects both away and towards the center were hard to focus on. After thinking about how the eyes look at objects close and far, it seems obvious that objects far away approach being straight in front of each eye. In no case in reality do the objects move further out. So I settled on objects at the same center being the furthest out and only moving them both towards the center, up to 10 pixels each, as the 3D effect. I actually work quite well!\n\nI made a few 3D images from actual camera shots. They are OK, but not too impressive. I honestly didn't spend too much time on this. It may better to use high resolution dithered images.\n\nOf course you can't have fun on a c64 without playing video games. So I made a VR64 game. This is only my third computer game and my second ML/Assembler program.", "sentiment": 0.11621527777777775},
{"link_title": "Don\u2019t Fall for Loan Sharks Just Because They Have Hip Branding", "url": "http://twocents.lifehacker.com/don-t-fall-for-loan-sharks-just-because-they-have-hip-b-1801045951#", "text": "We all know payday lenders, loan sharks, and credit cards profit when you go into debt and, therefore, they can be dangerous. But many of these companies conceal their danger with clever marketing. Beware: a debt trap by any other branding is just as dangerous.\n\nOver at the Outline, writer Gaby Del Valle discusses one such company, Affirm. Affirm works with over 1,000 retailer partners to allow customers to take out loans for expensive items: a $400 pair of pants, for example. Del Valle writes:\n\nThe difference between this service and a typical subprime loan seems to mostly lie in the marketing. Unlike other loans, Affirm is a bit more upfront about the terms you\u2019re getting into. \u201cWe\u2019re committed to clear, upfront pricing. You\u2019ll never have to worry about extra costs buried in fine print,\u201d their website reads.\n\nGranted, their interest rates, which range from 10 to 30 percent with the average customer taking on 21 percent, aren\u2019t as bad as payday loan rates. But as Del Valle points out, you\u2019d have more luck financing crap you don\u2019t need with a credit card, since credit cards have an average rate of 17 percent. And everyone knows buying crap you don\u2019t need with a credit card is typically a terrible idea.\n\nSomehow, though, this seems different, and it\u2019s all in the branding. Most criticism around Affirm and other fintech products is that they\u2019re just another way to dupe consumers into bad financial decisions. Absolutely, but I think the even more shameful problem is that these companies do this under the guise of helping people. As Affirm\u2019s founder told TechCrunch:\n\n\u201cWe cannot be judgmental but we must be proscriptive. If you can\u2019t afford a $200 dress [or presumably, a $400 pair of pants], maybe we\u2019re not helping those people.\u201d\n\nOkay, fair. But the thing is, Affirm makes it seem like they are helping those people when their website boasts, \u201c...a dwindling number of people can say \u2018I trust my bank to look out for me.\u2019 It doesn\u2019t have to be this way. Affirm\u2019s mission is to fix this problem.\u201d They want to \u201cfix this problem,\u201d and then they advertise the following:\n\nEveryone is picking on Affirm here, but the issue is not unique to them. This reminds me of the recent fiasco with Navient, the student loan servicer that was sued by the Consumer Financial Protection Bureau (CFPB) over shady business practices like misapplying student loan payments. In the lawsuit, Navient said they have no obligation to act in their customers\u2019 best interest. But that\u2019s not exactly the message that comes across on their \u201cFinancial Tips Blog.\u201d These companies use financial literacy to hook you into making bad financial moves.\n\nThey promote transparency and good financial decisions and flexibility (you can get into massive debt, but hey, it\u2019s on your terms!) but the bottom line is, they make money when you fail. Worse, when someone calls them out on it, they say \u201cwe\u2019re a financial product, we\u2019re not here to help people.\u201d That\u2019s fine, but it\u2019s extra irresponsible to pretend you\u2019re not a loan shark with hip, \u201cwe\u2019re not a debt trap\u201d branding.\n\nAs consumers, it seems like we already spend half our lives making sure we\u2019re not getting scammed, but here\u2019s yet another trap to watch out for. Don\u2019t fall for companies that piggyback on financial literacy as if they\u2019re helping you when all they want to do is make money off of your bad decisions.", "sentiment": -0.035298742138364765},
{"link_title": "The first man at trial over a \u201cgig economy\u201d got dismantled on cross-examination", "url": "https://arstechnica.com/tech-policy/2017/09/plaintiff-suing-grubhub-admits-lying-on-application-barely-working-his-shifts/", "text": "The sole plaintiff going to trial over his treatment in the \"gig economy\" has a serious problem. Under cross-examination yesterday, former GrubHub deliveryman Raef Lawson admitted that he lied on his applications to GrubHub, got paid for shifts he barely worked, and took steps to avoid doing some deliveries.\n\nLawson also acknowledged that, before applying to GrubHub, he consulted with his attorney, who has specialized in lawsuits against so-called \"gig economy\" companies, like Uber and Lyft. These companies typically provide workers with part-time work and flexible shifts but few other benefits. And Lawson was fired from another gig economy platform, Postmates, which directly accused him of fraud.\n\nLawson, an aspiring actor who made ends meet with various day jobs, sued GrubHub in 2015. He said that he should have been classified as an employee, not a contractor. He's suing for back wages and overtime. A magistrate judge denied class-action status to Lawson's case, but Lawson and his lawyer have persisted, despite having a total damage claim that amounts to less than $600, and they are now at trial in San Francisco. A win for Lawson could set the stage for future, bigger litigation wins against GrubHub and other gig economy companies.\n\nThere's a substantial body of law about the definitions of employee and independent contractor, but Lawson v. GrubHub is the first case that will result in a judicial ruling on whether a gig economy worker is an employee or an independent contractor.\n\nUnder California law, employees are entitled to minimum wage, overtime, meal periods, and rest breaks. In addition, employees are entitled to have their employers pay for workers' compensation insurance, unemployment insurance, disability insurance, and part of their social security. Those payments aren't required when workers are independent contractors.\n\nWhile many US workers choose to be employees because they get health benefits, there's no legal requirement that any business provide health care to its employees. Under the federal Affordable Care Act, businesses with more than 50 employees must pay a tax penalty if they don't offer health care.\n\nThe line between contractor and worker isn't a bright or clear one, which is one of the reasons Lawson's case is expected to be a two-week trial. Many of the factors involve control over elements of one's work life, including scheduling.\n\nOver the course of Tuesday and Wednesday, Lawson took a few hours of direct questioning from his lawyer, Shannon Liss-Riordan. During that time, Lawson explained that he felt compelled to take every order that GrubHub offered him so that he wouldn't get de-activated. If he didn't get an \"acceptance rate\" of at least 75 percent, he wouldn't get paid the \"true-up\" guaranteed rate, which was $15 in 2015.\n\nLawson said he believed there were \"ghost orders\" that showed up on his pay orders, but they were never sent to him on the app as a result of an \"app glitch\" or poor reception.\n\nWin for ex-Grubhub driver in pending trial may profoundly impact \u201cgig economy\u201d He also testified that he did work for Postmates and Caviar, and that he was afraid of managers at GrubHub finding out about it. Being active on multiple platforms was explicitly authorized by GrubHub's driver contract, but Lawson said he didn't know that because he hadn't read the contract closely when he signed it.\n\nIn early 2016, Lawson was terminated from the GrubHub platform. \"We notice you have not been able to receive orders and have not provided delivery services during the blocks you signed up for,\" read the e-mail from GrubHub management. \"You are being notified that GrubHub is terminating your agreement, effective immediately.\"\n\nAt the end of that testimony, Lawson said that he has also filed worker misclassification claims against other \"gig economy\" companies as well. \"Why have you done that?\" asked his lawyer.\n\n\"Because I think they're taking advantage of people like me,\" said Lawson. \"We need to work, need to pay rent, and they give us a little flexibility. Then they deny we're employees so they can get around wage laws, like paying overtime and reimbursing expenses... I don't think that's right.\"\n\nGrubHub lawyer Michele Maryott began her cross-examination by asking Lawson about the online contract he signed with GrubHub. He had sent an e-mail to the company specifically to opt out of arbitration\u2014and yet he also said he hadn't really read the contract.\n\n\"How did you know there was an arbitration provision in the agreement if you didn't read it?\" Maryott asked.\n\n\"I think I skimmed parts of it,\" Lawson said.\n\n\"You read parts, but you don't know which parts?\" Maryott said.\n\n\"I don't know which parts,\" Lawson said.\n\nIn another part of the contract, Lawson signed that he understood he was creating an independent contractor relationship. In GrubHub's legal lingo, he was a \"primary delivery-service provider,\" or PDSP.\n\n\"Did you think it was OK to lie to get what you wanted?\" Maryott asked.\n\n\"I wouldn't phrase it that way,\" Lawson answered.\n\n\"You wanted to get a job as an independent contractor, and you lied to try to accomplish that?\"\n\n\"I didn't have a choice, so I guess, yes,\" he said.\n\n\"You agreed that, if your relationship was something other than an independent contractor relationship, you would notify GrubHub of that, right?\" Maryott asked, pointing to another spot on the contract where Lawson had initialed.\n\nLawson said that GrubHub started to \"rub him the wrong way\" around November 2015 and that, when he would \"toggle\" the app on to indicate he was available for work, he would get sent more orders than he wanted. Then Maryott entered into a line of questioning about when he first established a relationship with his lawyer.\n\nMaryott: You had already spoken to Ms. Liss-Riordan about a lawsuit, before you had even toggled on [indicating he was available to the app for work] for the first time, right? Maryott: Isn't it true that you contacted Ms. Liss-Riordan's firm in summer 2015? Maryott: And you knew about the Uber independent contractor lawsuit [filed by Liss-Riordan] before you contacted her? Maryott: You asked if there was going to be a lawsuit against GrubHub, didn't you? [Here, Liss-Riordan objected that the question imposed on attorney-client privilege, and the objection was sustained.] Maryott: You signed up in August 2015. By that time, you already had a lawyer, correct? Maryott: But you had an intent to sue GrubHub, before you even signed up? Maryott: But you're not denying it. Lawson: Not denying it, not confirming it.", "sentiment": 0.09898904006046864},
{"link_title": "The History of Silicon Valley: Birth of Computers and the Internet", "url": "https://medium.com/founder-playbook/the-history-of-silicon-valley-part-2-computers-and-the-internet-7cb883eb522f", "text": "Before the development of electronic computers, the term \u201ccomputer\u201d referred to people, not machines. It was a job title, designated to someone who performed mathematical equations and calculations by hand. Typically these \u201ccomputers\u201d were women that, when grouped together were called a computer pool. These women played a major role in advancing the sciences at this time, however they often made mistakes that became quite costly. The world was in need of a machine that could run these calculations faster and more accurately. There were a number of inventors that devised such machines, but they were all built to solve highly specific problems. The notion of a computer as a general purpose machine did not become prominent until after World War II. During World War II, the British needed to devise a way to crack the code that the germans were using to communicate with each other. There were far too many possible combinations for anyone to do this manually before the codes reset each morning. A brilliant mathematician named Alan Turing found a way to use the vacuum tube technology used in radio and television to create a machine that could crack the german code very quickly. This computer was named Colossus (depicted above). In theory the Colossus could be reprogrammed to solve any similar problem, making it one of the first general purpose computers ever. There were a few other similar machines invented around this time frame, two notable ones were the Harvard Mark 1 and the ENIAC. These machines were huge leaps forward, but they had pretty significant drawbacks. They were extremely large, manually programming these machines was very laborious, and they were very prone to break down due to external factors. In fact the computer bug got its name because a moth was caught in the Harvard Mark 1 and it took weeks to be found. In the 1950s and 60s the semiconductor industry was just getting going out in Silicon Valley. William Shockley\u2019s transistors were many times smaller and much more cost effective than vacuum tubes (part 1). Because of this, the idea of computers moved away from top secret government laboratories, and out west, where the parts it needed were becoming commercialized. Soon, Jack Kilby and Robert Noyce (part 1) upgraded the transistor into the even more efficient integrated circuit (IC). Robert Noyce and Gordon Moore ended up creating their own company to create these integrated circuits, and called it Integrated Electronics (Intel for short). Their plan was to create specialized ICs for computers made for solving specific problems. However with the help of some notable venture capitalists and fellow scientists they realized that there was a much better opportunity in selling a general purpose IC that could be used to make just about any computer. This is what is now known as the microprocessor, and is used to power all of our modern day electronics today.\n\nWhen Intel released their 8080 microprocessor chip, they ignited a revolution in Silicon Valley. This revolution came in the form of the Homebrew Computer Club. The club held it\u2019s first meeting in March 1975 was held in one of its members\u2019 garage in Menlo Park in Santa Clara County. This meeting is more or less formally known as the launch of the personal computer revolution. The club arose from a spirit of sharing \u201chard-won technical information\u201d with other computer enthusiasts who developed their devices for the fun of tinkering around in this fascinating field of electronics. Despite their disdain of commercialism, some of these young hobbyists found themselves almost overnight as millionaires when their machines got acquired. Engineer and entrepreneur Len Shustek recalled the Homebrew meetings: \u201cWe sort of all united around this neat technology. There was this notion that we could do things, and we ought to be able to do things, that big companies could.\u201d Homebrew soon outgrew the garage and met in a Stanford auditorium, where engineers, hobbyists, entrepreneurs, activists, and social scientists gathered to share information, buy and trade electronics parts, and even form companies. Apple would likely not have been started if it wasn\u2019t for these meetings, which Steve Wozniak played a central role. This is what Woz had to say about the club: \u201cThe Homebrew Computer Club was the highlight of my life. I was too shy to ever talk in the club meeting, but the way that I could communicate sometimes was by doing good designs. I was very skilled at a certain type of circuit design.\u201d Homebrew\u2019s spirit of information sharing and entrepreneurship mirrored the rest of the Valley at the time. Because of this, they were able to advance the field of electronics into the personal computing industry much faster than any large company or government lab could have. In fact many people think that the Homebrew Computer Club was the origins of the open source movement that is spanning the globe today.\n\nWhen the USSR launched Sputnik satellite, the United States created the Advanced Research Projects Agency (ARPA) in order to stay one step ahead of the soviets. ARPA set up research laboratories at a number of top universities and businesses all throughout the country. The majority of them were at west coast universities where the majority of the top researchers were. One such researcher, Joseph Licklider, convinced ARPA to work on a network that would allow these distributed laboratories to share information more efficiently. The project was named ARPANET. One of ARPANET\u2019s first major innovations was the implementation of packet switching. This is where different computers send messages along the same set of wires instead of each getting one. Every packet had an address which it would look up on a table with all the addresses in the network on it. Then the packet would be sent to a nearby computer that was closest to the destination. Then the second computer would get the packet, look up the address and repeat the process. At the time packets were sent over phone lines. ARPANET was started in 1969. The first node was in UCLA, the second one was at Stanford. Later UC Santa Barbara and University of Utah were added as the third and fourth nodes. The first four nodes of the ARPANET Stanford was central to this early version of the internet. They were the official record keeper of every node on the ARPANET. This meant that they had an address of every computer that was on the network, so when a computer sent a packet to another computer they would look up the address on Stanford\u2019s database. This positioned Silicon Valley as the center of this new network, which greatly accelerated their understanding of how this network could eventually be used. This also brought a lot of government research money over to the Stanford and Silicon Valley area which helped spur even more innovation in the valley.\n\nIn the mid 1970s other countries started to develop their own networks. Everyone formatted their packets differently so it was very difficult to connect two networks to each other. The solution to this was a protocol called TCP/IP which we use today. The TCP was a standard way to format packets and the IP was a standard way of assigning addresses to every computer on the network. This \u201cinternetworking architecture\u201d is where the internet got its name. Map of the internet in 1993 This became an inflection point in the growth of the network. More and more computers joined the internet, which at the time was nothing more than a series of digital bulletin boards and a way to email fellow researchers. This growth eventually meant that Stanford couldn\u2019t continue to be the official record keepers of all of these computers. Thus the ARPANET engineers re-engineered the system, creating the DNS system. Instead of separating each host and arranging them in a random order the hosts were arranged into domains. First came the top level domains like .com and .edu. Within these domains there was a lower level such as mit.edu and apple.com. It\u2019s now the DNS\u2019s job to figure out where to send your packets, not yours. This new DNS system meant the centralization of the internet at Stanford was coming to an end. ARPANET too needed to remove themselves as the backbone of the internet, so ARPA handed control of the internet over to the National Science Foundation (NSF) who eventually handed over the reins to the internet service providers (ISPs) that control the internet today. The engineers at Stanford, previously tasked with maintaining the network, were the most knowledgeable people in the world in terms of its potential. Now that the internet was decentralized they could focus all of their attention on commercializing and building on top of the internet, instead of just maintaining it. This newfound freedom caused the birth of many of the first internet companies right there in the valley.", "sentiment": 0.1759816515914077},
{"link_title": "I downloaded an app. And suddenly, was part of the Cajun Navy", "url": "http://www.houstonchronicle.com/local/gray-matters/article/I-downloaded-an-app-And-suddenly-I-was-talking-12172506.php", "text": "After watching nonstop coverage of the hurricane and the incredible rescues that were taking place, I got in bed at 10:30 on Tuesday night. I had been glued to the TV for days. Every time I would change the channel in an attempt to get my mind on something else for a few minutes, I was drawn right back in.\n\nI finally turned off the TV and picked up my phone to do a quick check of email and Facebook. I read an article about the Cajun Navy and the thousands of selfless volunteers who have shown up to this city en masse. The article explained they were using a walkie-talkie-type app called Zello to communicate with each other, locate victims, get directions, etc. I downloaded the app, found the Cajun Navy channel and started listening.\n\nI was completely enthralled. Voice after voice after voice coming though my phone in the dark, some asking for help, some saying they were on their way. Most of the transmissions I was hearing when I first tuned in were from Houston, but within 30 minutes or so, calls started coming in from Port Arthur and Orange. Harvey had moved east from Houston and was pummeling East Texas.\n\nZELLO: Hurricane Irma just made a digital walkie-talkie the No. 1 app online\n\nCall after call from citizens saying they were trapped in their houses and needed boat rescue. None of the volunteer rescuers had made it to that area from Houston, but as soon as the calls started coming in, they were moving out, driving as fast as they could into the middle of Harvey.\n\nAs I was listening, I quickly figured out that there were a few moderators on the app that were in charge and very experienced in using this method of communication during emergencies. One in particular, Brittney, was giving directions, taking rescue requests, and prioritizing calls and rescues. At one point, she said something that made me realize she's a nurse, so I immediately understood why she was so effective in this situation.\n\nA couple of other women (who were working from other parts of the country, not Houston) who had been taking calls from victims and logging in the information came on the line around 12:30 and said they had to sign off so they could get to bed. They asked if there was anyone who could work through the night to keep taking rescue requests and log them.\n\nI sat up and turned on my light. I timidly pushed the \"talk\" button and said, \"I can.\"\n\nREAD ALSO: How to get around Houston's horrible traffic jams after Harvey\n\nI GOT a two-minute \"training\" session and a \"good luck!\" One of the key suggestions of the training session was that when I received a rescue request, I needed to try to call the person making the request if possible to get more details and to ensure that it was a legitimate request. Unfortunately, there had been reports of people calling in fake rescue requests and then robbing the volunteers when they arrived. Despicable.\n\nAfter I received each request and had called the person making the request, I was to log their information on a designated website, let the requester know the ID number they'd been assigned and move on to the next call.\n\nWithin minutes, I was on the phone with Karen. Karen was in a house in Port Arthur, sitting on her kitchen cabinet with seven other adults, two teenagers and a newborn. The water was almost to the counter tops. I assured here we would get someone to her as soon as we could and told her to stay safe.\n\nREAD ALSO: Flooding causes sinkhole on Beltway 8 frontage road\n\nIt was 1:15 a.m..\n\nBy this time, Cajun Navy rescuers had begun arriving in Port Arthur. They were begging to be let in the water, but the Coast Guard understandably wouldn't grant them permission because the storm was just too strong.\n\nIt was gut-wrenching to hear so many calls coming in and having to tell them there was nothing we could do until the storm calmed down a little. The local authorities were doing the best they could, but they were far outnumbered and also unable to get to everyone in the treacherous conditions.\n\nI took several more calls and quickly realized there was no way I could call to verify every request. They were coming in faster than I could type them into the website data bank. I would listen to the request, write down their info and start typing it in. In the time I could enter one request, three more would come in.\n\nI was originally just sitting up in bed with my laptop on my lap, phone in hand and a notepad on my nightstand. Pretty quickly, I moved to my dining room table, plugged in my computer and phone and poured a huge glass of iced tea.\n\nI started out taking notes nice and neat on printer paper. That quickly turned into chaotic scribbles. I was having trouble reading my own handwriting at times.\n\nI got a request from Chad. I had enough time to call him. Trapped in their house, he and his wife had water up to their chests. He told me they were about to go to their attic. I begged him not to do that and told him he had to go to his roof instead. He said there was no way for them to do that. I told him he didn't have a choice. I asked him to keep calling 911, over and over. When we hung up, I texted him other numbers to try \u2014 the Coast Guard, the Jefferson County Office of Emergency Management, the Air Force.\n\nIt was 2:20 a.m..\n\nI spoke to another woman whose name I can't even remember. I didn't call her directly but we had a few exchanges through the app. She told me she and her kids were sitting on their kitchen counter and needed rescuing, but she was scared to get off the counter when boats arrived because there were snakes in the water in their house.\n\nI took request after request after request. Name...phone number...address...number of adults...number of children...number of elderly...medical conditions. I would then type this information in as fast I could so the dispatchers could send the rescuers out. After submitting the information, I received an ID number that I was supposed to relay to the person requesting the rescue. We asked them to remember the number so they could give it to their rescuers when they were finally picked up. We could then mark them safe in the system, avoiding the dilemma of rescuers looking for people who had already been saved by someone else.\n\nIt was around this time that I heard one of the dispatchers who goes by Goose ping in to our channel to let us know that the Cajun Navy still had no boats on the water. Conditions were still too dangerous. I had mistakenly assumed we had boats in the water by then.\n\nNo wonder we had so many people desperately begging for rescue. No one was coming for them.\n\nAll night long I had been telling them to \"hang on, we'll be there soon.\" I didn't know I had been lying to them.\n\nAROUND 3 a.m., I got a request from a teenage boy in Orange who was screaming so hysterically I couldn't even understand him. I got his phone number and told him I'd call him directly. The second he answered, he was screaming that his brother and cousin were laying in the backyard, unresponsive, possibly electrocuted.\n\nI'm sad to say that I don't even remember this boy's name. I know I asked, but in the conversation that ensued, I forgot it. He told me that his brother and cousin had been near a shed in the backyard for over an hour, but they couldn't get to them because of the rising water and the storm.\n\nI told him they needed to try to get to them and that I was getting help to them as soon as I could. I think he thought I was an official 911 dispatcher, as he kept asking me why the police weren't there. He said he'd called 911 \"at least 100 times\" and they never answered. He then told me he and another cousin were going to go outside to check on the young men in the yard. I told him I'd wait. He put the phone down. I listened. And waited.\n\nI could hear panicked conversation and rain and sloshing water. After a very long seven or eight minutes, I suddenly heard the most blood-curdling, gut-wrenching screaming I've ever heard.\n\nI heard a little girl screaming at the top of her lungs.\n\nI heard a boy's voice screaming \"no, no, no, noooooo\" over and over.\n\nI felt nauseated. And completely helpless. I started screaming into the phone...\"Hello! Hello!\"\n\nHe picked up the phone.\n\n\"Miss, I think my brother is dead! He's not breathing! Should we do CPR? What do we do?\"\n\n\"Do you know CPR? Yes, try CPR!\"\n\n\"What do I do?\" he screamed.\n\nBefore I could answer, he dropped the phone again. More chaos. More screaming. Guttural. Desperate. He came back to the phone.\n\n\"He's not moving! I don't know what to do! I have to go get my cousin!\"\n\nI asked him to put his mom on the phone.\n\n\"Hello, I'm Holly. I'm trying to get some help to you. Tell me what's going on. What's your name?\"\n\n\"Margaret. My boy is gone! His lips are purple. He's gone.\"\n\n\"Margaret, I'm so very sorry. Where is your nephew?\"\n\n\"He's in the yard. They're trying to get him now.\"\n\n\"Who else is with you?\"\n\nMargaret told me she was with her other kids \u2014 four or five people total, if I remember correctly \u2014 and that they were up to their waists in water.\n\n\"My boy is on the table.\" Her voice cracked. \"They're out there trying to get my nephew now. Please get someone here, please,\" she begged.\n\nI assured her we would. But I knew there were still no boats in the water.\n\nI hung up and called the Coast Guard number we'd been given. They answered immediately, but the person I was talking to was actually in Houston. I quickly explained who I was and what I had just experienced and gave them Margaret's address. He assured me he would let the Coast Guard in Orange know about the Davis family.\n\nI hung up and called the Jefferson County Office of Emergency Management. Shockingly, he answered on the second ring.\n\n\"Hi, my name is Holly Har-\"\n\n\"I know why you're calling! Where are you?\"\n\n\"I don't need help. I'm working with the Cajun Navy dispatchers and need someone to get to a family I just spoke with.\"\n\nI explained the situation and gave them the Davises' address\n\n\"I know you're doing the best you can. Just please get to this family.\"\n\n\"We will. We're going to have a lot of deaths here tonight.\"\n\nI got up from my table to take a break and try to process what had just happened. I had just interjected myself into a family's most horrible moment. As quickly as I had crossed paths with them, they were gone. A 15-minute interaction that will stay with me for a lifetime.\n\nI went to the bathroom, refilled my tea, walked around a bit, thinking to myself, \"What are you doing?? You're not qualified to do this!\"\n\nThen I sat back down and went back to it.\n\nAROUND 4:30, I got a request from a young woman in Beaumont who was trying to get her 87-year-old grandfather, Chester, rescued in Port Arthur. He lived alone and had water to his shins. I couldn't hear her well through the app, so I called her directly. She told me her grandfather couldn't get through to 911 and she was really scared for him. I assured her someone would get to him and that he would be okay.\n\nThere were still no Cajun Navy boats in the water.\n\nAt some point, I'd heard another volunteer mention that a woman who lived on Sassine Street and her three kids had retreated to their attic to escape rising waters. I pinged in and told the volunteer that she had to call the woman back and tell her to get out of the attic and go to her roof.\n\nThe volunteer came back on the line and said that she'd talked to the woman, but she refused to move because her kids couldn't swim. I asked if she had anything they could use to break through the attic roof. No.\n\nWe got word around 7:30 a.m. Wednesday \u2014 seven hours after the first calls stared coming in from Port Arthur \u2014 that the Cajun Navy had finally been let in the water. Reports of rescues started coming in. I was finally able to mark one of my cases \"safe.\"\n\nI kept taking calls all day Wednesday. Throughout the night and into Wednesday, I was texting with Chad and Shaundra, the young woman calling for her grandfather.\n\nChad told me the water was almost to their necks and they still hadn't gone to the roof.\n\nShaundra texted me repeatedly, asking why no one had gotten to her grandfather. The water had risen to his chest. I promised her someone would get there.\n\nThe rescues and the \"safe\" status reports were increasing by the hour. I turned on the TV at some point and started seeing scenes of the same people and situations I was listening to on the app.\n\nAround 10:00, I heard one of the rescuers who uses the handle Cowboy ask about \"the woman in the attic on Sassine Street.\" I immediately pinged in, and Cowboy asked me to call him. He wanted the address again and wanted to know when we had last heard from the lady in the attic. I told him I had no idea because the volunteer who originally took that call had signed off.\n\nCowboy said he was a few minutes away from Sassine St. and didn't know if he should request another boat with \"breaching equipment\" or a helicopter. I suggested helicopter, hoping the family had somehow made it to the roof.\n\nThe calls for rescue were slowing down but continued to come in at a steady pace. Every 20 to 30 minutes, I'd remind the rescuers that Chester, Shaundra's grandfather, still needed a rescue from 19th Street. And I kept telling Shaundra that they would get there.\n\nShe finally said she was just going to get in the car and drive from Beaumont to Port Arthur to get him herself. I told her to be careful and let me know she made it. 20 minutes later she texted me to say that they'd been stopped by flood waters and couldn't get there. She told she was afraid he was going to die.\n\nAround 11:30, I realized I hadn't heard Cowboy on the line with a report about Sassine Street. I asked on the app if we had had any update.\n\nMy phone rang. It was Cowboy.\n\n\"We got to Sassine. It's confirmed.\"\n\n\"Confirmed?\" I frantically asked. \"Confirmed what? What does that mean? Does that mean they're dead?\"\n\n\"Yes. Water past the roof. They never left the attic. We sent divers in.\"\n\nI thanked him for letting me know and off he went to the next rescue.\n\nAT 3:02 p.m., I got a text from Shaundra that said \"[Mam], I thank you so much. He is on his way to the bowling alley.\" A few minutes later: \"Thank you [mam]. He was on a boat at first now he is on a truck.\"\n\nI let out a huge sigh of relief. I think I may have actually said \"Thank you, God\" out loud.\n\nI texted Chad at 5:30 p.m. to see if he was safe. I didn't hear back from him until 7:30 Thursday morning: \"We are safe now.\"\n\nI pinged Goose to ask him if he knew if Margaret, the mother who lost her son and her nephew, and her other kids had been rescued. He said they had.\n\nI have texted Margaret to ask her how she was doing. I still haven't heard from her. I've been scanning reports from Orange to see if her family has been mentioned. I need to know the names of the two boys who died.\n\nAt 6 p.m. Wednesday, I closed my laptop. I'd been awake 34 hours and wasn't even tired. I was emotionally drained, but there was no way I could've slept right then. I thought back on the last day and half and couldn't believe what I had just heard and experienced.\n\nEven as I type this, it seems surreal. I don't know how police officers and firefighters and 911 dispatchers and EMTs do this every day.\n\nWhat I do know: I am grateful beyond measure that they do it.\n\nAnd thank God for the Cajun Navy. How many more people would be dead today if not for our first responders and the thousands of volunteers here? What if a flood of this magnitude had happened 20 years ago, before cell phones and social media? The deaths would be in the hundreds.\n\nI saw a meme on Facebook today that said, \"Someone needs to erect a statue honoring the regular dude with a bass boat.\" It was meant to be funny, but it's actually spot-on.\n\nOn Thursday, I got another text from Shaundra. It was a picture of her and her grandfather. I sent a selfie back to her and told her I was going to find a way to meet them in person someday. I really hope I get to do that.\n\nHolly Hartman has been a teacher for 22 years. She currently teaches journalism and is the yearbook and newspaper adviser at Memorial High School in Spring Branch ISD. This story originally appeared as a post on her Facebook page.\n\nBookmark Gray Matters. Then pour yourself a huge glass of iced tea.", "sentiment": 0.08370162231559292},
{"link_title": "There are 2.4M rental bicycles in Beijing, city says enough is enough", "url": "http://www.scmp.com/tech/start-ups/article/2110210/there-are-24-million-rental-bicycles-beijing-and-city-says-enough", "text": "", "sentiment": 0.0},
{"link_title": "How Apple Finally Made Siri Sound More Human", "url": "https://www.wired.com/story/how-apple-finally-made-siri-sound-more-human/", "text": "The first time Alex Acero saw Her , he watched it like a normal person. The second time, he didn't watch the movie at all. Acero, the Apple executive in charge of the tech behind Siri , sat there with his eyes closed, listening to how Scarlett Johansson voiced her artificially intelligent character Samantha. He paid attention to how she talked to Theodore Twombly, played by Joaquin Phoenix, and how Twombly talked back. Acero was trying to discern what about Samantha could make someone fall in love without ever seeing her.\n\nWhen I ask Acero what he learned about why the voice worked so well, he laughs because the answer is so obvious. \"It is natural!\" he says. \"It was not robotic!\" This hardly counts as a revelation for Acero. Mostly, it confirmed that his team at Apple has spent the last few years on the right project: making Siri sound more human.\n\nThis fall, when iOS 11 hits millions of iPhones and iPads around the world, the new software will give Siri a new voice. It doesn't include many new features or tell better jokes, but you'll notice the difference. Siri now takes more pauses in sentences, elongates syllables right before a pause, and the speech lilts up and down as it speaks. The words sound more fluid and Siri speaks more languages, too. It's nicer to listen to, and to talk to.\n\nApple spent years re-architecting the technology behind Siri, transforming it from a virtual assistant into the catch-all term for all the artificial intelligence powering your phone. It has relentlessly expanded into new countries and languages (for all its faults, Siri's by far the most worldly assistant on the market). And slowly at first but more quickly now, Apple has worked to make Siri available anywhere and everywhere. Siri now falls under the control of Craig Federighi, Apple's head of software, indicating that Siri's now as important to Apple as iOS.\n\nIt'll still be a while before the tech's good enough to make you fall in love with your virtual assistant. But Acero and his team think they've taken a giant leap forward. And they believe firmly that if they can make Siri sound less like a robot and more like someone you know and trust, they can make Siri great even when it fails. And that, in these early days of AI and voice technology, might be the best-case scenario.\n\nIf you want a good example of why Apple likes to control everything about its products, just look at Siri. Six years after its launch, Siri has by most accounts fallen behind in the virtual assistant race . Amazon's Alexa has more developer support; Google Assistant knows more stuff; both are available in many kinds of devices from many different companies.\n\nApple says it's not its fault. When Siri first launched, another company provided the back-end technology for voice recognition. All signs point to Nuance as that company, though neither Apple nor Nuance ever confirmed a partnership. Whoever it was, Apple happily blames them for Siri's early issues. \"It was like running a race and, you know, somebody else was holding us back,\" says Greg Joswiak, Apple's VP of product marketing. Joswiak says Apple always had big plans for Siri, \"this idea of an assistant you could talk to on your phone, and have it do these things for you in a more easy way,\" but the tech just wasn't good enough. \"You know, garbage in, garbage out,\" he says.\n\nA few years ago, the team at Apple, led by Acero, took control of Siri's back-end and revamped the experience. It's now based on deep learning and AI, and has improved vastly as a result. Siri's raw voice recognition rivals all its competitors, correctly identifying 95 percent of users' speech. The AI works in two distinct and critical parts of the system: speech-to-text, in which Siri tries to figure out what you said; and text-to-speech, in which Siri speaks back.\n\nAmong Siri's most important jobs entails distinguishing your voice from everyone else's, especially as these systems become more personalized. The more data Siri has, and the better Apple's models become, the more it can discern between people and understand even heavy accents. It's also a security concern: Researchers recently found they could communicate with Siri at frequencies too high for humans to hear, rendering the hack invisible. Siri needs to learn to separate human speech from machine speech, and your speech from everyone else's.\n\nOne helpful way to understand how these systems work is through Apple's process of teaching Siri a new language. When bringing Siri into a new market\u2014say, Shanghai\u2014the team first finds pre-existing databases of local speech. They supplement that by hiring local voice talent, and having them read books, newspapers, web articles, and more.\n\nApple's team transcribes those recordings, matching words to sounds\u2014and more importantly, identifying phonemes, the individual sounds that make up all speech. (In English, \"fourteen\" is a word, the toothy \"e\" sound in the middle is a phoneme.) They try to capture these phonemes spoken in every imaginable way: trailing off at the end of the word, harder at the beginning, longer before a pause, rising in a question. Each utterance has a slightly different sound wave, which Apple's algorithms analyze to find the best fit for any given sentence. Every sentence Siri speaks contains dozens or hundreds of these phonemes, assembled like magazine cut-outs in a ransom note. It's likely that none of the words you hear Siri say were actually recorded the way they're spoken.\n\nAcero offers an example: \"You want to watch this?\" versus \"I like your watch.\" In the first case, Acero's voice naturally ticks upward as he says \"watch,\" but moves down in the latter. \"It's the same word, but it sounds completely different,\" Acero says. He couldn't use the same recording of the word \"watch,\" or even the same individual phonemes, in both sentences. Systems that do sound like your old GPS navigating to \u201cone Siiiix NINE fourteenth STREET PhilaDELphia.\u201d It's hard to listen to, especially for more than a few words at a time.\n\nEven a few years ago, computers and servers didn't offer enough processing power to pore over a vast database to find the perfect combination of sounds for every call and response. Now that they do, Acero and his team want as much data as possible. So once they've built an initial model, they roll out Siri in what they call \"dictation-only mode.\" You can't talk to Siri, but you can tap the microphone button and dictate a text message or web search. This gives Apple's machines inputs from many accents, different-quality microphones, and a variety of situations, all of which make Siri work better for more people . Apple collects (anonymously, it says) and transcribes that data, improving the algorithms and training the networks. They supplement with location-specific data and spoken customs\u2014you'd say the score is three-zero in the US, but three-nil in the UK\u2014and continue to refine the system until Siri has a near-perfect understanding both of what Shanghainese words are, and how people say them.\n\nAt the same time, Apple launches an epic search for the right voice talent. They begin with hundreds of people, all brought in to record a sampling of things Siri might say. Acero then works with Apple's designers and user-interface team to decide which voices they like best. This part skews more art than science\u2014they're listening for some ineffable sense of helpfulness and camaraderie, spunky without being sharp, happy without being cartoonish.\n\nThe next part is all science. \"There are many voice talents that sound good,\" Acero says, \"but it doesn't mean they'd be a good text-to-speech voice.\" They run speech through the models they've built looking for what's called phoneme variability\u2014essentially, the sound-wave difference between the left and right side of each tiny utterance. More variability within a phoneme makes it hard to stitch a lot of them together in a natural-sounding way, but you'd never hear the problems listening to them speak. Only the computer sees the difference. \"It\u2019s almost like when you\u2019re doing wallpaper on a wall, and you have to look at the seams to make sure they line up,\" Acero says.\n\nWhen they find the person who sounds right to both human and computer, Apple records them for weeks at a time, and that becomes the voice of Siri. This has been the process for each of Siri's 21 supported languages, localized for 36 countries\u2014more than all its major competitors combined. In all, 375 million people use Siri every month. That's a big number, especially for a much-panned voice assistant with a long list of serious flaws.\n\nStill, 375 million people pales next to the billion-plus Apple devices in use around the world. Nearly everything Apple sells includes Siri, from iPhone to Apple Watch to MacBook to Apple TV . At some point soon, analysts estimate more than a billion iPhones alone will be active simultaneously. Siri's a popular and important feature, but it\u2019s not quite ubiquitous. And for most people, it\u2019s definitely not essential; you don't need Siri to function the way you need your phone. Now that Apple has an assistant it trusts, it has to teach people how to use it.\n\nAll you need to know about Apple's intentions for Siri can be gleaned from one commercial . The spot follows Dwayne Johnson through a day in his life with his sidekick Siri. Johnson uses Siri to check his calendar while working out and zen-gardening; he checks his reminders; he summons a Lyft, which of course he drives; he checks the weather while speeding recklessly; he checks his email while painting the Sistine Chapel; he does centiliter conversions with his hands full; he FaceTimes and takes selfies from space. Siri calls him \"Mr. Big, Bald, and Beautiful,\" in a way that hopefully will feel slightly less uncomfortable in iOS 11.\n\nFrom the beginning, Joswiak says, Apple wanted Siri to be a get-shit-done machine. It drives him crazy that people compare virtual assistants by asking trivia questions, which always makes Siri look bad. \"We didn't engineer this thing to be Trivial Pursuit!\" he says.\n\nInstead, Joswiak is still focused on helping people do more with the help of an automated friend. He points to Siri's ability to do complicated file-search on the Mac, or the upcoming HomePod 's deep knowledge of music. Another example came a few days after our meeting, when Siri won a technical Emmy for its voice search and controls. There really is something wonderful about saying, \"Hey Siri, rewind two minutes,\" and watching it happen.\n\nSiri can't do everything, or even most things. It's most useful for saving you a few taps and types, not solving complicated trivia or debating whether we're living in a simulation. Yet because Siri shows no bounds\u2014you can ask it anything\u2014users will try everything. \"It is not trivial for users to know what they can say,\" Acero says. Part of his job entails helping Siri communicate its skills better, and fail gracefully when it must. \"We're trying to endow Siri with these kind of capabilities, where it may know what it doesn't know,\u201d he says. \u201cBut that's a tough problem.\u201d Apple's website, and even its commercials, are designed to help people better understand what Siri can and can't do.\n\nAnother challenge is just getting people to remember Siri exists. \"People have their habits of doing something,\" Acero says. \"If they're used to typing, all of a sudden changing that, it takes a while.\" So Apple's trying to nudge users in the right direction. In iOS 11, Siri becomes a lot more present and a lot more proactive. It'll watch you browse the web and then suggest Apple News stories for you to read, or help you add a calendar event for the massage you just booked through Groupon. The new Siri is a shape-shifter, syncing your settings between devices so no matter what gadget you're using, Siri knows you as well as always.\n\nOver the years, Apple's been slow to let developers integrate with Siri. While Alexa and, to a lesser extent, Google Assistant have encouraged others to build apps for and including their assistants, Siri's walls have stayed closed. All those things The Rock can do, he can only do in Apple's own apps. It refuses to acknowledge the existence of Google Maps or Outlook on your phone, and certainly won't turn on any light bulbs made without HomeKit. Last year, the company cautiously let more developers in, allowing users to use Siri to make calls with WhatsApp, summon a ride from Uber, or send money with Venmo. The doors creak wider in iOS 11, but only slightly.\n\nSuch slow-moving has cost Apple its lead in many people's eyes, as Amazon and Google hoover up developer support and race ahead in features. Joswiak at least projects patience. The question, he says, is not how many things Siri could do. \"It's 'how do you do it right?' Because what we didn't want to do is become prescriptive.\" He bristles at Amazon's and Google's demanding syntax, which require you to say things like, \"Alexa, ask Daily Horoscopes about Taurus\u201d or \"OK Google, let me talk to Todoist.\" He'd rather wait until you just say what you want, however you want, and have it happen. Apple, as always, prefers doing nothing to doing something halfway.\n\nThe syntax problem ultimately comes back to the same thing Acero heard listening to Samantha and Theodore Twombly fall in love on-screen. The best computers\u2014even the science-fiction ones\u2014sound human. \"It has the right pauses, the right intonations, smooth voice,\" he says. \"And just a little bit metallic in the sound.\" He wants to build something that good, and give it to everyone. Anytime you want to check the progress, just check in with Siri.\n\nUPDATE: This story now spells Greg Joswiak's name correctly.", "sentiment": 0.1822367260779278},
{"link_title": "The Rohingya Insurgency", "url": "https://en.wikipedia.org/wiki/Rohingya_insurgency_in_Western_Myanmar?wprov=sfla1", "text": "The Rohingya insurgency in Western Myanmar is an ongoing insurgency in northern Rakhine State, Myanmar (formerly known as Arakan, Burma), waged by insurgents belonging to the Rohingya ethnic minority. Most clashes have occurred in the Maungdaw District, which borders Bangladesh.\n\nFrom 1947 to 1961, local mujahideen fought government forces in an attempt to have the mostly Rohingya populated Mayu peninsula in northern Rakhine State secede from Myanmar, so it could be annexed by East Pakistan (present-day Bangladesh).[24] During the late 1950s and early 1960s, the mujahideen lost most of its momentum and support, resulting in most of them surrendering to government forces.[25][26]\n\nIn the 1970s Rohingya Islamist movements began to emerge from remnants of the mujahideen, and the fighting culminated with the Burmese government launching a massive military operation named Operation King Dragon in 1978.[27] In the 1990s, the well-armed Rohingya Solidarity Organisation was the main perpetrator of attacks on Burmese authorities near the Myanmar-Bangladesh border.[28]\n\nIn October 2016, clashes erupted on the Myanmar-Bangladesh border between government security forces and a new insurgent group, Harakah al-Yaqin, resulting in the deaths of at least 40 people (excluding civilians).[29][30][31] It was the first major resurgence of the conflict since 2001.[2] In November 2016, violence erupted again, bringing the death toll to 134.[12]\n\nDuring the early hours of 25 August 2017, up to 150 insurgents launched coordinated attacks on 24 police posts and the 552nd Light Infantry Battalion army base in Rakhine State, leaving 71 dead (12 security personnel and 59 insurgents). It was the first major attack by Rohingya insurgents since November 2016.[32][22][33]\n\nThe Rohingya people are an ethnic minority that live mainly in the northern region of Rakhine State, Myanmar, and have been described as one of the world's most persecuted minorities.[34][35][36] They describe themselves as descendants of Arab traders who settled in the region many generations ago.[34] Some scholars have stated that they have been present in the region since the 15th century.[37] However, they have been denied citizenship by the government of Myanmar, which sees them as illegal immigrants from Bangladesh.[34] In modern times, the persecution of Rohingyas in Myanmar dates back to the 1970s.[38] Since then, Rohingya people have regularly been made the target of persecution by the government and nationalist Buddhists.[39]\n\nIn May 1946, Muslim leaders from Arakan, Burma (present-day Rakhine State, Myanmar) met with Muhammad Ali Jinnah, the founder of Pakistan, and asked for the formal annexation of two townships in the Mayu region, Buthidaung and Maungdaw, by East Pakistan (present-day Bangladesh). Two months later, the North Arakan Muslim League was founded in Akyab (present-day Sittwe, capital of Rakhine State), which also asked Jinnah to annex the region.[40] Jinnah refused, saying that he could not interfere with Burma's internal matters. After Jinnah's refusal, proposals were made by Muslims in Arakan to the newly formed post-independence government of Burma, asking for the concession of the two townships to Pakistan. The proposals were rejected by the Burmese parliament.[41]\n\nLocal mujahideen were subsequently formed against the Burmese government,[42] and began targeting government soldiers stationed in the area. Led by Mir Kassem, the newly formed mujahideen movement began gaining territory, driving out local Rakhine communities from their villages, some of whom fled to East Pakistan.[43]\n\nIn November 1948, martial law was declared in the region, and the 5th Battalion of the Burma Rifles and the 2nd Chin Battalion were sent to liberate the area. By June 1949, the Burmese government's control over the region was reduced to the city of Akyab, whilst the mujahideen had possession of nearly all of northern Arakan. After several months of fighting, Burmese forces were able to push the mujahideen back into the jungles of the Mayu region, near the country's border with East Pakistan.\n\nIn 1950, the Pakistani government warned its counterparts in Burma about their treatment of Muslims in Arakan. Burmese Prime Minister U Nu immediately sent a Muslim diplomat, Pe Khin, to negotiate a memorandum of understanding, so that Pakistan would cease sending aid to the mujahideen. In 1954, Kassem was arrested by Pakistani authorities, and many of his followers surrendered to the government.[4]\n\nThe post-independence government accused the mujahideen of encouraging the illegal immigration of thousands of Bengalis from East Pakistan into Arakan during their rule of the area, a claim that has been highly disputed over the decades, as it brings into question the legitimacy of the Rohingya as an ethnic group of Myanmar.[25]\n\nBetween 1950 and 1954, the Burma Army launched several military operations against the remaining mujahideen in northern Arakan.[44] The first military operation was launched in March 1950, followed by a second named Operation Mayu in October 1952. Several mujahideen leaders agreed to disarm and surrender to government forces following the successful operations.[40]\n\nIn the latter half of 1954, the mujahideen again began to carry out attacks on local authorities and military units stationed around Maungdaw, Buthidaung and Rathedaung. In protest, hundreds of Rakhine Buddhist monks began hunger strikes in Rangoon (present-day Yangon),[25] and in response the government launched Operation Monsoon in October 1954.[40] The Tatmadaw managed to capture the main strongholds of the mujahideen and managed to kill several of their leaders. The operation successfully reduced the mujahideen's influence and support in the region.[11]\n\nIn 1957, 150 mujahideen, led by Shore Maluk and Zurah, surrendered to government forces. On 7 November 1957, 214 additional mujahideen under the leadership of al-Rashid disarmed and surrendered to government forces.[26]\n\nIn the beginning of the 1960s, the mujahideen began to lose its momentum after the governments of Myanmar (Burma) and Pakistan (which controlled Bangladesh at the time) began negotiating on how to deal with the insurgents at their border. On 4 July 1961, 290 mujahideen in southern Maungdaw Township surrendered their arms in front of Brigadier-General Aung Gyi, the then Deputy Commander-in-Chief of the Burmese Army.[45] On 15 November 1961, the few remaining mujahideen surrendered to Aung Gyi in the eastern region of Buthidaung.[25]\n\nA few dozen insurgents remained under the command of Zaffar Kawal, another group of 40 insurgents were led by Abdul Latif, and a mujahideen faction of 80 insurgents were led by Annul Jauli. All these groups lacked local support and a unifying ideology, which lead them to become rice smugglers around the end of the 1960s.[26]\n\nOn 15 July 1972, former mujahideen leader Zaffar Kawal founded the Rohingya Liberation Party (RLP), after mobilising various former mujahideen factions under his command. Zaffar appointed himself Chairman of the party, Abdul Latif as Vice Chairman and Minister of Military Affairs, and Muhammad Jafar Habib as the Secretary General, a graduate from Rangoon University. Their strength increased from 200 fighters in the beginning to 500 by 1974. The RLP was largely based in the jungles of Buthidaung, and were armed with weapons smuggled from Bangladesh. After a massive military operation by the Tatmadaw (Myanmar Armed Forces) in July 1974, Zaffar and most of his men fled across the border into Bangladesh.[26][46]\n\nIn 1974, Muhammad Jafar Habib, the former Secretary of the RLP, founded the Rohingya Patriotic Front (RPF), after the failure and dissolution of the RLP. The RPF had around 70 fighters,[26][2] Habib as self-appointed Chairman, Nurul Islam, a Yangon-educated lawyer, as Vice-Chairman, and Muhammad Yunus, a medical doctor, as Secretary General.[26]\n\nIn March 1978, government forces launched a massive military operation named Operation King Dragon in northern Arakan (Rakhine State), with the focus of expelling Rohingya insurgents in the area.[27] As the operation extended farther northwest, hundreds of thousands of Rohingyas crossed the border seeking refuge in Bangladesh.[2][47][48]\n\nIn 1982, more radical elements broke away from the Rohingya Patriotic Front (RPF), and formed the Rohingya Solidarity Organisation (RSO).[1][2] It was led by Muhammad Yunus, the former Secretary General of the RPF. The RSO became the most influential and extreme faction amongst Rohingya insurgent groups; by basing itself on religious grounds it gained support from various Islamist groups, such as Jamaat-e-Islami in Bangladesh and Pakistan, Hizb-e-Islami in Afghanistan, Hizb-ul-Mujahideen (HM) in the Indian state of Jammu and Kashmir, and Angkatan Belia Islam sa-Malaysia (ABIM) and the Islamic Youth Organisation of Malaysia in Malaysia.[2][48]\n\nOn 15 October 1982, the Burmese Citizenship Law was introduced, and with the exception of the Kaman people, most Muslims in the country were denied an ethnic minority classification, and thus were denied Burmese citizenship.[49]\n\nA more moderate Rohingya insurgent group, the Arakan Rohingya Islamic Front (ARIF), was founded in 1986 by Nurul Islam, the former Vice-Chairman of the Rohingya Patriotic Front (RPF), after uniting remnants of the old RPF and a handful of defectors from the RSO.[2]\n\nIn the early 1990s, the military camps of the Rohingya Solidarity Organisation (RSO) were located in the Cox's Bazar District in southern Bangladesh. RSO possessed a significant arsenal of light machine-guns, AK-47 assault rifles, RPG-2 rocket launchers, claymore mines and explosives, according to a field report conducted by correspondent Bertil Lintner in 1991.[28] The Arakan Rohingya Islamic Front (ARIF) was mostly armed with British manufactured 9mm Sterling L2A3 sub-machine guns, M-16 assault rifles and .303 rifles.[28]\n\nThe military expansion of the RSO resulted in the government of Myanmar launching a massive counter-offensive to expel RSO insurgents along the Bangladesh-Myanmar border. In December 1991, Tatmadaw soldiers crossed the border and accidentally attacked a Bangladeshi military outpost, causing a strain in Bangladeshi-Myanmar relations. By April 1992, more than 250,000 Rohingya civilians had been forced out of northern Rakhine State (Arakan) as a result of the increased military operations in the area.[2]\n\nIn April 1994, around 120 RSO insurgents entered Maungdaw Township in Myanmar by crossing the Naf River which marks the border between Bangladesh and Myanmar. On 28 April 1994, nine out of twelve bombs planted in different areas in Maungdaw by RSO insurgents exploded, damaging a fire engine and a few buildings, and seriously wounding four civilians.[50]\n\nOn 28 October 1998, the Rohingya Solidarity Organisation merged with the Arakan Rohingya Islamic Front and formed the Arakan Rohingya National Organisation (ARNO), operating in-exile in Cox's Bazaar.[2] The Rohingya National Army (RNA) was established as its armed wing.\n\nOne of the several dozen videotapes obtained by CNN from Al-Qaeda's archives in Afghanistan in August 2002 allegedly showed fighters from Myanmar training in Afghanistan.[3] Other videotapes were marked with \"Myanmar\" in Arabic, and it was assumed that the footage was shot in Myanmar, though this has not been validated.[2][48] According to intelligence sources in Asia,[who?] Rohingya recruits in the RSO were paid a 30,000 Bangladeshi taka ($525 USD) enlistment reward, and a salary of 10,000 taka ($175) per month. Families of fighters who were killed in action were offered 100,000 taka ($1,750) in compensation, a promise which lured many young Rohingya men, who were mostly very poor, to travel to Pakistan, where they would train and then perform suicide attacks in Afghanistan.[2][48]\n\nThe Islamic extremist organisations Harkat-ul-Jihad al-Islami[51] and Harkat-ul-Ansar[52] also claimed to have branches in Myanmar.\n\nOn 9 October 2016, hundreds of unidentified insurgents attacked three Burmese border posts along Myanmar's border with Bangladesh.[53] According to government officials in the mainly Rohingya border town of Maungdaw, the attackers brandished knives, machetes and homemade slingshots that fired metal bolts. Several dozen firearms and boxes of ammunition were looted by the attackers from the border posts. The attack resulted in the deaths of nine border officers.[30] On 11 October 2016, four soldiers were killed on the third day of fighting.[31] Following the attacks, reports emerged of several human rights violations allegedly perpetrated by Burmese security forces in their crackdown on suspected Rohingya insurgents.[54]\n\nGovernment officials in Rakhine State originally blamed the Rohingya Solidarity Organisation (RSO), an Islamist insurgent group mainly active in the 1980s and 1990s, for the attacks;[55] however, on 17 October 2016, a group calling itself Harakah al-Yaqin (Faith Movement in English) claimed responsibility.[56] In the following days, six other groups released statements, all citing the same leader.[57]\n\nThe Myanmar Army announced on 15 November 2016 that 69 Rohingya insurgents and 17 security forces (10 policemen, 7 soldiers) had been killed in recent clashes in northern Rakhine State, bringing the death toll to 134 (102 insurgents and 32 security forces). It was also announced that 234 people suspected of being connected to the attack were arrested.[12][58]\n\nNearly two dozen prominent human rights activists, including Malala Yousafzai, Archbishop Desmond Tutu and Richard Branson, called on the United Nations Security Council to intervene and end the \"ethnic cleansing and crimes against humanity\" being perpetrated in northern Rakhine State.[59]\n\nA police document obtained by Reuters in March 2017 listed 423 Rohingyas detained by the police since 9 October 2016, 13 of whom were children, the youngest being ten years old. Two police captains in Maungdaw verified the document and justified the arrests, with one of them saying, \"We the police have to arrest those who collaborated with the attackers, children or not, but the court will decide if they are guilty; we are not the ones who decide.\" Myanmar police also claimed that the children had confessed to their alleged crimes during interrogations, and that they were not beaten or pressured during questioning. The average age of those detained is 34, the youngest is 10, and the oldest is 75.[14][15]\n\nThe Myanmar Armed Forces (Tatmadaw) stated on 1 September that the death toll had risen to 370 insurgents, 13 security personnel, two government officials and 14 civilians.[13]", "sentiment": 0.05387949949593785},
{"link_title": "Remains from Viking Warrior\u2019s Grave Identified as Female", "url": "https://www.thelocal.se/20170908/confirmed-viking-warrior-was-a-woman", "text": "Have researchers finally discovered Sweden's real-life version of Lady Brienne of Tarth or Xena the Warrior Princess? New evidence suggest they actually have\u2026\n\nFor more than a century, archaeologists and historians have assumed that the remains of a person found buried along with arms and horses in one of the most spectacular graves discovered in the Viking Age town of Birka, in Sweden, belonged to a man. Turns out they were wrong. Osteology- and DNA tests now show that that he has always been a she, and she was most likely a powerful military leader.\n\n\u201cIt\u2019s actually a woman, somewhere over the age of 30 and fairly tall too, measuring around 170 centimetres,\u201d Charlotte Hedenstierna-Jonson, an archeologist at Uppsala University, told The Local of the findings that were published in the American Journal of Physical Anthropology on Friday.\n\n\u201cAside from the complete warrior equipment buried along with her \u2013 a sword, an axe, a spear, armour-piercing arrows, a battle knife, shields, and two horses \u2013 she had a board game in her lap, or more of a war-planning game used to try out battle tactics and strategies, which indicates she was a powerful military leader. She\u2019s most likely planned, led and taken part in battles,\u201d she said.\n\nREAD ALSO: Ten words you didn't know came from the Vikings\n\n\n\n What the grave may have looked like. Illustration by Evald Hansen based on Hjalmar Stolpe's excavations at Birka in the 19th century (Stolpe 1889).\n\nREAD MORE: Why Games of Thrones is based on Sweden\n\nThe grave, which Hedenstierna-Jonson describes as the world\u2019s \u201cultimate warrior Viking grave\u201d, was discovered and excavated by Swedish archeologist Hjalmar Stolpe at the end of the 19th century. Because of the \u201cmanly\u201d warrior equipment found in the grave, it was just assumed \u2013 rather than proven \u2013 that the remains were that of a man.\n\nBut a few years ago, Anna Kjellstr\u00f6m, an osteologist at the Stockholm University, brought out the remains to study them for another research project and noticed that something was amiss. The cheekbones were finer and thinner than that of a man, and the hip bones were typically feminine. An osteological analysis was carried out, lending even more support to her suspicion.\n\nNow, however, a DNA-analysis has been carried out, clearly confirming that the Viking warrior was indeed a woman.\n\n\u201cThis image of the male warrior in a patriarchal society was reinforced by research traditions and contemporary preconceptions. Hence, the biological sex of the individual was taken for granted,\u201d Hedenstierna-Jonson, Kjellstr\u00f6m and the eight other researchers behind the study, wrote in their report.\n\n\n\n Another drawing of what the grave may have looked like. Illustration: \u00de\u00f3rhallur \u00der\u00e1insson (copyright: Neil Price)\n\n\u201cThough some Viking women buried with weapons are known, a female warrior of this importance has never been determined and Viking scholars have been reluctant to acknowledge the agency of women with weapons,\u201d they said.\n\nHedenstierna-Jonson said the woman was likely a warrior herself.\n\n\u201cYou can\u2019t reach such a high (military) position without having warrior experience, so it\u2019s reasonable to believe that she took part in battles.\u201d\n\nHedenstierna-Jonson described it as a fantastic find, but said it is unlikely to completely up-end historians\u2019 view of the Viking society as being patriarchal, mainly constituting of male warriors.\n\n\u201cIt was probably quite unusual (for a woman to be a military leader), but in this case, it probably had more to do with her role in society and the family she was from, and that carrying more importance than her gender.\u201d\n\nHedenstierna-Jonson said that since the first indications came through that the warrior was a woman, the researchers have been met with a fair share of skepticism, with critics questioning whether the bones analyzed are actually from that specific grave.\n\n\u201cI think that\u2019s because of how we view history, and many of us would like to think that we live in the best (and more gender-equal) of worlds now.\u201d", "sentiment": 0.13770159219311762},
{"link_title": "Cost of Push Notifications for Smartphones Using Tor Hidden Services", "url": "http://ieeexplore.ieee.org/document/7966975/", "text": "Some users are experiencing issues with previewing PDFs. We are working on a solution.For more information, see our Resources and Help FAQs", "sentiment": 0.5},
{"link_title": "NASA's plan to save Earth from a supervolcano", "url": "http://www.bbc.com/future/story/20170817-nasas-ambitious-plan-to-save-earth-from-a-supervolcano", "text": "Lying beneath the tranquil settings of Yellowstone National Park in the US lies an enormous magma chamber. It\u2019s responsible for the geysers and hot springs that define the area, but for scientists at Nasa, it\u2019s also one of the greatest natural threats to human civilisation as we know it: a potential supervolcano.\n\nFollowing an article we published about supervolcanoes last month, a group of Nasa researchers got in touch to share a report previously unseen outside the space agency about the threat \u2013 and what could be done about it.\n\n\u201cI was a member of the Nasa Advisory Council on Planetary Defense which studied ways for Nasa to defend the planet from asteroids and comets,\u201d explains Brian Wilcox of Nasa\u2019s Jet Propulsion Laboratory (JPL) at the California Institute of Technology. \u201cI came to the conclusion during that study that the supervolcano threat is substantially greater than the asteroid or comet threat.\u201d\n\nThere are around 20 known supervolcanoes on Earth, with major eruptions occurring on average once every 100,000 years. One of the greatest threats an eruption may pose is thought to be starvation, with a prolonged volcanic winter potentially prohibiting civilisation from having enough food for the current population. In 2012, the United Nations estimated that food reserves worldwide would last 74 days.\n\nWhen Nasa scientists came to consider the problem, they found that the most logical solution could simply be to cool a supervolcano down. A volcano the size of Yellowstone is essentially a gigantic heat generator, equivalent to six industrial power plants. Yellowstone currently leaks about 60-70% of the heat coming up from below into the atmosphere, via water which seeps into the magma chamber through cracks. The remainder builds up inside the magma, enabling it to dissolve more and more volatile gases and surrounding rocks. Once this heat reaches a certain threshold, then an explosive eruption is inevitable.\n\nBut if more of the heat could be extracted, then the supervolcano would never erupt. Nasa estimates that if a 35% increase in heat transfer could be achieved from its magma chamber, Yellowstone would no longer pose a threat. The only question is how?\n\nOne possibility is to simply increase the amount of water in the supervolcano. But from a practical perspective, it would likely be impossible to convince politicians to sanction such an initiative.\n\n\u201cBuilding a big aqueduct uphill into a mountainous region would be both costly and difficult, and people don\u2019t want their water spent that way,\u201d Wilcox says. \u201cPeople are desperate for water all over the world and so a major infrastructure project, where the only way the water is used is to cool down a supervolcano, would be very controversial.\u201d\n\nInstead Nasa have conceived a very different plan. They believe the most viable solution could be to drill up to 10km down into the supervolcano, and pump down water at high pressure. The circulating water would return at a temperature of around 350C (662F), thus slowly day by day extracting heat from the volcano. And while such a project would come at an estimated cost of around $3.46bn (\u00a32.69bn), it comes with an enticing catch which could convince politicians to make the investment.\n\n\u201cYellowstone currently leaks around 6GW in heat,\u201d Wilcox says. \u201cThrough drilling in this way, it could be used to create a geothermal plant, which generates electric power at extremely competitive prices of around $0.10/kWh. You would have to give the geothermal companies incentives to drill somewhat deeper and use hotter water than they usually would, but you would pay back your initial investment, and get electricity which can power the surrounding area for a period of potentially tens of thousands of years. And the long-term benefit is that you prevent a future supervolcano eruption which would devastate humanity.\u201d\n\nBut drilling into a supervolcano does not come without certain risks. Namely triggering the eruption you\u2019re intending to prevent.\n\n\u201cThe most important thing with this is to do no harm,\u201d Wilcox says. \u201cIf you drill into the top of the magma chamber and try and cool it from there, this would be very risky. This could make the cap over the magma chamber more brittle and prone to fracture. And you might trigger the release of harmful volatile gases in the magma at the top of the chamber which would otherwise not be released.\u201d\n\nInstead, the idea is to drill in from the supervolcano from the lower sides, starting outside the boundaries of Yellowstone National Park, and extracting the heat from the underside of the magma chamber. \u201cThis way you\u2019re preventing the heat coming up from below from ever reaching the top of the chamber which is where the real threat arises,\u201d Wilcox says.\n\nHowever those who instigate such a project will never see it to completion, or even have an idea whether it might be successful within their lifetime. Cooling Yellowstone in this manner would happen at a rate of one metre a year, taking of the order of tens of thousands of years until just cold rock was left. Although Yellowstone\u2019s magma chamber would not need to be frozen solid to reach the point where it no longer posed a threat, there would be no guarantee that the endeavour would ultimately be successful for at least hundreds and possibly thousands of years.\n\nBut to prevent a catastrophe, such long-term thinking and planning may be the only choice. \u201cWith a project like this, you\u2019d start the process and the main ongoing benefit you\u2019d see in everyday terms is this new supply of electrical power,\u201d Wilcox says.\n\nSuch a plan could be potentially applied to every active supervolcano on the planet, and Nasa\u2019s scientists are hoping that their blueprints will encourage more practical scientific discussion and debate for tackling the threat.\n\n\u201cWhen people first considered the idea of defending the Earth from an asteroid impact, they reacted in a similar way to the supervolcano threat,\u201d Wilcox says. \u201cPeople thought, \u2018As puny as we are, how can humans possibly prevent an asteroid from hitting the Earth.\u2019 Well, it turns out if you engineer something which pushes very slightly for a very long time, you can make the asteroid miss the Earth. So the problem turns out to be easier than people think. In both cases it requires the scientific community to invest brain power and you have to start early. But Yellowstone explodes roughly every 600,000 years, and it is about 600,000 years since it last exploded, which should cause us to sit up and take notice.\u201d\n\nJoin 800,000+ Future fans by liking us on Facebook, or follow us on Twitter.\n\nIf you liked this story, sign up for the weekly bbc.com features newsletter, called \u201cIf You Only Read 6 Things This Week\u201d. A handpicked selection of stories from BBC Future, Earth, Culture, Capital, and Travel, delivered to your inbox every Friday.", "sentiment": 0.0983439007681432},
{"link_title": "What Clocks Have to Do with Quantum Computation", "url": "https://quantumfrontiers.com/2017/09/07/what-clocks-have-to-do-with-quantum-computation/", "text": "Have you ever played the game \u201ctelephone\u201d? You might remember it from your nursery days, blissfully oblivious to the fact that quantum mechanics governs your existence, and not yet wondering why Fox canceled Firefly. For everyone who forgot, here is the gist of the game: sit in a circle with your friends. Now you think of a story (prompt: a spherical weapon that can destroy planets). Once you have the story laid out in your head, tell it to your neighbor on your left. She takes the story and tells it to her friend on her left. It is important to master the art of whispering for this game: you don\u2019t want to be overheard when the story is passed on. After one round, the friend on your right tells you what he heard from his friend on his right. Does the story match your masterpiece?\n\nIf your story is generic, it probably survived without alterations. Tolstoy\u2019s War and Peace, on the other hand, might turn into a version of Game of Thrones. Passing along complex stories seems to be more difficult than passing on easy ones, and it also becomes more prone to errors the more friends join your circle\u2014which makes intuitive sense.\n\nLet\u2019s add maths to this game, because why not. Take a difficult calculation that follows a certain procedure, such as long division of two integer numbers.\n\nNow you perform one step of the division and pass the piece of paper on to your left. Your friend there is honest and trusts you: she doesn\u2019t check what you did, but happily performs the next step in the division. Once she\u2019s done, she passes the piece of paper on to her left, and so on. By the time the paper reaches you again, you hopefully have the result of the calculation, given you have enough friends to divide your favorite numbers, and given that everyone performed their steps accurately.\n\nI\u2019m not sure if Feynman thought about telephone when he, in 1986, proposed a method of embedding computation into eigenstates (e.g. the ground state) of a Hamiltonian, but the fact remains that the similarity is striking. Remember that writing down a Hamiltonian is a way of describing a quantum-mechanical system, for instance how the constituents of a multi-body system are coupled with each other. The ground state of such a Hamiltonian describes the lowest energy state that a system assumes when it is cooled down as far as possible. Before we dive into how the Hamiltonian looks, let\u2019s try to understand how, in Feynman\u2019s construction, a game of telephone can be represented as a quantum state of a physical system.\n\nIn this picture, represents a snapshot of the story or calculation at time t\u2014in the division example, this would be the current divisor and remainder terms; so e.g. the snapshot represents the initial dividend and divisor, and the person next to you is thinking of , one step into the calculation. The label in front of the tensor sign is like a tag that you put on files on your computer, and uniquely associates the snapshot with the t-th time step. We say that the story snapshot is entangled with its label.\n\nThis is also an example of quantum superposition: all the are distinct states (the time labels, if not the story snapshots, are all unique), and by adding these states up we put them into superposition. So if we were to measure the time label, we would obtain one of the snapshots uniformly at random\u2014it\u2019s as if you had a cloth bag full of cards, and you blindly pick one. One side of the card will have the time label on it, while the other side contains the story snapshot. But don\u2019t be fooled\u2014you cannot access all story snapshots by successive measurements! Quantum states collapse; whatever measurement outcome you have dictates what the quantum state will look like after the measurement. In our example, this means that we burn the cloth bag after you pick your card; in this sense, the quantum state behaves differently than a simple juxtaposition of scraps of paper.\n\nNonetheless, this is the reason why we call such a quantum state a history state: it preserves the history of the computation, where every step that is performed is appropriately tagged. If we manage to compare all pairs of successively-labeled snapshots (without measuring them!), one can verify that the end result does, in fact, stem from a valid computation\u2014and not just a random guess. In the division example, this would correspond to checking that each of your friends performs a correct division step.\n\nSo history states are clearly useful. But how do you design a Hamiltonian with a history state as the ground state? Is it even possible? The answer is yes, and it all boils down to verifying that two successive snapshots and are related to each other in the correct manner, e.g. that your friend on seat t+1 performs a valid division step from the snapshot prepared by the person on seat t. In fancy physics speak (aka Bra-Ket notation), we can for example write\n\nThe actual Hamiltonian will then be a sum of such terms, and one can verify that its ground state is indeed the one representing the history state we introduced above.\n\nI\u2019m glossing over a few details here: there is a minus sign in front of this term, and we have to add its Hermitian conjugate (flip the labels and snapshots around). But this is not essential for the argument, so let\u2019s not go there for now. However, you\u2019re totally right with one thing: it wouldn\u2019t make sense to write down all snapshots themselves into the Hamiltonian! After all, if we had to calculate every snapshot transition like in advance, there would be no use to this construction. So instead, we can write\n\nPerfect. We now have a Hamiltonian which, in its ground state, can encode the history of a computation, and if we replace the transition operator with another desired transition operator (a unitary matrix), we can perform any computation we want (more precisely, any computation that can be written as a unitary matrix; this includes anything your laptop can do). However, this is only half of the story, since we need to have a way of reading out the final answer. So let\u2019s step back for a moment, and go back to the telephone game.\n\nOk, let\u2019s assume we give them a little incentive: offer $1 to the person on your right in case the result is an even number. Will he cheat? With so much at stake?\n\nIn fact, maybe your friend is not only greedy but also dishonest: he wants to hide the fact that he miscalculates on purpose, and sometimes tells his friend on his right to make a mistake instead (maybe giving him a share of the money). So for a few of your friends close to the person at the end of the chain, there is a real incentive to cheat!\n\nWe already discussed how to write down a Hamiltonian that verifies valid computational steps. But can we do the same thing as bribing your friends to procure a certain outcome? Can we give an energy bonus to certain outcomes of the computation?\n\nIn fact, we can. Alexei Kitaev proposed adding a term to Feynman\u2019s Hamiltonian which raises the energy of an unwanted outcome, relative to a desirable outcome. How? Again in fancy physics language,\n\nWhat this term does is that it takes the history state and yields a negative energy contribution (signaled by the minus sign in front) if the last snapshot is an even number. If it isn\u2019t, no bonus is felt; this would correspond to you keeping the dollar you promised to your friend. This simply means that in case the computation has a desirable outcome\u2014i.e. an even number\u2014the Hamiltonian allows a lower energy ground state than for any other output. Et voil\u00e0, we can distinguish between different outputs of the computation.\n\nThe true picture is, of course, a tad more complicated; generally, we give penalty terms to unwanted states instead of bonus terms to desirable ones. The reason for this is somewhat subtle, but can potentially be explained with an analogy: humans fear loss much more than they value gains of the same magnitude. Quantum systems behave in a completely opposite manner: the promise of a bonus at the end of the computation is such a great incentive that most of the weight of the history state will flock to the bonus term (for the physicists: the system now has a bound state, meaning that the wave function is localized around a specific site, and drops off exponentially quickly away from it). This makes it difficult to verify the computation far away from the bonus term.\n\nSo the Feynman-Kitaev Hamiltonian consists of three parts: one which checks each step of the computation, one which penalizes invalid outcomes\u2014and obviously we also need to make sure the input of the computation is valid. Why? Well, are you saying you are more honest than your friends?\n\nIf there is one thing I\u2019ve learned throughout my PhD it is that we should always ask what use a theory is. So what can we learn from this construction? Almost 20 years ago, Alexei Kitaev used Feynman\u2019s idea to prove that estimating the ground state energy of a physical system with local interactions is hard, even on a quantum computer (for the experts: QMA-hard under the assumption of a promise gap splitting the embedded YES and NO instances). Why is estimating the ground state energy hard? The energy shift induced by the output penalty depends on the outcome of the computation that we embed (e.g. even or odd outcome). And as fun as long division is, there are much more difficult tasks we can write down as a history state Hamiltonian\u2014in fact, it is this very freedom which makes estimating the ground state energy difficult: if we can embed any computation we want, estimating the induced energy shift should be at least as hard as actually performing the computation on a quantum computer. This has one curious implication: if we don\u2019t expect that we can estimate the ground state energy efficiently, the physical system will take a long time to actually assume its ground state when cooled down, and potentially behave like a spin glass!\n\nFeynman\u2019s history state construction and the QMA-hardness proof of Kitaev were a big part of the research I did for my PhD. I formalized the case where the message is not passed on along a unique path from neighbor to neighbor, but can take an arbitrary path between beginning and end in a more complicated graph; in this way, computation can in some sense be parallelized.\n\nWell, to be honest, the last statement is not entirely true: while there can be parallel tracks of computation from A to B, these tracks have to perform the same computation (albeit in potentially different steps); otherwise the system becomes much more complicated to analyze. The reason why this admittedly quite restricted form of branching might still be an advantage is somewhat subtle: if your computation has a lot of classical if-else cases, but you don\u2019t have enough space on your piece of paper to store all the variables to check the conditions, it might be worth just taking a gamble: pass your message down one branch, in the hope that the condition is met. The only thing that you have to be careful about is that in case the condition isn\u2019t met, you don\u2019t produce invalid results. What use is that in physics? If you don\u2019t have to store a lot of information locally, it means you can get away using a much lower local spin dimension for the system you describe.\n\nSuch small and physically realistic models have as of late been proposed as actual computational devices (called Hamiltonian quantum computers), where a prepared initial state is evolved under such a history state Hamiltonian for a specific time, in contrast to the static property of a history ground state we discussed above. Yet whether or not this is something one could actually build in a lab remains an open question.\n\nLast year, Thomas Vidick invited me to visit Caltech, and I worked with IQIM postdoc Elizabeth Crosson to improve the analysis of the energy penalty that is assigned to any history state that cheats the constraints in the Feynman-Kitaev Hamiltonian. We identified some open problems and also proved limitations on the extent of the energetic penalty that these kinds of Hamiltonians can have. This summer I went back to Caltech to further develop these ideas and make progress towards a complete understanding of such \u201cclock\u201d Hamiltonians, which Elizabeth and I are putting together in a follow-up work that should appear soon.\n\nIt is striking how such simple idea can have so profound an implication across fields, and remain relevant, even 30 years after its first proposal.\n\nFeynman concludes his 1986 Foundations of Physics paper with the following words.\n\nFor my part, I hope that he was right and that history state constructions will play a part in this future.", "sentiment": 0.041298558897243105},
{"link_title": "Mark Wise's long-lost tribute for Joe Polchinski's 60th birthday", "url": "https://www.youtube.com/watch?v=hGJSkd2BP-c", "text": "", "sentiment": 0.0},
{"link_title": "The chili queens of San Antonio", "url": "http://www.uiw.edu/sanantonio/jenningschiliqueens.html", "text": "From the 1860s until the late 1930s, one of the primary amusements of both visitors and locals was the food and entertainment offered in the plazas of San Antonio by the Chili Queens.\n\nThese women served chili con carne and other Mexican American delicacies from dusk until dawn at various San Antonio plazas over the years -- setting up tables and benches and bringing pots of food to cook or reheat over their flickering mesquite fires and to serve by the light of their oil lanterns. As morning came, their families helped them cart everything away. Wandering musicians and singers provided a festive air to the unique proceedings\u2014unique, that is, outside Mexico. In Mexico, the open-air plaza restaurants were not celebrated for their charming food-servers. Only San Antonio had Chili Queens\u2014and while they liked to joke, banter, and flirt with customers, they were well chaperoned by family members who guarded their virtue.\n\nAt first, only a few women -- such as Sadie and Martha, sometimes pictured in old books about San Antonio -- were called Chili Queens. Sadie was called \"Anglo-Celtic;\" Martha was Hispanic. Eventually, the royal title was applied to all the women\u2014most of them young and virtually all of them Hispanic.\n\nVisiting writers such as Stephen Crane, author of Red Badge of Courage, were charmed by the Chili Queens. He recalled in 1895 that \"upon one of the plazas, Mexican vendors with open-air stands sell food that tastes exactly like pounded fire-brick from Hades -- chili con carne, tamales, enchiladas, chili verde, frijoles.\" Crane depicted a romantic scene: \"In the soft atmosphere of the southern night, the cheap glass bottles upon the stands shine like crystal and lamps glow with a tender radiance. A hum of conversation ascends from the strolling visitors who are at their social shrine.\"\n\nO. Henry, who visited San Antonio in the 1880s and 1890s, wrote in his short story, \u201cThe Enchanted Kiss\u201d, that \u201cthe nightly encampments upon the historic Alamo Plaza, in the heart of the city, had been a carnival, a saturnalia that was renowned throughout the land.\u201d Then the caterers numbered hundreds, the patrons thousands. \"Drawn by the coquettish senoritas, the music of the weird Spanish minstrels, and the strange piquant Mexican dishes served at a hundred competing tables, crowds thronged the Alamo Plaza all night.\"\n\nMaury Maverick, U. S. Congressman and Mayor of San Antonio, recalled that when he was a small boy, around 1900, \"Life in San Antonio was free and easy. The band played on the square in front of the Alamo, and San Antonians came out on the public square and walked around in circles just as they do in Mexico. In front of the Alamo there were chili stands.\"\n\nAt other times in the city's history, the Chili Queens cooked in Market Square and, after the city built the spectacular Municipal Market House in the square in 1900, they moved west to Haymarket Plaza and Milam Park, near Commerce and Santa Rosa streets. Originally, diners found the Chili Queen tables in the city's first marketplace, Military Plaza, but only until City Hall was built there in 1889.\n\nTheatrical groups loved to visit the plazas. Trying the meat stew for the first time, visitors from the East and North discovered a tantalizing new taste, as hot peppers seared their mouths. Most of them loved the novel sensation.\n\nFred Mosebach wrote in the San Antonio Express in 1937 that \u201cthe evangelist Sam Jones, when he conducted a big revival meeting in the old car barns at San Pedro Park, partook of the peppery viands served in steaming dishes at the chili stands on the plaza as a guest of N. R. Jennings, a newspaper writer.\u201d Railroad brochures aimed at potential travelers to San Antonio in the 1890s spoke brightly of the \"revelry and gormandizing\" at the tables \"presided over by dark-eyed Mexican girls\" in Milam Square. Later, a 1930s city booklet described the night scene in Haymarket Square, saying \"There is no light on the plaza but that of lanterns and of the distant street lamps, which shine garishly through bottles of highly colored soda pop along the tables and pick out pleasing patterns of color. Someone is sure to have a guitar. Proprieters of various tables pay the musicians with food to sing and draw trade. The tunes are always the same.\"\n\nFrank Bushick, author of 1934\u2019s Glamorous Days in Old San Antonio, said that \"travelers and tourists who came to San Antonio usually got around to these open-air Mexican restaurants before they took time to visit our world- famous patriotic shrine which so many of our visitors mispronounce as 'the A-lay-mo.' \"Bushick also noted that \"during the cholera epidemic that once visited San Antonio, when hundreds of people died or were stricken daily, the waitresses at the chili stands were called upon to abandon their tables and lend assistance. They administered to the sick and dying freely and faithfully, never abandoning a patient until death or recovery and sometimes themselves caught the malady and lost their lives.\"\n\nThe chili stands were closed by the City Council at various times over the years for sanitary reasons, but public outcry would soon cause them to reopen. Slowly, the number of Chili Queens dwindled, and finally, in the early 1940s, the City Health Department closed their stands permanently because they deemed the dishwashing methods unsanitary.\n\nOver the years, the Chili Queens became the forerunners of today's nationwide Tex-Mex food industry -- and San Antonio had become the hometown for large-scale production by William Gebhardt of chili powder, canned chili and canned tamales. In 1932, Elmer Doolin started selling in San Antonio a favorite Tejano snack made from toasted corn tortillas; he called them Fritos. Later, the city also saw the first Pace Picante Sauce and the first commercially successful fajitas. The Texas Legislature proclaimed chili con carne as the state dish in 1977.", "sentiment": 0.11539237893889058},
{"link_title": "A visualisation of decision data in protein AI", "url": "https://www.linkedin.com/feed/update/urn:li:activity:6312365391985287168", "text": "", "sentiment": 0.0},
{"link_title": "Bandai\u2019s Joy Family", "url": "http://metopal.com/2017/07/21/bandais-joy-family/", "text": "A Japanese toymaker\u2019s role in the history of board games\n\nIf you played NES in the 80s, watched Power Rangers in the 90s, or dipped into the world of Japanese toys in the past five decades, you\u2019ve likely heard of Bandai. Established in 1950, the toymaker managed to insinuate their name into the American consciousness like many other Japanese companies of the time\u2014via toys, television, and videogames. Though much younger than the House of Mario, Bandai\u2019s own rise to toy stardom reads like an alt-history version of Nintendo\u2019s. They came of age with model cars and toys at the same time Nintendo shifted focus to the toy and (non-electronic) game business. Like Nintendo, Bandai attempted to capitalize on the emerging videogame market in the 1970s with their TV Jack ball-and-paddle units. They produced pocket electronic games in the wake of Nintendo\u2019s Game & Watch series. They even tried their hand at licensed versions of American consoles, including the Arcadia 2001, Intellivision, and Vectrex\u2014all of which would fail to make an impact in Japan, in part due to the success of the Family Computer\u2014courtesy of Nintendo, of course.\n\nAs Nintendo sought worldwide dominance of the videogame market in the 1980s, Bandai played to their strengths as toymakers, bolstering their brand with popular TV, anime, and manga licenses. Ultraman, Dragon Ball Z, Fist of the North Star, Macross, Gundam, Sailor Moon, and Kamen Rider were all among their stable of toys and models. But Bandai\u2019s rise to toy market supremacy began in 1973, when they became sponsor and exclusive merchandiser of Toei Studio\u2019s Himitsu Sentai Gorenger. Twenty years later, the series would cross over to America as the Mighty Morphin Power Rangers, one of the most successful children\u2019s television shows of all time.\n\nDespite Bandai\u2019s decades of international success, a major part of their history is almost completely unknown outside Japan. Between 1980 and 1990, Bandai released nearly 250 board games under the \u30b8\u30e7\u30a4\u30d5\u30a1\u30df\u30ea\u30fc Joy Family, \u30d1\u30fc\u30c6\u30a3\u30b8\u30e7\u30a4 Party Joy, and affiliated spin-off series. And like their toys and models, these games would feature an impressive breadth of original and licensed characters, from horror stalwarts like Dracula to an up-and-coming pair of action-plumbing brothers named Mario and Luigi. In the U.S., however, Bandai\u2019s board game oeuvre is known only by a few collectors and a handful of entries on Board Game Geek. These omissions overlook not only a landmark in Japan\u2019s board game history, but an important bellwether for changing tastes in popular culture and media during the 1980s.\n\nIf you trawl Yahoo! Auctions (Japan\u2019s equivalent to eBay) for vintage board games, you\u2019ll often run across the term sugoroku \u53cc\u516d in the item descriptions. Superficially, sugoroku describes a genre of board game that community database sites like Board Game Geek classify as roll/spin and move. Among game designers, the x-and-move genre is relegated to kids\u2019 game status (e.g., Candyland), because the reliance on luck and random die rolls divests players of any strategic options. In many games of this style, she who rolls best wins. Historically, there is some truth to the claim\u2014in the licensing mania that dominated board games in the 60s and 70s, x-and-move games were the design of least resistance for companies that wanted to capitalize on fleeting media trends.\n\nBut reducing sugoroku to its mechanical core misses centuries of cultural context. In Japan, the genre traces back to two key stylistic/material variations: \u76e4\u53cc\u516d ban-sugoroku (or board sugoroku) and \u7d75\u53cc\u516d e-sugoroku (or picture sugoroku). The former is a Chinese import that dates to at least the 7th century in Japan. An abstract game, ban-sugoroku\u2019s columnar spatial divisions and black & white tokens have clear ties to backgammon, though there are slight rule variations that distinguish it from its ancestor. Like many dice-based games of its vintage, ban-sugoroku waxed and waned in public favor (and legality) due to its associations with gambling.\n\nE-sugoroku, as the name implies, ditched the abstract spaces of ban-sugoroku in favor of vibrant pictorial spaces. These ranged from grids of related vignettes that resemble manga to maps of tourist destinations in Japan. Movement through these spaces could be metaphorical, similar to the slides into moral turpitude in snakes \u2018n\u2019 ladders, or representational and literal, for instance, by moving one\u2019s piece from point to point on a real map. Whether literal or metaphorical, in sugoroku, players raced to move their pawns from a starting point along a route to an end goal.\n\nE-sugoroku\u2019s reliance on images tied its growth to technological innovations in printing modes and media. Many sugoroku, for instance, were printed on paper in the ukiyo-e \u6d6e\u4e16\u7d75 style during Japan\u2019s Edo period (c. 17th to mid-19th century). In the subsequent Meiji period (1868\u20131912), improvements in printing technology transitioned sugoroku from rarified to mass media. Magazines in particular became the primary vehicle for sugoroku\u2019s distribution and consumption. Publications would include sugoroku supplements to celebrate holidays, instruct young people, sell products, and more.\n\nSugoroku\u2019s ties to mass media and printing technology have important implications for how the genre is understood today, as historian Anthony Bryant explains:\n\nBryant\u2019s quote illustrates how sugoroku is not simply a set of genre or mechanical conventions. Over centuries, the label has come to signify an aggregate of material, procedural, and thematic properties. You can think of sugoroku as \u201cmobile gaming for the people,\u201d i.e., portable, low-cost, quick and easy to play, and tied to popular themes and figures in history, literature, and other media.\n\nThe landscape of Japanese board games in the 1950s, 60s, and 70s looked similar to that of the United States. The advent and unprecedented adoption of television shifted the tenor of games from teaching tools, novel diversions, and sports simulations to cross-media tie-ins that aimed to leverage the fleeting popularity of TV shows and stars. As Polizzi and Schaefer wrote in Spin Again:\n\nDuring television\u2019s pre-regulation era, advertisers could target children directly. In Millenial Monsters, Anne Allison notes how Bandai aggressively capitalized on TV\u2019s popularity in Japan:\n\nBandai\u2019s board games naturally followed this strategy, leveraging many of the licenses the company had used for their models and toys.\n\nAmong Bandai and its publishing competitors, there was a concurrent shift in the target demographics for board games. What was once the exclusive province of families or children was broadened to include new, and specifically older, players. Bandai\u2019s awkwardly named Game for Adult \u201cif\u201d series (i.e., \u201cWhat if you played as X in this historical battle\u2026?\u201d) and competitor Epoch\u2019s SLG (i.e., Simulation Games) series shared lineage with both the rules-heavy wargames of American hobbyists and 3M\u2019s groundbreaking, adult-marketed Bookshelf Games. By the end of the 70s, game companies were realizing that adults, university students, and other mature audiences might enjoy games among their peers (or independent of their children).\n\nIn short, while Japanese board game publishers were mirroring trends in Western games, they offered a unique cultural spin on licensing and marketing. Where the U.S. veered hard toward D&D-inspired fantasy roleplaying in the 70s, Japan leaned more toward homegrown sci-fi and folklore like Macross or GeGeGe no Kitar\u014d (\u30b2\u30b2\u30b2\u306e\u9b3c\u592a\u90ce). Japanese board games adopted familiar models but filled them with different content.\n\nThese prevailing trends set the context for Bandai\u2019s Joy Family series, which debuted its first games in 1980 (see note below). The series\u2019 first games covered a range of pop cultural subjects, including horror (\u30c9\u30e9\u30ad\u30e5\u30e9\u30b2\u30fc\u30e0 aka Dracula Game), travel (JALPAK \u4e16\u754c\u4e00\u5468\u30b2\u30fc\u30e0 aka JALPAK Around the World Game), manga (Dr.\u30b9\u30e9\u30f3\u30d7 \u30a2\u30e9\u30ec\u3061\u3083\u3093 \u30b2\u30fc\u30e0 aka Dr. Slump: Arale-chan Game), and even one of the first videogame licenses (\u30af\u30ec\u30a4\u30b8\u30fc\u30af\u30e9\u30a4\u30de\u30fc aka Crazy Climber). Though ostensibly oversized sugoroku, Bandai\u2019s Joy Family games featured numerous and varied components, colorful art, strong visual design, and novel uses of diorama-like 3D features (many that harken to U.S. board game manufacturer Ideal\u2019s toy/game hybrids of the 1960s). And by all accounts, the Joy Family series was an early success. One of its first games, 1980\u2019s Haunted House \u304a\u3070\u3051\u5c4b\u6577\u30b2\u30fc\u30e0, for instance, features heavily in Japanese popular press histories of board games and had a reissue in the early 2010s.\n\nThere are nearly no English sources for Bandai\u2019s Joy Family releases. I\u2019ve derived dates and titles from both primary sources (i.e., buying the games and checking their copyright dates or finding images online) and secondary sources (e.g., Japanese books, blog posts, interviews, etc.). Since my Japanese is elementary, many translations are machine-assisted or rely upon English text printed on the games themselves. When dates are indecipherable or unlisted, I rely on available context clues. Bandai\u2019s logo, for instance, had four distinct designs between 1980 to 1990, so it can provide approximate date ranges. In sum, this is difficult and time-consuming work. The language barrier is steep, and, perhaps worse, Yahoo! Auctioneers have a predilection for blurry, off-axis photos that obscure important text or dates. So if you see any glaring errors or you can help with clarifications, please let me know.\n\nJoy Family games were noticeably larger than their U.S. contemporaries. The standard Milton Bradley box in the 1980s, for instance, measured 19 x 9.5 x 2 in. (48.3 x 24.1 x 5 cm), while the Joy Family version of Super Mario Bros. 2 (\u30b9\u30fc\u30d1\u30fc\u30de\u30ea\u30aa\u30d6\u30e9\u30b6\u30fc\u30ba\u30b2\u30fc\u30e0\uff12) from 1986 measured 20 x 12 x 2 in. (50.8 x 30.5 x 5 cm). Similarly, unlike standard single-fold U.S. boards made from chipboard backed with buckram, Joy Family boards were made with a lighter card stock, printed on both sides (with a manga or other illustrated text on the verso), and, in lieu of a flat center fold, folded into a J- or U-shape that wrapped around the game\u2019s interior components. There was often a colorful sliding drawer box (for storing pawns, dice, etc.) that fit the box height- and width-wise. The lighter board construction and standardized storage/component construction must have eased production costs, a trait inherited from the mass media sugoroku found in magazines decades prior.\n\nJoy Family games had strong branding from their inception. Early boxes featured nearly full-size artwork on the cover with a white band along the right edge. At the top of this band was the Joy Family logo: three fanned, red playing cards superimposed by two lines of katakana text, \u30dc\u30fc\u30c9\u30b2\u30fc\u30e0 (board game) and \u30b8\u30e7\u30a4\u30d5\u30a1\u30df\u30ea\u30fc (family joy). (Why fanned cards? Interestingly, four of the earliest Joy Family games I\u2019ve found read \u30ab\u30fc\u30c9\u30b2\u30fc\u30e0, or \u201ccard game\u201d, rather than \u201cboard game\u201d in the logo text, despite being games with boards. Based on the limited contextual evidence I have, my only guess is that these games appear to have used cards, rather than spinners or dice, to dictate movement around the board. Perhaps Bandai considered this to be a novel mechanic for sugoroku.) The logo was typically \u201cBandai red\u201d, though it could change according to the game\u2019s design.\n\nDepending on their complexity and construction, Joy Family games cost roughly \u00a52,000\u2013\u00a53,500. If this historical currency calculator can be trusted, that range in 1980 equates to about $15\u201325 in 2015 dollars (though the yen appears to have been much weaker vs. the dollar at that time). For Joy Family\u2019s intended audience of \u201c8 to adult\u201d, that was a pretty reasonable amount. If you were shopping at Target today, you\u2019d spend a little less for Monopoly and a bit more for Settlers of Catan.\n\nHowever, Bandai\u2019s true price breakthrough happened three years after Joy Family\u2019s debut. In 1983, the Party Joy series number 1, \u60aa\u970a\u5cf6\u30b2\u30fc\u30e0 Demon Island inaugurated a breakneck, eight-year stretch of continual board game releases. Priced at a uniform \u00a51,000 (~$7), the Party Joy series squarely targeted the allowances of its primary school demographic.\n\nParty Joy games had a prominent logo mark that echoed the red, tripartite brand of its Joy Family sibling. But instead of cards, the logo resembled a folded map, referencing the play board tucked within its diminutive sliding box. Beneath the katakana \u30d1\u30fc\u30c6\u30a3\u30b8\u30e7\u30a4 Party Joy, Bandai included a small yellow circle with the series number, a sly marketing tactic that certainly compelled more obsessive compulsive children (and certain older game scholars) to collect as many as they could.\n\nBut the Party Joy games\u2019 most striking material feature was their size. Each game measured 8.5 x 6.2 x 1 in. (22 x 15.5 x 2.8 cm), only slightly smaller than B5 paper size. For a Japanese child in the 80s, this would have traveled conveniently alongside both the pervasive, B5-sized Japonica learning books (\u30b8\u30e3\u30dd\u30cb\u30ab\u5b66\u7fd2\u5e33) or the latest manga digest. Like the sugoroku of decades past, Party Joy were designed for simplicity and travel. And as commodities, they were designed for collection, display, and licensing appeal.\n\nUnlike Joy Family\u2019s traditional lidded box design, Party Joy games had an inner tray that slid out from the outer box\u2019s right edge. The tray was initially printed cardboard that duplicated the box artwork in monotone and had a small adhesive pull tab to help grasp the inner box. In 1985, Bandai replaced the cardboard with molded plastic trays in pastel hues of green, blue, yellow, and (for at least one game) pink. The trays included a short folding flap on the right side that clicked into place when closed. At the top of this flap was an embossed Party Joy logo, and at its center, replacing the green pull tab, was a ridged thumb grip. The tray\u2019s bottom had six recessed posts, four at the corners and two at the center of each longer side, as well as a small center hole. This clever design feature allowed the tray to serve as a platform for game accessories. The same plastic standees that held players\u2019 cardboard pawns would fit into the recessed holes in order to hold, for example, cardboard walls for a 3D diorama. Similarly, the center hole could be used to fix a through-hole spinner mechanism, converting an unused storage tray into an active game component.\n\nThe Party Joy box became the key affordance for Bandai\u2019s game designers. All components necessary for play, from the rule book to the game board, had to fit within the small sliding compartment. As the logo indicated, double-sided game boards were printed on thick card stock that could be folded or cut into, say, fourths or sixths, and stored within the tray. To further save space and manufacturing costs, playing cards were printed on perforated sheets that players would separate during setup. These printed sheets allowed Bandai to deviate from standard playing card size, so a single game could have myriad card types and shapes\u2014standard, square, miniature, and more. Other standardized elements would appear across multiple games (and even non-Bandai games), including four plastic standees in primary colors (yellow, blue, red, green) and a miniature six-sided die with a large red circle for the one face, presumably evoking the Japanese flag.\n\nMost Party Joy games used sugoroku-style mechanics. With few exceptions, either dice rolls or spinners drove player movement, while cards and other accessories factored in additional chance or strategy mechanics. Owing to the series\u2019 target audience, Bandai\u2019s design focus was on simple setup and play, eye-catching artwork, and novel, toy-like constructions. In survey postcards included in many Party Joy games, Bandai asked, \u201cWhat kind of game would you like to see in the future?\u201d The answer choices included, \u201cmore three-dimensional,\u201d \u201cquick play,\u201d \u201c more thrilling game subjects,\u201d \u201cfun board designs,\u201d and \u201cnice components.\u201d\n\nWhile Bandai\u2019s idea of \u201cthrilling subjects\u201d predictably included licensed properties, the range of genres the series covered in 135 games is impressive, spanning horror, sports, travel, adventure, exploration, cooking, fantasy, humor, racing, and more. At the series\u2019 peak, Bandai was releasing a game or more per month, so their stable of designers were expected to churn out game prototypes at a remarkable pace. In a 2015 interview, game designer Ikuo Nomura explains that he was among several outsourced contractors that Bandai hired to create their Joy-affiliated games. Due to limitations on copyright terms, game production had a short life span. No matter how well a particular game sold, after six months, it was taken out of production. As a result, Nomura explains, he had to focus on game designs that, inspired by sugoroku, did not require extensive rules to play. But conversely, speedy turnarounds allowed the designers to experiment with mechanics and styles, because failures and successes alike would be short-lived.\n\nNomura describes Party Joy\u2019s material limitations as a kind of \u201cplatform\u201d. Since the game\u2019s price was fixed at \u00a51,000, he explains, this set a hard limit on how much paper he could use or how many components he could include. Nomura tended to use spinners rather than dice, for example, because they were cheaper, and he could weight the random results easier. And no matter how clever a game component might be, it had to fit within the plastic tray. Ultimately, these same constraints would lead to the series\u2019 demise\u2014at \u00a51,000, it was difficult to design board games with enough strategic interest to encourage long-term play. Such limitations would be a significant handicap as videogames began to attract young peoples\u2019 time and money.\n\nBandai\u2019s Party Joy and Nintendo\u2019s Family Computer both arrived in 1983, and the impact of the latter would eventually spell defeat for the former. Videogames certainly didn\u2019t catch Bandai by surprise. They\u2019d been trying to make the new medium a viable business since its inception, dipping into consoles, electronic handhelds, and third-party development. In fact, the Joy Family series\u2019 Crazy Climber, released in 1981, was among the first, if not the first, licensed board game adaptation of a popular arcade game. But it wasn\u2019t until the Famicom gained momentum that Bandai\u2019s videogame licensing kicked into high gear.\n\nNintendo\u2019s Family Computer initially didn\u2019t set the homeland on fire, but by 1985, buoyed by the breakout phenomenon of Super Mario Bros., the landscape of Japanese videogames was irrevocably changed. Japan already had a vibrant arcade culture, but Famicom brought the arcade home for millions of eager players. Bandai showed no hesitation licensing this new trend. Party Joy #51, \u30b9\u30fc\u30d1\u30fc\u30de\u30ea\u30aa\u30d6\u30e9\u30b6\u30fc\u30ba Super Mario Bros., was released the same year as the Famicom cartridge and featured the same cover artwork. Propped side-by-side, it\u2019s remarkable how the Party Joy game looks like a deluxe version of the Famicom game, and Bandai certainly didn\u2019t miss this comparison either. In the next year, Party Joy releases were almost exclusively dedicated to videogame licenses, including \u30c4\u30a4\u30f3\u30d3\u30fc\u30b2\u30fc\u30e0 TwinBee (#56), \u30b2\u30b2\u30b2\u306e\u9b3c\u592a\u90ce \u5996\u602a\u5927\u9b54\u5883 aka Ninja Kid (#58), \u30b0\u30e9\u30c7\u30a3\u30a6\u30b9 Gradius (#60), \u30bc\u30eb\u30c0\u306e\u4f1d\u8aac\u30b2\u30fc\u30e0 aka The Legend of Zelda (#61), \u30de\u30a4\u30c6\u30a3\u30dc\u30f3\u30b8\u30e3\u30c3\u30af\u30b2\u30fc\u30e0 Mighty Bomb Jack (#62), \u8b0e\u306e\u6751\u96e8\u57ce\u30b2\u30fc\u30e0 aka The Mysterious Castle Murasame (#63), and \u8b0e\u306e\u6751\u96e8\u57ce\u30b2\u30fc\u30e0 aka Commando (#64). On the Joy Family side, there were adaptations of Super Mario Bros. (in a DX, or deluxe, version), Super Mario Bros. 2, and \u9b54\u754c\u6751 aka Ghosts \u2018n Goblins. Whether these adaptions proved more or less popular than previous board games is unknown, but after Bandai\u2019s initial licensing barrage, their videogame licenses precipitously decreased.\n\nBandai\u2019s competitors tried to hitch their wagon to videogames as well. Takahashi released a six-part Family Computer Board Game series clad in silver boxes that were nearly indistinguishable from Famicom boxes apart from their size, which was identical to Party Joy boxes. Namco decided to release board game versions of their own properties in two short-lived runs. Their Fantasy Board Game series released conversions of Tower of Druaga, Dragon Buster, and Pac-Land in 1986, followed by the Namcot Handy Board Game series, comprising only two games: Valkyrie\u2019s Adventure and Super Xevious. (In English, \u201cHandy Board\u201d reads like an awkward linguistic construction for \u201cportability,\u201d but in Japanese, the name was strategic. Note that the characters for \u30cf\u30f3\u30c7\u30a3 \u201chandy,\u201d at a glance, are strikingly similar to the \u30d1\u30fc\u30c6\u30a3 \u201cparty\u201d in Party Joy, plus and minus a few diacritics.) A few years later, Enix would adapt their Dragon Quest franchise to multiple board and card games (which continue today), but beyond a few one-offs, no other company besides Bandai would attempt such a sustained videogame licensing schedule.\n\nOne recent account of Japanese board game history (\u65e5\u672c\u61d0\u304b\u3057\u30dc\u30fc\u30c9\u30b2\u30fc\u30e0\u5927\u5168) claims that the rise of videogame adaptations demonstrates how the Famicom bolstered rather than damaged the 80s board game market. While key licenses may have driven some videogame players to board game adaptations, the market crossover must have had a significant impact on Bandai\u2019s bottom line. Time spent playing videogames was time not spent playing board games. And despite the price differential between a Party Joy box and a Famicom cassette, millions of consumers clearly didn\u2019t mind paying 3x or 4x more for the latest electronic diversion. Bandai obviously didn\u2019t abandon the board game business post-1986, but Joy Family\u2019s and Party Joy\u2019s balance of original versus licensed games clearly swung toward the latter, signaling the need to tie their series to established media properties. But even Gundam couldn\u2019t keep Joy afloat\u2014by 1990, all of Bandai\u2019s board game series were sputtering to a finish. Releases became more staggered (there were only three Party Joy games in \u201990, including Super Mario World), licenses were less diverse (nearly every property was owned by Toei Animation), and sequels were prevalent. Consumer entertainment dollars were clearly shifting to videogames.\n\nOne of Bandai\u2019s most interesting last-ditch efforts to revive the line was the spin-off \u30d1\u30fc\u30c6\u30a3\u30b8\u30e7\u30a4\u6307\u5357\u5f79 Party Joy Instructor series, which began in 1991. The games were all licensed Super Famicom adaptations (including Nintendo\u2019s \u30bc\u30eb\u30c0\u306e\u4f1d\u8aac\u30fb\u795e\u3005\u306e\u30c8\u30e9\u30a4\u30d5\u30a9\u30fc\u30b9 aka The Legend of Zelda: A Link to the Past and Capcom\u2019s Rockman 4) sheathed in silver boxes featuring a prominent line drawing of a SFC game cartridge. The \u201cInstructor\u201d gimmick was a hybrid board game/strategy guide. Original artwork was interspersed with in-game screen captures, and portions of the instruction manual and game board were dedicated to strategies for the titular videogame. Ostensibly, while you were playing the board game, you were learning skills that would transfer to the console. Whether the Instructor series served its intended purpose is unclear, but judging by the series\u2019 quick demise after seven games (and their relative scarcity today), videogame players weren\u2019t biting.\n\nBandai\u2019s decade-long run of board game releases are remarkable not only for their sheer quantity, but for their breadth of genres, their shrewd attention to construction and design, and their willingness to experiment within proscribed constraints. It\u2019s easy to dismiss these games as children\u2019s fare or simplistic spin-and-move games, but there\u2019s a rich vein of Japanese culture hiding beneath these mere sugoroku. Just as picture sugoroku superseded board sugoroku as players\u2019 tastes changed, a significant sea change in entertainment consumption re-shaped board games\u2019 design parameters in 1980s Japan. While prior media like television, manga, and anime had provided lucrative fodder for board game design in prior decades, videogames proved to be a competitor hewn too close to board games\u2019 procedural core. Bandai tried both cooption and conciliation as strategies to compete with the new medium, but concession proved to be the only viable course. And while they ultimately didn\u2019t survive videogames\u2019 market incursion, Bandai\u2019s board game adaptations are a significant, yet still largely overlooked, branch of (video)game history.", "sentiment": 0.12019777562862671},
{"link_title": "Are catastrophic disasters striking more often?", "url": "https://theconversation.com/are-catastrophic-disasters-striking-more-often-83599", "text": "No sooner had Hurricane Harvey\u2019s record rains receded from Houston and neighboring cities than the residents of Florida began bracing for a wallop from an even more powerful storm. And hurricane season hasn\u2019t even peaked yet.\n\nThis begs the question: Is the number of major natural disasters striking the United States actually increasing, or does the media\u2019s natural tendency to overhype conflict only make it seem so?\n\nThe federal government\u2019s National Oceanic and Atmospheric Administration actually maintains a website that can help answer this question. Its list of billion-dollar disasters goes back to 1980 and records the date, location, number of deaths and total cost of each one.\n\nAs an economist, I take a different approach from how a climate or environmental scientist might answer the question. Major disasters can wreak tremendous damage, such as destroying wild habitats, rerouting major rivers and killing innocent people. However, assigning specific values to these kinds of damages is difficult.\n\nAssessing the direct economic damage of a disaster is comparatively simpler. Insurance companies and government agencies receive damage and loss claims for specific amounts. These figures are then adjusted to account for the uninsured to produce a total.\n\nIn my own analysis, I will start with these figures but make an additional adjustment which I believe more accurately answers whether catastrophic disasters are indeed occurring at a greater frequency.\n\nThe government has gone to great efforts to make sure its disaster data are reliable by combining data from government agencies like FEMA, the USDA and private insurance claims.\n\nAnd it doesn\u2019t just track the hurricanes making headlines right now, but all kinds of large disasters, from winter storms and heat waves to droughts and floods. The data also include winters when subfreezing temperatures destroy billions of dollars of crops and kill large numbers of livestock.\n\nThe government ensures the costs are measured accurately. The total cost of each event includes both losses covered and not covered by insurance. The losses include damage to buildings, roads and infrastructure, as well as items destroyed within buildings when a major disaster strikes. The figures also include some amounts lost by businesses because they were temporarily forced to shut down.\n\nThe figures, however, do not assign any value to lives lost. Even if a storm kills hundreds, no adjustment is made for these deaths.\n\nFinally, the figures are adjusted for inflation. This is very important because US$1 billion in 1980 is actually equivalent to $3.15 billion today after adjusting for price changes. Hollywood movies consistently break box office records because the industry does not adjust ticket sales for inflation. Without an adjustment, disasters would consistently look more expensive over time and, like Hollywood films, seem to always shatter records for damage \u2013 even when they don\u2019t.\n\nSince 1980 there have been 212 disasters, which NOAA calculates resulted in over $1.2 trillion in damage.\n\nMy analysis of the NOAA data shows that the number of billion-dollar disasters has indeed been increasing over time. A typical year in the 1980s experienced on average 2.7 such disasters in the U.S. In the 1990s and 2000s, that average had climbed to 4.6 and 5.4 a year, respectively.\n\nSince then, the frequency of costly disasters has soared. In this decade so far, each year has seen an average of 10.5 disasters. The scale of this increase amounts to one additional billion-dollar disaster every four years.\n\nEven with the inflation adjustment, a key reason we have more costly disasters is simply that the economy is much bigger today than it was in the 1980s.\n\nWhen the economy was smaller, disasters caused less economic damage. There were fewer homes, factories and office buildings to destroy, so it was harder for a natural disaster to cause a billion dollars of damage.\n\nSince 1980, the U.S. economy has more than doubled. The economy grew from $6.5 trillion back then to $17 trillion as measured by inflation-adjusted GDP. To account for this, the NOAA data should be adjusted for the economy\u2019s size. In other words, a storm happening today will cause more damage than an identical one occurring decades earlier simply because there is more to destroy.\n\nA simple adjustment that incorporates economic growth divides each year\u2019s inflation-adjusted GDP figure by the 1980 value and multiplies the answer by $1 billion. This results in a figure, in each year, that equates to the minimum amount of damage needed to be economically equivalent to $1 billion of destruction in 1980.\n\nFor example, the resulting figure for 2010 is $2.3 billion. Or put another way, a storm that caused $1 billion in damage in 1980 would have caused about $2.3 billion worth in 2010.\n\nWhile this isn\u2019t a perfect adjustment and assumes all parts of the economy grow at roughly the same rate, it creates a more accurate measure of meaningfully billion-dollar disasters than the official data.\n\nExcluding storms under the adjusted cutoff and redoing the statistical analysis shows we gain an additional billion-dollar disaster about every 25 years, not every four years. So the frequency of these natural disasters is increasing, but not nearly as fast as the raw NOAA data suggest.\n\nIn general, people tend to overestimate the impact of small-probability events. For example, many people must think they have a pretty decent chance at winning giant lotteries; otherwise, no one would buy tickets.\n\nDisasters are just like lotteries except people lose when their number comes up. While millions of people have been affected by Harvey, hundreds of millions more were not in the hurricane\u2019s path. However, the news and dramatic live coverage make all of us worry that we are in danger.\n\nEven still, given the number of costly disasters is rising \u2013 if slowly \u2013 policymakers and politicians should think about strengthening building codes and making other changes to ensure we\u2019re better prepared and less at risk. If commercial and residential buildings are designed to handle a higher chance of being inundated by water, wind and fire the cost of cleaning up and time needed to recover from natural disasters will fall.\n\nAnd what can you do? Be prepared. You never know when disaster might strike. Make a plan. Have spare water, flashlights, food and your important papers accessible in one place in case it is time to flee.", "sentiment": 0.1360942568961437},
{"link_title": "2017 CUFP Keynote \u2013 Are We There Yet?", "url": "https://github.com/bodil/are-we-there-yet/blob/master/notes.org", "text": "\u201cEngineer\u201d used to be a protected academic title in Norway, where I grew up, equivalent to a Master of Science degree, but more special. I grew up believing that those who were granted the title had a deep understanding of what they were doing.\n\nCivil engineers understood down to the most minute detail how to build roads and bridges, and what would happen if you get it wrong.\n\nChemical engineers designed and oversaw the processes burning in the oil refineries that fuelled the Norwegian economy.\n\nMechanical engineers designed and built the oil rigs and supply ships.\n\nEngineers were the people who led the charge forward into the bright, oily future.\n\nI grew up, in short, thinking the word \u201cengineer\u201d held some special magic, some sort of confident infallibility. These people had their science figured out down to the smallest detail, I thought, and to some extent I still believe that.\n\nThen, when I was grown and started working as a programmer, I encountered the idea of \u201csoftware engineering.\u201d\n\nAnd nobody had the slightest clue how to write good software, but everybody had an opinion.\n\nThere was no demonstrably correct process for how to go about constructing and verifying software. But everybody had very strong opinions. Whether or not to write tests, and in which order. Whether static analysis was a waste of time or not, and if not, how many lines a function was allowed to be. Whether to write documentation or whether tests are better docs than docs.\n\nWe didn\u2019t know anything about what makes a good programming language, or how to build one. But everybody had a language they swore was superior to any other. It was often the only language they knew. Sometimes it was XML.\n\nNobody had any idea how to properly organise and manage the process of building software. But a lot of people were selling certifications.\n\nThe only thing we could all agree on was that academics knew nothing about writing software in the real world. Academics were all useless posers spouting mathematical gibberish. \u201cA monad is just a monoid in the category of endofunctors.\u201d Clearly trying to start a fight.\n\nWell, there was this guy Dijkstra, whose algorithm was pretty useful for network topology and such. But he probably couldn\u2019t write a Maven plugin to save his life.\n\nThis is my story. This is the industry I grew up in.\n\nNow, let me be clear, I\u2019m generalising. There\u2019s a part of the industry that\u2019s always stayed in touch with both science and academia. Usually this part has evolved in the crucible of hypercompetitive markets, such as investment banking. Where the best programs give you a measurable competitive edge, you go find the smartest people to write them. These people often come from academia, and if they don\u2019t, they\u2019ll still be up to date on the research. And a lot of them are probably here today.\n\nBut the rest of us? We don\u2019t really care, we just need to deliver those features on the scrum board before the end of the sprint, and pray to Uncle Bob they don\u2019t come back as bugs, or we\u2019ll spoil the burndown chart.\n\nOr whatever the process jargon of the day may be. It doesn\u2019t matter - what matters is that the industry, for the most part, is a fashion driven clown show and we. Have. No. Idea. What. We\u2019re. Doing.\n\nSoftware is everywhere now. It\u2019s in cars. It\u2019s in medical devices. It\u2019s in things that could fall out of the sky and kill a lot of people if you put a bug in there.\n\nWe\u2019re not at the point yet where individual developers are held legally liable for such accidents - perhaps only because we haven\u2019t had a sufficiently dramatic incident of this sort yet. We do know that individual developers can be held liable, along with their bosses, for software that breaks the law. The programmer who wrote the software for the cheating Volkswagen cars just got sentenced to a lot of jail time.\n\nSo it\u2019s becoming apparent that we really do need to get our act together. Programming is a lovely hobby for a lot of people, but for more and more people doing it for a living, it\u2019s suddenly become Very Serious Business.\n\nAnd this is where I\u2019d like some help, please.\n\nWe have a huge problem of process, management, all that squishy humanities stuff that\u2019s necessary for building things at large scale. Someone more qualified will have to figure out that bit.\n\nBut I need you to help build the hard science of solid engineering. To a surprisingly large extent, I think we\u2019re already there, but like the man said, the future is unevenly distributed outside of academia.\n\nWe can perform amazing feats of formal verification, but we don\u2019t yet know how to make it readily accessible to everybody.\n\nWe\u2019ve already figured out a lot about the structure and composition of good software, but whenever we try to explain it to the average developer, they just think we\u2019re trying to start a fight.\n\nWe need to figure out how to explain why this stuff is important, in the large as well as the small, even before we explain how to do it.\n\nWe need to think in terms of a unified theory of software engineering. We have most of the pieces of the puzzle, and we need to get started on the work of putting them together into a coherent whole.\n\nAnd then we need to go teach it. Perhaps we even need to bring back that protected engineer title which says, without a doubt, \u201cI know what I\u2019m doing.\u201d At the very least, we need to make sure that critical software which could have a serious impact on people\u2019s lives is actually written by people qualified to do so, with the tools necessary to get it right. And there\u2019s more and more of that software every day.\n\nThey figured out how to send dogs to the moon, so we\u2019ve got to be able to figure out how to write software that isn\u2019t terrible.\n\nSo let\u2019s get to work.", "sentiment": 0.15403791887125218},
{"link_title": "Weighing Justice with a Jury of Her \u2018Peers\u2019", "url": "https://longreads.com/2017/09/08/weighing-justice-with-a-jury-of-her-peers/", "text": "I received the notice for jury duty with mild annoyance. I hoped I wouldn\u2019t get picked as I put the date of the summons on my calendar. I thought about how jury duty would throw me off my work schedule; how I didn\u2019t want to participate in this particular part of civic life in small town Alabama; how I didn\u2019t want to help someone, probably another Black person, go to jail.\n\nBut I didn\u2019t spend too much time worrying. It was summertime and the date, during a week in the middle of September, seemed an unpleasant blip on the road far ahead. I pushed it out of my mind and tried to enjoy the remaining pieces of a waning summer in my sleepy southern town.\n\nEventually the summer break gave way to the fall semester, though the weather stayed oppressively muggy. Living in a college town where God and football are rivals for people\u2019s undying devotion meant there was also an air of jubilance and anticipation everywhere. I care little for football and even less for their God, so I did not have much to look forward to except the return of my regular paycheck and the eventual end of sultry weather. Otherwise, the date of my summons \u2014 September 12th \u2014 loomed unpleasantly before me.\n\nIt was 2011, the tenth anniversary of the attacks on September 11th. The decade had rushed by impossibly fast, but there it was, on the news and emblazoned in public memory like an unwanted tattoo. I had been a college senior when the attacks on the World Trade Center and the Pentagon happened and now here I was, a grownup with a job. Maybe it was growing up with my mother always reminding us that \u201cthe days are being shortened for the sake of God\u2019s elect\u201d \u2014 those chosen for salvation \u2014 plus our being unaware of the day or the hour of God\u2019s return, but even though I was scared, I was not shocked about terrorism on American soil. Or maybe it was having grown up in Caribbean immigrant communities where America was loved more pragmatically than patriotically. Curiously, when I moved to the white, rural South in 2007, far away from New York, D.C., and the Pennsylvania field where the third plane went down, there seemed to be more anger, more panicked rhetoric about terrorism and violence than in my hometown of Fort Lauderdale. At first it didn\u2019t make sense. What would terrorists want with a state in which memories of the Confederacy were wistful and sweetly savored? Still, on the tenth anniversary, there didn\u2019t seem to be any commemorations in town, aside from faded t-shirts and bumper stickers proclaiming, \u201cThese colors never run,\u201d and \u201cNever forget.\u201d\n\nOn September 12th, dread bloomed in my stomach as I made my way to the Justice Center. I was a ball of anxiety: nervous about missing work, nervous about jury duty, nervous about the specter of another terrorist attack. I didn\u2019t think anyone was coming for me in Alabama, but I feared that if some tenth anniversary massacre happened it would be Black and Brown people who\u2019d suffer and be handed the blame.\n\nThe Justice Center is a series of large buildings at a busy intersection, directly across from a bank and kitty-corner from a sprawling mall. Not exactly where I imagined finding a county jail and court offices, but it sort of made sense. The Justice Center had the appearance of a business like so many of the other businesses that surrounded it. I parked and made my way inside. Dozens of solemn, dutiful citizens milled around, waiting to hear our fates. Everywhere, white men in khaki uniforms barked orders.\n\n\u201cStand in line, single file!\u201d \u201cWalk through the metal detector!\u201d \u201cOpen your purse!\u201d \u201cLift up your arms!\u201d\n\nAll of a sudden I was not a 30-year-old college professor with her own apartment and car and independence and freedom. I was a child again, being told where to move, sit, and stand. I tried to shake off that feeling as we were ushered up the stairs and informed that the penalty for leaving the building without being properly vetted was getting arrested. We were being considered for a grand jury, and more than 12 of us would be picked.\n\nThere were 30 or 40 of us who eventually made it to a room to be questioned. After a while, we were separated and made to answer a series of questions. Did we live in the county? Had we served on a jury before? Did we know anyone involved in a case or in the courthouse?\n\nYes, I lived in the county, I said. No, I never served on a jury. Yes, I think I do know someone here.\n\nI have this thing where I remember faces but not names. I used to pride myself on being able to remember everything from my childhood \u2014 even the horrible things. I would joke that I had a cast iron stomach and a mind like a steel trap. Unlike folks who could float up towards the ceiling and disappear when terrible things happened, it seemed as if I would become even more present in my body, and then my mind had the nerve to replay painful, shameful moments over and over and over again that night like a perverse broken record. Or, my mind would morph these memories into the strangest dreams of me falling, getting lost, or being killed. Well, the cast iron stomach is definitely of yesteryear and the steel trap is somewhat rusty or perhaps even unhinged. Not unlike the rest of me.\n\nAs I have gotten older, the past has become increasingly fuzzy and indistinct like a Monet up close; the details are less distinct but the feelings, the experiences conjured, remain. Having lived in five different states means that sometimes I can\u2019t remember where I\u2019ve met someone. And after over a decade of teaching, students run together in my mind as an amalgamation of benign white faces with faded novelty t-shirts, baseball caps, and Uggs. The ones that stand out are either terrible or lovely.\n\nThat day in the Justice Center I remembered a person. Well, his face anyway. He was blond and small, with a pinched expression like a startled possum. He was an unremarkable student in my class, maybe even below average. But he was a recent student. I remembered him. I thought maybe he was there with his lawyers because he had committed a crime.\n\n\u201cI know this young man here.\u201d My voice was not my own. I spoke in a quavering whisper. \u201cHe\u2019s one of my former students. We\u2019re supposed to say if we know someone, right?\u201d\n\nAt once, everyone in the room laughed. I saw their heads thrown back and their sharp, pointed teeth. I froze. Oh my God, I thought. I\u2019ve said the wrong thing. It was an excruciating moment that stretched out as if I was being drawn and quartered. I wanted to turn around, run, disappear, or maybe grow wings and fly out the window in the next room to get far away from the building and out of the state.\n\nBut I just stood there, blinking slowly, waiting for them to stop.\n\n\u201cOh, him. Yeah, he\u2019s a real big criminal alright,\u201d one man said, wiping his eyes after laughing hard. \u201cHe works here.\u201d\n\nHe explained that my mediocre former student had some sort of fairly important job. I don\u2019t know whether it was my nervousness, or my incredulity that a student who could barely put in any effort in English class now worked at the county courthouse, but none of what was said really made sense to me. What I knew, felt, even tasted, was my own thick shame. It was a heavy stench ready to smother me under its weight. I was ready to buckle.\n\nI was told to sit in an adjacent room. As I walked over, an official, a bailiff maybe, held a door for me.\n\n\u201cYou shouldn\u2019t have said that, young lady.\u201d His watery blue eyes twinkled at me. \u201cThey think you\u2019re funny. Now they\u2019re definitely going to pick you.\u201d\n\nIt was a familiar nigger joke: white joy brought on by Black shame and vulnerability. I was an unwitting coon, a funny story to punctuate the morning, for them to tell over their coffee and donuts. My fear and shame was just a punchline.\n\nAlarm sang out through my whole body. I felt hot and cold at the same time. My stomach flip-flopped, all jittery and nauseous. I thought I was going to have a panic attack like I used to get when I first started driving. But there was no side of the road or parking lot to pull over in. The rage and embarrassment was bile in my throat that I had to just choke down. I sat in silence until they called me back.\n\n\u201cCongratulations, professor! You have been selected for the grand jury,\u201d the district attorney\u2019s voice rang out. He was tickled.\n\nMy shoulders might have slumped a little but my face was a blank mask. I closed my eyes and nodded stiffly, fearing that if I opened my mouth I would let out a strangled cry, or a curse that would get me held in contempt.\n\n\u201cOh, and by the way, since you\u2019re a professor I thought it best that you lead the proceedings as a foreperson. I\u2019m sure you\u2019ll be able to keep these folks in line.\u201d\n\nEveryone laughed again at the nigger joke, but the district attorney\u2019s smile did not reach his eyes. Later, he would tell me he chose me because I was a professional Black woman, thought I\u2019d be the best candidate for the job. \u201cI like to have people of color be the foreperson. Adds a bit of diversity to the proceedings.\u201d\n\nThere I was, the foreperson for a grand jury, joined by a little over a dozen other county residents. Our task was to review a slew of cases \u2014 over three hundred \u2014 and figure out whether or not they should go to trial. The district attorney, a jovial, dark-haired, white man, slick-talked in the manner of a politician or used car salesman. He was always polite, but there was something discomfiting about him just beneath the surface, like he could stab you in the chest with a smile. He warned us to complete our cases quickly or else jury duty would spill over into another week.\n\nNone of us wanted to stay the day, much less over a week. We promised to go through the cases quickly and efficiently.\n\nWhat exactly is a jury of your peers? In this particular experience, it was mostly white men. Conservative, evangelical, Republican. The men dressed like grownup versions of my students: fancier versions of khaki shorts, polo shirts, baseball caps, tinted sunglasses, and shorter hair. Some were businessmen, others were laborers. Every last one of them mansplained, espousing universal truths with their arms crossed or their hands casually clasped, talking over each other and over the women unless I interjected \u2014 which I did, often.\n\nThe women were housewives, clerical workers, and retired schoolteachers. They sat uneasily among men, mostly silent among the cacophony of smug male voices.\n\nMy group was probably more racially mixed than most for the area. In addition to the white men, there was a smattering of white women, and two other Black women besides myself. I don\u2019t recall there being any non-Black people of color.\n\nDay One. The Justice Center is a jail, no matter what they want to call it. And it felt that way, even as we sat in a newish conference room with tiered gray desks, gray walls, and gray carpet, like one of the classrooms in a business school. The air conditioning was, thankfully, turned up high against the late summer heat. I sat in the middle seat of the front row, a reluctant teacher\u2019s pet. The district attorney and a legal secretary faced us, briefing us on the day\u2019s cases.\n\nKickstart your weekend reading by getting the week\u2019s best Longreads delivered to your inbox every Friday afternoon.\n\nThe first few hours were a sort of orientation. We began our term as a grand jury by being shown pictures of people addicted to meth. Up until that point I knew very little about the drug and had no idea that the countryside \u2014 the idyllic part of the state known as The Plains \u2014 was riddled with meth addiction. But it makes sense. What are poor, underemployed, and unemployed folks to do with few resources and even less compassion? I stared at the side-by-side view of mugshots, taking in the progression of sad but relatively healthy faces that morphed into ghoulish bulging eyes, graying skin, sunken cheeks, pockmarks, and missing teeth.\n\nWe then got the cases. Police officers, all men and mostly white, came in and presented them to us. They were all tall men, firm and serious. They had buzz cuts and bald heads and swung their arms wide when they walked. When the Black officers came in I made eye contact with them. They never looked away, but there was no recognition there either. All of the police officers spoke to us in clipped, somber tones, but laughed with the Justice Center staff like old buddies sharing inside jokes. They recounted the facts of each case starkly, succinctly. The accused was pulled over for running a red light. The officer smelled weed. They searched the car and found a blunt and a portable meth lab \u2014 a Sprite bottle filled with fertilizer, battery acid, pseudoephedrine, and other ingredients used to make the drug that keeps you up for days and makes you look like an extra in The Walking Dead. The police officer noted that part of their crackdown on drug offenders is making the most of traffic stops. If someone has a busted taillight or runs a red light, it\u2019s also likely that they have drugs in the car. If you talk to them long enough, they\u2019ll break.\n\nWhen the policeman said this I heard a small click as a forgotten memory slid into place. Now it all made sense. I remembered being stopped around the corner from my house a few months earlier. I remembered it was a good morning. I had an appointment at a weight loss clinic, the latest attempt in my efforts to take off the thirty pounds I gained after grad school, and I was feeling good about my choices. I made a left out of my complex and a right on the next street. I glided by row after row of cookie-cutter suburban homes. Soon after, I heard sirens behind me. I pulled over to let the cop car pass but, to my surprise, it stopped a few feet behind me. I gathered my license and registration and tried to still my beating heart as the police officer stomped out of the car toward me, red-faced and sweaty.\n\n\u201cDo you know why I pulled you over?\u201d His voice was loud and brusque as his eyes darted around the cab of my car.\n\n\u201cNo, sir.\u201d My voice was small. I had no idea. Was I speeding? He asked for my license and registration and I handed them over.\n\n\u201cDid you know you ran that stop sign back there?\u201d His stubby fingers jabbed the air with incredulity. \u201cThis is a nice neighborhood. You can\u2019t just disregard stop signs.\u201d\n\n\u201cI\u2019m sorry. I had no idea I\u2026\u201d I stammered.\n\nHe interrupted me. \u201cWhat are you doing in this neighborhood?\u201d He was scanning my ID as he asked.\n\n\u201cI live here. Around the corner.\u201d Some of my fear dissipated. I could see what he was doing. What he was trying to get at. What was I doing, what was a random Black woman doing, in a predominantly white area, apparently running stop signs with reckless abandon?\n\nHe looked skeptical. \u201cWell, where are you going?\u201d\n\nI panicked. Shit, where was I going? I said the first thing that came to mind. \u201cI\u2019m going to work.\u201d\n\nHe wanted to know where I worked and what I did and who I did it with. When I told him I was a professor at the university his disbelief was only eclipsed by his dismissal.\n\n\u201cWhy are you heading this way if you\u2019re going to work? You\u2019re headed in the wrong direction.\u201d He spat the words out and his beady eyes scanned the interior of my car once again.\n\nI finally remembered about the weight loss clinic and said as much. He went away to run my plates and came back, cooled down, and with a ticket in his hand. He advised me to \u201cbe more careful\u201d and sent me on my way. I drove away with my hands shaking, choking down a familiar rage.\n\nNow, as officers droned on about cases, I was still thinking about how close I had come to being arrested myself. I pushed that out of my mind to learn about another defendant, who was accused of breaking into a pharmacy and clearing the place out of Sudafed. We heard about several more folks, arrested for passing off checks or writing their own prescriptions for Xanax and Vicodin. The cases piled on. Almost all the cases were non-violent drug offenses. My fellow jurors sighed and clucked their tongues in weary sympathy and vocal judgment when yet another young person was caught with meth in their car, or stole copper wire from an abandoned house, or passed off bad checks to finance a drug habit.\n\nWhen Black people were involved, as victims, perpetrators, and in a few cases both, the discussions carried on coldly, with plenty of victim-blaming. And there was extra concern for the \u201cfacts\u201d and \u201cprocedure.\u201d I tried to use my feeble power as foreperson to rebuke this thinly-veiled racism. I asked questions, steered conversations. I was successful, mostly.\n\nJust when I was about to explode from the monotony, we got a reprieve. \u201cTake an hour \u2014 no more \u2014 for lunch,\u201d the D.A. said congenially. We all scattered without a word, worried that he\u2019d change his mind.\n\nDuring my lunch break I went to my car, drove to a nearby parking lot, and wept. I wanted to drive home and get under the covers and stay there for the rest of my life. I called a friend. \u201cThey picked me for jury duty,\u201d I said, my voice cracking. She started to groan in sympathy but stopped. I could hear the worry in her voice; she had never heard me so upset. I could tell she wondered why someone like me, so normally stoic, detached even, was freaking out. I got my voice under control and told her, \u201cI\u2019m fine,\u201d and hurried off the phone.\n\nI wondered why I was having such a dramatic reaction to a commonplace thing like jury duty. Previously, I\u2019d thought of it like voting \u2014 just something you did as a part of citizenship. I think I never realized how physically sickening it might be to play a part in the carceral state, condemning people to jail for petty crimes of poverty and despair. And there\u2019s something else \u2014 something about not being able to control my movements and my own time brought out a deep, almost irrational, sense of claustrophobia, reminding me how unfit I might be for confining circumstances like the military, motherhood, or marriage.\n\nDay Two. The second day was hard. There were dozens more meth cases and we agreed to indict the accused for offenses like drug possession or trafficking. But we also heard cases about violent crimes.\n\nThat day three Black women came forward as survivors of sexual assault. When the women faced me I wore a mask of benign concern. But when each woman spoke I held my breath and clasped my hands in front of me hard, so I wouldn\u2019t scream. The first woman was a university student who was raped by a so-called friend. This young man, a popular figure on campus, had taunted and stalked the survivor, trying to strongarm her into dropping her case.\n\nShe sat before us nicely coiffed, tears ruining her tasteful makeup. She sobbed, saying she wanted her life back, that she just wanted to finish her degree and just move on. All of my fellow jurors cried and shook their heads. \u201cWhat a shame,\u201d they said. \u201cWhat a shame.\u201d\n\nShe was the perfect victim, a proper respectable Negro. But she was still a victim. The D.A. gently asked if she was sure she wanted to drop the case. The young woman was resolute: \u201cYes.\u201d We gave her our tearful goodbyes and good lucks. I was a bit taken aback at the care and concern shown to this woman.\n\nIn the next two cases, though, the dismissal of the women\u2019s concerns was as palpable as an open wound. One woman was working class and young, no older than 21, just as the previous woman had been. She had already been married for three or four years to a much older man, who was violent and possessive. She wore a long denim skirt and short bobbed wig that made her look older than her years. She spoke in a low, dispassionate monotone about the soon-to-be ex-husband who attacked her because she had started seeing another man \u2014 someone who may or may not have been a drug dealer \u2014 during their separation. Her husband broke into her trailer, took a knife from the kitchen, held her at knifepoint, and raped her. My fellow jurors could not believe this. They asked intimate, probing questions, questions that were startling and gratuitous and inappropriate.\n\n\u201cWhen did he stab you?\u201d \u201cWhere did he stab you?\u201d \u201cHow many times did he stab you?\u201d\n\nTheir tone was accusatory, as if this woman had stabbed herself. When she responded, her voice was even, neutral, as if she were describing an entirely reasonable thing to a particularly slow audience.\n\nI looked at this young woman who stared down at her dark brown hands as she spoke. All that she had gone through, and now she had to sit, defiantly composed, in a room of (mostly) unforgiving strangers. While her testimony was hard to hear, the questions my fellow jurors asked of her were even harder. When she left the room, the textbook misogynoir continued to rear its head. \u201cWhy didn\u2019t she just run out?\u201d \u201cShe looks way older than she says she is.\u201d Even some of the women who had been reticent finally spoke up to echo the mens\u2019 disregard for the woman\u2019s case.\n\nThis was a clear-cut case that needed to go to trial. There were mounds of evidence and the victim was firm about wanting her abuser to be held accountable for what he did to her. Why was she on trial? It was as if I was in an after-school special about racism and patriarchy and everyone had gotten the script but me, or a nightmare I had to find a way out of.\n\nThe third woman was treated the same way. When we got to her \u2014 a former university student assaulted by a friend, an athlete, during a party \u2014 I knew what to do: cut the men off at the pass. Steer the discussion towards our duty. Ask the simple but hard questions:\n\n\u201cDo we think that wrongdoing could have occurred? Yes or no?\u201d\n\nRemind my peers that it was not our job to prosecute, but to make a judgment about whether or not we had enough reason for this case to come to trial.\n\nI looked to the women to do the right thing. In both cases, we (the women swaying the men) turned the tide.\n\nWe asked, \u201cWhat about her life?\u201d \u201cHow will she carry on?\u201d \u201cWhat about the other women in his life, whom he can hurt?\u201d\n\nI should have felt satisfied, or even hopeful, but at the end of the day I just felt exhausted.\n\nDay 3. On the third day we toured the jail located on the right side of the Justice Center. I wanted to opt out and say, \u201cNo, I have no interest in \u2018touring\u2019 a jail.\u201d But I stayed quiet.\n\nThere was really not much separating our jury room and the jail. We filed our way down a series of long hallways and locked rooms to the other side of the building and made it there in a few minutes. We were shown where people are held and booked. We walked through concrete-walled corridors and were shown common areas and individual cells. We walked into the kitchen, where inmates stood about washing dishes and wiping down surfaces. They took no notice of us jurors, even as some of my peers got up close to them like kids pressing their faces up against the glass at an aquarium. When we walked into the laundry room there were two Black women stuffing sheets into dryers.\n\nMy mouth automatically smiled when I saw another Black woman, but this was all wrong. My smile was crooked, lopsided, and rueful. The woman stared back at me blankly at first, then defiantly, rejecting my pity, my empathy, or whatever else I had to give. Then she turned her back, returning to her fitted sheets.\n\nLater in the afternoon, we heard a case of a family cookout gone wrong. According to the police, a fight over a love triangle broke out at a gathering at an apartment complex a couple miles from my house. Two people ended up hurt, one stabbed pretty badly. One of the witnesses for the prosecution was being held in the Justice Center. He was brought in with his wrists and ankles shackled, wearing a white jumpsuit and a bemused expression. There was very little space on his body, that I could see anyway, that was not tattooed. Even his light brown face sported a series of teardrops near his eyes.\n\nHe spoke slowly, thoughtfully, recounting his version of events. My fellow jurors were mesmerized both by his polite and deliberate slow drawl and the gold teeth in his mouth. He was tall, handsome, and strapping; perhaps in another life he could have been a football player for their favorite team. The D.A. asked him if he would be willing to testify in the case if it went to trial. He said yes and added, \u201cI\u2019ll be in here just for a little bit longer. Just 18 months.\u201d\n\nDay Four. The feeling of panic did not dissipate as the week dragged on. Instead, my anxiety and dis-ease expanded without ceasing. My heart raced every time I entered the Justice Center and I fought the urge to run out of the building several times a day. Lunchtime was my only reprieve. That week whatever diet I was on was abandoned without apology. Normally, I felt guilty about eating my feelings and tried to rationalize my way out of it. This week I embraced food\u2019s soothing, numbing power. Some days I headed over to Chik-Fil-A for a large vanilla milkshake, with whipped cream and a cherry. Or I ate pizza and guzzled orange soda.\n\nOn this day I got a large burrito and nachos, two of my favorite things to binge eat. But when I went to take a bite I couldn\u2019t swallow the food; my stomach felt like a closed fist, folded in on itself. Later, I tried to eat more food and the same thing happened. The food felt like it was hitting a wall and going nowhere. After jury duty I would find out that I had a bad stomach infection. My doctor would tell me to take it easy and that it would have become an ulcer if I had waited any longer to see her.\n\nI have a history of stomach violence fueled by anxiety. Often, when I travel, I have \u201ctraveler\u2019s stomach,\u201d especially if I\u2019m running late. The year before jury duty I got my gallbladder removed because it was horribly inflamed. These psychosomatic illnesses crop up when I\u2019m stressed, overwhelmed, or feeling defeated. That week I felt all three.\n\nDay Five. I woke up with a tiny piece of hope, small enough to put in my pocket or zip up in my purse for safekeeping. This was, hopefully, the last day of jury duty. At the Justice Center, we were quiet and uncomplaining, like kids hoping the teacher would let us out of class early. After a week of working at breakneck speed we had seen hundreds of mostly small cases and a few big ones. So many of the crimes involved Black people or poor rural whites. Most of the offenses were for petty crimes concerning drugs: poor folk busted for small amounts of weed and meth, folks whose lives were wrecked and ravaged by poverty, abuse, and judicial indifference.\n\nAt the end of the day, I said goodbye to my fellow jurors but did not linger, though many of them came up to me and thanked me for my service. Months later I would occasionally run into some of the women of the jury \u2014 at the supermarket or at the mall \u2014 and we\u2019d grimace and say, \u201cWe made it!\u201d as we rushed past each other in the aisles. What I never said to them was how angry, hurt, and ashamed I felt and still feel about that week, and how I\u2019m not entirely sure I\u2019m entitled to those feelings. After all, I simply served on a grand jury; I wasn\u2019t on trial. But still, I feel traumatized, and have less respect for our system because I know what happens when a jury of one\u2019s peers get together. For people of color and women and the poor, there is nothing close to justice.\n\nSusana Morris is the co-editor, along with Brittney Cooper and Robin Boylorn, of The Crunk Feminist Collection. She teaches African American literature and Black media studies at the Georgia Institute of Technology.\n\n", "sentiment": 0.023171496713163403},
{"link_title": "Equifax hack checker: For \u201cTest\u201d and \u201c123456\u201d, data has been breached", "url": "https://twitter.com/zackwhittaker/status/906247688768905216", "text": "", "sentiment": 0.0},
{"link_title": "Web IDL", "url": "https://heycam.github.io/webidl/", "text": "This document defines an interface definition language, Web IDL, that can be used to describe interfaces that are intended to be implemented in web browsers. Web IDL is an IDL variant with a number of features that allow the behavior of common script objects in the web platform to be specified more readily. How interfaces described with Web IDL correspond to constructs within ECMAScript execution environments is also detailed in this document. It is expected that this document acts as a guide to implementors of already-published specifications, and that newly published specifications reference this document to ensure conforming implementations of interfaces are interoperable.\n\nThis document is governed by the 1 March 2017 W3C Process Document .\n\nThis document was produced by a group operating under the 5 February 2004 W3C Patent Policy . W3C maintains a public list of any patent disclosures made in connection with the deliverables of the group; that page also includes instructions for disclosing a patent. An individual who has actual knowledge of a patent which the individual believes contains Essential Claim(s) must disclose the information in accordance with section 6 of the W3C Patent Policy .\n\nPublication as an Editors Draft does not imply endorsement by the W3C Membership. This is a draft document and may be updated, replaced or obsoleted by other documents at any time. It is inappropriate to cite this document as other than work in progress.\n\nFeedback and comments on this specification are welcome, please send them to public-webapps@w3.org ( subscribe , archives ) with %5Bwebidl%5D at the start of your email\u2019s subject.\n\nThis document was published by the Web Platform Working Group as an Editors Draft. This document is intended to become a W3C Recommendation.\n\nThis section describes the status of this document at the time of its publication. Other documents may supersede this document. A list of current W3C publications and the latest revision of this technical report can be found in the W3C technical reports index at https://www.w3.org/TR/.\n\nTechnical reports published by the W3C that include programming language interfaces have typically been described using the Object Management Group\u2019s Interface Definition Language (IDL) [OMGIDL]. The IDL provides a means to describe these interfaces in a language independent manner. Usually, additional language binding appendices are included in such documents which detail how the interfaces described with the IDL correspond to constructs in the given language.\n\nHowever, the bindings in these specifications for the language most commonly used on the web, ECMAScript, are consistently specified with low enough precision as to result in interoperability issues. In addition, each specification must describe the same basic information, such as DOM interfaces described in IDL corresponding to properties on the ECMAScript global object, or the IDL type mapping to the Number type in ECMAScript.\n\nThis specification defines an IDL language similar to OMG IDL for use by specifications that define interfaces for Web APIs. A number of extensions are given to the IDL to support common functionality that previously must have been written in prose. In addition, precise language bindings for ECMAScript Edition 6 are given.\n\nThis section describes a language, Web IDL, which can be used to define interfaces for APIs in the Web platform. A specification that defines Web APIs can include one or more that describe the interfaces (the state and behavior that objects can exhibit) for the APIs defined by that specification. An IDL fragment is a sequence of definitions that matches the grammar symbol. The set of IDL fragments that an implementation supports is not ordered. See IDL grammar for the complete grammar and an explanation of the notation used.\n\nThe different kinds of that can appear in an IDL fragment are: interfaces, partial interface definitions, namespaces, partial namespace definitions, dictionaries, partial dictionary definitions, typedefs and implements statements. These are all defined in the following sections.\n\nEach definition (matching ) can be preceded by a list of extended attributes (matching ), which can control how the definition will be handled in language bindings. The extended attributes defined by this specification that are language binding agnostic are discussed in \u00a72.12 Extended attributes, while those specific to the ECMAScript language binding are discussed in \u00a73.3 ECMAScript-specific extended attributes.\n\nEvery interface, partial interface definition, namespace, partial namespace definition, dictionary, partial dictionary definition, enumeration, callback function and typedef (together called ) and every constant, attribute, and dictionary member has an , as do some operations. The identifier is determined by an token somewhere in the declaration:\n\nNote: Operations can have no identifier when they are being used to declare a special kind of operation, such as a getter or setter.\n\nFor all of these constructs, the identifier is the value of the token with any leading U+005F LOW LINE (\"_\") character (underscore) removed.\n\nNote: A leading \"_\" is used to escape an identifier from looking like a reserved word so that, for example, an interface named \u201cinterface\u201d can be defined. The leading \"_\" is dropped to unescape the identifier.\n\nOperation arguments can take a slightly wider set of identifiers. In an operation declaration, the identifier of an argument is specified immediately after its type and is given by either an token or by one of the keywords that match the symbol. If one of these keywords is used, it need not be escaped with a leading underscore.\n\nIf an token is used, then the identifier of the operation argument is the value of that token with any leading U+005F LOW LINE (\"_\") character (underscore) removed. If instead one of the keyword token is used, then the identifier of the operation argument is simply that token.\n\nThe identifier of any of the abovementioned IDL constructs must not be \u201cconstructor\u201d, \u201ctoString\u201d, or begin with a U+005F LOW LINE (\"_\") character. These are known as .\n\nAlthough the \u201ctoJSON\u201d identifier is not a reserved identifier, it must only be used for regular operations that convert objects to JSON types, as described in \u00a72.2.3.1 toJSON.\n\nNote: Further restrictions on identifier names for particular constructs may be made in later sections.\n\nWithin the set of IDL fragments that a given implementation supports, the identifier of every interface, namespace, dictionary, enumeration, callback function and typedef must not be the same as the identifier of any other interface, namespace, dictionary, enumeration, callback function or typedef.\n\nWithin an IDL fragment, a reference to a definition need not appear after the declaration of the referenced definition. References can also be made across IDL fragments.\n\nIDL fragments are used to describe object oriented systems. In such systems, objects are entities that have identity and which are encapsulations of state and behavior. An is a definition (matching or ) that declares some state and behavior that an object implementing that interface will expose.\n\nAn interface is a specification of a set of (matching ), which are the constants, attributes, operations and other declarations that appear between the braces in the interface declaration. Attributes describe the state that an object implementing the interface will expose, and operations describe the behaviors that can be invoked on the object. Constants declare named constant values that are exposed as a convenience to users of objects in the system.\n\nInterfaces in Web IDL describe how objects that implement the interface behave. In bindings for object oriented languages, it is expected that an object that implements a particular IDL interface provides ways to inspect and modify the object\u2019s state and to invoke the behavior described by the interface.\n\nAn interface can be defined to from another interface. If the identifier of the interface is followed by a U+003A COLON (\":\") character and an identifier, then that identifier identifies the inherited interface. An object that implements an interface that inherits from another also implements that inherited interface. The object therefore will also have members that correspond to the interface members from the inherited interface.\n\nThe order that members appear in has significance for property enumeration in the ECMAScript binding.\n\nInterfaces may specify an interface member that has the same name as one from an inherited interface. Objects that implement the derived interface will expose the member on the derived interface. It is language binding specific whether the overridden member can be accessed on the object.\n\nThe of a given interface is the set of all interfaces that inherits from, directly or indirectly. If does not inherit from another interface, then the set is empty. Otherwise, the set includes the interface that inherits from and all of \u2019s inherited interfaces.\n\nAn interface must not be declared such that its inheritance hierarchy has a cycle. That is, an interface cannot inherit from itself, nor can it inherit from another interface that inherits from , and so on.\n\nNote that general multiple inheritance of interfaces is not supported, and objects also cannot implement arbitrary sets of interfaces. Objects can be defined to implement a single given interface , which means that it also implements all of \u2019s inherited interfaces. In addition, an implements statement can be used to define that objects implementing an interface will always also implement another interface.\n\nEach interface member can be preceded by a list of extended attributes (matching ), which can control how the interface member will be handled in language bindings.\n\nA is an interface that uses the keyword at the start of its definition. Callback interfaces are ones that can be implemented by user objects and not by platform objects, as described in \u00a72.10 Objects implementing interfaces.\n\nNote: See also the similarly named callback function definition.\n\nCallback interfaces must not inherit from any non-callback interfaces, and non-callback interfaces must not inherit from any callback interfaces. Callback interfaces must not have any consequential interfaces.\n\nStatic attributes and static operations must not be defined on a callback interface.\n\nPerhaps this warning shouldn\u2019t apply if you are planning to extend the callback interface in the future. That\u2019s probably a good reason to start off with a single operation callback interface.\n\nI think we need to support operations not being implemented on a given user object implementing a callback interface. If specs extending an existing callback interface, we probably want to be able to avoid calling the operations that aren\u2019t implemented (and having some default behavior instead). So we should perhaps define a term that means whether the operation is implemented, which in the ECMAScript binding would correspond to checking for the property\u2019s existence.\n\nThe IDL for interfaces can be split into multiple parts by using definitions (matching ). The identifier of a partial interface definition must be the same as the identifier of an interface definition. All of the members that appear on each of the partial interfaces are considered to be members of the interface itself.\n\nNote: Partial interface definitions are intended for use as a specification editorial aide, allowing the definition of an interface to be separated over more than one section of the document, and sometimes multiple documents.\n\nThe order of appearance of an interface definition and any of its partial interface definitions does not matter.\n\nNote: A partial interface definition cannot specify that the interface inherits from another interface. Inheritance must be specified on the original interface definition.\n\nExtended attributes can be specified on partial interface definitions, with some limitations. The following extended attributes must not be specified on partial interface definitions: [ ], [ ], [ ], [ ], [ ].\n\nNote: The above list of extended attributes is all of those defined in this document that are applicable to interfaces except for [ ], [ ], [ ], [ ], and [ ].\n\nAny extended attribute specified on a partial interface definition is considered to appear on the interface itself.\n\nThe relevant language binding determines how interfaces correspond to constructs in the language.\n\nThe following extended attributes are applicable to interfaces: [ ], [ ], [ ], [ ], [ ], [ ], [ ], [ ], [ ], and [ ].\n\nA is a declaration (matching ) used to bind a constant value to a name. Constants can appear on interfaces.\n\nConstants have in the past primarily been used to define named integer codes in the style of an enumeration. The Web platform is moving away from this design pattern in favor of the use of strings. Specification authors who wish to define constants are strongly advised to discuss this on the public-script-coord@w3.org mailing list before proceeding.\n\nThe identifier of a constant must not be the same as the identifier of another interface member defined on the same interface. The identifier also must not be \u201clength\u201d, \u201cname\u201d or \u201cprototype\u201d.\n\nNote: These three names are the names of properties that may exist on function objects at the time the function object is created in the ECMAScript language binding.\n\nThe type of a constant (matching ) must not be any type other than a primitive type or a nullable primitive type. If an identifier is used, it must reference a typedef whose type is a primitive type or a nullable primitive type.\n\nThe part of a constant declaration gives the value of the constant, which can be one of the two boolean literal tokens ( and ), the token, an token, a token, or one of the three special floating point constant values ( , and ).\n\nNote: These values \u2013 in addition to strings and the empty sequence \u2013 can also be used to specify the default value of a dictionary member or of an optional argument. Note that strings and the empty sequence cannot be used as the value of a constant.\n\nThe value of the boolean literal tokens and are the IDL values and .\n\nThe type of an token is the same as the type of the constant, dictionary member or optional argument it is being used as the value of. The value of the token must not lie outside the valid range of values for its type, as given in \u00a72.11 Types.\n\nThe value of a constant value specified as , or is either an IEEE 754 single-precision floating point number or an IEEE 754 double-precision floating point number, depending on the type of the constant, dictionary member or optional argument is is being used as the value for:\n\nThe type of a token is the same as the type of the constant, dictionary member or optional argument it is being used as the value of. The value of the token must not lie outside the valid range of values for its type, as given in \u00a72.11 Types. Also, , and must not be used as the value of a or .\n\nThe value of the token is the special value that is a member of the nullable types. The type of the token is the same as the type of the constant, dictionary member or optional argument it is being used as the value of.\n\nIf is the type of the value assigned to a constant, and is the type of the constant, dictionary member or optional argument itself, then these types must be compatible, which is the case if and are identical, or is a nullable type whose inner type is .\n\nConstants are not associated with particular instances of the interface on which they appear. It is language binding specific whether constants are exposed on instances.\n\nThe following extended attributes are applicable to constants: [ ], [ ].\n\nAn is an interface member (matching , , , or ) that is used to declare data fields with a given type and identifier whose value can be retrieved and (in some cases) changed. There are two kinds of attributes:\n\nIf an attribute has no keyword, then it declares a . Otherwise, it declares a static attribute. Note that in addition to being interface members, read only regular attributes can be namespace members as well.\n\nTo get the underlying value of an attribute given a value , return the result of performing the actions listed in the description of that occur on getting, or those listed in the description of the inherited attribute, if is declared to inherit its getter, on if is not null.\n\nThe identifier of an attribute must not be the same as the identifier of another interface member defined on the same interface. The identifier of a static attribute must not be \u201cprototype\u201d.\n\nThe type of the attribute is given by the type (matching ) that appears after the keyword. If the is an identifier or an identifier followed by , then the identifier must identify an interface, enumeration, callback function or typedef.\n\nThe type of the attribute, after resolving typedefs, must not be a nullable or non-nullable version of any of the following types:\n\nThe attribute is if the keyword is used before the keyword. An object that implements the interface on which a read only attribute is defined will not allow assignment to that attribute. It is language binding specific whether assignment is simply disallowed by the language, ignored or an exception is thrown.\n\nAttributes whose type is a promise type must be read only. Additionally, they cannot have any of the extended attributes [ ], [ ], [ ], or [ ].\n\nA regular attribute that is not read only can be declared to from an ancestor interface. This can be used to make a read only attribute in an ancestor interface be writable on a derived interface. An attribute inherits its getter if its declaration includes in the declaration. The read only attribute from which the attribute inherits its getter is the attribute with the same identifier on the closest ancestor interface of the one on which the inheriting attribute is defined. The attribute whose getter is being inherited must be of the same type as the inheriting attribute, and must not appear on a read only attribute or a static attribute.\n\nWhen the keyword is used in a regular attribute declaration, it indicates that objects implementing the interface will be stringified to the value of the attribute. See \u00a72.2.4.2 Stringifiers for details.\n\nIf an implementation attempts to get or set the value of an attribute on a user object (for example, when a callback object has been supplied to the implementation), and that attempt results in an exception being thrown, then, unless otherwise specified, that exception will be propagated to the user code that caused the implementation to access the attribute. Similarly, if a value returned from getting the attribute cannot be converted to an IDL type, then any exception resulting from this will also be propagated to the user code that resulted in the implementation attempting to get the value of the attribute.\n\nThe following extended attributes are applicable to regular and static attributes: [ ], [ ], [ ].\n\nThe following extended attributes are applicable only to regular attributes: [ ], [ ], [ ], [ ], [ ].\n\nAn is an interface member (matching , , or ) that defines a behavior that can be invoked on objects implementing the interface. There are three kinds of operation:\n\nIf an operation has an identifier but no keyword, then it declares a . If the operation has one or more special keywords used in its declaration (that is, any keyword matching , or the keyword), then it declares a special operation. A single operation can declare both a regular operation and a special operation; see \u00a72.2.4 Special operations for details on special operations. Note that in addition to being interface members, regular operations can also be namespace members.\n\nIf an operation has no identifier, then it must be declared to be a special operation using one of the special keywords.\n\nThe identifier of a regular operation or static operation must not be the same as the identifier of a constant or attribute defined on the same interface. The identifier of a static operation must not be \u201cprototype\u201d.\n\nNote: The identifier can be the same as that of another operation on the interface, however. This is how operation overloading is specified.\n\nThe identifier of a static operation also must not be the same as the identifier of a regular operation defined on the same interface.\n\nThe of the operation is given by the type (matching ) that appears before the operation\u2019s optional identifier. A return type of indicates that the operation returns no value. If the return type is an identifier followed by , then the identifier must identify an interface, dictionary, enumeration, callback function or typedef.\n\nAn operation\u2019s arguments (matching ) are given between the parentheses in the declaration. Each individual argument is specified as a type (matching ) followed by an identifier (matching ).\n\nNote: For expressiveness, the identifier of an operation argument can also be specified as one of the keywords matching the symbol without needing to escape it.\n\nIf the of an operation argument is an identifier followed by , then the identifier must identify an interface, enumeration, callback function or typedef. If the operation argument type is an identifier not followed by , then the identifier must identify any one of those definitions or a dictionary.\n\nIf the operation argument type, after resolving typedefs, is a nullable type, its inner type must not be a dictionary type.\n\nThe identifier of each argument must not be the same as the identifier of another argument in the same operation declaration.\n\nEach argument can be preceded by a list of extended attributes (matching ), which can control how a value passed as the argument will be handled in language bindings.\n\nAn operation is considered to be if the final argument uses the token just after the argument type. Declaring an operation to be variadic indicates that the operation can be invoked with any number of arguments after that final argument. Those extra implied formal arguments are of the same type as the final explicit argument in the operation declaration. The final argument can also be omitted when invoking the operation. An argument must not be declared with the token unless it is the final argument in the operation\u2019s argument list.\n\nExtended attributes that take an argument list ([ ] and [ ], of those defined in this specification) and callback functions are also considered to be variadic when the token is used in their argument lists.\n\nAn argument is considered to be an if it is declared with the keyword. The final argument of a variadic operation is also considered to be an optional argument. Declaring an argument to be optional indicates that the argument value can be omitted when the operation is invoked. The final argument in an operation must not explicitly be declared to be optional if the operation is variadic.\n\nOptional arguments can also have a specified. If the argument\u2019s identifier is followed by a U+003D EQUALS SIGN (\"=\") and a value (matching ), then that gives the optional argument its default value. The implicitly optional final argument of a variadic operation must not have a default value specified. The default value is the value to be assumed when the operation is called with the corresponding argument omitted.\n\nIt is strongly suggested not to use a default value of for -typed arguments, as this can be confusing for authors who might otherwise expect the default conversion of to be used (i.e., ).\n\nIf the type of an argument is a dictionary type or a union type that has a dictionary as one of its flattened member types, and that dictionary type and its ancestors have no required members, and the argument is either the final argument or is followed only by optional arguments, then the argument must be specified as optional. Such arguments are always considered to have a default value of an empty dictionary, unless otherwise specified.\n\nWhen a boolean literal token ( or ), the token, an token, a token or one of the three special floating point literal values ( , or ) is used as the default value, it is interpreted in the same way as for a constant.\n\nIf the type of the optional argument is an enumeration, then its default value if specified must be one of the enumeration\u2019s values.\n\nOptional argument default values can also be specified using the two token value , which represents an empty sequence value. The type of this value is the same as the type of the optional argument it is being used as the default value of. That type must be a sequence type, a nullable type whose inner type is a sequence type or a union type or nullable union type that has a sequence type in its flattened member types.\n\nIf an implementation attempts to invoke an operation on a user object (for example, when a callback object has been supplied to the implementation), and that attempt results in an exception being thrown, then, unless otherwise specified, that exception will be propagated to the user code that caused the implementation to invoke the operation. Similarly, if a value returned from invoking the operation cannot be converted to an IDL type, then any exception resulting from this will also be propagated to the user code that resulted in the implementation attempting to invoke the operation.\n\nThe following extended attributes are applicable to operations: [ ], [ ], [ ], [ ], [ ].\n\nBy declaring a regular operation, an interface specifies how to convert the objects that implement it to JSON types.\n\nThe regular operation is reserved for this usage. It must take zero arguments and return a JSON type.\n\nHow the regular operation is made available on an object in a language binding, and how exactly the JSON types are converted into a JSON string, is language binding specific.\n\nNote: In the ECMAScript language binding, this is done by exposing a method which returns the JSON type converted into an ECMAScript value that can be turned into a JSON string by the function. Additionaly, in the ECMAScript language binding, the operation can take a [ ] extended attribute, in which case the default toJSON operation is exposed instead.\n\nA is a declaration of a certain kind of special behavior on objects implementing the interface on which the special operation declarations appear. Special operations are declared by using one or more in an operation declaration.\n\nThere are five kinds of special operations. The table below indicates for a given kind of special operation what special keyword is used to declare it and what the purpose of the special operation is:\n\nNot all language bindings support all of the five kinds of special object behavior. When special operations are declared using operations with no identifier, then in language bindings that do not support the particular kind of special operations there simply will not be such functionality.\n\nDefining a special operation with an identifier is equivalent to separating the special operation out into its own declaration without an identifier. This approach is allowed to simplify prose descriptions of an interface\u2019s operations.\n\nA given special keyword must not appear twice on an operation.\n\nGetters and setters come in two varieties: ones that take a as a property name, known as and , and ones that take an as a property index, known as and . There is only one variety of deleter: . See \u00a72.2.4.3 Indexed properties and \u00a72.2.4.4 Named properties for details.\n\nOn a given interface, there must exist at most one stringifier, at most one named property deleter, and at most one of each variety of getter and setter. Multiple legacy callers can exist on an interface to specify overloaded calling behavior.\n\nIf an interface has a setter of a given variety, then it must also have a getter of that variety. If it has a named property deleter, then it must also have a named property getter.\n\nSpecial operations declared using operations must not be variadic nor have any optional arguments.\n\nSpecial operations must not be declared on callback interfaces.\n\nIf an object implements more than one interface that defines a given special operation, then it is undefined which (if any) special operation is invoked for that operation.\n\nWhen an interface has one or more legacy callers, it indicates that objects that implement the interface can be called as if they were functions. As mentioned above, legacy callers can be specified using an operation declared with the keyword.\n\nIf multiple legacy callers are specified on an interface, overload resolution is used to determine which legacy caller is invoked when the object is called as if it were a function.\n\nLegacy callers can only be defined on interfaces that also support indexed or named properties.\n\nNote: This artificial restriction allows bundling all interfaces with exotic object behavior into a single platform object category: legacy platform objects. This is possible because all existing interfaces which have a legacy caller also supports indexed or named properties.\n\nLegacy callers must not be defined to return a promise type.\n\nLegacy callers are universally recognised as an undesirable feature. They exist only so that legacy Web platform features can be specified. Legacy callers should not be used in specifications unless required to specify the behavior of legacy APIs, and even then this should be discussed on the public-script-coord@w3.org mailing list before proceeding.\n\nWhen an interface has a stringifier, it indicates that objects that implement the interface have a non-default conversion to a string. As mentioned above, stringifiers can be specified using an operation declared with the keyword.\n\nIf an operation used to declare a stringifier does not have an identifier, then prose accompanying the interface must define the of the interface. If the operation does have an identifier, then the object is converted to a string by invoking the operation to obtain the string.\n\nStringifiers declared with operations must be declared to take zero arguments and return a .\n\nAs a shorthand, if the keyword is declared using an operation with no identifier, then the operation\u2019s return type and argument list can be omitted.\n\nThe keyword can also be placed on an attribute. In this case, the string to convert the object to is the value of the attribute. The keyword must not be placed on an attribute unless it is declared to be of type or . It also must not be placed on a static attribute.\n\nAn interface that defines an indexed property getter is said to . By extension, a platform object is said to support indexed properties if it implements an interface that itself does.\n\nIf an interface supports indexed properties, then the interface definition must be accompanied by a description of what indices the object can be indexed with at any given time. These indices are called the .\n\nInterfaces that support indexed properties must define an integer-typed attribute named \u201clength\u201d.\n\nIndexed property getters must be declared to take a single argument. Indexed property setters must be declared to take two arguments, where the first is an .\n\nThe following requirements apply to the definitions of indexed property getters and setters:\n\nAn interface that defines a named property getter is said to . By extension, a platform object is said to support named properties if it implements an interface that itself does.\n\nIf an interface supports named properties, then the interface definition must be accompanied by a description of the ordered set of names that can be used to index the object at any given time. These names are called the .\n\nNamed property getters and deleters must be declared to take a single argument. Named property setters must be declared to take two arguments, where the first is a .\n\nThe following requirements apply to the definitions of named property getters, setters and deleters:\n\nNote: As with indexed properties, if an named property getter, setter or deleter is specified using an operation with an identifier, then indexing an object with a name that is not a supported property name does not necessarily elicit the same behavior as invoking the operation with that name; the behavior is language binding specific.\n\nand are ones that are not associated with a particular instance of the interface on which it is declared, and is instead associated with the interface itself. Static attributes and operations are declared by using the keyword in their declarations.\n\nIt is language binding specific whether it is possible to invoke a static operation or get or set a static attribute through a reference to an instance of the interface.\n\nStatic attributes and operations must not be declared on callback interfaces.\n\nIf a regular operation or static operation defined on an interface has an identifier that is the same as the identifier of another operation on that interface of the same kind (regular or static), then the operation is said to be . When the identifier of an overloaded operation is used to invoke one of the operations on an object that implements the interface, the number and types of the arguments passed to the operation determine which of the overloaded operations is actually invoked. If an interface has multiple legacy callers defined on it, then those legacy callers are also said to be overloaded. In the ECMAScript language binding, constructors can be overloaded too. There are some restrictions on the arguments that overloaded operations, legacy callers and constructors can be specified to take, and in order to describe these restrictions, the notion of an effective overload set is used.\n\nOperations and legacy callers must not be overloaded across interface and partial interface definitions.\n\nAn represents the allowable invocations for a particular operation, constructor (specified with [ ] or [ ]), legacy caller or callback function. The algorithm to compute an effective overload set operates on one of the following six types of IDL constructs, and listed with them below are the inputs to the algorithm needed to compute the set.\n\nAn effective overload set is used, among other things, to determine whether there are ambiguities in the overloaded operations, constructors and callers specified on an interface.\n\nThe items of an effective overload set are tuples of the form (callable, type list, optionality list) whose items are described below:\n\nEach tuple represents an allowable invocation of the operation, constructor, legacy caller or callback function with an argument value list of the given types. Due to the use of optional arguments and variadic operations and constructors, there may be multiple items in an effective overload set identifying the same operation or constructor.\n\nTwo types are if the following algorithm returns true.\n\nIf there is more than one item in an effective overload set that has a given type list size, then for those items there must be an index such that for each pair of items the types at index are distinguishable. The lowest such index is termed the for the items of the effective overload set with the given type list size.\n\nIn addition, for each index , where is less than the distinguishing argument index for a given type list size, the types at index in all of the items\u2019 type lists must be the same, and the optionality values at index in all of the items\u2019 optionality lists must be the same.\n\nAn interface can be declared to be by using an (matching ) in the body of the interface.\n\nObjects implementing an interface that is declared to be iterable support being iterated over to obtain a sequence of values.\n\nNote: In the ECMAScript language binding, an interface that is iterable will have \u201centries\u201d, \u201cforEach\u201d, \u201ckeys\u201d, \u201cvalues\u201d and @@iterator properties on its interface prototype object.\n\nIf a single type parameter is given, then the interface has a and provides values of the specified type. If two type parameters are given, then the interface has a and provides value pairs, where the first value is a key and the second is the value associated with the key.\n\nA value iterator must only be declared on an interface that supports indexed properties. The value-type of the value iterator must be the same as the type returned by the indexed property getter. A value iterator is implicitly defined to iterate over the object\u2019s indexed properties.\n\nA pair iterator must not be declared on an interface that supports indexed properties. Prose accompanying an interface with a pair iterator must define what the list of value pairs to iterate over is.\n\nNote: This is how array iterator objects work. For interfaces that support indexed properties, the iterator objects returned by \u201centries\u201d, \u201ckeys\u201d, \u201cvalues\u201d and @@iterator are actual array iterator objects.\n\nInterfaces with iterable declarations must not have any interface members named \u201centries\u201d, \u201cforEach\u201d, \u201ckeys\u201d or \u201cvalues\u201d, or have any inherited or consequential interfaces that have interface members with these names.\n\nAn interface must not have more than one iterable declaration. The inherited and consequential interfaces of an interface with an iterable declaration must not also have an iterable declaration. An interface with an iterable declaration and its inherited and consequential interfaces must not have a maplike declaration or setlike declaration.\n\nThe following extended attributes are applicable to iterable declarations: [ ], [ ].\n\nAn interface can be declared to be by using a (matching or ) in the body of the interface.\n\nObjects implementing an interface that is declared to be maplike represent an ordered list of key\u2013value pairs known as its . The types used for the keys and values are given in the angle brackets of the maplike declaration. Keys are required to be unique.\n\nThe map entries of an object implementing a maplike interface is empty at the of the object\u2019s creation. Prose accompanying the interface can describe how the map entries of an object change.\n\nMaplike interfaces support an API for querying the map entries appropriate for the language binding. If the keyword is not used, then it also supports an API for modifying the map entries.\n\nNote: In the ECMAScript language binding, the API for interacting with the map entries is similar to that available on ECMAScript objects. If the keyword is used, this includes \u201centries\u201d, \u201cforEach\u201d, \u201cget\u201d, \u201chas\u201d, \u201ckeys\u201d, \u201cvalues\u201d, @@iterator methods and a \u201csize\u201d getter. For read\u2013write maplikes, it also includes \u201cclear\u201d, \u201cdelete\u201d and \u201cset\u201d methods.\n\nMaplike interfaces must not have any interface members named \u201centries\u201d, \u201cforEach\u201d, \u201cget\u201d, \u201chas\u201d, \u201ckeys\u201d, \u201csize\u201d, or \u201cvalues\u201d, or have any inherited or consequential interfaces that have interface members with these names. Read\u2013write maplike interfaces must not have any attributes or constants named \u201cclear\u201d, \u201cdelete\u201d, or \u201cset\u201d, or have any inherited or consequential interfaces that have attributes or constants with these names.\n\nNote: Operations named \u201cclear\u201d, \u201cdelete\u201d, or \u201cset\u201d are allowed on read\u2013write maplike interfaces and will prevent the default implementation of these methods being added to the interface prototype object in the ECMAScript language binding. This allows the default behavior of these operations to be overridden.\n\nAn interface must not have more than one maplike declaration. The inherited and consequential interfaces of a maplike interface must not also have a maplike declaration. A maplike interface and its inherited and consequential interfaces must not have an iterable declaration or setlike declaration.\n\nNo extended attributes defined in this specification are applicable to maplike declarations.\n\nAn interface can be declared to be by using a (matching or ) in the body of the interface.\n\nObjects implementing an interface that is declared to be setlike represent an ordered list of values known as its . The type of the values is given in the angle brackets of the setlike declaration. Values are required to be unique.\n\nThe set entries of an object implementing a setlike interface is empty at the of the object\u2019s creation. Prose accompanying the interface can describe how the set entries of an object change.\n\nSetlike interfaces support an API for querying the set entries appropriate for the language binding. If the keyword is not used, then it also supports an API for modifying the set entries.\n\nNote: In the ECMAScript language binding, the API for interacting with the set entries is similar to that available on ECMAScript objects. If the keyword is used, this includes \u201centries\u201d, \u201cforEach\u201d, \u201chas\u201d, \u201ckeys\u201d, \u201cvalues\u201d, @@iterator methods and a \u201csize\u201d getter. For read\u2013write setlikes, it also includes \u201cadd\u201d, \u201cclear\u201d, and \u201cdelete\u201d methods.\n\nSetlike interfaces must not have any interface members named \u201centries\u201d, \u201cforEach\u201d, \u201chas\u201d, \u201ckeys\u201d, \u201csize\u201d, or \u201cvalues\u201d, or have any inherited or consequential interfaces that have interface members with these names.. Read\u2013write setlike interfaces must not have any attributes or constants named \u201cadd\u201d, \u201cclear\u201d, or \u201cdelete\u201d, or have any inherited or consequential interfaces that have attributes or constants with these names.\n\nNote: Operations named \u201cadd\u201d, \u201cclear\u201d, or \u201cdelete\u201d are allowed on read\u2013write setlike interfaces and will prevent the default implementation of these methods being added to the interface prototype object in the ECMAScript language binding. This allows the default behavior of these operations to be overridden.\n\nAn interface must not have more than one setlike declaration. The inherited and consequential interfaces of a setlike interface must not also have a setlike declaration. A setlike interface and its inherited and consequential interfaces must not have an iterable declaration or maplike declaration.\n\nNo extended attributes defined in this specification are applicable to setlike declarations.\n\nA is a definition (matching ) that declares a global singleton with associated behaviors.\n\nA namespace is a specification of a set of (matching ), which are the regular operations and read only regular attributes that appear between the braces in the namespace declaration. These operations and attributes describe the behaviors packaged into the namespace.\n\nAs with interfaces, the IDL for namespaces can be split into multiple parts by using definitions (matching ). The identifier of a partial namespace definition must be the same as the identifier of a namespace definition. All of the members that appear on each of the partial namespace definitions are considered to be members of the namespace itself.\n\nNote: As with partial interface definitions, partial namespace definitions are intended for use as a specification editorial aide, allowing the definition of a namespace to be separated over more than one section of the document, and sometimes multiple documents.\n\nThe order that members appear in has significance for property enumeration in the ECMAScript binding.\n\nNote that unlike interfaces or dictionaries, namespaces do not create types.\n\nOf the extended attributes defined in this specification, only the [ ] and [ ] extended attributes are applicable to namespaces.\n\nA is a definition (matching ) used to define an ordered map data type with a fixed, ordered set of entries, termed , where keys are strings and values are of a particular type specified in the definition.\n\nDictionaries are always passed by value. In language bindings where a dictionary is represented by an object of some kind, passing a dictionary to a platform object will not result in a reference to the dictionary being kept by that object. Similarly, any dictionary returned from a platform object will be a copy and modifications made to it will not be visible to the platform object.\n\nA dictionary can be defined to from another dictionary. If the identifier of the dictionary is followed by a colon and a identifier, then that identifier identifies the inherited dictionary. The identifier must identify a dictionary.\n\nA dictionary must not be declared such that its inheritance hierarchy has a cycle. That is, a dictionary cannot inherit from itself, nor can it inherit from another dictionary that inherits from , and so on.\n\nThe of a given dictionary is the set of all dictionaries that inherits from, directly or indirectly. If does not inherit from another dictionary, then the set is empty. Otherwise, the set includes the dictionary that inherits from and all of \u2019s inherited dictionaries.\n\nA dictionary value of type can have key\u2013value pairs corresponding to the dictionary members defined on and on any of \u2019s inherited dictionaries. On a given dictionary value, the presence of each dictionary member is optional, unless that member is specified as required. When specified in the dictionary value, a dictionary member is said to be , otherwise it is not present. Dictionary members can also optionally have a , which is the value to use for the dictionary member when passing a value to a platform object that does not have a specified value. Dictionary members with default values are always considered to be present.\n\nIn the ECMAScript binding, a value of is treated as not present, or will trigger the default value where applicable.\n\nAn ordered map with string keys can be implicitly treated as a dictionary value of a specific dictionary if all of its entries correspond to dictionary members, in the correct order and with the correct types, and with appropriate entries for any required dictionary members.\n\nAs with operation argument default values, it is strongly suggested not to use as the default value for -typed dictionary members, as this can be confusing for authors who might otherwise expect the default conversion of to be used (i.e., ).\n\nEach dictionary member (matching ) is specified as a type (matching ) followed by an identifier (given by an token following the type). The identifier is the key name of the key\u2013value pair. If the is an identifier followed by , then the identifier must identify an interface, enumeration, callback function or typedef. If the dictionary member type is an identifier not followed by , then the identifier must identify any one of those definitions or a dictionary.\n\nIf the type of the dictionary member, after resolving typedefs, is a nullable type, its inner type must not be a dictionary type.\n\nIf the identifier is followed by a U+003D EQUALS SIGN (\"=\") and a value (matching ), then that gives the dictionary member its default value.\n\nWhen a boolean literal token ( or ), the token, an token, a token, one of the three special floating point literal values ( , or ), a token or the two token sequence used as the default value, it is interpreted in the same way as for an operation\u2019s optional argument default value.\n\nIf the type of the dictionary member is an enumeration, then its default value if specified must be one of the enumeration\u2019s values.\n\nIf the type of the dictionary member is preceded by the keyword, the member is considered a and must be present on the dictionary. A required dictionary member must not have a default value.\n\nThe type of a dictionary member must not include the dictionary it appears on. A type includes a dictionary if at least one of the following is true:\n\nAs with interfaces, the IDL for dictionaries can be split into multiple parts by using definitions (matching ). The identifier of a partial dictionary definition must be the same as the identifier of a dictionary definition. All of the members that appear on each of the partial dictionary definitions are considered to be members of the dictionary itself.\n\nNote: As with partial interface definitions, partial dictionary definitions are intended for use as a specification editorial aide, allowing the definition of an interface to be separated over more than one section of the document, and sometimes multiple documents.\n\nThe order of the dictionary members on a given dictionary is such that inherited dictionary members are ordered before non-inherited members, and the dictionary members on the one dictionary definition (including any partial dictionary definitions) are ordered lexicographically by the Unicode codepoints that comprise their identifiers.\n\nThe identifier of a dictionary member must not be the same as that of another dictionary member defined on the dictionary or on that dictionary\u2019s inherited dictionaries.\n\nDictionaries must not be used as the type of an attribute or constant.\n\nNo extended attributes are applicable to dictionaries.\n\nAn is a type of object that represents an error and which can be thrown or treated as a first class value by implementations. Web IDL does not allow exceptions to be defined, but instead has a number of pre-defined exceptions that specifications can reference and throw in their definition of operations, attributes, and so on. Exceptions have an , a , which is the type of error the exception represents, and a , which is an optional, user agent-defined value that provides human readable details of the error.\n\nThere are two kinds of exceptions available to be thrown from specifications. The first is a , which is identified by one of the following types:\n\nThese correspond to all of the ECMAScript error objects (apart from and , which are deliberately omitted as they are reserved for use by the ECMAScript parser and by authors, respectively). The meaning of each simple exception matches its corresponding error object in the ECMAScript specification.\n\nThe second kind of exception is a , which is an exception that encapsulates a name and an optional integer code, for compatibility with historically defined exceptions in the DOM.\n\nFor simple exceptions, the error name is the type of the exception. For a , the error name must be one of the names listed in the error names table below. The table also indicates the 's integer code for that error name, if it has one.\n\nThere are two types that can be used to refer to exception objects: and , where the latter encompasses simple exceptions and . This allows for example an operation to be declared to have a return type or an attribute to be of type .\n\nSimple exceptions can be by providing their error name. A can be created by providing its error name followed by . Exceptions can also be , by providing the same details required to create one.\n\nThe resulting behavior from creating and throwing an exception is language binding-specific.\n\nNote: See \u00a73.13.3 Creating and throwing exceptions for details on what creating and throwing an exception entails in the ECMAScript language binding.\n\nThe below lists all the allowed error names for , a description, and legacy code values.\n\nThe names marked as deprecated are kept for legacy purposes but their usage is discouraged.\n\nNote: If an error name is not listed here, please file a bug as indicated at the top of this specification and it will be addressed shortly. Thanks!\n\nAn is a definition (matching ) used to declare a type whose valid values are a set of predefined strings. Enumerations can be used to restrict the possible values that can be assigned to an attribute or passed to an operation.\n\nThe are specified as a comma-separated list of literals. The list of enumeration values must not include duplicates.\n\nIt is strongly suggested that enumeration values be all lowercase, and that multiple words be separated using dashes or not be separated at all, unless there is a specific reason to use another value naming scheme. For example, an enumeration value that indicates an object should be created could be named or . Consider related uses of enumeration values when deciding whether to dash-separate or not separate enumeration value words so that similar APIs are consistent.\n\nThe behavior when a string value that is not one a valid enumeration value is used when assigning to an attribute, or passed as an operation argument, whose type is the enumeration, is language binding specific.\n\nNote: In the ECMAScript binding, assignment of an invalid string value to an attribute is ignored, while passing such a value as an operation argument results in an exception being thrown.\n\nNo extended attributes defined in this specification are applicable to enumerations.\n\nThe \u201cCustom DOM Elements\u201d spec wants to use callback function types for platform object provided functions. Should we rename \u201ccallback functions\u201d to just \u201cfunctions\u201d to make it clear that they can be used for both purposes?\n\nA is a definition (matching ) used to declare a function type.\n\nNote: See also the similarly named callback interfaces.\n\nThe identifier on the left of the equals sign gives the name of the callback function and the return type and argument list (matching and ) on the right side of the equals sign gives the signature of the callback function type.\n\nCallback functions must not be used as the type of a constant.\n\nThe following extended attribute is applicable to callback functions: [ ].\n\nA is a definition (matching ) used to declare a new name for a type. This new name is not exposed by language bindings; it is purely used as a shorthand for referencing the type in the IDL.\n\nThe type being given a new name is specified after the keyword (matching ), and the token following the type gives the name.\n\nThe must not be the identifier of the same or another typedef.\n\nNo extended attributes defined in this specification are applicable to typedefs.\n\nAn is a definition (matching ) used to declare that all objects implementing an interface (identified by the first identifier) must additionally implement interface (identified by the second identifier), including all other interfaces that inherits from.\n\nTransitively, if objects implementing are declared with an implements statement to additionally implement interface , then all objects implementing do additionally implement interface .\n\nThe two identifiers must identify two different interfaces.\n\nThe interface identified on the left-hand side of an implements statement must not inherit from the interface identifier on the right-hand side, and vice versa. Both identified interfaces also must not be callback interfaces.\n\nIf each implements statement is considered to be an edge in a directed graph, from a node representing the interface on the left-hand side of the statement to a node representing the interface on the right-hand side, then this graph must not have any cycles.\n\nInterfaces that a given object implements are partitioned into those that are considered and those that are not. An interface is considered to be a supplemental interface of an object if:\n\nThe of an interface are:\n\nFor a given interface, there must not be any member defined on any of its consequential interfaces whose identifier is the same as any other member defined on any of those consequential interfaces or on the original interface itself.\n\nNo extended attributes defined in this specification are applicable to implements statements.\n\nIn a given implementation of a set of IDL fragments, an object can be described as being a , a , or neither. There are two kinds of object that are considered to be platform objects:\n\nare platform objects that implement an interface which does not have a [ ] or [ ] extended attribute, supports indexed or named properties, and may have one or multiple legacy callers.\n\nIn a browser, for example, the browser-implemented DOM objects (implementing interfaces such as and ) that provide access to a web page\u2019s contents to ECMAScript running in the page would be platform objects. These objects might be exotic objects, implemented in a language like C++, or they might be native ECMAScript objects. Regardless, an implementation of a given set of IDL fragments needs to be able to recognize all platform objects that are created by the implementation. This might be done by having some internal state that records whether a given object is indeed a platform object for that implementation, or perhaps by observing that the object is implemented by a given internal C++ class. How exactly platform objects are recognised by a given implementation of a set of IDL fragments is implementation specific.\n\nAll other objects in the system would not be treated as platform objects. For example, assume that a web page opened in a browser loads an ECMAScript library that implements DOM Core. This library would be considered to be a different implementation from the browser provided implementation. The objects created by the ECMAScript library that implement the interface will not be treated as platform objects that implement by the browser implementation.\n\nUser objects are those that authors would create, implementing callback interfaces that the Web APIs use to be able to invoke author-defined operations or to send and receive values to the author\u2019s program through manipulating the object\u2019s attributes. In a web page, an ECMAScript object that implements the interface, which is used to register a callback that the DOM Events implementation invokes, would be considered to be a user object.\n\nNote that user objects can only implement callback interfaces and platform objects can only implement non-callback interfaces.\n\nThis section lists the types supported by Web IDL, the set of values corresponding to each type, and how constants of that type are represented.\n\nThe following types are known as : , , , , , , and .\n\nThe following types are known as : the integer types, , , and .\n\nThe are and the numeric types.\n\nThe are , all enumeration types, and .\n\nThe are and .\n\nThe are , , , , , , , and .\n\nThe are , , and the typed array types.\n\nThe type, all interface types and the exception types are known as .\n\nEvery type has a , which is a string, not necessarily unique, that identifies the type. Each sub-section below defines what the type name is for each type.\n\nWhen conversions are made from language binding specific types to IDL types in order to invoke an operation or assign a value to an attribute, all conversions necessary will be performed before the specified functionality of the operation or attribute assignment is carried out. If the conversion cannot be performed, then the operation will not run or the attribute will not be updated. In some language bindings, type conversions could result in an exception being thrown. In such cases, these exceptions will be propagated to the code that made the attempt to invoke the operation or assign to the attribute.\n\nThe type is the union of all other possible non-union types. Its type name is \u201cAny\u201d.\n\nThe type is like a discriminated union type, in that each of its values has a specific non- type associated with it. For example, one value of the type is the 150, while another is the 150. These are distinct values.\n\nThe particular type of an value is known as its . (Values of union types also have specific types.)\n\nThe type has two values: and .\n\nconstant values in IDL are represented with the and tokens.\n\nThe type name of the type is \u201cBoolean\u201d.\n\nThe type is a signed integer type that has values in the range [\u2212128, 127].\n\nconstant values in IDL are represented with tokens.\n\nThe type name of the type is \u201cByte\u201d.\n\nThe type is an unsigned integer type that has values in the range [0, 255].\n\nconstant values in IDL are represented with tokens.\n\nThe type name of the type is \u201cOctet\u201d.\n\nThe type is a signed integer type that has values in the range [\u221232768, 32767].\n\nconstant values in IDL are represented with tokens.\n\nThe type name of the type is \u201cShort\u201d.\n\nThe type is an unsigned integer type that has values in the range [0, 65535].\n\nconstant values in IDL are represented with tokens.\n\nThe type name of the type is \u201cUnsignedShort\u201d.\n\nThe type is a signed integer type that has values in the range [\u22122147483648, 2147483647].\n\nconstant values in IDL are represented with tokens.\n\nThe type name of the type is \u201cLong\u201d.\n\nThe type is an unsigned integer type that has values in the range [0, 4294967295].\n\nconstant values in IDL are represented with tokens.\n\nThe type name of the type is \u201cUnsignedLong\u201d.\n\nThe type is a signed integer type that has values in the range [\u22129223372036854775808, 9223372036854775807].\n\nconstant values in IDL are represented with tokens.\n\nThe type name of the type is \u201cLongLong\u201d.\n\nThe type is an unsigned integer type that has values in the range [0, 18446744073709551615].\n\nconstant values in IDL are represented with tokens.\n\nThe type name of the type is \u201cUnsignedLongLong\u201d.\n\nThe type is a floating point numeric type that corresponds to the set of finite single-precision 32 bit IEEE 754 floating point numbers. [IEEE-754]\n\nconstant values in IDL are represented with tokens.\n\nThe type name of the type is \u201cFloat\u201d.\n\nUnless there are specific reasons to use a 32 bit floating point type, specifications should use rather than , since the set of values that a can represent more closely matches an ECMAScript Number.\n\nThe type is a floating point numeric type that corresponds to the set of all possible single-precision 32 bit IEEE 754 floating point numbers, finite, non-finite, and special \"not a number\" values (NaNs). [IEEE-754]\n\nconstant values in IDL are represented with tokens.\n\nThe type name of the type is \u201cUnrestrictedFloat\u201d.\n\nThe type is a floating point numeric type that corresponds to the set of finite double-precision 64 bit IEEE 754 floating point numbers. [IEEE-754]\n\nconstant values in IDL are represented with tokens.\n\nThe type name of the type is \u201cDouble\u201d.\n\nThe type is a floating point numeric type that corresponds to the set of all possible double-precision 64 bit IEEE 754 floating point numbers, finite, non-finite, and special \"not a number\" values (NaNs). [IEEE-754]\n\nconstant values in IDL are represented with tokens.\n\nThe type name of the type is \u201cUnrestrictedDouble\u201d.\n\nThe type corresponds to the set of all possible sequences of code units. Such sequences are commonly interpreted as UTF-16 encoded strings [RFC2781] although this is not required. While is defined to be an OMG IDL boxed sequence< > valuetype in DOM Level 3 Core \u00a7The DOMString Type, this document defines to be an intrinsic type so as to avoidspecial casing that sequence type in various situations where a string is required.\n\nNote: Note also that is not a value of type . To allow , a nullable , written as in IDL, needs to be used.\n\nNothing in this specification requires a value to be a valid UTF-16 string. For example, a value might include unmatched surrogate pair characters. However, authors of specifications using Web IDL might want to obtain a sequence of Unicode scalar values given a particular sequence of code units.\n\nThere is no way to represent a constant value in IDL, although dictionary member and operation optional argument default values can be specified using a literal.\n\nThe type name of the type is \u201cString\u201d.\n\nThe type corresponds to the set of all possible sequences of bytes. Such sequences might be interpreted as UTF-8 encoded strings [RFC3629] or strings in some other 8-bit-per-code-unit encoding, although this is not required.\n\nThere is no way to represent a constant value in IDL, although dictionary member and operation optional argument default values can be specified using a literal.\n\nThe type name of the type is \u201cByteString\u201d.\n\nSpecifications should only use for interfacing with protocols that use bytes and strings interchangably, such as HTTP. In general, strings should be represented with values, even if it is expected that values of the string will always be in ASCII or some 8 bit character encoding. Sequences, frozen arrays or Typed Arrays with or elements should be used for holding 8 bit data rather than .\n\nThe type corresponds to the set of all possible sequences of Unicode scalar values, which are all of the Unicode code points apart from the surrogate code points.\n\nThere is no way to represent a constant value in IDL, although dictionary member and operation optional argument default values can be specified using a literal.\n\nThe type name of the type is \u201cUSVString\u201d.\n\nSpecifications should only use for APIs that perform text processing and need a string of Unicode scalar values to operate on. Most APIs that use strings should instead be using , which does not make any interpretations of the code units in the string. When in doubt, use .\n\nThe type corresponds to the set of all possible non-null object references.\n\nThere is no way to represent a constant value in IDL.\n\nTo denote a type that includes all possible object references plus the value, use the nullable type .\n\nThe type name of the type is \u201cObject\u201d.\n\nThe type corresponds to the set of all possible symbol values. Symbol values are opaque, non- values which nevertheless have identity (i.e., are only equal to themselves).\n\nThere is no way to represent a constant value in IDL.\n\nThe type name of the type is \u201cSymbol\u201d.\n\nAn identifier that identifies an interface is used to refer to a type that corresponds to the set of all possible non-null references to objects that implement that interface.\n\nFor non-callback interfaces, an IDL value of the interface type is represented just by an object reference. For callback interfaces, an IDL value of the interface type is represented by a tuple of an object reference and a . The callback context is a language binding specific value, and is used to store information about the execution context at the time the language binding specific object reference is converted to an IDL value.\n\nNote: For ECMAScript objects, the callback context is used to hold a reference to the incumbent settings object at the time the Object value is converted to an IDL callback interface type value. See \u00a73.2.14 Interface types.\n\nThere is no way to represent a constant object reference value for a particular interface type in IDL.\n\nTo denote a type that includes all possible references to objects implementing the given interface plus the value, use a nullable type.\n\nThe type name of an interface type is the identifier of the interface.\n\nAn identifier that identifies a dictionary is used to refer to a type that corresponds to the set of all dictionaries that adhere to the dictionary definition.\n\nThe literal syntax for ordered maps may also be used to represent dictionaries, when it is implicitly understood from context that the map is being treated as an instance of a specific dictionary type. However, there is no way to represent a constant dictionary value inside IDL fragments.\n\nThe type name of a dictionary type is the identifier of the dictionary.\n\nAn identifier that identifies an enumeration is used to refer to a type whose values are the set of strings (sequences of code units, as with ) that are the enumeration\u2019s values.\n\nLike , there is no way to represent a constant enumeration value in IDL, although enumeration-typed dictionary member default values can be specified using a literal.\n\nThe type name of an enumeration type is the identifier of the enumeration.\n\nAn identifier that identifies a callback function is used to refer to a type whose values are references to objects that are functions with the given signature.\n\nAn IDL value of the callback function type is represented by a tuple of an object reference and a callback context.\n\nNote: As with callback interface types, the callback context is used to hold a reference to the incumbent settings object at the time an ECMAScript Object value is converted to an IDL callback function type value. See \u00a73.2.17 Callback function types.\n\nThere is no way to represent a constant callback function value in IDL.\n\nThe type name of a callback function type is the identifier of the callback function.\n\nA is an IDL type constructed from an existing type (called the ), which just allows the additional value to be a member of its set of values. Nullable types are represented in IDL by placing a U+003F QUESTION MARK (\"?\") character after an existing type. The inner type must not be:\n\nNote: Although dictionary types can in general be nullable, they cannot when used as the type of an operation argument or a dictionary member.\n\nNullable type constant values in IDL are represented in the same way that constant values of their inner type would be represented, or with the token.\n\nThe type name of a nullable type is the concatenation of the type name of the inner type and the string \u201cOrNull\u201d.\n\nThe type is a parameterized type whose values are (possibly zero-length) lists of values of type .\n\nSequences are always passed by value. In language bindings where a sequence is represented by an object of some kind, passing a sequence to a platform object will not result in a reference to the sequence being kept by that object. Similarly, any sequence returned from a platform object will be a copy and modifications made to it will not be visible to the platform object.\n\nThe literal syntax for lists may also be used to represent sequences, when it is implicitly understood from context that the list is being treated as a sequences. However, there is no way to represent a constant sequence value inside IDL fragments.\n\nSequences must not be used as the type of an attribute or constant.\n\nNote: This restriction exists so that it is clear to specification writers and API users that sequences are copied rather than having references to them passed around. Instead of a writable attribute of a sequence type, it is suggested that a pair of operations to get and set the sequence is used.\n\nThe type name of a sequence type is the concatenation of the type name for and the string \u201cSequence\u201d.\n\nAny list can be implicitly treated as a , as long as it contains only items that are of type .\n\nA is a parameterized type whose values are ordered maps with keys that are instances of and values that are instances of . must be one of , , or .\n\nThe literal syntax for ordered maps may also be used to represent records, when it is implicitly understood from context that the map is being treated as a record. However, there is no way to represent a constant record value inside IDL fragments.\n\nRecords are always passed by value. In language bindings where a record is represented by an object of some kind, passing a record to a platform object will not result in a reference to the record being kept by that object. Similarly, any record returned from a platform object will be a copy and modifications made to it will not be visible to the platform object.\n\nRecords must not be used as the type of an attribute or constant.\n\nThe type name of a record type is the concatenation of the type name for , the type name for and the string \u201cRecord\u201d.\n\nAny ordered map can be implicitly treated as a , as long as it contains only entries whose keys are all of of type and whose values are all of type .\n\nA is a parameterized type whose values are references to objects that \u201cis used as a place holder for the eventual results of a deferred (and possibly asynchronous) computation result of an asynchronous operation\u201d. See section 25.4 of the ECMAScript specification for details on the semantics of promise objects.\n\nPromise types are non-nullable, but may be nullable.\n\nThere is no way to represent a promise value in IDL.\n\nThe type name of a promise type is the concatenation of the type name for and the string \u201cPromise\u201d.\n\nA is a type whose set of values is the union of those in two or more other types. Union types (matching ) are written as a series of types separated by the keyword with a set of surrounding parentheses. The types which comprise the union type are known as the union\u2019s .\n\nLike the type, values of union types have a specific type, which is the particular member type that matches the value.\n\nNote: For example, the flattened member types of the union type are the six types , , , , and .\n\nThe type must not be used as a union member type.\n\nThe number of nullable member types of a union type must be 0 or 1, and if it is 1 then the union type must also not have a dictionary type in its flattened member types.\n\nEach pair of flattened member types in a union type, and , must be distinguishable.\n\nUnion type constant values in IDL are represented in the same way that constant values of their member types would be represented.\n\nThe type name of a union type is formed by taking the type names of each member type, in order, and joining them with the string \u201cOr\u201d.\n\nAdditional types can be created from existing ones by specifying certain extended attributes on the existing types. Such types are called , and the types they annotate are called .\n\nThe following extended attributes are : [ ], [ ], and [ ].\n\nFor any type, the extended attributes associated with it must only contain extended attributes that are applicable to types.\n\nThe type name of a type associated with extended attributes is the concatenation of the type name of the original type with the set of strings corresponding to the identifiers of each extended attribute associated with the type, sorted in lexicographic order.\n\nThe type corresponds to the set of all possible non-null references to exception objects, including simple exceptions and objects.\n\nThere is no way to represent a constant value in IDL.\n\nThe type name of the type is \u201cError\u201d.\n\nThere are a number of types that correspond to sets of all possible non-null references to objects that represent a buffer of data or a view on to a buffer of data. The table below lists these types and the kind of buffer or view they represent.\n\nNote: These types all correspond to classes defined in ECMAScript.\n\nThere is no way to represent a constant value of any of these types in IDL.\n\nThe type name of all of these types is the name of the type itself.\n\nAt the specification prose level, IDL buffer source types are simply references to objects. To inspect or manipulate the bytes inside the buffer, specification prose must first either get a reference to the bytes held by the buffer source or get a copy of the bytes held by the buffer source . With a reference to the buffer source\u2019s bytes, specification prose can get or set individual byte values using that reference.\n\nAttempting to get a reference to or get a copy of the bytes held by a buffer source when the has been detached will fail in a language binding-specific manner.\n\nNote: See \u00a73.2.25 Buffer source types below for how interacting with buffer source types works in the ECMAScript language binding.\n\nWe should include an example of specification text that uses these types and terms.\n\nA is a parameterized type whose values are references to objects that hold a fixed length array of unmodifiable values. The values in the array are of type .\n\nSince FrozenArray< > values are references, they are unlike sequence types, which are lists of values that are passed by value.\n\nThere is no way to represent a constant frozen array value in IDL.\n\nThe type name of a frozen array type is the concatenation of the type name for and the string \u201cArray\u201d.\n\nAn is an annotation that can appear on definitions, types, interface members, namespace members, dictionary members, and operation arguments, and is used to control how language bindings will handle those constructs. Extended attributes are specified with an , which is a square bracket enclosed, comma separated list of s.\n\nThe grammar symbol matches nearly any sequence of tokens, however the extended attributes defined in this document only accept a more restricted syntax. Any extended attribute encountered in an IDL fragment is matched against the following five grammar symbols to determine which form (or forms) it is in:\n\nThis specification defines a number of extended attributes that are applicable to the ECMAScript language binding, which are described in \u00a73.3 ECMAScript-specific extended attributes. Each extended attribute definition will state which of the above five forms are allowed.\n\nThis section describes how definitions written with the IDL defined in \u00a72 Interface definition language correspond to particular constructs in ECMAScript, as defined by the [ECMA-262].\n\nObjects defined in this section have internal properties as described in ECMA-262 sections 9.1 and 9.3.1 unless otherwise specified, in which case one or more of the following are redefined in accordance with the rules for exotic objects: [[Call]], [[Set]], [[DefineOwnProperty]], [[GetOwnProperty]], and [[Delete]].\n\nOther specifications may override the definitions of any internal method of a platform object that is an instance of an interface.\n\nAs overriding internal ECMAScript object methods is a low level operation and can result in objects that behave differently from ordinary objects, this facility should not be used unless necessary for security or compatibility. The expectation is that this will be used for Location objects and possibly WindowProxy objects.\n\nUnless otherwise specified, the [[Extensible]] internal property of objects defined in this section has the value .\n\nUnless otherwise specified, the [[Prototype]] internal property of objects defined in this section is %ObjectPrototype%.\n\nSome objects described in this section are defined to have a , which is the string to include in the string returned from Object.prototype.toString. If an object has a class string, then the object must, at the time it is created, have a property whose name is the @@toStringTag symbol and whose value is the specified string.\n\nShould define whether @@toStringTag is writable, enumerable and configurable. All @@toStringTag properties in the ES6 spec are non-writable and non-enumerable, and configurable.\n\nIf an object is defined to be a built-in function object, then it has characteristics as described in ECMA-262 section 9.3.\n\nAlgorithms in this section use the conventions described in ECMA-262 section 5.2, such as the use of steps and substeps, the use of mathematical operations, and so on. The ToBoolean, ToNumber, ToUint16, ToInt32, ToUint32, ToString, ToObject, IsAccessorDescriptor and IsDataDescriptor abstract operations and the Type(x) notation referenced in this section are defined in ECMA-262 sections 6 and 7.\n\nWhen an algorithm says to a then this means to construct a new ECMAScript object in the current Realm and to throw it, just as the algorithms in ECMA-262 do.\n\nNote that algorithm steps can call in to other algorithms and abstract operations and not explicitly handle exceptions that are thrown from them. When an exception is thrown by an algorithm or abstract operation and it is not explicitly handled by the caller, then it is taken to end the algorithm and propagate out to its caller, and so on.\n\nIn an ECMAScript implementation of a given set of IDL fragments, there will exist a number of ECMAScript objects that correspond to definitions in those IDL fragments. These objects are termed the , and comprise the following:\n\nEach ECMAScript global environment must have its own unique set of each of the initial objects, created before control enters any ECMAScript execution context associated with the environment, but after the global object for that environment is created. The [[Prototype]]s of all initial objects in a given global environment must come from that same global environment.\n\nUnless otherwise specified, each ECMAScript global environment all interfaces that the implementation supports. If a given ECMAScript global environment does not expose an interface, then the requirements given in \u00a73.6 Interfaces are not followed for that interface.\n\nNote: This allows, for example, ECMAScript global environments for Web Workers to expose different sets of supported interfaces from those exposed in environments for Web pages.\n\nAlthough at the time of this writing the ECMAScript specification does not reflect this, every ECMAScript object must have an . The mechanisms for associating objects with Realms are, for now, underspecified. However, we note that in the case of platform objects, the associated Realm is equal to the object\u2019s relevant Realm, and for non-exotic function objects (i.e. not callable proxies, and not bound functions) the associated Realm is equal to the value of the function object's [[Realm]] internal slot.\n\nThis section describes how types in the IDL map to types in ECMAScript.\n\nEach sub-section below describes how values of a given IDL type are represented in ECMAScript. For each IDL type, it is described how ECMAScript values are converted to an IDL value when passed to a platform object expecting that type, and how IDL values of that type are when returned from a platform object.\n\nNote that the sub-sections and algorithms below also apply to annotated types created by applying extended attributes to the types named in their headers.\n\nSince the IDL type is the union of all other IDL types, it can correspond to any ECMAScript value type.\n\nAn IDL value is converted to an ECMAScript value as follows. If the value is an reference to a special object that represents an ECMAScript value, then it is converted to the ECMAScript value. Otherwise, the rules for converting the specific type of the IDL value as described in the remainder of this section are performed.\n\nThe only place that the type may appear in IDL is as the return type of an operation. Functions on platform objects that implement an operation whose IDL specifies a return type must return the value.\n\nECMAScript functions that implement an operation whose IDL specifies a return type may return any value, which will be discarded.\n\nThe IDL value is converted to the ECMAScript value and the IDL value is converted to the ECMAScript value.\n\nMathematical operations used in this section, including those defined in ECMA-262 section 5.2, are to be understood as computing exact mathematical results on mathematical real numbers.\n\nIn effect, where is a Number value, \u201coperating on \u201d is shorthand for \u201coperating on the mathematical real number that represents the same numeric value as \u201d.\n\nThe result of converting an IDL value to an ECMAScript value is a Number that represents the same numeric value as the IDL value. The Number value will be an integer in the range [\u2212128, 127].\n\nThe result of converting an IDL value to an ECMAScript value is a Number that represents the same numeric value as the IDL value. The Number value will be an integer in the range [0, 255].\n\nThe result of converting an IDL value to an ECMAScript value is a Number that represents the same numeric value as the IDL value. The Number value will be an integer in the range [\u221232768, 32767].\n\nThe result of converting an IDL value to an ECMAScript value is a Number that represents the same numeric value as the IDL value. The Number value will be an integer in the range [0, 65535].\n\nThe result of converting an IDL value to an ECMAScript value is a Number that represents the same numeric value as the IDL value. The Number value will be an integer in the range [\u22122147483648, 2147483647].\n\nThe result of converting an IDL value to an ECMAScript value is a Number that represents the same numeric value as the IDL value. The Number value will be an integer in the range [0, 4294967295].\n\nThe result of converting an IDL value to an ECMAScript value is a Number value that represents the closest numeric value to the , choosing the numeric value with an even significand if there are two equally close values. If the is in the range [\u2212253 + 1, 253 \u2212 1], then the Number will be able to represent exactly the same value as the .\n\nThe result of converting an IDL value to an ECMAScript value is a Number value that represents the closest numeric value to the , choosing the numeric value with an even significand if there are two equally close values. If the is less than or equal to 253 \u2212 1, then the Number will be able to represent exactly the same value as the .\n\nThe result of converting an IDL value to an ECMAScript value is the Number value that represents the same numeric value as the IDL value.\n\nNote: Since there is only a single ECMAScript value, it must be canonicalized to a particular single precision IEEE 754 NaN value. The NaN value mentioned above is chosen simply because it is the quiet NaN with the lowest value when its bit pattern is interpreted as an unsigned 32 bit integer.\n\nThe result of converting an IDL value to an ECMAScript value is the Number value that represents the same numeric value as the IDL value.\n\nNote: Since there is only a single ECMAScript value, it must be canonicalized to a particular double precision IEEE 754 NaN value. The NaN value mentioned above is chosen simply because it is the quiet NaN with the lowest value when its bit pattern is interpreted as an unsigned 64 bit integer.\n\nThe result of converting an IDL value to an ECMAScript value is the String value that represents the same sequence of code units that the IDL represents.\n\nThe result of converting an IDL value to an ECMAScript value is a String value whose length is the length of the , and the value of each element of which is the value of the corresponding element of the .\n\nThe result of converting an IDL value to an ECMAScript value is the Object value that represents a reference to the same object that the IDL represents.\n\nThe result of converting an IDL value to an ECMAScript value is the Symbol value that represents a reference to the same symbol that the IDL represents.\n\nThe result of converting an IDL interface type value to an ECMAScript value is the Object value that represents a reference to the same object that the IDL interface type value represents.\n\nIDL dictionary type values are represented by ECMAScript Object values. Properties on the object (or its prototype chain) correspond to dictionary members.\n\nNote: The order that dictionary members are looked up on the ECMAScript object are not necessarily the same as the object\u2019s property enumeration order.\n\nThe result of converting an IDL enumeration type value to an ECMAScript value is the String value that represents the same sequence of code units as the enumeration value.\n\nIDL callback function types are represented by ECMAScript function objects, except in the [ ] case, when they can be any object.\n\nThe result of converting an IDL callback function type value to an ECMAScript value is a reference to the same object that the IDL callback function type value represents.\n\nIDL nullable type values are represented by values of either the ECMAScript type corresponding to the inner IDL type, or the ECMAScript value.\n\nThe result of converting an IDL promise type value to an ECMAScript value is the value that represents a reference to the same object that the IDL promise type represents.\n\nInclude an example of how to write spec text using this term.\n\nIDL union type values are represented by ECMAScript values that correspond to the union\u2019s member types.\n\nAn IDL union type value is converted to an ECMAScript value as follows. If the value is an reference to a special object that represents an ECMAScript value, then it is converted to the ECMAScript value. Otherwise, the rules for converting the specific type of the IDL union type value as described in this section (\u00a73.2 ECMAScript type mapping).\n\nIDL values are represented by native ECMAScript objects and by platform objects.\n\nThe result of converting an IDL value to an ECMAScript value is the value that represents a reference to the same object that the IDL represents.\n\nThe result of converting an IDL value to an ECMAScript value is the Object value that represents a reference to the same object that the IDL represents.\n\nValues of the IDL buffer source types are represented by objects of the corresponding ECMAScript class, with the additional restriction that unless the type is associated with the [ ] extended attribute, they can only be backed by ECMAScript objects, and not objects.\n\nThe result of converting an IDL value of any buffer source type to an ECMAScript value is the Object value that represents a reference to the same object that the IDL value represents.\n\nValues of frozen array types are represented by frozen ECMAScript Array object references.\n\nThe result of converting an IDL FrozenArray< > value to an ECMAScript value is the Object value that represents a reference to the same object that the IDL FrozenArray< > represents.\n\nThis section defines a number of extended attributes whose presence affects only the ECMAScript binding.\n\nIf the [ ] extended attribute appears on one of the buffer source types, it creates a new IDL type that allows the buffer source type to be backed by an ECMAScript , instead of only by a non-shared .\n\nThe [ ] extended attribute must take no arguments.\n\nA type that is not a buffer source type must not be associated with the [ ] extended attribute.\n\nSee the rules for converting ECMAScript values to IDL buffer source types in \u00a73.2.25 Buffer source types for the specific requirements that the use of [ ] entails.\n\nIf the [ ] extended attribute appears on one of the integer types, it creates a new IDL type such that that when an ECMAScript Number is converted to the IDL type, out-of-range values will be clamped to the range of valid values, rather than using the operators that use a modulo operation (ToInt32, ToUint32, etc.).\n\nThe [ ] extended attribute must take no arguments.\n\nA type annotated with the [ ] extended attribute must not appear in a read only attribute. A type must not be associated with both the [ ] and [ ] extended attributes. A type that is not an integer type must not be associated with the [ ] extended attribute.\n\nSee the rules for converting ECMAScript values to the various IDL integer types in \u00a73.2.4 Integer types for the specific requirements that the use of [ ] entails.\n\nIf the [ ] extended attribute appears on an interface, it indicates that the interface object for this interface will have an [[Construct]] internal method, allowing objects implementing the interface to be constructed.\n\nMultiple [ ] extended attributes may appear on a given interface.\n\nThe [ ] extended attribute must either take no arguments or take an argument list. The bare form, , has the same meaning as using an empty argument list, . For each [ ] extended attribute on the interface, there will be a way to construct an object that implements the interface by passing the specified arguments.\n\nThe prose definition of a constructor must either return an IDL value of a type corresponding to the interface the [ ] extended attribute appears on, or throw an exception.\n\nThe [ ] and [ ] extended attributes must not be specified on the same interface.\n\nThe [ ] extended attribute must not be used on a callback interface.\n\nSee \u00a73.6.1 Interface object for details on how a constructor for an interface is to be implemented.\n\nIf the [ ] extended attribute appears on a regular operation, then it indicates that steps described in the corresponding default operation must be carried out when the operation is invoked.\n\nThe [ ] extended attribute must take no arguments.\n\nThe [ ] extended attribute must not be used on anything other than a regular operation for which a corresponding default operation has been defined.\n\nIf the [ ] extended attribute appears on one of the integer types, it creates a new IDL type such that that when an ECMAScript Number is converted to the IDL type, out-of-range values will cause an exception to be thrown, rather than being converted to a valid value using using the operators that use a modulo operation (ToInt32, ToUint32, etc.). The Number will be rounded toward zero before being checked against its range.\n\nThe [ ] extended attribute must take no arguments.\n\nA type annotated with the [ ] extended attribute must not appear in a read only attribute. A type must not be associated with both the [ ] and [ ] extended attributes. A type that is not an integer type must not be associated with the [ ] extended attribute.\n\nSee the rules for converting ECMAScript values to the various IDL integer types in \u00a73.2 ECMAScript type mapping for the specific requirements that the use of [ ] entails.\n\nIf the [ ] extended attribute appears on an interface, partial interface, namespace, partial namespace, or an individual interface member or namespace member, it indicates that the construct is exposed on a particular set of global interfaces, rather than the default of being exposed only on the primary global interface.\n\nThe [ ] extended attribute must either take an identifier or take an identifier list. Each of the identifiers mentioned must be a global name.\n\nEvery construct that the [ ] extended attribute can be specified on has an , which is a set of interfaces defining which global environments the construct can be used in. The exposure set for a given construct is defined as follows:\n\nIf [ ] appears on an overloaded operation, then it must appear identically on all overloads.\n\nThe [ ] extended attribute must not be specified on both an interface member and a partial interface definition the interface member is declared on. Similarly, the [ ] extended attribute must not be specified on both a namespace member and a partial namespace definition the namespace member is declared on.\n\nIf [ ] appears an interface member, then the interface member\u2019s exposure set must be a subset of the exposure set of the interface or partial interface it\u2019s a member of. Similarly, if [ ] appears on a namespace member, then the namespace member\u2019s exposure set must be a subset of the exposure set of the namespace or partial namespace it\u2019s a member of.\n\nAn interface\u2019s exposure set must be a subset of the exposure set of all of the interface\u2019s consequential interfaces.\n\nIf an interface inherits from another interface then the exposure set of must be a subset of the exposure set of .\n\nAn interface, namespace, interface member, or namespace member is in a given ECMAScript global environment if the ECMAScript global object implements an interface that is in the construct\u2019s exposure set, and either:\n\nNote: Since it is not possible for the relevant settings object for an ECMAScript global object to change whether it is a secure context or not over time, an implementation\u2019s decision to create properties for an interface or interface member can be made once, at the time the initial objects are created.\n\nSee \u00a73.6 Interfaces, \u00a73.6.6 Constants, \u00a73.6.7 Attributes, \u00a73.6.8 Operations and \u00a73.6.9 Common iterator behavior for the specific requirements that the use of [ ] entails.\n\nIf the [ ] or [ ] extended attribute appears on an interface, it indicates that objects implementing this interface can be used as the global object in an ECMAScript environment, and that the structure of the prototype chain and how properties corresponding to interface members will be reflected on the prototype objects will be different from other interfaces. Specifically:\n\nIf the [ ] or [ ] extended attributes is used on an interface, then:\n\nIf [ ] or [ ] is specified on a partial interface definition, then that partial interface definition must be the part of the interface definition that defines the named property getter.\n\nThe [ ] and [ ] extended attribute must not be used on an interface that can have more than one object implementing it in the same ECMAScript global environment.\n\nNote: This is because the named properties object, which exposes the named properties, is in the prototype chain, and it would not make sense for more than one object\u2019s named properties to be exposed on an object that all of those objects inherit from.\n\nIf an interface is declared with the [ ] or [ ] extended attribute, then there must not be more than one interface member across the interface and its consequential interfaces with the same identifier. There also must not be more than one stringifier or more than one iterable declaration, maplike declaration or setlike declaration across those interfaces.\n\nNote: This is because all of the members of the interface and its consequential interfaces get flattened down on to the object that implements the interface.\n\nThe [ ] and [ ] extended attributes can also be used to give a name to one or more global interfaces, which can then be referenced by the [ ] extended attribute.\n\nThe [ ] and [ ] extended attributes must either take no arguments or take an identifier list.\n\nIf the [ ] or [ ] extended attribute is declared with an identifier list argument, then those identifiers are the interface\u2019s ; otherwise, the interface has a single global name, which is the interface\u2019s identifier.\n\nNote: The identifier argument list exists so that more than one global interface can be addressed with a single name in an [ ] extended attribute.\n\nThe [ ] and [ ] extended attributes must not be declared on the same interface. The [ ] extended attribute must be declared on at most one interface. The interface [ ] is declared on, if any, is known as the .\n\nSee \u00a73.6.5 Named properties object for the specific requirements that the use of [ ] and [ ] entails for named properties, and \u00a73.6.6 Constants, \u00a73.6.7 Attributes and \u00a73.6.8 Operations for the requirements relating to the location of properties corresponding to interface members.\n\nIf the [ ] extended attribute appears on an interface that is not defined to inherit from another, it indicates that the internal [[Prototype]] property of its interface prototype object will be the intrinsic object %ArrayPrototype% rather than %ObjectPrototype%. This allows Array methods to be used more easily with objects implementing the interface.\n\nThe [ ] extended attribute must take no arguments. It must not be used on an interface that has any inherited interfaces.\n\nNote: Interfaces using [ ] will need to define a \u201clength\u201d attribute of type that exposes the length of the array-like object, in order for the inherited Array methods to operate correctly. Such interfaces would typically also support indexed properties, which would provide access to the array elements.\n\nSee \u00a73.6.3 Interface prototype object for the specific requirements that the use of [ ] entails.\n\nThe [ ] extended attribute is an undesirable feature. It exists only so that legacy Web platform features can be specified. It should not be used in specifications unless required to specify the behavior of legacy APIs, or for consistency with these APIs. Instead, interface names can be formed with a naming convention of starting with a particular prefix for a set of interfaces, as part of the identifier, rather than using a namespace. Editors who wish to use this feature are strongly advised to discuss this by filing an issue before proceeding.\n\nIf the [ ] extended attribute appears on an interface, it indicates that the interface object for this interface will not be created as a property of the global object, but rather as a property of the namespace identified by the argument to the extended attribute.\n\nThe [ ] extended attribute take an identifier. This identifier must be the identifier of a namespace.\n\nThe [ ] and [ ] extended attributes must not be specified on the same interface.\n\nThe [ ] extended attribute must not be used on a callback interface.\n\nSee \u00a73.12.1 Namespace object for details on how an interface is exposed on a namespace.\n\nIf the [ ] extended attribute appears on a interface that supports named properties, it indicates that all the interface\u2019s named properties are unenumerable.\n\nThe [ ] extended attribute must take no arguments and must not appear on an interface that does not define a named property getter.\n\nIf the [ ] extended attribute is specified on an interface, then it applies to all its derived interfaces and must not be specified on any of them.\n\nSee \u00a73.9.1 [[GetOwnProperty]] for the specific requirements that the use of [ ] entails.\n\nIf the [ ] extended attribute appears on an interface, it indicates that the primary global interface will have a property for each identifier mentioned in the extended attribute, whose value is the interface object for the interface.\n\nThe [ ] extended attribute must either take an identifier or take an identifier list. The s that occur after the \u201c \u201d are the [ ]'s .\n\nEach of the identifiers of [ ] must not be the same as one used by a [ ] extended attribute on this interface or another interface, must not be the same as the identifier used by a [ ] extended attribute on this interface or another interface, must not be the same as an identifier of an interface that has an interface object, and must not be one of the reserved identifiers.\n\nThe [ ] and [ ] extended attributes must not be specified on the same interface.\n\nThe [ ] extended attribute must not be specified on an interface that does not include the primary global interface in its exposure set.\n\nThe [ ] extended attribute must not be specified on a callback interface.\n\nAn interface must not have more than one [ ] extended attributes specified.\n\nSee \u00a73.6 Interfaces for details on how legacy window aliases are to be implemented.\n\nIf the [ ] extended attribute appears on a read only regular attribute, it indicates that a no-op setter will be generated for the attribute\u2019s accessor property. This results in erroneous assignments to the property in strict mode to be ignored rather than causing an exception to be thrown.\n\nThe [ ] extended attribute must take no arguments. It must not be used on anything other than a read only regular attribute.\n\nAn attribute with the [ ] extended attribute must not also be declared with the [ ] or [ ] extended attributes.\n\nThe [ ] extended attribute must not be used on an attribute declared on a namespace.\n\nSee the Attributes section for how [ ] is to be implemented.\n\nIf the [ ] extended attribute appears on a regular attribute, it indicates that invocations of the attribute\u2019s getter or setter with a value that is not an object that implements the interface on which the attribute appears will be ignored.\n\nThe [ ] extended attribute must take no arguments. It must not be used on a static attribute.\n\nThe [ ] extended attribute must not be used on an attribute declared on a namespace.\n\nSee the Attributes section for how [ ] is to be implemented.\n\nIf the [ ] extended attribute appears on an interface, it indicates that the ECMAScript global object will have a property with the specified name whose value is a constructor that can create objects that implement the interface. Multiple [ ] extended attributes may appear on a given interface.\n\nThe [ ] extended attribute must either take an identifier or take a named argument list. The that occurs directly after the \u201c \u201d is the [ ]'s . The first form, , has the same meaning as using an empty argument list, . For each [ ] extended attribute on the interface, there will be a way to construct an object that implements the interface by passing the specified arguments to the constructor that is the value of the aforementioned property.\n\nThe identifier used for the named constructor must not be the same as that used by a [ ] extended attribute on another interface, must not be the same as an identifier used by a [ ] extended attribute on this interface or another interface, must not be the same as an identifier of an interface that has an interface object, and must not be one of the reserved identifiers.\n\nThe [ ] extended attribute must not be used on a callback interface.\n\nSee \u00a73.6.2 Named constructors for details on how named constructors are to be implemented.\n\nIf the [ ] extended attribute appears on a regular or static operation, then it indicates that when calling the operation, a reference to a newly created object must always be returned.\n\nThe [ ] extended attribute must take no arguments.\n\nThe [ ] extended attribute must not be used on anything other than a regular or static operation whose return type is an interface type or a promise type.\n\nIf the [ ] extended attribute appears on an interface, it indicates that an interface object will not exist for the interface in the ECMAScript binding.\n\nThe [ ] extended attribute should not be used on interfaces that are not solely used as supplemental interfaces, unless there are clear Web compatibility reasons for doing so. Editors who wish to use this feature are strongly advised to discuss this by filing an issue before proceeding.\n\nThe [ ] extended attribute must take no arguments.\n\nIf the [ ] extended attribute is specified on an interface, then the [ ] extended attribute must not also be specified on that interface. A [ ] extended attribute is fine, however.\n\nThe [ ] extended attribute must not be specified on an interface that has any static operations defined on it.\n\nThe [ ] extended attribute must not be specified on a callback interface.\n\nAn interface that does not have the [ ] extended attribute specified must not inherit from an interface that has the [ ] extended attribute specified.\n\nSee \u00a73.6 Interfaces for the specific requirements that the use of [ ] entails.\n\nIf the [ ] extended attribute appears on an interface, it indicates that for a legacy platform object implementing the interface, properties corresponding to all of the object\u2019s supported property names will appear to be on the object, regardless of what other properties exist on the object or its prototype chain. This means that named properties will always shadow any properties that would otherwise appear on the object. This is in contrast to the usual behavior, which is for named properties to be exposed only if there is no property with the same name on the object itself or somewhere on its prototype chain.\n\nThe [ ] extended attribute must take no arguments and must not appear on an interface that does not define a named property getter or that also is declared with the [ ] or [ ] extended attribute. If the extended attribute is specified on a partial interface definition, then that partial interface definition must be the part of the interface definition that defines the named property getter.\n\nSee \u00a73.9 Legacy platform objects and \u00a73.9.3 [[DefineOwnProperty]] for the specific requirements that the use of [ ] entails.\n\nIf the [ ] extended attribute appears on a read only regular attribute declaration whose type is an interface type, it indicates that assigning to the attribute will have specific behavior. Namely, the assignment is \u201cforwarded\u201d to the attribute (specified by the extended attribute argument) on the object that is currently referenced by the attribute being assigned to.\n\nThe [ ] extended attribute must take an identifier. Assuming that:\n\nthen there must be another attribute declared on whose identifier is . Assignment of a value to the attribute on an object implementing will result in that value being assigned to attribute of the object that references, instead.\n\nNote that [ ]-annotated attributes can be chained. That is, an attribute with the [ ] extended attribute can refer to an attribute that itself has that extended attribute. There must not exist a cycle in a chain of forwarded assignments. A cycle exists if, when following the chain of forwarded assignments, a particular attribute on an interface is encountered more than once.\n\nAn attribute with the [ ] extended attribute must not also be declared with the [ ] or [ ] extended attributes.\n\nThe [ ] extended attribute must not be used on an attribute that is not read only.\n\nThe [ ] extended attribute must not be used on a static attribute.\n\nThe [ ] extended attribute must not be used on an attribute declared on a callback interface or namespace.\n\nSee the Attributes section for how [ ] is to be implemented.\n\nIf the [ ] extended attribute appears on a read only regular attribute, it indicates that setting the corresponding property on the platform object will result in an own property with the same name being created on the object which has the value being assigned. This property will shadow the accessor property corresponding to the attribute, which exists on the interface prototype object.\n\nThe [ ] extended attribute must take no arguments.\n\nAn attribute with the [ ] extended attribute must not also be declared with the [ ] or [ ] extended attributes.\n\nThe [ ] extended attribute must not be used on an attribute that is not read only.\n\nThe [ ] extended attribute must not be used on a static attribute.\n\nThe [ ] extended attribute must not be used on an attribute declared on a callback interface.\n\nSee \u00a73.6.7 Attributes for the specific requirements that the use of [ ] entails.\n\nIf the [ ] extended attribute appears on a read only attribute, then it indicates that when getting the value of the attribute on a given object, the same value must always be returned.\n\nThe [ ] extended attribute must take no arguments.\n\nThe [ ] extended attribute must not be used on anything other than a read only attribute whose type is an interface type or .\n\nIf the [ ] extended attribute appears on an interface, partial interface, namespace, partial namespace, interface member, or namespace member, it indicates that the construct is exposed only within a secure context.\n\nThe [ ] extended attribute must take no arguments.\n\nThe [ ] extended attribute must not be used on anything other than an interface, partial interface, namespace, partial namespace, interface member, or namespace member.\n\nWhether a construct that the [ ] extended attribute can be specified on is available only in secure contexts is defined as follows:\n\nNote: Whether a construct is available only in secure contexts influences whether it is exposed in a given ECMAScript global environment.\n\nIf [ ] appears on an overloaded operation, then it must appear on all overloads.\n\nThe [ ] extended attribute must not be specified on both an interface member and the interface or partial interface definition the interface member is declared on, or on both a namespace member and the namespace or partial namespace definition the namespace member is declared on.\n\nAn interface without the [ ] extended attribute must not inherit from another interface that does specify [ ].\n\nIf the [ ] extended attribute appears on a callback function, then it indicates that any value assigned to an attribute whose type is a nullable callback function that is not an object will be converted to the value.\n\nSee \u00a73.2.18 Nullable types \u2014 T? for the specific requirements that the use of [ ] entails.\n\nThe [ ] extended attribute is an undesirable feature. It exists only so that legacy Web platform features can be specified. It should not be used in specifications unless required to specify the behavior of legacy APIs, or for consistency with these APIs. Editors who wish to use this feature are strongly advised to discuss this by filing an issue before proceeding.\n\nIf the [ ] extended attribute appears on the type, it creates a new IDL type such that that when an ECMAScript is converted to the IDL type, it will be handled differently from its default handling. Instead of being stringified to , which is the default, it will be converted to the empty string.\n\nThe [ ] extended attribute must take the identifier .\n\nThe [ ] extended attribute must not be associated with a type that is not .\n\nNote: This means that even must not use [ ], since is a valid value of that type.\n\nSee \u00a73.2.9 DOMString for the specific requirements that the use of [ ] entails.\n\nIf the [ ] extended attribute appears on a non-static attribute or non-static operations, it indicates that the attribute or operation will be reflected as an ECMAScript property in a way that means its behavior cannot be modified and that performing a property lookup on the object will always result in the attribute\u2019s property value being returned. In particular, the property will be non-configurable and will exist as an own property on the object itself rather than on its prototype.\n\nAn attribute or operation is said to be on a given interface if the attribute or operation is declared on or one of \u2019s consequential interfaces, and is annotated with the [ ] extended attribute.\n\nThe [ ] extended attribute must take no arguments.\n\nThe [ ] extended attribute must not appear on anything other than an attribute or a non-static operation. If it does appear on an operation, then it must appear on all operations with the same identifier on that interface.\n\nThe [ ] extended attribute must not be used on an attribute declared on a namespace.\n\nIf an attribute or operation is unforgeable on an interface , and is one of the inherited interfaces of another interface , then and all of its consequential interfaces must not have a non-static attribute or regular operation with the same identifier as .\n\nSee \u00a73.6.7 Attributes, \u00a73.6.8 Operations, \u00a73.8 Platform objects implementing interfaces, \u00a73.9 Legacy platform objects and \u00a73.9.3 [[DefineOwnProperty]] for the specific requirements that the use of [ ] entails.\n\nIf the [ ] extended attribute appears on a regular attribute or regular operation, it indicates that an object that implements an interface with the given interface member will not include its property name in any object environment record with it as its base object. The result of this is that bare identifiers matching the property name will not resolve to the property in a statement. This is achieved by including the property name on the interface prototype object\u2019s @@unscopables property\u2019s value.\n\nThe [ ] extended attribute must take no arguments.\n\nThe [ ] extended attribute must not appear on anything other than a regular attribute or regular operation.\n\nThe [ ] extended attribute must not be used on an attribute declared on a namespace.\n\nSee \u00a73.6.3 Interface prototype object for the specific requirements that the use of [ ] entails.\n\nCertain algorithms in the sections below are defined to on a given object. This check is used to determine whether a given operation invocation or attribute access should be allowed. The security check takes the following three inputs:\n\nNote: The HTML Standard defines how a security check is performed. [HTML]\n\nFor every non-callback interface that is exposed in a given ECMAScript global environment and that is not declared with the [ ] or [ ] extended attributes, a corresponding property must exist on the ECMAScript environment\u2019s global object. The name of the property is the identifier of the interface, and its value is an object called the . The property has the attributes { [[Writable]]: , [[Enumerable]]: , [[Configurable]]: }. The characteristics of an interface object are described in \u00a73.6.1 Interface object.\n\nIf the [ ] extended attribute was specified on an exposed interface, then for each identifier in [ ]'s identifiers there must be a corresponding property on the primary global interface. The name of the property is the given identifier, and its value is a reference to the interface object for the interface. The property has the attributes { [[Writable]]: , [[Enumerable]]: , [[Configurable]]: }.\n\nIn addition, for every [ ] extended attribute on an exposed interface, a corresponding property must exist on the ECMAScript global object. The name of the property is the [ ]'s identifier, and its value is an object called a , which allows construction of objects that implement the interface. The property has the attributes { [[Writable]]: , [[Enumerable]]: , [[Configurable]]: }. The characteristics of a named constructor are described in \u00a73.6.2 Named constructors.\n\nThe interface object for a given interface is a built-in function object. It has properties that correspond to the constants and static operations defined on that interface, as described in sections \u00a73.6.6 Constants and \u00a73.6.8 Operations.\n\nIf the interface is declared with a [ ] extended attribute, then the interface object can be called as a constructor to create an object that implements that interface. Calling that interface as a function will throw an exception.\n\nInterface objects whose interfaces are not declared with a [ ] extended attribute will throw when called, both as a function and as a constructor.\n\nAn interface object for a non-callback interface has an associated object called the interface prototype object. This object has properties that correspond to the regular attributes and regular operations defined on the interface, and is described in more detail in \u00a73.6.3 Interface prototype object.\n\nNote: Since an interface object is a function object the operator will return \"function\" when applied to an interface object.\n\nA named constructor that exists due to one or more [ ] extended attributes with a given identifier is a built-in function object. It allows constructing objects that implement the interface on which the [ ] extended attributes appear.\n\nIf the actions listed in the description of the constructor return normally, then those steps must return an object that implements interface . This object\u2019s relevant Realm must be the same as that of the named constructor.\n\nThere must exist an for every non-callback interface defined, regardless of whether the interface was declared with the [ ] extended attribute. The interface prototype object for a particular interface has properties that correspond to the regular attributes and regular operations defined on that interface. These properties are described in more detail in sections \u00a73.6.7 Attributes and \u00a73.6.8 Operations.\n\nAs with the interface object, the interface prototype object also has properties that correspond to the constants defined on that interface, described in \u00a73.6.6 Constants.\n\nIf the [ ] extended attribute was not specified on the interface, then the interface prototype object must also have a property named \u201cconstructor\u201d with attributes { [[Writable]]: , [[Enumerable]]: , [[Configurable]]: } whose value is a reference to the interface object for the interface.\n\nIf the interface is declared with the [ ] or [ ] extended attribute, or the interface is in the set of inherited interfaces for any other interface that is declared with one of these attributes, then the interface prototype object must be an immutable prototype exotic object.\n\nThe class string of an interface prototype object is the concatenation of the interface\u2019s identifier and the string \u201cPrototype\u201d.\n\nFor every callback interface that is exposed in a given ECMAScript global environment and on which constants are defined, a corresponding property must exist on the ECMAScript environment\u2019s global object. The name of the property is the identifier of the interface, and its value is an object called the . The property has the attributes { [[Writable]]: , [[Enumerable]]: , [[Configurable]]: }.\n\nThe legacy callback interface object for a given callback interface is a built-in function object. It has properties that correspond to the constants defined on that interface, as described in sections \u00a73.6.6 Constants.\n\nNote: Since a legacy callback interface object is a function object the operator will return \"function\" when applied to a legacy callback interface object.\n\nFor every interface declared with the [ ] or [ ] extended attribute that supports named properties, there must exist an object known as the for that interface on which named properties are exposed.\n\nThe class string of a named properties object is the concatenation of the interface\u2019s identifier and the string \u201cProperties\u201d.\n\nWhen the [[SetPrototypeOf]] internal method of a named properties object is called, the same algorithm must be executed as is defined for the [[SetPrototypeOf]] internal method of an immutable prototype exotic object.\n\nFor each exposed constant defined on an interface , there must be a corresponding property. The property has the following characteristics:\n\nIn addition, a property with the same characteristics must exist on the interface object or the legacy callback interface object, if either of those objects exists.\n\nFor each exposed attribute of the interface, whether it was declared on the interface itself or one of its consequential interfaces, there must exist a corresponding property. The characteristics of this property are as follows:\n\nNote: Although there is only a single property for an IDL attribute, since accessor property getters and setters are passed a value for the object on which property corresponding to the IDL attribute is accessed, they are able to expose instance-specific data.\n\nNote: Attempting to assign to a property corresponding to a read only attribute results in different behavior depending on whether the script doing so is in strict mode. When in strict mode, such an assignment will result in a being thrown. When not in strict mode, the assignment attempt will be ignored.\n\nFor each unique identifier of an exposed operation defined on the interface, there must exist a corresponding property, unless the effective overload set for that identifier and operation and with an argument count of 0 has no entries.\n\nThe characteristics of this property are as follows:\n\nThe above description has some bugs, especially around partial interfaces. See issue #164.\n\nFor namespaces, the properties corresponding to each declared operation are described in \u00a73.12.1 Namespace object. (We hope to eventually move interfaces to the same explicit property-installation style as namespaces.)\n\nOnly regular operations which have a defined below may be declared with a [ ] extended attribute.\n\nThe corresponding default operation of the operation is the default toJSON operation.\n\nThe return type of the default toJSON operation must be .\n\nIf the interface has an exposed stringifier, then there must exist a property with the following characteristics:\n\nIf the interface has any of the following:\n\nthen a property must exist whose name is the @@iterator symbol, with attributes { [[Writable]]: , [[Enumerable]]: , [[Configurable]]: } and whose value is a function object.\n\nThe location of the property is determined as follows:\n\nIf the interface defines an indexed property getter, then the function object is %ArrayProto_values%.\n\nThe value of the @@iterator function object\u2019s \u201clength\u201d property is the Number value .\n\nThe value of the @@iterator function object\u2019s \u201cname\u201d property is the String value \u201centries\u201d if the interface has a pair iterator or a maplike declaration and the String \u201cvalues\u201d if the interface has a setlike declaration.\n\nIf the interface has any of the following:\n\nthen a property named \u201cforEach\u201d must exist with attributes { [[Writable]]: , [[Enumerable]]: , [[Configurable]]: } and whose value is a function object.\n\nThe location of the property is determined as follows:\n\nIf the interface defines an indexed property getter, then the function object is the initial value of the \u201cforEach\u201d data property of %ArrayPrototype%.\n\nThe value of the function object\u2019s \u201clength\u201d property is the Number value .\n\nThe value of the function object\u2019s \u201cname\u201d property is the String value \u201cforEach\u201d.\n\nIf the interface has an iterable declaration, then a property named \u201centries\u201d must exist with attributes { [[Writable]]: , [[Enumerable]]: , [[Configurable]]: } and whose value is a function object.\n\nThe location of the property is determined as follows:\n\nIf the interface has a value iterator, then the function object is the initial value of the \u201centries\u201d data property of %ArrayPrototype%.\n\nIf the interface has a pair iterator, then the function object is the value of the @@iterator property.\n\nIf the interface has an iterable declaration, then a property named \u201ckeys\u201d must exist with attributes { [[Writable]]: , [[Enumerable]]: , [[Configurable]]: } and whose value is a function object.\n\nThe location of the property is determined as follows:\n\nIf the interface has a value iterator, then the function object is the initial value of the \u201ckeys\u201d data property of %ArrayPrototype%.\n\nThe value of the function object\u2019s \u201clength\u201d property is the Number value .\n\nThe value of the function object\u2019s \u201cname\u201d property is the String value \u201ckeys\u201d.\n\nIf the interface has an iterable declaration, then a property named \u201cvalues\u201d must exist with attributes { [[Writable]]: , [[Enumerable]]: , [[Configurable]]: } and whose value is a function object.\n\nThe location of the property is determined as follows:\n\nIf the interface has a value iterator, then the function object is the value of the @@iterator property.\n\nThe value of the function object\u2019s \u201clength\u201d property is the Number value .\n\nThe value of the function object\u2019s \u201cname\u201d property is the String value \u201cvalues\u201d.\n\nA for a given interface, target and iteration kind is an object whose internal [[Prototype]] property is the iterator prototype object for the interface.\n\nNote: Default iterator objects are only used for pair iterators; value iterators, as they are currently restricted to iterating over an object\u2019s supported indexed properties, use standard ECMAScript Array iterator objects.\n\nWhen a default iterator object is first created, its index is set to 0.\n\nThe class string of a default iterator object for a given interface is the result of concatenting the identifier of the interface and the string \u201cIterator\u201d.\n\nThe for a given interface is an object that exists for every interface that has a pair iterator. It serves as the prototype for default iterator objects for the interface.\n\nThe internal [[Prototype]] property of an iterator prototype object must be %IteratorPrototype%.\n\nThe class string of an iterator prototype object for a given interface is the result of concatenting the identifier of the interface and the string \u201cIterator\u201d.\n\nAny object that implements an interface that has a maplike declaration must have a [[BackingMap]] internal slot, which is initially set to a newly created object. This object\u2019s [[MapData]] internal slot is the object\u2019s map entries.\n\nIf an interface is declared with a maplike declaration, then there exists a number of additional properties on \u2019s interface prototype object. These additional properties are described in the sub-sections below.\n\nThere must exist a property named \u201csize\u201d on \u2019s interface prototype object with the following characteristics:\n\nA property named \u201centries\u201d must exist on \u2019s interface prototype object with attributes { [[Writable]]: , [[Enumerable]]: , [[Configurable]]: } and whose value is the function object that is the value of the @@iterator property.\n\nFor both of \u201ckeys\u201d and \u201cvalues\u201d, there must exist a property with that name on \u2019s interface prototype object with the following characteristics:\n\nThe value of the function objects\u2019 \u201clength\u201d properties is the Number value .\n\nThe value of the function object\u2019s \u201cname\u201d property is the String value \u201ckeys\u201d or \u201cvalues\u201d, correspondingly.\n\nFor both of \u201cget\u201d and \u201chas\u201d, there must exist a property with that name on \u2019s interface prototype object with the following characteristics:\n\nThe value of the function object\u2019s \u201clength\u201d properties is the Number value .\n\nThe value of the function object\u2019s \u201cname\u201d property is the String value \u201cget\u201d or \u201chas\u201d, correspondingly.\n\nIf and \u2019s consequential interfaces do not declare an interface member with identifier \u201cclear\u201d, and was declared with a read\u2013write maplike declaration, then a property named \u201cclear\u201d and the following characteristics must exist on \u2019s interface prototype object:\n\nThe value of the function object\u2019s \u201clength\u201d property is the Number value .\n\nThe value of the function object\u2019s \u201cname\u201d property is the String value \u201cclear\u201d.\n\nIf and \u2019s consequential interfaces do not declare an interface member with identifier \u201cdelete\u201d, and was declared with a read\u2013write maplike declaration, then a property named \u201cdelete\u201d and the following characteristics must exist on \u2019s interface prototype object:\n\nThe value of the function object\u2019s \u201clength\u201d property is the Number value .\n\nThe value of the function object\u2019s \u201cname\u201d property is the String value \u201cdelete\u201d.\n\nIf and \u2019s consequential interfaces do not declare an interface member with identifier \u201cset\u201d, and was declared with a read\u2013write maplike declaration, then a property named \u201cset\u201d and the following characteristics must exist on \u2019s interface prototype object:\n\nThe value of the function object\u2019s \u201clength\u201d property is the Number value .\n\nThe value of the function object\u2019s \u201cname\u201d property is the String value \u201cset\u201d.\n\nAny object that implements an interface that has a setlike declaration must have a [[BackingSet]] internal slot, which is initially set to a newly created object. This object\u2019s [[SetData]] internal slot is the object\u2019s set entries.\n\nIf an interface is declared with a setlike declaration, then there exists a number of additional properties on \u2019s interface prototype object. These additional properties are described in the sub-sections below.\n\nThere must exist a property named \u201csize\u201d on \u2019s interface prototype object with the following characteristics:\n\nA property named \u201cvalues\u201d must exist on \u2019s interface prototype object with attributes { [[Writable]]: , [[Enumerable]]: , [[Configurable]]: } and whose value is the function object that is the value of the @@iterator property.\n\nFor both of \u201centries\u201d and \u201ckeys\u201d, there must exist a property with that name on \u2019s interface prototype object with the following characteristics:\n\nThe value of the function object\u2019s \u201clength\u201d properties is the Number value .\n\nThe value of the function object\u2019s \u201cname\u201d property is the String value \u201centries\u201d or \u201ckeys\u201d, correspondingly.\n\nThere must exist a property with named \u201chas\u201d on \u2019s interface prototype object with the following characteristics:\n\nThe value of the function object\u2019s \u201clength\u201d property is a Number value .\n\nThe value of the function object\u2019s \u201cname\u201d property is the String value \u201chas\u201d.\n\nFor both of \u201cadd\u201d and \u201cdelete\u201d, if:\n\nthen a property with that name and the following characteristics must exist on \u2019s interface prototype object:\n\nThe value of the function object\u2019s \u201clength\u201d property is the Number value .\n\nThe value of the function object\u2019s \u201cname\u201d property is the String value \u201cadd\u201d or \u201cdelete\u201d, correspondingly.\n\nIf and \u2019s consequential interfaces do not declare an interface member with a matching identifier, and was declared with a read\u2013write setlike declaration, then a property named \u201cclear\u201d and the following characteristics must exist on \u2019s interface prototype object:\n\nThe value of the function object\u2019s \u201clength\u201d property is the Number value .\n\nThe value of the function object\u2019s \u201cname\u201d property is the String value \u201cclear\u201d.\n\nThe interface prototype object of an interface must have a copy of each property that corresponds to one of the constants, attributes, operations, iterable declarations, maplike declarations and setlike declarations that exist on all of the interface prototype objects of \u2019s consequential interfaces. For operations, where the property is a data property with a function object value, each copy of the property must have distinct function objects. For attributes, each copy of the accessor property must have distinct function objects for their getters, and similarly with their setters.\n\nEvery platform object is associated with a global environment, just as the initial objects are. It is the responsibility of specifications using Web IDL to state which global environment (or, by proxy, which global object) each platform object is associated with.\n\nThe of a platform object that implements one or more interfaces is the most-derived non-supplemental interface that it implements. The value of the internal [[Prototype]] property of the platform object is the interface prototype object of the primary interface from the platform object\u2019s associated global environment.\n\nThe global environment that a given platform object is associated with can after it has been created. When the global environment associated with a platform object is changed, its internal [[Prototype]] property must be immediately updated to be the interface prototype object of the primary interface from the platform object\u2019s newly associated global environment.\n\nThe class string of a platform object that implements one or more interfaces must be the identifier of the primary interface of the platform object.\n\nThe class string of an interface with the [ ] extended attribute is the concatenation of the class string of the namespace, \".\", and the class string that the interface would otherwise have without this extended attribute.\n\nThe internal [[SetPrototypeOf]] method of every platform object that implements an interface with the [ ] or [ ] extended attribute must execute the same algorithm as is defined for the [[SetPrototypeOf]] internal method of an immutable prototype exotic object.\n\nNote: For objects, it is unobservable whether this is implemented, since the presence of the object ensures that [[SetPrototypeOf]] is never called on a object directly. For other global objects, however, this is necessary.\n\nLegacy platform objects will appear to have additional properties that correspond to their indexed and named properties. These properties are not \u201creal\u201d own properties on the object, but are made to look like they are by being exposed by the [[GetOwnProperty]] internal method .\n\nIt is permissible for an object to implement multiple interfaces that support indexed properties. However, if so, and there are conflicting definitions as to the object\u2019s supported property indices, or if one of the interfaces is a supplemental interface for the legacy platform object, then it is undefined what additional properties the object will appear to have, or what its exact behavior will be with regard to its indexed properties. The same applies for named properties.\n\nThe indexed property getter that is defined on the derived-most interface that the legacy platform object implements is the one that defines the behavior when indexing the object with an array index. Similarly for indexed property setters. This way, the definitions of these special operations from ancestor interfaces can be overridden.\n\nA property name is an on a given platform object if the object implements an interface that has an interface member with that identifier and that interface member is unforgeable on any of the interfaces that implements.\n\nSupport for getters is handled in \u00a73.9.1 [[GetOwnProperty]], and for setters in \u00a73.9.3 [[DefineOwnProperty]] and \u00a73.9.2 [[Set]].\n\nThis document does not define a complete property enumeration order for platform objects implementing interfaces (or for platform objects representing exceptions). However, it does for legacy platform objects by defining the [[OwnPropertyKeys]] internal method as follows.\n\nAs described in \u00a72.10 Objects implementing interfaces, callback interfaces can be implemented in script by an ECMAScript object. The following cases determine whether and how a given object is considered to be a user object implementing a callback interface:\n\nNote that ECMAScript objects need not have properties corresponding to constants on them to be considered as user objects implementing interfaces that happen to have constants declared on them.\n\nA is a list of values each of which is either an IDL value or the special value \u201cmissing\u201d, which represents a missing optional argument.\n\nAn ECMAScript callable object that is being used as a callback function value is called in a manner similar to how operations on user objects are called (as described in the previous section).\n\nSome callback functions are instead used as constructors. Such callback functions must not have a return type that is a promise type.\n\nFor every namespace that is exposed in a given ECMAScript global environment, a corresponding property must exist on the ECMAScript environment\u2019s global object. The name of the property is the identifier of the namespace, and its value is an object called the .\n\nThe property has the attributes { [[Writable]]: , [[Enumerable]]: , [[Configurable]]: }. The characteristics of a namespace object are described in \u00a73.12.1 Namespace object.\n\nIn the ECMAScript binding, the type has some additional requirements:\n\nSimple exceptions are represented by native ECMAScript objects of the corresponding type.\n\nA is represented by a platform object that implements the interface.\n\nUnless specified otherwise, whenever ECMAScript runtime semantics are invoked due to requirements in this document and end due to an exception being thrown, that exception must propagate to the caller, and if not caught there, to its caller, and so on.\n\nPer Document conventions, an algorithm specified in this document may intercept thrown exceptions, either by specifying the exact steps to take if , or by explicitly handling abrupt completions.\n\nThis section specifies some common definitions that all conforming implementations must support.\n\nThe typedef is used to represent objects that provide a view on to an .\n\nThe typedef is used to represent objects that are either themselves an or which provide a view on to an .\n\nThe type is an interface type defined by the following IDL fragment:\n\nNote: as discussed in \u00a73.13.1 DOMException custom bindings, the ECMAScript binding imposes additional requirements beyond the normal ones for interface types.\n\nEach object has an associated and , both JavaScript strings.\n\nThe constructor, when invoked, must run these steps:\n\nThe attribute\u2019s getter must return this object\u2019s name.\n\nThe attribute\u2019s getter must return this object\u2019s message.\n\nThe attribute\u2019s getter must return the legacy code indicated in the error names table for this object\u2019s name, or 0 if no such entry exists in the table.\n\nThe type is used for representing a number of milliseconds, either as an absolute time (relative to some epoch) or as a relative amount of time. Specifications that use this type will need to define how the number of milliseconds is to be interpreted.\n\nThe callback function type is used for representing function values with no restriction on what arguments are passed to it or what kind of value is returned from it.\n\nThe callback function type is used for representing function values that take no arguments and do not return any value.\n\nExtensions to language binding requirements can be specified using extended attributes that do not conflict with those defined in this document. Extensions for private, project-specific use should not be included in IDL fragments appearing in other specifications. It is recommended that extensions that are required for use in other specifications be coordinated with the group responsible for work on , which at the time of writing is the W3C Web Platform Working Group, for possible inclusion in a future version of this document.\n\nExtensions to any other aspect of the IDL language are strongly discouraged.\n\nIt is expected that other specifications that define Web platform interfaces using one or more IDL fragments will reference this specification. It is suggested that those specifications include a sentence such as the following, to indicate that the IDL is to be interpreted as described in this specification:\n\nIn addition, it is suggested that the conformance class for user agents in referencing specifications be linked to the conforming implementation class from this specification:\n\nThe editor would like to thank the following people for contributing to this specification: Glenn Adams, David Andersson, L. David Baron, Art Barstow, Nils Barth, Robin Berjon, David Bruant, Jan-Ivar Bruaroey, Marcos C\u00e1ceres, Giovanni Campagna, Domenic Denicola, Chris Dumez, Michael Dyck, Brendan Eich, Jo\u00e3o Eiras, Gorm Haug Eriksen, Sigbjorn Finne, David Flanagan, Aryeh Gregor, Dimitry Golubovsky, James Graham, Aryeh Gregor, Tiancheng \u201cTimothy\u201d Gu, Kartikaya Gupta, Marcin Hanclik, Jed Hartman, Stefan Haustein, Dominique Haza\u00ebl-Massieux, Ian Hickson, Bj\u00f6rn H\u00f6hrmann, Kyle Huey, Lachlan Hunt, Oliver Hunt, Jim Jewett, Wolfgang Keller, Anne van Kesteren, Olav Junker Kj\u00e6r, Magnus Kristiansen, Takeshi Kurosawa, Yves Lafon, Travis Leithead, Jim Ley, Kevin Lindsey, Jens Lindstr\u00f6m, Peter Linss, \u5442\u5eb7\u8c6a (Kang-Hao Lu), Kyle Machulis, Mark Miller, Ms2ger, Andrew Oakley, \u5ca1\u5742 \u53f2\u7d00 (Shiki Okasaka), Jason Orendorff, Olli Pettay, Simon Pieters, Andrei Popescu, Fran\u00e7ois Remy, Tim Renouf, Alex Russell, Takashi Sakamoto, Doug Schepers, Jonas Sicking, Garrett Smith, Geoffrey Sneddon, Jungkee Song, Josh Soref, Maciej Stachowiak, Anton Tayanovskyy, Peter Van der Beken, Jeff Walden, Allen Wirfs-Brock, Jeffrey Yasskin and, Collin Xu.\n\nSpecial thanks also go to Sam Weinig for maintaining this document while the editor was unavailable to do so.\n\nThis section defines an LL(1) grammar whose start symbol, , matches an entire IDL fragment.\n\nEach production in the grammar has on its right hand side either a non-zero sequence of terminal and non-terminal symbols, or an epsilon (\u03b5) which indicates no symbols. Symbols that begin with an uppercase letter are non-terminal symbols. Symbols in monospaced fonts are terminal symbols. Symbols in sans-serif font that begin with a lowercase letter are terminal symbols that are matched by the regular expressions (using Perl 5 regular expression syntax [PERLRE]) as follows:\n\nThe tokenizer operates on a sequence of Unicode characters [UNICODE]. When tokenizing, the longest possible match must be used. For example, if the input text is \u201ca1\u201d, it is tokenized as a single , and not as a separate and . If the longest possible match could match one of the above named terminal symbols or one of the other terminal symbols from the grammar, it must be tokenized as the latter. Thus, the input text \u201clong\u201d is tokenized as the quoted terminal symbol rather than an called \u201clong\u201d, and \u201c.\u201d is tokenized as the quoted terminal symbol rather than an .\n\nThe IDL syntax is case sensitive, both for the quoted terminal symbols used in the grammar and the values used for terminals. Thus, for example, the input text \u201cConst\u201d is tokenized as an rather than the terminal symbol , an interface with identifier \u201cA\u201d is distinct from one named \u201ca\u201d, and an extended attribute [ ] will not be recognized as the [ ] extended attribute.\n\nImplicitly, any number of and terminals are allowed between every other terminal in the input text being parsed. Such and terminals are ignored while parsing.\n\nThe following LL(1) grammar, starting with , matches an IDL fragment:\n\nNote: The non-terminal matches any single terminal symbol except for , , , , , and .\n\nWhile the non-terminal matches any non-empty sequence of terminal symbols (as long as any parentheses, square brackets or braces are balanced, and the token appears only within those balanced brackets), only a subset of those possible sequences are used by the extended attributes defined in this specification \u2014 see \u00a72.12 Extended attributes for the syntaxes that are used by these extended attributes.\n\nThe following typographic conventions are used in this document:\n\nThe following conventions are used in the algorithms in this document:\n\nEverything in this specification is normative except for diagrams, examples, notes and sections marked as being informative.\n\nThe keywords \u201cmust\u201d, \u201cmust not\u201d, \u201crequired\u201d, \u201cshall\u201d, \u201cshall not\u201d, \u201cshould\u201d, \u201cshould not\u201d, \u201crecommended\u201d, \u201cmay\u201d and \u201coptional\u201d in this document are to be interpreted as described in Key words for use in RFCs to Indicate Requirement Levels [RFC2119].\n\nRequirements phrased in the imperative as part of algorithms (such as \u201cstrip any leading space characters\u201d or \u201creturn false and abort these steps\u201d) are to be interpreted with the meaning of the key word (\u201cmust\u201d, \u201cshould\u201d, \u201cmay\u201d, etc) used in introducing the algorithm.\n\nConformance requirements phrased as algorithms or specific steps can be implemented in any manner, so long as the end result is . In particular, the algorithms defined in this specification are intended to be easy to understand and are not intended to be performant. Implementers are encouraged to optimize.\n\nThe following conformance classes are defined by this specification:", "sentiment": 0.07607011920622765},
{"link_title": "The Incredible Growth of Python", "url": "https://stackoverflow.blog/2017/09/06/incredible-growth-python/?cb=1&imm_mid=0f5f86&cmp=em-prog-na-na-newsltr_20170909", "text": "We recently explored how wealthy countries (those defined as high-income by the World Bank) tend to visit a different set of technologies than the rest of the world. Among the largest differences we saw was in the programming language Python. When we focus on high-income countries, the growth of Python is even larger than it might appear from tools like Stack Overflow Trends, or in other rankings that consider global software development.\n\nIn this post, we\u2019ll explore the extraordinary growth of the Python programming language in the last five years, as seen by Stack Overflow traffic within high-income countries. The term \u201cfastest-growing\u201d can be hard to define precisely, but we make the case that Python has a solid claim to being the fastest-growing major programming language.\n\nAll the numbers discussed in this post are for high-income countries; they\u2019re generally representative of trends in the United States, United Kingdom, Germany, Canada, and other such countries, which in combination make up about 64% of Stack Overflow\u2019s traffic. Many other countries such as India, Brazil, Russia, and China also make enormous contributions to the global software development ecosystem, and this post is less descriptive of those economies, though we\u2019ll see that Python has shown growth there as well.\n\nIt\u2019s worth emphasizing up front that the number of users of a language isn\u2019t a measure of the language\u2019s quality: we\u2019re describing the languages developers use, but not prescribing anything. (Full disclosure: I used to program primarily in Python, though I have since switched entirely to R).\n\nYou can see on Stack Overflow Trends that Python has been growing rapidly in the last few years. But for this post we\u2019ll focus on high-income countries, and consider visits to questions rather than questions asked (this tends to give similar results, but has less month-by-month noise, especially for smaller tags).\n\nWe have data on Stack Overflow question views going back to late 2011, and in this time period we can consider the growth of Python relative to five other major programming languages. (Note that this is therefore a shorter time scale than the Trends tool, which goes back to 2008). These are currently six of the ten most-visited Stack Overflow tags in high-income countries; the four we didn\u2019t include are CSS, HTML, Android, and JQuery.\n\nJune 2017 was the first month that Python was the most visited tag on Stack Overflow within high-income nations. This included being the most visited tag within the US and the UK, and in the top 2 in almost all other high income nations (next to either Java or JavaScript). This is especially impressive because in 2012, it was less visited than any of the other 5 languages, and has grown by 2.5-fold in that time.\n\nPart of this is because of the seasonal nature of traffic to Java. Since it\u2019s heavily taught in undergraduate courses, Java traffic tends to rise during the fall and spring and drop during the summer. Will it catch up with Python again by the end of the year? We can try forecasting the next two years of growth with a model called \u201cSTL\u201d, which combines growth with seasonal trends to make a prediction about future values.\n\nAccording to this model, Python could either stay in the lead or be overtaken by Java in the fall (it\u2019s roughly within the variation of the model\u2019s predictions), but it\u2019s clearly on track to become the most visited tag in 2018. STL also suggests that JavaScript and Java will remain at similar levels of traffic among high income countries, just as they have for the last two years.\n\nThe above was looking only at the six most-visited programming languages. Among other notable technologies, which are currently growing the fastest in high-income countries?\n\nWe defined the growth rate in terms of the ratio between 2017 and 2016 share of traffic. We decided to consider only programming languages (like Java and Python) and platforms (such as iOS, Android, Windows and Linux) in this analysis, as opposed to frameworks like Angular or libraries like TensorFlow (although many of those showed notable growth that may be examined in a future post).\n\nBecause of the challenges in defining \u201cfastest-growing\u201d described in this comic, we compare the growth to the overall average in a mean-difference plot.\n\nWith a 27% year-over year-growth rate, Python stands alone as a tag that is both large and growing rapidly; the next-largest tag that shows similar growth is R. We see that traffic to most other large tags has stayed pretty steady within high-income countries, with visits to Android, iOS, and PHP decreasing slightly. We previously examined some of the shrinking tags like Objective-C, Perl and Ruby in our post on the death of Flash). We can also notice that among functional programming languages, Scala is the largest and growing, while F# and Clojure are smaller and shrinking, with Haskell in between and remaining steady.\n\nThere\u2019s an important omission from the above chart: traffic to TypeScript questions grew by an impressive 142% in the last year, enough that we left it off to avoid overwhelming the rest of the scale. You can also see that some other smaller languages are growing similarly or faster than Python (like R, Go and Rust), and there are a number of tags like Swift and Scala that are also showing impressive growth. How does their traffic over time compare to Python\u2019s?\n\nThe growth of languages like R and Swift is indeed impressive, and TypeScript has shown especially rapid expansion in an even shorter time. Many of these smaller languages grew from getting almost no question traffic to become notable presences in the software ecosystem. But as this graph shows, it\u2019s easier to show rapid growth when a tag started relatively small.\n\nNote that we\u2019re not saying these languages are in any way \u201ccompeting\u201d with Python. Rather, we\u2019re explaining why we\u2019d treat their growth in a separate category; these were lower-traffic tags to start with. Python is an unusual case for being both one of the most visited tags on Stack Overflow and one of the fastest-growing ones. (Incidentally, it is also accelerating! Its year-over-year growth has become faster each year since 2013).\n\nSo far in this post we\u2019ve been analyzing the trends in high-income countries. Does Python show a similar growth in the rest of the world, in countries like India, Brazil, Russia and China?\n\nIndeed it does.\n\nOutside of high-income countries Python is still the fastest growing major programming language; it simply started at a lower level and the growth began two years later (in 2014 rather than 2012). In fact, the year-over-year growth rate of Python in non-high-income countries is slightly higher than it is in high-income countries. We don\u2019t examine it here, but R, the other language whose usage is positively correlated with GDP, is growing in these countries as well.\n\nMany of the conclusions in this post about the growth and decline of tags (as opposed to the absolute rankings) in high-income countries hold true for the rest of the world; there\u2019s a 0.979 Spearman correlation between the growth rates in the two segments. In some cases, you can see a \u201clagging\u201d phenomenon similar to what happened with Python, where a technology was widely adopted within high-income countries a year or two before it expanded in the rest of the world. (This is an interesting phenomenon and may be the subject of a future blog post!)\n\nWe\u2019re not looking to contribute to any \u201clanguage war.\u201d The number of users of a language doesn\u2019t imply anything about its quality, and certainly can\u2019t tell you which language is more appropriate for a particular situation. With that perspective in mind, however, we believe it\u2019s worth understanding what languages make up the developer ecosystem, and how that ecosystem might be changing.\n\nThis post demonstrated that Python has shown a surprising growth in the last five years, especially within high-income countries. In our next post, we\u2019ll start to explore the \u201cwhy\u201d. We\u2019ll segment the growth by country and by industry, and examine what other technologies tend to be used alongside Python (to estimate, for example, how much of the growth has been due to increased usage of Python for web development versus for data science).\n\nIn the meantime, if you work in Python and are looking to take the next step in your career, here are some companies hiring Python developers right now on Stack Overflow Jobs.", "sentiment": 0.12783723274181288},
{"link_title": "Jeff Bezos Mandates Programming Shift at Amazon Studios", "url": "http://variety.com/2017/tv/news/amazon-studios-jeff-bezos-roy-price-zelda-1202552532/", "text": "The mandate from Jeff Bezos is clear: Bring me \u201cGame of Thrones.\u201d\n\nThat\u2019s the word that has the creative community buzzing this week about a major strategy shift underway for Amazon Studios\u2019 original series efforts.\n\nThe CEO of the e-commerce giant is said to have tasked Amazon Studios chief Roy Price with honing the focus on high-end drama series with global appeal. Amazon\u2019s decision this week to scrap plans for a second season of period drama \u201cZ: The Beginning of Everything\u201d reflects the new marching orders.\n\nOn Friday, Amazon confirmed five new projects \u2014 series greenlights for a period drama from Paul Attanasio and Wong Kar-wai and a comedy starring Fred Armisen and Maya Rudolph; two comedy pilots; and a Seth Rogen-produced comic book adaptation eyed as a straight-to-series order \u2014 that reflect the drive to find shows that deliver sizzle in the water-cooler environs of social media and can travel around the world.\n\nIn an interview on Friday, Price told Variety that there is a new focus on finding \u201cbig shows that can make the biggest difference around the world\u201d in growing Amazon Video\u2019s reach and Amazon Prime subscribers. \u201cTong Wars,\u201d the drama penned by Paul Attanasio and directed by Wong, is a prime example of a period piece that blends the epic history of Chinese immigration to the U.S. with a crime potboiler. \u201cIt\u2019s a very compelling show,\u201d he said.\n\nPrice said the strategic course has been informed by the wealth of data available to Amazon and is the consensus of senior management, including Bezos.\n\n\u201cIt comes out of analysis of the data and conversations among the leadership team,\u201d Price said. \u201cWe\u2019ve been looking at the data for some time, and as a team we\u2019re increasingly focused on the impact of the biggest shows. It\u2019s pretty evident that it takes big shows to move the needle.\u201d\n\nPrice cited Amazon\u2019s \u201cMan in the High Castle,\u201d the unscripted \u201cGrand Tour,\u201d and the new comedy \u201cThe Tick\u201d as examples of existing shows that fit the bill of having global appeal. And he doesn\u2019t mince words about his interest in finding a show that packs the wallop of HBO\u2019s \u201cGame of Thrones.\u201d\n\n\u201cI do think \u2018Game of Thrones\u2019 is to TV as \u2018Jaws\u2019 and \u2018Star Wars\u2019 was to the movies of the 1970s,\u201d Price said. \u201cIt\u2019ll inspire a lot of people. Everybody wants a big hit and certainly that\u2019s the show of the moment in terms of being a model for a hit.\u201d\n\nPrice pointed to the move Amazon made in January to recruit former Fox International Channels exec Sharon Tal Yguado to lead a new event series development unit focused specifically on sci-fi, fantasy and genre series. Price pointed to AMC\u2019s \u201cPreacher\u201d and Starz\u2019s \u201cAmerican Gods,\u201d shows that Amazon carries in multiple markets outside the U.S.\n\n\u201cThe biggest shows make the biggest difference around the world,\u201d Price said. \u201cIf you have one of the top five or 10 shows in the marketplace, it means your show is more valuable because it drives conversations and it drive subscriptions. \u2026 We\u2019re a mass-market brand. We have a lot of video customers and we need shows that move the needle at a high level.\u201d\n\nWith this focus, Amazon could not justify moving ahead with season 2 of \u201cZ.\u201d Industry sources said Karl Gajdusek, the showrunner recruited to steer season 2 of \u201cZ,\u201d was plainly told of the shift in strategy when the surprise call came down on Thursday that the show was being shuttered. Gajdusek and his team of writers had been working for several weeks on getting the 10-episode order ready for production. \u201cZ\u201d starred Christina Ricci as Zelda Fitzgerald, the socialite wife of writer F. Scott Fitzgerald and a legendary figure from 1920s Jazz Age lore.\n\nPrice said the decision on \u201cZ\u201d came down to a simple matter of priorities. He notes that Amazon has an ongoing development pact on the film side with Killer Films, one of the show\u2019s producers.\n\n\u201cWe\u2019re glad we did \u2018Z.\u2019 We\u2019re proud of the work done on it and the team we had on it,\u201d Price said. \u201cAt the end of the day you only have so many slots. With those slots you have to drive viewership and drive subscriptions. Sometimes there are shows that are a little bit on the bubble in terms of their viewership. We went down the road with it but ultimately decided in light of the full spectrum of opportunities we were looking at we would not be able to proceed with the show.\u201d\n\nAmazon is also expected to cut a significant number of current development prospects off of its plate. The service already has several big-ticket series orders in the works for 2018, including the two-season order for Amy Sherman-Palladino\u2019s \u201cThe Marvelous Mrs. Maisel,\u201d the John Krasinski-led adaptation of Tom Clancy\u2019s \u201cJack Ryan\u201d from Carlton Cuse, Matthew Weiner\u2019s \u201cThe Romanoffs\u201d anthology series, and David O. Russell\u2019s untitled crime drama starring Robert De Niro and Julianne Moore.\n\nMultiple industry sources who work with Amazon say it is clear there is pressure on Price and his team to deliver. There has been speculation about the prospect of major management changes at Amazon Studios given the number of industry insiders who have complained about what they see has a difficult working environment at the streaming giant.\n\n\u201cIt\u2019s not a good sign when Seattle overrules your decision,\u201d said one prominent producer of Amazon\u2019s reversal on \u201cZ.\u201d\n\nThe overhaul of priorities comes amid what sources said is some frustration with the fruits of its foray into original TV content during the past few years. Amazon Studios made an early splash with comedy \u201cTransparent\u201d in 2014, which helped propel the national conversation about transgender issues and has collected high-profile Emmy wins for star Jeffrey Tambor and creator Jill Soloway.\n\nBut Amazon hasn\u2019t had much traction in pop culture with many other original series, even after comedy \u201cMozart in the Jungle\u201d was an underdog winner for comedy series at the 2016 Golden Globe Awards. For all of Amazon\u2019s investment in original series, it\u2019s been eclipsed this season by its smaller rival Hulu with the critically praised \u201cThe Handmaid\u2019s Tale.\u201d\n\nThere\u2019s been speculation about Amazon reining in its development expenditures \u2014 something that Price flatly denies. Amazon\u2019s aggregate spending on original content will be up in 2018 versus this year, he said, although he would not cite specific dollar figures. He also noted that Amazon is shelling out big bucks this season for a marquee sports franchise, \u201cThursday Night Football.\u201d\n\n\u201cWe\u2019re very interested in getting those top shows \u2014 something that is broadly popular and admired,\u201d he said. \u201cWe want to allocate a lot of our attention and resources going forward to that kind of thing.\u201d\n\nThere have already been signals of Amazon\u2019s heightened focus on event and spectacle series. Tal Yguado has been given ample resources to go after big-name talent. In August, she secured an overall deal with \u201cThe Walking Dead\u201d creator Robert Kirkman, luring him away from his longtime home AMC. At Fox, Tal Yguado made the savvy decision to help finance and license \u201cThe Walking Dead\u201d for the more than 200 Fox-branded international channels. She also worked with Kirkman in developing \u201cOutcast,\u201d which airs across the Fox international channels group and on Cinemax in the U.S. She is said to be targeting other \u201cWalking Dead\u201d talent to make the jump to Amazon.\n\nTal Yguado came to the streaming service three months after the development team under Price had been reorganized, with comedy head Joe Lewis taking oversight of half-hour and drama series development. The move has caused some confusion among TV literary agents, who see no clear lines between Lewis\u2019 team and Tal Yguado\u2019s event focus.\n\nAmazon faced another black eye in the creative community this week when reports of strife behind the scenes on another drama series, \u201cGoliath,\u201d emerged along with the news of the show\u2019s third showrunner in two seasons. Clyde Phillips, who took over from creator David E. Kelley for season two, departed the show of his own volition after creative conflicts with star Billy Bob Thornton.\n\n\u201cGoliath\u201d was in production in Los Angeles on its episode five of the 10-episode order at the point when Phillips left last month, according to sources. Lawrence Trilling, a producer on the first season of \u201cGoliath,\u201d has taken over.\n\nPrice said he spoke with Thornton on Thursday and was feeling \u201cvery hopeful\u201d about the future of the show. He also asserted that Amazon has not had a higher incidence of behind-the-scenes changes on shows than other networks doing comparable volume.\n\n\u201cThe reality is it can be a complicated task to create a show. and sometimes it goes smoothly and other times it does not,\u201d Price said.\n\nAs for the big-picture of Amazon\u2019s programming focus, Price said there are more deals to be unveiled in the coming weeks that will make the company\u2019s priorities very clear to the creative community. \u201cThere are a lot more big, exciting announcements to come, and you\u2019ll see where it\u2019s all going,\u201d he said.", "sentiment": 0.13596224340175955},
{"link_title": "Evasi0n \u2013 an untethered jailbreak for all iPhone, iPod touch, iPad and iPad mini", "url": "https://github.com/OpenJailbreak/evasi0n6", "text": "", "sentiment": 0.0},
{"link_title": "Show HN: Watch YouTube from Menubar with MenuTube for MacOs", "url": "https://edanchenkov.github.io/MenuTube/", "text": "Do you enjoy listening to YouTube\u2019s podcasts, audiobooks, interviews or anything else that doesn\u2019t require to focus on video? If yes, then MenuTube is for you! Put entire full functional YouTube website into your macOs\u2019s menu bar.\n\nWhat you get:", "sentiment": 0.2833333333333333},
{"link_title": "Log Book with Computer Bug (Sep 9, 1947)", "url": "http://americanhistory.si.edu/collections/search/object/nmah_334663", "text": "Enter the terms you wish to search for.", "sentiment": 0.0},
{"link_title": "Ansible Open Sources Ansible Tower with AWX", "url": "https://www.jeffgeerling.com/blog/2017/ansible-open-sources-ansible-tower-awx", "text": "Ever since Red Hat acquired Ansible, I and many others have anticipated whether or when Ansible Tower would be open sourced. Ansible Tower is one of the nicest automation tools I've used... but since I haven't been on a project with the budget to support the Tower licensing fees, I have only used it for testing small-scale projects.\n\nI wrote a guide for Automating your Automation with Ansible Tower, and it's both on the web and in Chapter 11 of Ansible for DevOps, and in the guide, I wrote:\n\nThere are a lot of companies (mine included!) using Jenkins as a substitute for Ansible Tower, and this can work pretty well, but Jenkins doesn't integrate deeply with all Ansible's powerful inventory management, secret management, and playbook management. Tower also supports a lot more flexible authentication and role-based playbook permissions model which makes it a perfect fit for team-based playbook management.\n\nTo be clear though, Ansible Tower itself will still be a licensed product offering from Red Hat, but the code that builds Ansible Tower releases is open sourced, and is available in the AWX Project. According to the AWX Project FAQ, the best way to think of this open source model is in the analogy Fedora is to Red Hat Enterprise Linux as AWX is to Ansible Tower:\n\nAWX is designed to be a frequently released, fast-moving project where all new development happens. Ansible Tower is produced by taking selected releases of AWX, hardening them for long-term supportability, and making them available to customers as the Ansible Tower offering. This is a tested and trusted method of software development for Red Hat, which follows a similar model to Fedora and Red Hat Enterprise Linux.\n\nI'm excited to see the code behind Tower has finally been open sourced via AWX, and I hope to start using it for a few services like Hosted Apache Solr shortly. I'll be updating my book's chapter on Ansible Tower and AWX as soon as I'm able\u2014and if you buy the book on LeanPub, you'll get that updated content for free, as soon as I finish writing it!\n\nI'm currently working on adding an AWX example to my popular Ansible Vagrant Examples GitHub repository. Read through the AWX example README file for instructions in getting everything set up, and read the AWX example addition issue for further development of the example (I'm working to make it run in more environments, more easily!).\n\nAfter you install it, you'll be greeted by this angry potato:\n\nLog in with the credentials and .\n\nOriginally, Ansible Tower was called \"AWX\" \u2014 see this old blog post from 2013. And apparently that was kind of a short-hand for 'AnsibleWorks', the original name of the company that became Ansible, that became Ansible by Red Hat. Straight from the horse's mouth:\n\nRight now, Ansible for DevOps is 25% off in celebration of AnsibleFest, so pick up a copy today and supercharge your automation with Ansible!", "sentiment": 0.18826463391680778},
{"link_title": "Equifax Breach a Reminder That Cybersecurity Whistleblowers Have Protections", "url": "https://www.natlawreview.com/article/equifax-breach-reminder-cybersecurity-whistleblowers-have-protections", "text": "The latest mega breach is a big one. This week Equifax announced that it had suffered a breach of data belonging to as many as 143 million Americans. That\u2019s about half the country. Worse, the breached data was sensitive: names, social security numbers, birth dates, addresses, and some driver\u2019s license numbers. Even in a world where mega breaches are commonplace, this one is staggering in both scope and severity. The total impact is impossible to foresee, but it so far it has been swift and harsh for the company.\n\nEquifax stock tumbled 13% today, though there has been a modest rebound in afterhours trading, as of writing. Multiple state and federal agencies are initiating investigations. News broke that three Equifax executives sold stock after the company discovered the breach in June, but before Equifax announced the stock. The company responded that the executives had no knowledge of the breach at the time of the transactions, but the timing could not be much worse. (And right after the SEC put consideration of an insider trading rule on the backburner despite some uncertainty arising in the courts.) To add to the problem of perception \u2013 there\u2019s no indication that the sales were prescheduled under a 10b5-1 plan.\n\nHowever, I want to focus on a particular drop in the bucket: the fact that at least one class action lawsuit has already been filed alleging that Equifax was negligent in its information security. A similar case arising from the Yahoo mega breach recently passed a big hurdle.\n\nThis development is significant to me because it relates to a much smaller potential class of victims: innocent Equifax employees, who did the right thing. As the Enron scandal roiled in 2001, many rank-and-file employees who had nothing to do with the fraud lost not only their jobs, but also their life savings as Enron\u2019s stock value evaporated. Enron whistleblower Sherron Watkins suffered retaliation for trying to bring the misconduct to light.\n\nThankfully, corporate whistleblowers have much more robust protections today than they did in 2001. I have written and talked previously about how cybersecurity whistleblowers can often enjoy those same protections, for example here and here. The developing Equifax story provides an opportunity to present a hypothetical opportunity to demonstrate that legal theory.\n\nSuppose a large, publicly-traded corporation was arguably negligent with its cybersecurity controls. Perhaps the budget was anemic, or the company did not have adequate safeguards and procedures to protect customer data, or company executives violated information security protocols. Perhaps the company delayed in reporting a breach that significantly affected its business, or did not report the breach at all.\n\nWould an employee who reported the deficient cybersecurity have any protections under the law? Though the answer must depend on the specific facts of any given matter, the answer would often be yes, even though there is no specific federal law protecting cybersecurity whistleblowers. For example, I often analyze cybersecurity whistleblower claims under the anti-retaliation provision of the Sarbanes-Oxley Act. Though it\u2019s a gross overgeneralization, suffice to say that the Sarbanes-Oxley Act protects corporate whistleblowers. However, as I have explained in a blog post, cybersecurity issues often involve securities law issues. This type of hypothetical provides ample opportunity to draw those connections. Failing to disclose material deficiencies in a firm\u2019s information security could violate a public corporation\u2019s duty to disclose known risks, especially if cybersecurity is the public corporation\u2019s business. The company may have had a duty to file an 8K, or the company may have misrepresented its information security efforts in its public filings.\n\nBut what about an employee who has information about the misconduct but did not come forward before the cybersecurity issue was disclosed? Even then, the whistleblower laws can be of assistance. Disclosing information to the SEC that significantly contributes to an existing investigation can entitle the whistleblower to an award if certain criteria are met.\n\nIn summary, the Equifax breach reminds me that when a corporate scandal erupts, innocent employees sometimes suffer the same or worse as other victims, while also frequently being lumped in with the alleged wrongdoers. Thankfully, whistleblower laws exist to help make that tough road a little less bumpy.", "sentiment": 0.04270282186948855},
{"link_title": "Tesla Unlocks Battey Packs to Aid Evacuation", "url": "https://www.reddit.com/r/teslamotors/comments/6z2fwd/did_tesla_just_upgrade_my_60d_due_to_the_hurricane/", "text": "Did Tesla just upgrade my 60D due to the hurricane?\n\nChina to Ban Sale of Fossil Fuel Cars in Electric Vehicle Push\n\nPrediction - Tesla Model 3 will become the new Prius taxicab?\n\nAnyone Evacuate Florida with their Teslas? Bonus photo of Supercharger in Davie, FL from day before Irma!\n\nNew model S75 has over 300 miles range?", "sentiment": 0.0606060606060606},
{"link_title": "How Social Security numbers became our insecure national ID (2012)", "url": "https://www.theverge.com/2012/9/26/3384416/social-security-numbers-national-ID-identity-theft-nstic", "text": "Last week, the White House announced $9 million in funding for five pilot projects as part of its National Strategy for Trusted Identities in Cyberspace initiative, a federal effort to establish a secure, universal online identity ecosystem led by the private sector.\n\nCritics say any kind of top-down identification system would be a security risk and an encroachment on civil rights. But the fact is that the United States already has a universal ID: the unique nine-digit number issued to US citizens and residents by the Social Security Administration, which has turned out to be no less than a gift to identity thieves. While Social Security numbers work pretty well for tracking Social Security, they weren't designed to be secure.\n\nAmericans are reluctant to institute a national ID. But in the absence of one, the market adopted a poor substitute \u2014 and the millions of Social Security numbers for sale online for cheaper than a cup of coffee is one of the consequences of that disastrous indecision.\n\nSocial Security numbers were poorly understood from the beginning. In 1938, a leather factory in Lockport, New York attempted to capitalize on the excitement around the country\u2019s newly-formed social insurance program by tucking duplicate Social Security cards into its wallets. Company vice president and treasurer Douglas Patterson thought it would be cute to use the actual Social Security number of his secretary, Hilda Schrader Whitcher.\n\nReal Social Security cards had just begun circulating the year before, so many Americans were confused. Even though the display card was marked \"specimen\" and sold at Woolworth\u2019s, more than 40,000 people adopted Hilda\u2019s number as their own. According to the Social Security Administration, no fewer than 12 people were still using their Woolworth\u2019s-issued SSN in 1977.\n\nThe SSN was originally intended for one purpose: tracking a worker\u2019s lifetime earnings\n\nThe SSN was originally intended for one purpose: tracking a worker\u2019s lifetime earnings in order to calculate retirement benefits after age 65. But birth certificates are issued state-by-state and not everyone has a passport, making the SSN the closest thing we have to a national ID number. As a result, the SSN is now widely used within the government and private sector \u2014 and most people are no less confused about it than they were 75 years ago.\n\nOnly a very narrow set of government agencies and financial organizations are required to ask for a customer\u2019s SSN by law. That list does not include landlords, cable companies, cell phone providers, or even credit reporting agencies, which all habitually request SSNs simply because a number is more precise than a name. Americans have repeatedly rejected the idea of instituting a formal national identification system when it was proposed for use in health care or to prevent illegal immigration. As a result, the SSN has become the national ID by default.\n\nUnfortunately, SSNs are also appealing to identity thieves, who can use the numbers to open new bank accounts and credit cards. An SSN is also typically the first piece in building an identity profile that can be used for more elaborate crimes like insurance fraud. In addition to being unique and widely available, the vast majority of SSNs were assigned according to a publicly-available formula. Because it was never intended to be used for identification \u2014 the SSA added the disclaimer \"FOR SOCIAL SECURITY PURPOSES NOT FOR IDENTIFICATION\" to the card in 1946, then removed it in 1972 during a redesign \u2014 this was never anticipated as being a problem.\n\nIn 2009, researchers developed an algorithm that could guess an individual\u2019s SSN with up to ten percent accuracy depending on the size of the population in the state it was issued. Combined with phishing attacks that trick people into giving up their last four digits, malevolent hackers have become pretty adept at cracking any individual\u2019s number. Algorithms aren\u2019t necessary anymore, though. SSNs have become available through data resellers, security breaches at various companies and government agencies, unsuspecting customer service representatives, and even public records, if you know where to look. SSNs can be bought in bulk for $1 each on private online forums, and a specific person\u2019s SSN can reportedly be had for as little as $3.80.\n\nA screenshot of an underground forum where identity thieves do business. Source: RSA\n\nSocial Security numbers are the most common starting point for identity thieves, said Angel Grant, a senior manager at the information security firm RSA, which monitors the black markets where identity thieves traffic. In the last five years, SSNs have become so easy to obtain that thieves now usually bundle the number with extra identifying information like birth dates and even medical records in order to get the price up. \"A Social Security number is good, and it\u2019s very easy for a fraudster to obtain,\" she said. \"Social Security numbers are a commodity in the underground right now.\"\n\n\"Social Security numbers are a commodity in the underground right now.\"\n\nThe pitfalls of using SSNs for identification have been known for years, but there has been a recent push by state and local governments to make the number more secure. Last year, the SSA dropped the formula it had been using to assign new numbers in favor of a randomized method. The Social Security Protection Act of 2010 prohibits government agencies at all levels from displaying the number on checks. At least 20 states have enacted laws restricting the use of SSNs. Even private companies are starting to roll back their reliance on the SSN due to security concerns, Grant said.\n\nIn August, the state of New York passed a law that will prohibit most types of companies from refusing service to customers without an SSN, and subjects violators to a $500 fine. Jeffrey Dinowitz, the New York assemblyman who sponsored the law, said he gets complaints from his Bronx constituents who are constantly asked to give up their SSNs. Tellingly, the complaints come mostly from older people who remember a time before the SSN became a US resident\u2019s de facto ID.\n\nThe bill was introduced a few years ago, Dinowitz said, but for some reason it failed to gain traction with other state lawmakers until now. \"I think the dissemination of Social Security numbers should be done sparingly,\" he said. \"There are so many ways to take advantage of or steal people\u2019s identity, but the most important number is the Social Security number. I don\u2019t give it out.\"\n\nAfter year of warnings from groups like the Privacy Rights Clearinghouse and the Electronic Privacy Information Center, the public may be waking up to the dangers of systemic reliance on the SSN. High profile data leaks at companies like Sony and LinkedIn in the last year have increased general awareness of the underground market in personal information and the danger of losing control of one\u2019s data. The SSN is the ultimate password because it\u2019s always the same, whether you\u2019re talking to your bank or your doctor, and so many companies use it to authenticate customers over the phone.\n\nIt\u2019s unclear when private companies first had the bright idea of using SSNs to identify their customers. The evolution of laws that required the use of SSNs for various government purposes like food stamps is well-documented, and the government does require that SSNs be collected by certain financial institutions. In fact, the Social Security Administration is one of the most nostalgic agencies in government and maintains extensive historical records. It has its own (modest) history museum, its own historian\u2019s office, and a history of the historian\u2019s office on its website along with a thorough archive of historical documents.\n\nBut when asked about the proliferation of the SSN in the private sector, the historians were stumped. \"Over the decades, the uses of the number have expanded greatly beyond that original idea for it and it\u2019s become, for many private purposes, an identification thing,\" said Jane Zanca, a spokesperson for the agency. \"There really isn\u2019t a timeline or anything. It seems it's pretty early on that the issue must have come up.\"\n\nSimilarly, the Government Accountability Office issued a report in 2004 that acknowledged that the private sector entities \"routinely\" obtain and use SSNs, but does not say how the practice started.\n\nThe use of the number by the three credit reporting agencies, which are privately owned, would have encouraged the practice by other companies because having the number makes it easy to run a credit check on a new customer.\n\nBut Chris Hibbert, who published a widely-disseminated SSN explainer on Usenet and maintained it for 20 years, says the credit bureaus weren\u2019t even early adopters. \"They didn't take off until SSNs had been in use for quite a while,\" he said in an email. \"There has long been a confusion over whether the SSN is a secret (and so would be useful for verifying someone's identity) or a public identifier (and thus should be treated as well known).\" He hypothesized that local utilities may have been the first to use SSNs, or that it might have spontaneously occurred to companies throughout the private sector.\n\nSome privacy rights diehards prefer to give out Richard Nixon\u2019s SSN as their own\n\nThe SSN is still awkwardly short of being as public an identifier as a name, but perhaps it should be treated as such. For now, most consumers who try to withhold their SSN from private companies are in for an uphill battle. A resistant customer usually has to talk to a manager and may be asked to plunk down a deposit in exchange, and it\u2019s legal in most states for companies to refuse to do business without the number. Some privacy rights diehards prefer to give a fake SSN or one belonging to a dead person (Richard Nixon's is a favorite). This often works but is technically illegal.\n\n\"It's the only thing that's really unique,\" said Paul Stephens, director of policy and advocacy at the Privacy Rights Clearinghouse. But given the popularity of SSNs with legitimate service providers and identity thieves alike, it's become increasingly tough to protect. \"We tell people, 'if a private business wants your Social Security number... you should just say no.\"\n\nStill, the vast majority of customers don\u2019t realize that in many situations, including at the doctor\u2019s office, a SSN is not legally required. Until the problem of public perception is corrected, SSNs will continue to function as the national ID system that Americans on both sides of the political spectrum have said they do not want.\n\nThe National Strategy for Trusted Identities in Cyberspace is not really a push for a national ID, either. Each of the pilot projects has a specific focus, including healthcare, ecommerce, and senior citizens, with the hope that the government\u2019s investment will lead to multiple systems. \"The government will not require that you get a trusted ID,\" says NSTIC\u2019s website. \"If you want to get one, you will be able to choose among multiple identity providers \u2014 both private and public \u2014 and among multiple digital credentials. Such a marketplace will ensure that no single credential or centralized database can emerge.\"\n\nThe National Strategy for Trusted Identities in Cyberspace is not really a push for a national ID\n\nThe projects will test solutions that rely on mobile phones, biometrics, encryption, and other cutting-edge security technology that lets consumers browse anonymously but also validate their identities when needed. If one or more of the privately-developed online identity systems commissioned by the Obama administration proves effective, it may end up creeping into daily use the same way the SSN did \u2014 that is, through common practice instead of federal mandate. Unlike the SSN system, these systems are being designed for use as an identifier. If implemented, one of these systems could easily substitute for many less secure verification methods in place today, including typing in your mother\u2019s maiden name or reciting the last four digits of your SSN.\n\nMaking any identification system secure yet usable and universal yet private is a challenge, to say the least, and must be handled delicately in order to avoid comparisons to 1984. But even if the idea of a government-funded online identity system creeps you out, the existence of such alternatives would mean the private sector could give SSNs a rest.", "sentiment": 0.09519880094992202},
{"link_title": "Secret Government of Canada data stored on U.S. servers? Memo raises possibility", "url": "http://www.cbc.ca/news/politics/storage-data-cloud-government-canadian-shared-services-microsoft-secret-1.4277836", "text": "Two government agencies have been meeting with Microsoft Inc. to discuss ways to store secret Canadian data on American servers, a measure expressly forbidden by federal policy.\n\nMicrosoft's talks this year with Shared Services Canada and the Communications Security Establishment reviewed whether sensitive data about Canadians and other confidential matters could be securely encrypted on American \"cloud\" services.\n\nA May 2017 memo to the chief operating officer of Shared Services Canada (SSC) says the discussions examined in part how to protect Canada's sovereignty by insulating the data from legal demands under the USA Patriot Act, which forces firms to turn over confidential information to American law enforcement if demanded.\n\n\"This memorandum is to provide you with an update on the feasibility of Microsoft \u2013 or any other cloud vendor \u2013 to hold Government of Canada encrypted data in such a manner that Shared Services Canada holds and owns the decryption keys and is able to access the data while the vendor is not able to access to the data,\" says the memo.\n\nA copy of the heavily censored document was obtained by CBC News under the Access to Information Act.\n\nA spokesperson for Shared Services Canada, Monika Mazur, did not respond directly when asked whether the IT agency was still considering foreign cloud services for sensitive government data, but referred to a federal document that forbids it.\n\nOttawa's IT Strategic Plan 2016-2020 forbids storing secret data outside Canada's borders: \"To ensure Canada's sovereign control over its data, departments and agencies will adopt the policy that all sensitive or protected data under government control will be stored on servers that reside in Canada.\"\n\nSome low-risk Government of Canada data already reside on American and other non-Canadian \"cloud\" servers, including data for web pages with the Canada.ca suffix, which provide only general information. Amazon Web Services in the United States, for example, hosts such Canadian pages.\n\nThe previously undisclosed discussions with Microsoft are likely driven by a highly critical, $1.35-million report earlier this year on the repeated failures of Shared Services Canada since its creation in 2011 as Ottawa's IT department.\n\nThe Jan. 12 report by international experts, assembled by consultants Gartner Inc., said the struggling agency needs to find more cloud-based solutions: \"There is universal agreement from the Expert Panel that the progression of cloud and its continuing trajectory make this approach a vital component of a going-forward strategy for SSC.\"\n\nOutsourcing data storage and processing to commercial cloud services eliminates the need for costly hardware or software, and can be expanded and contracted as needed. The Gartner report, among other things, advised using a cloud service for Ottawa's badly delayed transition to a single email service across government.\n\nOttawa has been reviewing cloud options since April 2014, when then-treasury board president Tony Clement announced the Conservative government was launching expert consultations to look for savings by switching to cloud storage and processing.\n\nConsultations and reviews have been underway more or less continuously since then, including an endorsement of the approach in 2016 from Scott Brison, the Treasury Board president under the Liberals, who said cloud computing would \"get better value for taxpayers' dollars.\"\n\nThe latest round of cloud consultations ended last Sept. 30, and Treasury Board spokesperson Alain Belle-Isle said there's still no word on an updated cloud strategy.\n\nThe Gartner report in January cautioned against delays, calling on the government \"to fast-track the development of cloud capabilities for the GC [Government of Canada]. The Expert Panel and Gartner believe this should be given the highest priority \u2014 before examining managed service providers or other vendor supplied infrastructure service providers.\"\n\nThe May memo on discussions with Microsoft referred to several data-encryption options, including putting encrypted data on a U.S. cloud server but keeping the decoding key within Canada.\n\nHowever, the document outlines several daunting problems, including the cost, of such a \"hold your own key\" system, an option few Microsoft clients use.\n\nReferring in part to the USA Patriot Act, the memo says Microsoft \"always informs clients about any legal requests for access to information prior to releasing information. In some cases, Microsoft has sent the request through the client country's judicial system for the appropriate legal response.\"\n\n\"According to Microsoft, the company has never released the data of one country to a foreign government, including the United States.\"\n\nBut the memo also concludes with a warning that no data-storage cloud system is impervious to the USA Patriot Act or other legal challenges to Canada's data sovereignty.\n\nThe document by Raj Thuppal, of the Cyber and IT Security unit of Shared Services Canada, says \" \u2026 no mechanism is able to entirely prevent foreign access to data should legal requests be invoked.\"", "sentiment": 0.03557767369242779},
{"link_title": "XML? Be cautious", "url": "https://blog.pragmatists.com/xml-be-cautious-69a981fdc56a", "text": "I assume that everyone in their professional career has heard about XML. To be honest, it\u2019s so widely used that it\u2019s hard to miss. Simple structure, a little too verbose, yet easy to understand. You might think it is so easy and well-known, that there are no secrets and certainly no vulnerabilities! I will try to put you right in this post.\n\nI\u2019m pretty sure you already know that if you want to use special characters that cannot be typed into an XML document (<, &) you need to use the entity reference (< &). But did you know that you can define your own internal entity? It\u2019s pretty easy. Take a look:\n\nAs you can see, all we need to do is to write the doctype declaration and define the custom entity in it. After that, we can use our new internal entity in an XML document. As a result, the parser will replace every &company; occurrence with \u2018Pragmatists\u2019 text. Unfortunately, this feature puts us in danger.\n\nFirstly, let\u2019s take a look at the very simple, yet dangerous denial of service (DoS) attack called the Billion laughs attack.\n\nIn order to carry out the attack, you need to prepare malicious XML using internal entities described in the previous paragraph, and use it as an input.\n\nWhen an XML parser loads this document, it will try to resolve the lol9 entity. At first, lol9 expands to ten lol8 entities, each lol8 expands to ten lol7 entities and so on. As a result, we get 1 billion \u201clol\u201d strings. This creates a heavy burden on our machine, which can bury application responsiveness! If it doesn\u2019t sound scary to you, imagine that on my computer memory consumption increased up to 4GB in one minute.\n\nIn order to simulate that attack, I wrote a simple Java app. During tests, however, I encountered a Java error.\n\nIt seems that the default Java XML parser (at least in Java 8) is immune to the Billion Laughs Attack. Nevertheless, I was a little surprised because I\u2019d tried that code earlier on another computer and was able to succeed.\n\nI started to dig around and discovered that the method I was using to get DocumentBuilder can return different instances depending on its lookup procedure.\n\nWhen I added maven dependency with some old XML parser, DocumentBuilderFactory.newInstance() returned a different implementation, and I was able to get the expected result. It\u2019s important to be aware that even if you are safe at present, someone in the future can add simple dependency to your project, causing a security breach.\n\nHere you can find code used to create the Document from String:\n\nThere is another variation of this attack called quadratic blowup. This causes quadratic growth in storage requirements (Billion laughs takes an exponential amount of space).\n\nThe only difference is that instead of using nested entities, you can define one very large entity and repeat it over and over again.\n\nIn paragraph 2, we saw an entity that was defined in a document, but there is another way to use entities. We can import them from another file.\n\nIt looks simple but it is even more tricky than internal entities, and can lead to XXE attacks. To understand it better, imagine that we have two endpoints: one for listing all posts and one for creating new posts. With this in mind, we can prepare malicious XML input and use endpoint to create a new post. The input data could look like this:\n\nAfter that, we can use another endpoint to list all posts. If all goes well, the response will look like this:\n\nAs you can see, this is really dangerous and can lead to serious security problems!\n\nImagine that we use a different XML structure to create a post. In this example, we will use the element\u2019s attributes to describe author, topic, content instead of nested elements (as in previous examples).\n\nUnfortunately, there is one constraint when you use external entities: you cannot use them in XML attributes, but that doesn\u2019t mean there is no way to attack our application. If we are stubborn enough, we can try to workaround that problem with parameter entities.\n\nParameter entities are defined using % signs, and can only be used in DOCTYPE declarations. Moreover, they can be external.\n\nIf we want to attack our sample application, we need to try a little harder. Firstly, we need to create an external file with parameter entity definitions.\n\nSecondly, we need to refer to that file and use parameterEntityDefiningEntity in order to define entityWithResult.\n\nFinally, we can use entities in attributes, create new posts, list all of them and enjoy the outcome.\n\nYou might think you are safe if you don\u2019t provide an endpoint that can return data (like listing posts in the previous example) but that\u2019s not true. Instead of providing the file with external entity definition, we can prepare an endpoint that listens to every request and logs results.\n\nThe previous example had a small simplification. Referring to http://yourserver/log?%result; will work only if the result is on one line, otherwise we will get an error because the URL won\u2019t be correct. In fact, previous XXE examples had small constraints as well. Every file that we read and append as a result needs to be a grammatically valid in XML context and cannot contain \\x00 bytes.\n\nNevertheless, sometimes we can overcome these problems. In PHP, for example, we could use a filter to encode the result, or even execute the code remotely.\n\nAs always, everything depends on what technology you are using. Securing your application can be as easy as configuring the library or setting a couple of properties. Sadly, most Java XML parsers have XXE enabled by default.\n\nOn the OWASP site, you can read more about ways to prevent XXE attacks.", "sentiment": 0.07106315968675522},
{"link_title": "Ben Johnson, creator of BoltDB, thread on OSS burnout and businesses", "url": "https://twitter.com/benbjohnson/status/906208697738698753", "text": "", "sentiment": 0.0},
{"link_title": "Chinese government is working on a timetable to end sales of fossil-fuel cars", "url": "https://www.bloomberg.com/news/articles/2017-09-09/china-to-ban-sale-of-fossil-fuel-cars-in-electric-vehicle-push", "text": "China will set a deadline for automakers to end sales of fossil-fuel powered vehicles, a move aimed at pushing companies to speed efforts in developing electric vehicles for the world\u2019s biggest auto market.\n\nXin Guobin, the vice minister of industry and information technology, said the government is working with other regulators on a timetable to end production and sales. The move will have a profound impact on the environment and growth of China\u2019s auto industry, Xin said at an auto forum in Tianjin on Saturday.\n\nA ban on combustion-engine vehicles will help push both local and global automakers to shift toward electric vehicles, a carrot-and-stick approach that could boost sales of energy-efficient cars and trucks and reduce air pollution while serving the strategic goal of cutting oil imports. The government offers generous subsidies to makers of new-energy vehicles. It also plans to require automakers to earn enough credits or buy them from competitors with a surplus under a new cap-and-trade program for fuel economy and emissions.\n\nHonda Motor Co. will launch an electric car for the China market in 2018, China Chief Operating Officer Yasuhide Mizuno said at the same forum. The Japanese carmaker is developing the vehicle with Chinese joint ventures of Guangqi Honda Automobile Co. and Dongfeng Honda Automobile Co. and will create a new brand with them, he said.\n\nInternet entrepreneur William Li\u2019s Nio will start selling ES8, a sport-utility vehicle powered only with batteries, in mid-December. The startup is working with state-owned Anhui Jianghuai Automobile Group, which also is in a venture with Volkswagen AG to introduce an electric SUV next year.\n\nChina, seeking to meet its promise to cap its carbon emissions by 2030, is the latest country to unveil plans to phase out vehicles running on fossil fuels. The U.K. said in July it will ban sales of diesel- and gasoline-fueled cars by 2040, two weeks after France announced a similar plan to reduce air pollution and meet targets to keep global warming below 2 degrees Celsius (3.6 degrees Fahrenheit).", "sentiment": 0.01359180035650624},
{"link_title": "Robots Still Haven\u2019t Taken Over: A brief history of machine anxiety", "url": "http://lithub.com/its-been-100-years-and-the-robots-still-havent-taken-over/", "text": "The 20th-century symbol of ambivalence toward technology was the robot, invented in its modern form by the Czech playwright Karel \u010capek. The word \u201crobot\u201d comes from the Czech robota, meaning \u201cforced labor,\u201d a term that derives in turn from rab meaning \u201cslave.\u201d Initially this referred to programmed machines that released factory workers from repetitive toil but simultaneously threatened their livelihood, a comment on Henry Ford\u2019s moving assembly line, introduced in 1913 for the mass production of automobiles.\n\nSubsequently reinvented in increasingly humanlike forms as androids, with autonomous thought and intelligence, robots combined the lure of increased power and freedom for their owners with the threat that, like Frankenstein\u2019s creature and countless earlier mechanical creations in fiction, they would rebel against the controls imposed on them and dominate or destroy their masters. As an extension of this filmmakers have evolved a recurrent scenario of the development of a master race of highly intelligent robots, programmed or autonomously motivated to take over and destroy the human race. The Mechanical Man, The Terminator, Runaway, RoboCop, the Replicators in Stargate, the Cylons in Battlestar Galactica, the Cybermen and Daleks in Doctor Who, The Matrix, Enthiran, and I, Robot all play on, and exacerbate, such fears.\n\nAs presented in fiction and film, robots are essentially hybrids, combining the apparent intelligence of humans with the functionality and programmed obedience of machines. They epitomize efficiency, rationality, absence of emotion, objectivity, and the unswerving pursuit of a predetermined end regardless of the consequences. They became a powerful metaphor for technological culture itself, \u201cthe paradoxical status of the human body within the technological framework of modern society\u201d and the fear of technology invading and subsuming human identity. As such, robots and, later, androids have been used in fiction and film to indicate and explore the values and attitudes of their scientist-creators. In most cases they signify their designers\u2019 hubris, their obsession with controlling their world absolutely, and their cultivation of efficiency as an ultimate goal. Robotics engineers, it is suggested, have a close affinity to their mechanical creations because they themselves have lost their humanity.\n\nThe first in-depth literary study of the impact of robots and the motives of their creators was Karel \u010capek\u2019s seminal play R.U.R. (1921), which concerns the effect on society of robots designed to relieve their masters of toil, leaving them free to enjoy endless leisure. It is this feature\u2014their suggested potential for liberating humanity from the burden of heavy and repetitive tasks\u2014that has contributed to their continuing ambiguous status as objects of both desire and fear, an ambiguity that, as we have seen, has beset science itself from its origins in alchemy. \u010capek also explores the motives of the Rossums, uncle and nephew, who have created the firm Rossum\u2019s Universal Robots, designated by the play\u2019s title. Having accidentally discovered a substance that behaves like protoplasm, the elder Rossum, a physiologist, proceeds to experiment with making artificial beings. He thus exemplifies the hubris that leads scientists to believe that reason (science) is sufficient to create anything required. He is eventually killed by one of his creatures, but not before he has produced a manlike being, in modern terms, an android. Like Franken- stein and Moreau, Rossum, whose name derives from the Czech word rozum (reason), was driven by a desire to usurp the role of God, an intention about which he was more explicit than his predecessors. He \u201cwanted to become a sort of scientific substitute for God. He was a frightful materialist, and that\u2019s why he did it all . . . to prove that God was no longer necessary.\u201d\n\nThe younger Rossum, eager to exploit this discovery commercially, and impatient with his uncle\u2019s inefficient methods, has simplified the design, omitting as superfluous any aesthetic and emotional elements. Domin, the manager of the firm, explains: \u201cThe Robots are not people. Mechanically they are more perfect than we are, they have an enormously developed intelligence, but they have no soul\u201d (my italics). Domin himself cherishes a utopian vision of a world without work, but it is apparent that this ideal is also strongly tinged with hubris: \u201cMan shall have no other aim, no other labour, no other care than to perfect himself . . . He will be Lord of creation.\u201d\n\n\u010capek is also concerned to show how this philosophy of scientific materialism affects those who espouse it. The scientists at the Rossum factory are almost indistinguishable from each other and from the robots, for they too have become standardized, as though fresh from the assembly line. They are among the first and most stylized symbols of the anonymity and disappearance of individuality in a scientific, industrialized society, where people are interchangeable, defined only by the tasks assigned to them. Eventually the robots revolt against their exploitative, ineffectual masters and kill every human being except one, who is spared only on condition that he work to rediscover the lost formula for creating the robots. The parallel with Frankenstein, begged by the monster to create a mate for him, becomes clear as the play evolves to show that the reckless pursuit of science as an end in itself leads inevitably to the actual destruction of humanity, a humanity that, \u010capek suggests, has been effectively sterile and symbolically dead for years.\n\nR.U.R. became the prototype for a succession of robot stories and films, in most of which the scientist-inventors, like those in \u010capek\u2019s play, lose control over their creations. The first, appearing in the same year as R.U.R., was the Italian science fiction film L\u2019uomo meccanico (The mechanical man) (1921), in which a scientist creates a humanoid robot (really an android) with super- human speed and strength, controlled remotely by a machine. Ironically the scientist himself loses control of his robot to a criminal gang that employs the mechanical man to facilitate crimes before their leader is electrocuted by a short circuit in the controls.\n\nThe first film to associate robots with the mad, evil scientist stereotype was Fritz Lang\u2019s Metropolis (1927), based on the novel of the same name by Thea von Harbou. Rotwang, the deranged and obsessive scientist of Metropolis, has been called \u201cthe most influential scientist in the history of the cinema,\u201d largely because of the film\u2019s powerful visual symbolism. Metropolis is identified with both an ancient, Gothic house and the machinery of a bleak, urban wasteland depicted as an expressionist, futuristic city of 2026 characterized by skyscrapers where the \u201cmanagers\u201d live in luxury while the workers toil underground, running the machines and furnaces that power the city. This \u201cRaygun-Gothic\u201d decor was to become the prototype of mad scientists\u2019 laboratories in cinema throughout the century, while Rotwang himself, with his disordered mane of white hair, the high forehead associated with the mad inventor, an artificial, clawed right hand, and eyes \u201csmouldering with a hatred close to madness,\u201d is both the descendant of the obsessed alchemist and the ancestor of Dr. Strangelove. In his art deco laboratory, complete with industrial machinery and flashing lights, Rotwang creates not a male homunculus but a female robot, Maria, by imprinting the likeness of a human Maria onto a machine and then bringing her to life as an erotic and destructive woman, reminiscent of the beautiful automaton Olimpia in Hoffman\u2019s Der Sandmann (1814), also created by a mad and vengeful scientist, Mea of La femme endormie, and Frankenstein\u2019s monster.\n\nTraditionally robots had been creatures of masculine power and strength created by males independent of women, but the robot Maria emphasizes woman as Other, allowing Rotwang to claim power over both women and the process of human reproduction. As Andreas Huyssen remarks, \u201cBy creating a female android, Rotwang fulfils the male phantasm of a creation without mother; . . . he produces not just any natural life, but woman herself, the epitome of nature.\u201d As the evil counterpart of the good human Maria, who tries to quell the rebellion of the workers and who enacts her virgin mother status by collecting up their children to save them from impending destruction, the evil robot Maria is programmed to destroy the workers of Metropolis. When she is carried off by the rebels and burned at the stake like a witch (also an uncontrollable female Other), Rotwang, his life and meaning invested in the now-destroyed mechanical Maria, symbolically falls to his death from the cathedral roof. The moral is heavily underlined: evil scientists and their dangerous creations must be destroyed, especially if that creation is a sexually aggressive female who flouts the rules of society as robot Maria does during her erotic striptease in a brothel.\n\nAnother and more contentious variation of the amoral robot maker is Zapparoni, the scientist-creator of Ernst J\u00fcnger\u2019s Gl\u00e4serne Bienen (1957), translated as The Glass Bees (1961). This Prospero-like figure creates aesthetically perfect glass bees, tiny, sophisticated automata that, in their functionality and craftsmanship, combine technology and art, prefiguring the evolution during the 1980s of the powerful microchip. Symbolically, Zapparoni creates for himself a complete culture, endowing his robots not only with physical being but also with the ability to create other robots, a modern version of the \u201cphilosopher\u2019s stone\u201d the narrator thinks, emphasizing the connection between Zapparoni and the magus-alchemist Albertus Magnus, whose works Frankenstein studied.\n\nAt first Zapparoni\u2019s world appears utopian, a landscape of marvels, both functional and aesthetic, offered by technology; but through his narrator, the retired military man Captain Richard, J\u00fcnger expresses reservations about Zapparoni\u2019s pose of godlike detachment from his world, living in happy ignorance and refusing to accept accountability for a Brave New World society. In 1995, on his hundredth birthday, J\u00fcnger commented: \u201cOurs is the time of cybernetics, when machines wait on the threshold of thought and human beings are treated as components of the machine-world which can be cast aside when they are no longer needed. In such a period, what of ethics?\u201d In J\u00fcnger\u2019s analysis, Western society\u2019s complacent acceptance of the gifts and controls of technology gives a new and sinister meaning to Bacon\u2019s dictum \u201cknowledge is power.\u201d The serene and amoral Zapparoni may be even more dangerous to humanity than a recognizably evil dictator. J\u00fcnger\u2019s novel raises questions that were to be explored more explicitly by Donna Haraway in \u201cA Cyborg Manifesto\u201d (1991) regarding the blurring, possibly the erasure, of boundaries between animate and inanimate, between cyborgs and humans.\n\nWith few exceptions fictional robots are portrayed as a threat to humanity, raising the question of who controls the robots, but in stark contrast to the many dystopian narratives of such dangers, Eando Binder\u2019s science fiction short story I, Robot (1939) introduces an interesting twist in that the moral roles are reversed. The robot Adam Link, created by scientist Dr. Charles Link, is educated, self-aware, and desirous of serving a human master. However, when Dr. Link is accidentally killed, Adam is blamed and pursued by armed men intent on destroying him. At first he retaliates but then, having found and read a copy of Frankenstein, he understands the fear and revulsion he evokes, rejects revenge, and, after writing his confession, prepares to self-destruct. Binder\u2019s story was highly innovative for its time in breaking away from the Frankenstein clich\u00e9, and its popularity led to a series of Adam Link stories by Binder, which were later adapted for the American TV series The Outer Limits (1963\u201365).\n\nIsaac Asimov acknowledged the influence of Binder\u2019s concept on his robot stories: \u201cIt certainly caught my attention. Two months after I read it, I began \u2018Robbie,\u2019 about a sympathetic robot, and that was the start of my positronic robot series. Eleven years later, when nine of my robot stories were collected into a book, the publisher named the collection I, Robot over my objections. My book is now the more famous, but Otto\u2019s story was there first.\u201d Asimov\u2019s many stories about robots and the reactions of their scientist \u201cminders\u201d provide a striking contrast to the prevailing view in fiction about the effect of \u201cintelligent\u201d machines. As a biochemist and author of a large number of books on popular science, Asimov was scornful of what he called the \u201cFrankenstein complex\u201d and remained fundamentally optimistic about technological progress. Like H.G. Wells he saw it as relieving humanity of \u201cthose mental tasks that are dull, repetitive, stultifying and degrading, leaving to human beings themselves the far greater work of creative thought in every field from art and literature to science and ethics.\u201d\n\nAsimov conceded the possibility of danger from robots and computers only for those who feared change and had, in effect, already abrogated their autonomy in a technological age, essentially becoming like the machines they attacked. This is most apparent in his story \u201cProfession\u201d (1957), set in a future world where most of the inhabitants, fearful of change, have had their brains wired and programmed to act in a routine fashion so as to avoid the agony of decision making. Like the citizens of E.M. Forster\u2019s \u201cThe Machine Stops\u201d (1909), they have become voluntary appendages of the machine as a result of their reactionary paranoia.\n\nAsimov\u2019s most popular sequence of stories, beginning with \u201cI, Robot\u201d (1950), was based on an exploration of robots as necessarily \u201cbenign,\u201d because controlled by the Three Laws of Robotics, a system of ethics designed to prevent any takeover by robots such as \u010capek had depicted. Ironically, these laws were accepted and propagated by later science fiction writers as though they had inherent validity. In the broader context, however, Asimov\u2019s robots are atypical, for they are so humanized that they blur, if they do not actually deny, the issues being explored by other writers.\n\nContrary to Asimov\u2019s intention, and possibly unrealized by him, the stories themselves subvert this comfortable optimism, for not only are the human characters upstaged by the more intelligent robots that become the problem-solving heroes of these stories, but the more moral human characters are, themselves, governed by the Three Laws of Robotics. In \u201cEvidence\u201d (1946) the politician Byerly, the most \u201cethical\u201d human in the robot stories, is accused of being a humanoid (android) with a robot\u2019s brain, and the robopsychologist Susan Calvin concedes, \u201cActions such as his could come only from a robot, or from a very honorable and decent human being. But you see, you just can\u2019t differentiate between a robot and the very best of humans.\u201d Where problems in the robots\u2019 functioning occur, it is almost invariably attributed to a failure in perception or logic on the part of the humans. The interest of the stories thus centers, not on human qualities and emotions, but on the laws of logic and intellectual wordplay; that is, the plot interest is determined by the rules of the robots and, in the later stories, the computer.\n\nThe approved stereotype emerging from Asimov\u2019s stories is a thorough-going materialist and pragmatist who, without a qualm, exploits the solar system in the name of efficiency and human imperialism. In View from a Height (1963) Asimov discusses the most efficient means of colonizing the other planets, where existing life could provide an immediate source of food for the prospective Terran colonists; the use of space as a garbage dump for radioactive waste; and an ingenious real-estate scheme for selling off planets broken into asteroids. Indeed, Asimov\u2019s heroes bear a striking resemblance to C.S. Lewis\u2019s archvillain, Weston .\n\nA similar, optimistic view of artificial intelligence informs Frank Herbert\u2019s novel Destination: Void (1966). His four scientists aboard the spaceship Earthling\u2014a psychiatrist, a life-systems engineer, a doctor who specializes in brain chemistry, and a computer scientist\u2014represent the four disciplines most closely allied with the understanding and development of cognitive science. In the critical circumstances that attend their lone journey through space, they come to the realization that their survival depends on developing high-level artificial intelligence. Herbert\u2019s view is clearly that machine intelligence in cooperation with human intelligence is our only hope for the future and that scientists are therefore indispensable for the very reasons that led to their vilification by the majority of novelists discussed hitherto.\n\nFrom From Madman to Crime Fighter, by Roslynn D. Haynes, courtesy Johns Hopkins Press. Copyright 2017, by Roslynn D. Haynes.", "sentiment": 0.07509212332383063},
{"link_title": "Building Backup Server for Photography", "url": "https://medium.com/@trm42/building-backup-server-for-photography-981c32ab2f5", "text": "Few months ago I realised my seven years old desktop PC, which was nowadays used mostly as offline backup server, was running out of disk space. That\u2019s kind of hard to achieve unless you\u2019re storing lots of digital media like videos or photos.\n\nI happen to be enthusiast photographer (feel free to check my photos on Flickr) so my photo archive spans for over ten years and the size of the archive is about 3 terabytes these days. Of course my old PC had 3 terabytes of mirrored storage space and it was running out fast.\n\nA. just buy bunch of new hard drives and continue as it were with the big and noisy desktop PC.\n\nB. build new, more suitable home server that\u2019s small and quiet enough to reside in a bookshelf and stay always on.\n\nPlease note: I\u2019m not advising this route to anybody who isn\u2019t comfortable with PC hardware, Linux and command line tools. This is just to document my own approach in case it\u2019s useful for somebody. Please tell and comment how you have solved the same problems.\n\nThe reasons to build home server instead of buying ready made NAS are:\n\nAt first I was thinking about building the server around HPE ProLiant MicroServer Gen8 Entry mini server which is really interesting and inexpensive but has quite old specs and it\u2019s not that upgradeable and required custom parts and tweaks for some its problems, like additional SSD mount and MBR kludging to get the SSD boot from the secondary SATA connector meant for internal slim dvd drive.\n\nAnother option would\u2019ve been to use Raspberry Pi or Cubieboards but their IO capabilities are really, really limited.\n\nAfter researching for suitable home server enclosure I found the Fractal Design Node 304 for mini-ITX motherboards. While it\u2019s not really business class server enclosure (= no hot swap), it\u2019s quiet and can have six HDDs (but you need to open the box for changes) and have lots of room for mini-ITX components, cables and coolers. It comes with three quiet coolers, has air filters for all air intakes but requires you buying a power supply unit separately.\n\nFor the PSU I somewhat randomly selected modular Silverstone 550W Strider Platinum, which keeps quiet even on full load and is narrow enough for the Node 304 box. With wider PSU the cables may not fit into the box with case closed or fit full length PCIE video card (if there\u2019s a need for that). The only real nitpick about the PSU is the need to buy short cable set for it separately. That set is a must for mini-ITX enclosures for easier cabling.\n\nThe requirements for motherboard were mostly: gigabit ethernet, Kabylake support and at least six SATA ports. There were couple of really similar options and I ended up with Asrock H270-ITX/AC as it has bluetooth support in case there\u2019s some need for it in the future. The server is meant to be stored and used in our living room so wireless keyboard and mouse might be useful in the future. For now I\u2019m running the server headless and the integrated video chip in the CPU will be enough if I ever end up connecting the server to our living room TV.\n\nThe motherboard has two ethernet chips and AC Wifi so one could use them as bonded network interface for extra bandwidth. ATM I\u2019m not using the Wifi but use one ethernet for the host/backup Linux and the other ethernet is shared for Nextcloud container running on top of systemd-nspawn as it was simple to setup.\n\nAs the server\u2019s main usage is IO-bound, there\u2019s no need for really expensive, fast multi-core expensive CPU so I opted for the cheapest Kabylake Pentium G4560B which has two cores + hyper threading so four concurrent threads supported, plenty for home server. Unfortunately I didn\u2019t notice that its T-version had lower top frequency and lower TDP (54 W vs 35 W). Using the T-version would have kept the server cooler and quieter but it\u2019s a bit more expensive and would\u2019ve required buying separate cooler instead of the boxed one. Most of the time the CPU is mostly idling so higher TDP shouldn\u2019t be that much of an issue as the power saving features of the CPUs are nowadays awesome.\n\nNote: I would\u2019ve wanted to see, what kind of AMD Ryzens + mini-ITX motherboards are coming up but the need for the server grew faster than similar motherboards for Ryzen started getting to stores. Historically AMD setups are less expensive than their Intel counterparts so there could\u2019ve been a saving of 50\u2013100 EUR.\n\nFor the memory I bought single 16 GB DIMM, which is more than enough (although ZFS likes RAM). The motherboard\u2019s maximum memory support is 32 GB so it\u2019s easy to max the RAM by buying another memory module. I opted for Kingston as it should be really easy to buy new module with the same specs. Once again the actual speed of the memory isn\u2019t that important as the main usage pattern is IO-bound.\n\nPrices are in Euros, bought in Finland from Jimms.fi and Verkkokauppa.com.\n\nOf course this is one of the most important aspects of the setup so I bought three 6TB Western Digital Red drives meant for NAS usage. I put two inside the server and keep one as a offline-backup with a USB 3 SATA dock.\n\nNote: The disks cost more than the actual server. That cost is the same if you buy ready-made NAS.\n\nEnded up recycling a little used Samsung 840 Pro 256GB SSD disk from the old setup. Old Ubuntu 16.04 installation didn\u2019t even require reinstallation.\n\nFor the OS disk I\u2019ve been using the traditional Ext4 file system as it\u2019s robust enough and has worked really well for long time (I still kinda miss ReiserFS ;-), but for the backup storage drives I decided it\u2019s time to use something (hopefully) better: ZFS. It\u2019s not in Linux mainline kernel as its CDDL license is not compatible with the GPL license Linux kernel uses.\n\nLuckily it is served as separate project that provides Solaris compatibility layer and makes sure it works with Linux. Also, Ubuntu 16.04 has support for it included. That also means that either I\u2019m bound to the prebuilt Ubuntu kernel or have to compile ZFS module separately for newer kernels which also means downloading and compiling ZFS and SPL modules manually.\n\nZFS has quite awesome CLI tools (zpool and zfs), which are really well thought and documented. For Linux usage ArchWiki has good info and Aaron Toponce has made really informative documentation package about ZFS.\n\nOther option would\u2019ve been Linux-born BTRFS but even on 2017 it sounds (still) somewhat unreliable and with little documentation compared to ZFS. ZFS is not perfect or complete but it feels like the best option for now.\n\nThe advantage of ZFS and BTRFS is that instead of using separate RAID layer in kernel, they implement their own support for RAID and LVM features because RAID layer and filesystem needs to know about the stuff in each other to handle error situations better. ZFS is designed for large server setups and should handle data corruptions really well with checksums etc.\n\nThe two 6TB hard drives were configured as one big mirrored zpool. If it seems that the IO performance with tens of thousands of 10\u201350 megabyte files is too slow. There\u2019s an option to add SSD as a cache for the setup. That should speed up recently and often used files a lot.\n\nZFS supports snapshots and virtually they don\u2019t take any additional disk space** so you can use something like zfs-auto-snapshot, which creates new snapshots and removes older ones automagically multiple times per day.\n\n** ZFS Snapshots don\u2019t require additional space if the files don\u2019t change. If file changes, then the old and new ones are both preserved. The old version is removed if none of the existing snapshots are referring to it thus finally freeing the space. Using the snapshots requires some command line usage, but should be quite straightforward when the need arises.\n\nAnother sweet thing is that you don\u2019t need to partition mirrored pool to smaller logical partitions. You can create new sub volumes which may or may not have maximum disk space limits.\n\nZFS also supports compression on the fly so if you data set is easily compressible you can save some disk space with that. Mine is already mostly compressed so didn\u2019t see big gains from that.\n\nThe snapshots make creating new backups to external disk or another server easy. One can just send the fresh snapshots to new destination disk (even over network) and that\u2019s it.\n\nCurrently the most important backup software for me is the Unison backup which I\u2019m using to sync photos and files between my external HFS-partitioned disk and the server. This software has been awesome! I\u2019ve been using for my backups for over ten years and it still rocks!\n\nUnison keeps track of what has changed between syncs and syncs only changed files so making a backup sync of 3 TB takes less time than the full sync over network. It\u2019s like a rsync with steroids and with a better GUI. It\u2019s also cross platform and supports even Windows nicely albeit slowly compared to Linux.\n\nI\u2019m partially planning to move older photos of my external use disk to the server to skip the need to switch my 3 TB external disk to larger one. The older photos are used less and less so moving them to server disk shouldn\u2019t be that big problem. For this I need Samba to serve SMB/CIFS protocol. MacOS has switched from AFP to CIFS as the preferred network file protocol.\n\nNote: Storing photos in a network share can be done with Lightroom but it requires storing the catalog and preview files locally. That also should help with the network and disk latencies.\n\nIn case one wants an offline copy of the backup, one could set up additional sync from the server to one of the cloud backup services. Backblaze sounds interesting but they offer personal pricing plans only for Mac and Windows. Linux based backup requires corporate plan which makes it a lot more expensive.\n\nSo this is my backup solutions. Tell more and comment about how you\u2019ve solved the same issues :)", "sentiment": 0.13888017413298312},
{"link_title": "The Cruel Irony of the DACA Database", "url": "https://www.theringer.com/tech/2017/9/7/16268068/daca-database-trump", "text": "Societies are built on trust. Without it, no one would bother to cast a ballot in an election, pay college tuition to learn a vocation, or invest a portion of their paycheck in a retirement account. We stake our lives on these labyrinthine systems not because they\u2019re fair\u2014they fail us regularly\u2014but because they tend to reward risk rather than punish it.\n\nImmigrants who are in the United States illegally, typically shut out from accessing the fragile institutions that American citizens take for granted, may soon be punished for making a very American leap of faith. The Deferred Action for Childhood Arrivals program, implemented in 2012, has convinced hundreds of thousands of people residing in the United States illegally to provide the federal government their names, current and past addresses, immigration status, biometric data, and other personal identifiers, based on the understanding that the information would be used to grant them temporary legal residence in the country. On Tuesday, President Trump and Attorney General Jeff Sessions announced that DACA would be ended, meaning that about 800,000 immigrants who have been in the U.S. since they were children could be at risk of job termination, college expulsion, or deportation.\n\nThe data provided by the DACA recipients is now the permanent property of the federal government. The Department of Homeland Security, which both provides legal shelter to immigrants via DACA and rounds them up for deportation by deploying Immigration and Customs Enforcement, has said it won\u2019t explicitly target DACA recipients as it continues its crackdown on undocumented immigrants. But that\u2019s unlikely to assuage the fears of young adults who have built lives here in the United States. The question of who has a right to the DACA data and how they are authorized to use it will be a key point of contention as the immigration debate is renewed yet again. \u201cAs long as they have this information, it\u2019s going to be a temptation to be used,\u201d says Juli\u00e1n Gustavo G\u00f3mez, a 25-year-old Argentine who grew up in Miami and signed up for DACA in 2013. \u201cI think there\u2019s obviously a lack of trust between people with DACA and the Trump administration right now, considering what they\u2019ve just done.\u201d\n\nApplying to college is supposed to be a time for sizing up opportunities, but for G\u00f3mez, before DACA the process became more about identifying limitations. As an undocumented immigrant, he was forced to attend a local school since he didn\u2019t have the proper identification to board a plane. He had to pay out-of-state tuition, even though he\u2019d lived in Miami since he was a child. And he didn\u2019t know exactly what he planned to do with a college degree since he wouldn't qualify for legal employment. \u201cI didn\u2019t really know what I was going to do after college,\u201d he says. \u201cI was just kind of waiting and hoping for something to happen.\u201d\n\nThe summer before his freshman year, something did happen. DACA, the result of an executive action by President Obama in June 2012, emerged as a stopgap solution to help people like G\u00f3mez gain a more lasting foothold in society. Though Obama has always voiced empathy for the nation\u2019s undocumented immigrants, his first term actually saw a spike in immigrant removals. At the same time the DREAM Act (which would have provided a path to citizenship for people brought to the country illegally as children) died in the Senate in 2010, as immigration bills tend to do. Obama proved surprisingly tough on immigration enforcement but incapable of brokering a reform compromise to protect the immigrants already here.\n\nDACA was something of an end-run around congressional intransigence on the issue. The program granted a two-year stay to qualifying immigrants who were 30 or younger, had lived in the U.S. for at least five years, and were younger than 16 when they arrived. DACA recipients could use their status to legally get a driver\u2019s license, enroll in college, or apply for a job. However, they also had to renew their DACA status every two years (at a cost of about $500 in application fees) and were still not on a path to citizenship. Soon enough, immigrant activists hoped, the government would come up with a more permanent fix. \u201cThis would be the bridge between executive action and broader congressional action on the issue,\u201d says Deep Gulasekaram, a law professor at Santa Clara University specializing in immigration law.\n\nTo become DACA recipients, young residents who had spent most of their lives avoiding government scrutiny had to offer detailed personal information to U.S. Citizenship and Immigration Services (USCIS), the subdivision of the Department of Homeland Security that manages the DACA program. G\u00f3mez decided to take a chance. \u201cI had to weigh the risks, and to me, I was already living with a lot of risks being undocumented my whole life,\u201d he says. \u201cIt just was worth it to me at that point.\u201d\n\nG\u00f3mez\u2019s data\u2014fingerprints, photographs, bank accounts, school records, his original Argentine birth certificate\u2014would become a permanent part of his Alien File, the name for the dossier that the U.S. government keeps on each non-citizen. His fingerprints would be shared with the FBI to check whether he had an arrest record. His name would be checked across 26 different federal agencies to determine whether he was a suspected terrorist or a sex offender. Though his data would be housed within USCIS, ICE could get access to it if he was suspected of a crime. He was on the grid, for life.\n\nThat wouldn\u2019t have been quite so nerve-racking if Congress had decided to cement the ideals of DACA in legislation. But another effort for immigration reform in 2013 passed the Senate and stalled in the House. Obama\u2019s attempt to extend DACA\u2019s benefits to an older group of immigrants was rebuffed by the Supreme Court in 2016. And as Donald Trump used the demonization of immigrants as a stepping stone to the White House, it became clear that the United States\u2019 treatment of illegal residents was about to change drastically.\n\nFollowing Trump\u2019s election, G\u00f3mez, who now works for the immigration advocacy nonprofit Define American, penned an op-ed in The Washington Post urging Obama to delete the DACA database before Trump could use it as part of his deportation agenda. He wasn\u2019t alone in demanding action\u2014111 members of Congress penned a letter to Obama asking him to pass an executive order ensuring that the DACA data would be used only for its original intent. Such steps have been taken before: When New York launched a municipal ID program in 2015 to aid undocumented immigrants, it included a provision allowing for personal data to be destroyed at the end of 2016. But Obama took no such action.\n\nNow DACA is on its way out, but the data gathered through the program remains. In an FAQ about the DACA wind-down released by the Department of Homeland Security, the agency notes that \u201cInformation provided to USCIS in DACA requests will not be proactively provided to ICE and Customs and Border Protection for the purpose of immigration enforcement proceedings.\u201d The word \u201cproactively,\u201d which was not included in the original version of the DACA data-use policy in the Obama years, is doing a lot of work in this sentence. According to a former senior official in USCIS under Obama, ICE agents can already search the database of DACA recipients for a specific person if they have a name without having to first get clearance from USCIS. \u201cIf ICE wanted to examine an individual's record\u2014let's say they had a name of someone that interested them\u2014that is something that they could do,\u201d the official said. \u201cThere is not a legal barrier to their obtaining that information.\u201d\n\nIn response to questions about the department\u2019s data-use practices, a current ICE official said the department is not generally targeting \u201cactive DACA beneficiaries\u201d for deportation, absent any law enforcement interests. However the official would not specify when and how ICE agents can access immigrant data. \u201cThe transfer of information, from USCIS to ICE for purposes of enforcement action, is only going to be contemplated in instances where there is a significant law enforcement or national security interest,\u201d the ICE official said. \u201cBeyond that, we really can\u2019t speak to our investigative procedures or tactics\u2014certainly how we conduct national security investigations is law enforcement sensitive.\u201d\n\nEven the policy that ostensibly protects DACA data is impermanent. The Department of Homeland Security FAQ also notes that its current pledge to limit ICE\u2019s access to DACA recipients \u201cmay be modified, superseded, or rescinded at any time without notice.\u201d This was always part of the deal\u2014the old DACA boilerplate has the same disclaimer\u2014but the threat of a policy reversal looms much larger under Trump. It was only five days after his inauguration that the president issued an executive order to \u201c[e]nsure the faithful execution of the immigration laws \u2026 against all removable aliens,\u201d an order that DHS made sure to cite in its memo explaining the rescinding of DACA.\n\nPut plainly, the fate of the DACA data is subject to the whims of the Trump administration. \u201cThe government \u2026 always has with it a caveat, and that caveat is we can modify this rule at any time,\u201d says Gulasekaram. \u201cThat\u2019s the fear when you house both the benefit (adjudication [and] the giving of DACA) and the enforcement (ICE and prosecution) within the same government agency.\u201d\n\nBut even in this era when nothing feels private, playing fast and loose with people\u2019s personal data can still run afoul of the law.\n\nSocieties are built on trust. That\u2019s not just a feel-good axiom; it\u2019s an important legal distinction as well. In contract law, a notion known as \u201creliance\u201d stipulates that a person can be held responsible for breaking a promise if the person they brokered a deal with took action based on the assumption that the promise would be kept. The government promised to use DACA data to grant people residency, not deport them. Reneging on that promise after all these years might be ruled unlawful by a court, despite the disclaimer that enforcement policies can change in the future. \u201cThere are really strong arguments that the main thing that counts is the promise that they made\u2014that people who provide the information reasonably rely on that assurance of confidentiality,\u201d says Hiroshi Motomura, an immigration law professor at UCLA. \u201cIf someone were to file a lawsuit protecting that information, I think the chances are pretty good that the information would be protected. \u2026 The arguments are strong that the confidentiality promise is more robust and [takes precedent] over the boilerplate that the government puts into hundreds of documents.\u201d\n\nOther legal experts agree. \u201cOnce you start acting in a certain way, you might create reliance interest on behalf of certain people,\u201d Gulasekaram says. \u201cIn this case the government has created a reliance interest on behalf of people who gave their information underneath the promise that the government wasn\u2019t going to share it unless it fit narrow criteria. To go back on that would violate that reliance interest.\u201d\n\nAlready lawsuits that make this argument are being filed. On Wednesday, 15 states and the District of Columbia filed a suit to block Trump\u2019s decision, claiming in part that the government has not made it clear that DACA recipients\u2019 data won\u2019t be used for immigration enforcement. The suit also argues that the DACA decision is motivated by Trump\u2019s racial animus, citing the president\u2019s invective against Mexicans during his campaign. Words still matter, as Trump discovered when his attempted executive order to restrict travel from a handful of majority-Muslim countries was struck down in part due to his xenophobic campaign rhetoric. The new case\u2019s route through the courts is likely to be long and arduous, and it will run in parallel to a six-month deadline for Congress to somehow solve immigration reform\u2014an issue that has vexed legislators for decades\u2014before the protections for DACA recipients begin to elapse.\n\nThe lawsuit is a preemptive strike against the worst assumptions about the Trump administration\u2019s plans. If the DACA data is indeed used to target immigrants for deportation, engaging residents in collective action that requires a bit of faith will only be harder in the future. \u201cIf you breach that wall you have set up for the information that went to USCIS and you start to give it out, I don't think you can reasonably expect at any time in the future that anyone would ever trust the federal government and especially [the Department of Homeland Security] with any information about themselves,\u201d says Gulasekaram. \u201cAny future program like this I think is going to be scrutinized.\u201d\n\nIn the meantime, DACA recipients must sit and wait as the knots in their stomachs once again grow tight. G\u00f3mez was granted a green card in August, so he\u2019s now safe in the country he\u2019s called home since age 3. But hundreds of thousands of his peers aren\u2019t so lucky. They took a gamble putting their names on a list they were told could protect them. Now, it could expose them.\n\nAn earlier version of this piece incorrectly described DACA as granting recipients legal residency; it granted them a temporary stay.", "sentiment": 0.05672530534599503},
{"link_title": "Things found on GitHub: shell history", "url": "https://corte.si/posts/hacks/github-shhistory/index.html", "text": "Github recently introduced hugely improved code search, one of those rare moments when a service I use adds a feature that directly and measurably measurably improves my life. Predictably, there was soon a flurry of breathless stories about the security implications. This shouldn't have been news to anyone - by now, it should be clear that better search in almost any context has security or privacy implications, a law of the universe almost as solid as the second law of thermodynamics. We saw this with Google's own code search, as well as Google proper, Facebook's Graph Search and even Bing. A certain fraction of people will always make mistakes, and and any sufficiently powerful search will allow bad guys to find and take advantage of the outliers.\n\nAfter the dust had settled a bit I started wondering what else we could do with Github's search - other than snookering schmucks who checked in their private keys. I'm always enticed by data, and the combination of search and the ability to download raw checked-in files seemed like a promising avenue to explore. Lets see what we can come up with.\n\nFirst, some tooling. I've just released ghrabber, a simple tool that lets you grab all files matching a search specification from GitHub. Here, for instance, is an obvious wheeze - fetching all files with the extension \".key\":\n\nDownloaded files are saved locally to files named user.repository. Existing files with the same name are skipped, which means that you can reasonably efficiently stop and resume a ghrab.\n\nI've been having a lot of fun exploring Github with ghrabber. I'll return to this in future posts - today I'll start with a quick illustration of what can be done. One type of difficult-to-find information that is sometimes checked in to repos is shell history. Two simple ghrabber commands for the two most popular shells is all we need:\n\nAfter cleaning the data a bit, I had 234 history files varying in length from 1 line to just over 10 thousand, containing a total of 165k entries. I fed this into Pandas for analysis, parsing each command using a combination of hand-hacked heuristics and the built-in shlex module. The remainder of this post is a light exploration of some approaches to this dataset, steering clear of the obvious and tediously well-covered security implications.\n\nOne way to slice the data is to look at the percentage of history files a given command appears in. This gives us a nice listing of the top commands by user prevalence, which you can see in the graph on the left above. On the right, I've taken the same list of commands, and checked how many invocations are preceded by a man lookup for the command. This gives us an idea of which commonly-used commands have difficult or unintuitive interfaces. It's interesting that ln is right at the top of the list, considering how simple the command syntax is. My theory is that everyone forgets the order of the source and target files.\n\nSince we have a list of the most widely used commands, it's also trivial to do silly popularity comparisons. Above is the obvious look at the state of the editor wars (vim is winning, folks), and a check on how tmux is doing in supplanting screen (the faster the better).\n\nAnother interesting thing to do is to look at the most commonly used flags to commands. I think having \"real data\" of command use may well guide us to design better command-line interfaces. I'd love to know the most common invocation flags for some of the tools I write.\n\nI'll stop there. The data pool in this case is very deep, and there are a huge range of interesting bits of command-line ethnography that could be done. Stay posted for more in the coming weeks.", "sentiment": 0.16232239982239982},
{"link_title": "10 Random Ideas for September 3, 2017", "url": "https://medium.com/@x0054/10-random-ideas-for-september-3-2017-7ea2dc81cf6d", "text": "Everyday I try to think of 10 ideas. Sometimes interesting, sometimes thought provoking, and often enough pretty stupid. The point is to get the brain thinking, to exercise the brain muscle.\n\nNone of these have been researched. All of them are original, as in I thought of them, but many may not be novel, as you know what they say, geniuses think alike :)\n\nSince ideas are useless if you don\u2019t share them with the world, I decided to post them here. If you like any of them, go for it, make them a reality. If you ever want to chat about any of them with me, just look me up on sdbr.net or read my latest ideas on datarig.com. Thanks for reading!\n\nHere are my ideas for September 3, 2017:", "sentiment": 0.125},
{"link_title": "My two cents about Go", "url": "https://oscarforner.com/2017/09/09/My_two_cents_about_Go", "text": "There is plenty of meterials such as talks, tutorials and posts talking about Go, but I want to give my opinion about what I think are the strenghts and weaknesses of Go.\n\nFull disclosure: I do not use Go at work. I use Go in my personal projects. Therefore, take my opinions with a grain of salt.\n\nGo is one of the best programming languages I have seen in my whole life. I think that because of the following characteristics.\n\nIf you ever asked me \"Which programming language has the best and more complete standard library?\" before trying Go I would have answered Python. Nowadays, I would answer Go has the best and more complete standard library.\n\nTo back up my statement that Go has the best and more complete standard library:\n\nObviously that is no the only content of the standard library, but these were the packages that I found to be the more interesting.\n\nI really love the idea of the compiler of the language having the ability to run the unit testing and the benchmarking in such a simple way.\n\nTo run all the unit test you just have to do the following:\n\nYou can also run the unit test from a specific file with:\n\nTo run all the unit test that match a provided pattern you can do the following:\n\nTo perform the benchmarking of the unit test you have to do the following:\n\nFinally, you can limit the amount of time the benchmark is running:\n\nThis is one of the big features of Go. If you ever used a Unix system, this feature will remind you of pipes. This is because a channel is used to send data between a reader and a writer (usually Goroutines).\n\nThere is plenty of good documentation about channels in Go by Example and in A Tour of Go. So, there is no need for me to write examples.\n\nNot everything is perfect in Go. There are some perks that once they are fixed will make Go an even better programming language.\n\nOne of the main flaws of Go is the lack of a standard build system. I can understand that older languages such as C or C++ do not have a standard build system. Newer languages usually have a build system, not always from the beginning, such as Python or the more recent Rust.\n\nGetting a standard build system for Go would be a great advantage. Maybe for Go 2.0.\n\nThis issue is not my biggest concern, but it is something that could be problematic for some people. The main problem is having to change the environment variables $GOROOT and $GOPATH. In Python, I can choose to use Python2 or Python3 calling different executables. The same for C and C++, I can call different versions of the compiler because they can coexist in the filesystem.\n\nIn this section I talk about some features of Go that are both a Pro and a Con.\n\nSince I come from a systems programming background, I find hard to like a programming language that has a runtime. Unless such programming language brings more to the table to compensate for the runtime, I will try to avoid using it.\n\nGo is one of the few languages that actually brings cool features in its runtime like detection of concurrency problems such as deadlocks.\n\nThe lack of generics in the Go language may make some people doubt the usefulness of the language, however, I do not mind not having generics. It could be because I have not yet created a big project using Go, so I have not needed them.\n\nMaybe in Go 2.0 they will introduce some feature to address this perk. Nevertheless, I am not confident using this argument against Go. I have used C for long time and I never felt the need for generics. Therefore, I can live with a language that does not have them.\n\nThis is another of the big features of Go and some people will be surprised of it not being a Pro in this post. Don't get me wrong, I think Goroutines are awesome, but you need the runtime to make them work.\n\nThat is because Goroutines do not use the thread system provided by the operating system. It uses its own implementation of thread. And all that dependency, locality, lifetime, etc. of the thread is handled by the runtime.\n\nI think Go is already a great language, even with the small Cons I talk about in this post. Once the people in charge of the language fix this perks, Go will become an even better language. Go is a mature language with an awesome standard library and great tools. I will not be surprised if Go starts replacing Python in some niches even before Go gets a standard build system.", "sentiment": 0.20377816627816625},
{"link_title": "LIVE: 12 Cams from Florida \u2013 Irma Hurricane \u2013 Waiting for the Hurricane in Miami", "url": "https://lonews.ro/media-cinema/23944-uraganul-irma-asupra-miami-important-serverele-administratorilor-din-reteaua-lacasuri-ortodoxe-ar-putea-fi-afectate.html#livecamera", "text": "", "sentiment": 0.0},
{"link_title": "Presearch is building a blockchain-based search engine", "url": "http://linuxbsdos.com/2017/09/07/presearch-is-building-a-blockchain-based-search-engine/", "text": "Presearch is a Canadian crypto-startup building a blockchain-based search engine to take on Google. It is officially described as a decentralized search engine powered by the community.\n\nAs with crypto-starttups, the company is giving everybody the opportunity to get in on the ground floor by way of a token sale. Actually, this is the Lot 3 token sale, and each token is going for $0.15 USD, and you can get those after you create an account using BTC, ETH, or USD.\n\nAnd by getting your tokens using my referral link, you\u2019ll be helping to support this website. So go ahead and get those tokens. Who knows what they might be worth in the future.\n\nThe blockchain revolution is just getting under way. This is one small chance to get a piece of the action. Since this round started yesterday, 40% of the tokens have already been sold, so hurry, if you want some!", "sentiment": -0.016666666666666666},
{"link_title": "\u201cI\u2019ve Fallen in Love with the Lenovo ThinkPad X1 Yoga\u201d", "url": "https://arstechnica.com/gadgets/2017/09/ive-fallen-in-love-with-a-laptop-the-lenovo-thinkpad-x1-yoga/", "text": "When writing a review, whether of a computer game, a film, a book, or a piece of hardware, there is always a certain amount of pressure to be \"objective,\" to write from some kind of non-personal, neutral viewpoint divorced from any kind of emotional response.\n\nI've never subscribed to this view myself. Here at Ars, we don't try to review every piece of hardware that hits the market; our selection of review products is implicitly skewed toward those that we think are likely to be good, or if not good, then in some sense significant due to their profile, their positioning within the market, or whatever other factors we deem to be relevant. As such, someone reading the laptop reviews at Ars will always see a somewhat skewed representation of the market without being exposed to its full breadth. The same goes for laptop reviews virtually anywhere.\n\nThe truth is that for most reviews, especially when we look beyond the level of individual components, subjective considerations are equally, if not more, important than objective ones like benchmark scores or SSD storage space. Consider, for example, the keyboard on a laptop. We all have different preferences for keyboards, both in terms of layout and in terms of feel. On my desktop PC, for example, I have a Das Keyboard with Cherry MX Brown switches as a trade-off between tactile feel and sound. Other people prefer the clicky Cherry MX Blue switches. Some prefer laptop-style scissor switches.\n\nPersonal preference dominates on this particular detail. And it's important, especially on a laptop, because a keyboard is often not interchangeable. I would gladly take a laptop that was objectively \"worse\" (slower, lower battery life) than one that was objectively \"better\" (faster, longer battery life) if the first laptop had a keyboard that was crisp and well laid out and the second had a keyboard that felt spongy with a poor layout. Something like 20 percent worse performance, say, may make itself felt some of the time, but a bad keyboard will be frustrating every single time I use the device. Subjective experience matters.\n\nAll this brings me to the second generation Lenovo ThinkPad X1 Yoga. My first ever laptop was a ThinkPad back in the IBM days: an A30p. It was a large, high-end, heavy machine that ran hot and didn't have much in the way of battery life (but in those days, what did?). In spite of this, it ruined me for other laptops. I fell in love with the TrackPoint, the little red nipple between the G and H keys, that served as its pointing device. Touchpads are much better than they used to be, and some these days are even quite good. But for me, they will always be inferior to the IBM and Lenovo TrackPoints.\n\nI know, though I do not fully understand, that some people find the TrackPoint awkward to use. There is certainly a modest learning curve as one familiarizes oneself with the sensitivity and acceleration curve of the TrackPoint. They're delicate devices, and ham-fisted brute force is not rewarded. But once a little time investment has been made, the TrackPoint stands head and shoulders above any touchpad. The precision and fine control it offers is far beyond any touchpad, making things like precise text selection and even image editing comfortable and easy. The use of discrete buttons also means that traditional mouse operations, such as drag and drop and right clicking, do not require any new conventions or learning.\n\nGive me a laptop with a good keyboard and a great TrackPoint and I'm probably going to love it, and that's precisely the case with the X1 Yoga. The ThinkPad heritage is loud and clear with this machine, and it fills me with joy to use it.\n\nIf you're weird and hate the TrackPoint, the touchpad is good, too. It's a little smaller than it might otherwise be, due to the hardware TrackPoint buttons, but it feels good and supports the Precision Touchpad spec, so it offers all the Windows 10 gestures. I'd be happy to use it if there wasn't a TrackPoint.\n\nLikewise, the X1 Yoga has a fabulous keyboard. And it's a keyboard that lets you know it's a ThinkPad keyboard, right down to features that I know will annoy people: the Fn and Ctrl keys are \"backwards\" (though as has long been the case, you can swap them around in the system firmware), and if you end up pressing too many keys at the same time, the machine beeps at you in annoyance. The key action is positive and crisp, it's comfortable, and it's a keyboard that I can easily put thousands upon thousands of words into.\n\nI know I'm biased about the TrackPoint and keyboard, and it's possible that my love for the TrackPoint and keyboard have blinded me to the machine's flaws. But I don't think the X1 Yoga really has any flaws; a few areas that may have scope for improvement, perhaps, but flaws? Not really. My intense personal biases aside, the X1 Yoga is still a great machine. \"X1\" means it's a premium Ultrabook-type system. It's about 3 pounds (a hair under for OLED, a little over for LCD), with 7th generation Intel Core processors (which means dual core, Kaby Lake designs).\n\nAs a ThinkPad it is, of course, available in black, and black is always in style, but it's also available with a gray/silver option, which my review system used. I think I'd go for black just for the sake of tradition, but the silver doesn't look bad.\n\nEven at about 3 pounds, the X1 Yoga packs in a 14-inch screen. The review unit had a 1920\u00d71080 270 nit screen. I would have liked a little more brightness, but it looks decent. I'm intrigued by, but haven't had a chance to use, the OLED 2560\u00d71440 screen. Although I have concerns about OLED, especially around display longevity, the rich colors and high contrast ratios are very appealing. There's also an LCD option at that higher resolution.\n\nWhichever screen option you choose, it'll be a 10-finger touchscreen with pen support. In spite of its slim size, the X1 Yoga manages to include a pen garage to neatly dock the stylus when it's not in use. The pen is powered, and it recharges whenever it's docked. Combined with the final element of the system's name\u2014\"Yoga\" denotes that it has a 360-degree hinge, so the screen can fold all the way back to convert the system into a chunky tablet\u2014and the result is a device with a ton of versatility.\n\nIf I were buying a laptop today, I'd make touch screen support and a 360-degree hinge must-have features. After using Windows 8 and 10 devices for so long, touching the screen to scroll and tap dialog box buttons has become second nature. A laptop without a touch screen just feels broken. And the 360-degree hinge is fantastic on the plane or in the kitchen.\n\nHonestly, I'm probably never going to fold the screen all the way back for tablet mode. I don't really care for tablets, so it's just not that useful to me. But \"tent\" mode, where the system is bent into an inverted V, is great for watching movies on the plane or following recipes in the kitchen. On the plane, it greatly reduces the footprint of the machine\u2014invaluable for watching movies in cattle class where I normally find myself sitting\u2014and in the kitchen it not only shrinks the footprint, it also makes the keyboard a much less inviting target for accidental spills. The touch screen means you can still pause your movie or scroll through the instructions as you're following them. Frankly, all laptops should work this way in 2017.\n\nWith the screen folded back, the keys recess into the keyboard and become disabled, protecting them from damage and ensuring that there are no stray key presses when holding the thing like a tablet.", "sentiment": 0.13157853687783266},
{"link_title": "Casinos Aren\u2019t Enough as Native Tribe Makes Deal on Drug Patents", "url": "https://www.bloomberg.com/news/articles/2017-09-09/casinos-aren-t-enough-as-native-tribe-makes-deal-on-drug-patents", "text": "\u2018Our options beyond the casino are few and far between\u2019\n\nFor one Native American tribe, the money\u2019s not in the casino anymore. Patents are the future.\n\nOn Friday, drugmaker Allergan Plc said it would transfer intellectual property on a blockbuster drug to the Saint Regis Mohawk Tribe, in order to avoid attacks on the medicine\u2019s patents. For the tribe, it\u2019s a new revenue stream that could lead to more down the road.\n\n\u201cOur options beyond the casino are few and far between so an opportunity like this is attractive to us, because we have a lot of unmet needs for our community,\u201d said Saint Regis Mohawk Tribe\u2019s general counsel Dale White. The agreement will pay the tribe $13.75 million, plus $15 million a year in annual revenues, according to Allergan.\n\nThere are 13,000 Saint Regis Mohawk Tribe members nationwide, and they own a six-mile-by-six-mile square piece of land in a sparsely populated part of upstate New York. The Akwesasne Mohawk Casino Resort, is one of the tribe\u2019s main sources of revenue. It\u2019s open 24 hours a day, has 1,600 slot machines, 30 table games and a poker room, according to the website.\n\nThe attraction for a drug company is simple: The tribe has sovereignty, setting it apart from some legal proceedings -- such as expedited patent reviews. For Allergan, it can serve as a small legal island guarding its valuable intellectual property.\n\nRestasis\u2019s patents are under attack on two fronts, and moving the rights may shield them on one side. Last year, the drug -- which treats chronic dry eye -- brought in $1.49 billion in sales.\n\n\u201cI would expect it creates a playbook for other cases down the road both for us and for others,\u201d Bob Bailey, Allergan\u2019s chief legal officer, said of the agreement with the tribe.\n\nShares of Allergan, which is based in the U.S., ended Friday up 2.5 percent.\n\nThere\u2019s a legal basis for the strategy. The Patent Trial and Appeal Board recently ruled in two cases that state-university-owned patents aren\u2019t subject to the review process because states have sovereign immunity. In those cases, however, the university was the original owner of the patents.\n\nDrugmakers have been looking for any way they can to hold onto revenue. Several top pharmaceutical companies have recently reported slowing sales and shrinking profits due to the loss of exclusivity for top-selling medications, and the Trump administration has said it wants to drive down drug prices. At the same time, attacks on drug patents have been made easier under the patent office\u2019s recently created fast-track legal review process.\n\nAllergan said it had been approached by the tribe with what it called a \u201csophisticated opportunity to strengthen the defense\u201d of Restatis\u2019s patents, which have been challenged under a process called inter partes review, or IPR.\n\nDrugmakers have long railed against the IPR process, which enables challenges before the patent board, because it means they have to defend their patents against assaults in two different forums. They have to win in both the court and before the agency.\n\nMylan NV has filed petitions with the patent office to challenge Allergan\u2019s patents under the IPR process. The agency previously determined that Mylan has established a \u201creasonable likelihood\u201d of winning its arguments that the patents are invalid, though a hearing on the case is scheduled for next week in Alexandria, Virginia.\n\nBy transferring ownership to the tribe, Allergan can try to limit its legal battle to the courts, where it\u2019s harder to invalidate patents because of a more stringent legal standard.\n\n\u201cThe way I think about this is what we\u2019re doing here today is to really allow Allergan to focus the defense of the Restasis patent in the federal court system and avoid the double jeopardy of the IPR system,\u201d Allergan Chief Executive Officer Brent Saunders said in an interview.\n\nSuccess for Allergan\u2019s deal with the Saint Regis Mohawk Tribe would open a whole host of questions for the patent-review process, which was set up under a 2011 law. Tech companies are the biggest users of the system, intended as a lower-cost way to resolve patent disputes.\n\n\u201cIf they can do it, other drugmakers can do it, and not just other drugmakers but other companies,\u201d said Wright, the patent lawyer. \u201cIt\u2019s a massive loophole and you could drive a Mack truck through that.\u201d\n\nThe tribe is ready, if they do. White, the lawyer, said the tribe will work with Shore Chan DePumpo, a law firm that helped vet the Allergan deal.\n\n\u201cWe will vet them through the Shore firm to make sure that these are legitimate companies that we want to do business with,\u201d he said. \u201cWe\u2019ll probably take as many as the Shore firm can handle.\"", "sentiment": 0.1024675324675325},
{"link_title": "Science behind E-Commerce Checkout", "url": "https://uxplanet.org/science-of-e-commerce-app-checkout-8d5bf9bc0be0", "text": "Which is the most important e-commerce page? Is it the home page, listing page, product page, cart or checkout? Well, to be fair combination of all these pages make a successful e-commerce app.\n\nBut in this capitalist world, moneymaking is the most important and checkout is the moneymaking page and here we do not want to leave customers with sour taste in their mouth.\n\nCheckout is essentially a two step process:\n\n1. Where to deliver product (name, address)\n\n2. Payment Details.\n\nWith the above information, we can deliver the product. However, as a User Experience Designer, we wish to create something coveted by many people and ideally used by them on a regular basis without any hassle.\n\nNow, let us get to actual process.\n\nAs the user hits Proceed to Checkout, we need to create a safe environment for our users. They should be informed all the time where they are in the process. We do not want users to be confused at any step.\n\nThe three steps are Shipping, Billing and Confirmation. There are two types of users, those who have gone through the checkout process and others who are visiting/ buying for the first time. We need to be wary of those who are visiting it for the first time.\n\nEnough rumbling, lets get to the process.\n\nA major worry new users possess is what will the company do with my information. Explicitly state : We do not share your details with anyone. They will feel a lot better. You can also incentivise First Time Users to Sign Up.\n\nThere are 3 types of Signup: Email Sign Up, Social Media Sign Up and Mobile Sign Up. Email Sign Up are the worst.\n\nUser has to go back to their mail account to verify their Email ID. This is an extra hassle and distracts the user from their main goal.\n\nSocial Media media sign ups are the quickest way, as users do not have to fill anything and are verified instantly.\n\nMobile Signups are better than Email Sign Ups, because the user does not have to leave the app.\n\nAs the user hits continue, they are prompted to enter an OTP, it should fill automatically when user receives OTP.\n\n\u201cSit back and relax! while we verify your mobile number (Enter the OTP in case if we fail to detect the SMS automatically).\u201d \n\nMessages like these go a long way, they clearly prepare user for what is about to happen and what is fail safe.\n\nAfter this user should be asked to enter password. A few companies do not ask password and every time the user logs in, they are asked to enter OTP, many times users have bad experiences with OTP, \n\n1. Sometimes, they do not receive them, even if the Internet is working on their phone, which leads to frustration.\n\n2. While, logging on desktop website, and they do not have their phone near them.\n\n3. They lost their phone, which incidentally, happened to me, and I was not able to log on to my PayTm wallet, when you can log onto PayTm only via OTP, PayTm now supports password.\n\nPlaceholder, informing the user about minimum number of character and letting users check if they have entered password to their liking, is a good practice to avoid unnecessary errors. Many websites have confirm password which is an extra step for user to fill, and it can be removed by introducing eye.\n\nNo one wants to fill out a form, it\u2019s boring, but we can help users at each step and make it easier and fun for them (who am I kidding :P).\n\nFor customer to receive her items, she will have to fill her name, mobile no., shipping address, pin-code, city, state.\n\nFill the form with pin-code because with the help of google API, pin-code can provide City and State automatically, two details in the form fills quickly(Good Start).\n\nIf user has same name as on Facebook or Google, it can be filled on a click by introducing a checkbox, \u201cSame as registered on Facebook or Google\u201d after that user fills Mobile No., Shipping Address and Landmark is Optional field instead of asterisk(*) we should write optional in front of landmark, reason being, including the phrase optional is much clearer than using any symbol or icon.\n\nDefault address is the billing address and address used when user goes for quicker checkout.\n\nProceed to payment should not be active till user fills the whole form to prevent unnecessary errors.\n\nThere should be inline validation in the form\n\nFor e.g. if a user types 1234567890 as her phone number, and she goes onto next step, error should be displayed as user progresses through the form, as opposed to checking inputs in lump sum when she submits the form.\n\nGreat thing about Inline validation is that the user gets to know their mistake as soon as, error has occurred their, rather than after filling the form. It is frustrating for anyone to be pointed out their mistakes after they have done all their work and during checkout, we do not want to irritate the almighty user.\n\nShowing a lock icon on proceed to payment is another way of assuring the user of safety during payment.\n\nUpto this moment user has chosen what to buy and filled their address, just before payment, we can let the user review what they have done and give them a chance to edit address. Another reason to have order summary is that the user gets redirected to this part of checkout directly when they hit Buy Now on product page(If logged in), more on this in future articles.\n\nPromotional Code\n\nCoupon codes are more a business need then a UX need. In one usability test, removing the coupon code field led to increase in overall conversion from 3.8% to 5.1%(Increase of 34%). In another study by PayPal, 27% of the users accepted that they were leaving the app to look for a coupon code, here we are distracting the user from their main task i.e. Checkout.\n\nInstead of coupon codes, direct discounts can be given to users in their profile i.e. auto applying of coupons to targeted customers or on a particular product, category etc. But if the client wants coupon code in the app, instead of showing text box. We can ask user, \u201cHave you got Promo Code?\u201d and upon clicking it, text box will appear. We are trying to hide the coupon code box, those who have coupon code will try to find it.\n\nIt is very important to give the user as many option as possible (Cards, Wallets, Net Banking, EMI, Cash on Delivery etc.). This is the step where user needs to feel most secure cause they are just about to give their hard earned money to a machine. We need to assure our users, payments are safe and secure, even if they don\u2019t like what they get, they can return it back and we are not here to dupe them, we only deal in authentic products. Above points can be concisely explained \u201cSafe and Secure Payments. Easy Returns. 100% Authentic Product.\u201d \n\nSave details for quicker checkout: For returning users, this is a major help as they do not have to fill their card, wallet, net banking details every time they make a purchase while buying. It also reduces checkout time.\n\nAfter payment from secure gateway, user will reach thank you/confirmation page, but before that I will cover cases of returning users(logged in users).\n\nIf user has ordered before, they will have bunch of addresses saved with one marked as default address. A clear big button to add a new address and every old address from which they ordered.\n\nAfter choosing the address to deliver, user will reach the Billing page, here too their, payment details will be saved, they can select one of those and pay the amount.\n\nThis should be a cheerful page. Customer has bought from the website so be grateful and tell them what they ordered.\n\nWith the Thank You page we can create more leads and compel users to buy again.\n\nIncentivise Users\n\nWe can have a referral program or can give offers on the page, so that users have another reason to buy the product, other than just willingness. \n\nI worked on a Dental products\u2019, e-commerce app and this is how I portrayed Refer & Earn.\n\nWe can ask user\u2019s question to understand more about them. Here we are collecting data to understand what type of dentists buy from our website which can help us improve products and inventory.\n\nThe user has gone through the most stringent process, they went through its intricacies, and some active users can give really nice and insightful feedback which can help us improve the app immensely.\n\nHumanising the app is very important that is why user should be given feedback of their action in a fun and engaging manner.\n\nDuring Shipping and Billing, Header 1 should be used as the user is going through checkout, and therefore should not be distracted, all of their concentration should be on checkout process.\n\nOn the Thank You page, Header 2 should be used as the user is done with buying and now she should be able to navigate through the whole app.", "sentiment": 0.16464065433091982},
{"link_title": "On Hiding a Plaintext Length by Preencryption (2011) [pdf]", "url": "https://infoscience.epfl.ch/record/166704/files/lengthhiding-corrected.pdf", "text": "", "sentiment": 0.0},
{"link_title": "Equifax security freeze PINs are the timestamp of when you request the freeze", "url": "https://twitter.com/webster/status/906346071210778625", "text": "", "sentiment": 0.0},
{"link_title": "Stuff to Do to Register in Heat of the Code by FOSSASIA", "url": "https://docs.google.com/forms/d/e/1FAIpQLSfhIZHnDIFwHZjRHrp1LmzcBE5eaAtudCFgQKEoFwxb229Ulw/viewform", "text": "STEP 1: At first developers look through the FOSSASIA projects and get acquainted with it by reading the Best Practices ( blog.fossasia.org/open-source-developer-guide-and-best-practices-at-fossasia/ ) They also join FOSSASIA channels as follows:* Signup for the FOSSASIA Newsletter, fossasia.org/#subscribe * Star and fork repositories you are interested in on GitHub, github.com/fossasia * Join the FOSSASIA Chat on Gitter, gitter.im/fossasia/fossasia * Follow FOSSASIA on Twitter ( twitter.com/fossasia ) and the Codeheat Twitter account ( twitter.com/codeheat_ * Like FOSSASIA on Facebook, facebook.com/fossasia * Join the FOSSASIA Group on Linkedin, www.linkedin.com/groups/3762811 * Follow the FOSSASIA Linkedin Page, www.linkedin.com/company/2982840/ * Subscribe to the FOSSASIA YouTube Channel, www.youtube.com/fossasiaorg\n\nSTEP 2: Then developers sign up on this form to participate. Developers will receive an invite to the FOSSASIA GitHub within a week (usually earlier). All developers should then make their participation public by showing their membership as \u201cpublic\u201d in their GitHub profile.\n\nSTEP 3: Developers actively contribute code, submit blog posts, screencasts and organize outreach events.\n\n* Each developer then searches for issues that interests him/her and claims ownership of a particular issue. The developers work on the issues and if there are questions asks on the chat channel of the project. \n\n* Once finished, the developers makes a pull request from his/her own forked repository to the development branch of the project and submit their work for review. \n\n* Pull requests need to pass Travis builds, code CI tests and ensure migrations work. \n\n* Mentors and core-developers from the organization evaluate the work submitted.\n\n* After the pull request has been merged developers can claim another issue to work on if they wish.\n\n* Developers also submit blog posts, meet new developers in outreach events and provide screencasts\n\nSTEP 4: Final Evaluations will take place at the end of the contest. Results of winners will be announce on February 5, 2018.\n\nA detailed timeline and more details at http://codeheat.org", "sentiment": 0.2621212121212121},
{"link_title": "Decentralized Social Networks Sound Great. Too Bad They\u2019ll Never Work", "url": "https://www.wired.com/story/decentralized-social-networks-sound-great-too-bad-theyll-never-work/", "text": "Last year Jillian York, a free expression activist, was temporarily booted off Facebook for sharing partially nude images. The offending photos were part of a German breast cancer awareness campaign which featured, well, breasts. Facebook flagged the post as a violation of its Community Standards, which strictly prohibits most types of female nudity . Though the account suspension lasted only 24 hours, it had a powerful impact on York\u2019s ability to get things done.\n\nLocked out of Facebook, York was unable to complete her work or post comments on news sites that use Facebook\u2019s commenting tools. And without Facebook credentials, York could not access apps like Spotify and Tinder. Tick off Facebook and you may be unable to work, date, or listen to music. York\u2019s suspension highlights the ever-expanding ways in which we now rely on large private platforms to facilitate our online activities.\n\nOver the last 13 years, Facebook has evolved from a lifestyle site for college kids into a cornerstone of civic life. It is one of a handful of very large platforms that dominate our online world. As such platforms have gained traction, the web has transformed from an open space for free expression into a corporate-owned gated community of private platforms.\n\nThe power of giant platforms like Facebook, Google, and Twitter leads to problems ranging from the threat of government-ordered censorship to more subtle, algorithmic biases in the curation of content users consume. Moreover, as these platforms expand their reach, the ripple effects of exclusion can have serious consequences for people\u2019s personal and professional lives, and users have no clear path to recourse. The platforms that host and inform our networked public sphere are unelected, unaccountable, and often impossible to audit or oversee.\n\nIn response, there is a growing movement among free speech advocates to create new technology to address these concerns. Early web pioneers like Brewster Kahle have called for ways we might \u201clock the web open\u201d with code, enabling peer-to-peer interactions in place of mediated private platforms. The idea is to return to the good old days of the early '90\u2019s web, when users published content directly in a user-friendly decentralized fashion, without the need for corporate intermediaries and their aspirational approach.\n\nIt\u2019s an exciting idea. Instead of corporate-owned platforms, users could meet, flirt, argue, and share in user-run community forums. Many of these proposed platforms build on the concept of decentralization, which has grown increasingly popular with the rise of cryptocurrencies like bitcoin.\n\nSimilar to bitcoin, decentralized platforms have no single organization controlling the network. The decentralized web employs technology that eliminates such choke points in the technical infrastructure of the web. This has given rise to projects like Mastodon , a federated social media platform that resembles Twitter; Blockstack, a distributed system for online identity services; and Steemit, an online community using digital tokens to encourage people to contribute to a Reddit-like online community.\n\nThe three of us investigated several of these most promising efforts to \u201cre-decentralize\u201d the web, to better understand their potential to shake up the dominance of Facebook, Google, and Twitter. The projects we examined are pursuing deeply exciting new ideas. However, we doubt that decentralized systems alone will address the threats to free expression caused by today\u2019s mega-platforms, for several key reasons.\n\nFirst, these tools will face challenges acquiring users and gaining the attention of developers. Tools like Diaspora and FreedomBox encountered difficulties attracting a permanent user base, and it\u2019s likely new platforms will, too. Social networks, in particular, are difficult to bootstrap due to network effects\u2014we join them because our friends are there, not for ideological reasons like decentralization. And while existing social networks have perfected their interfaces based on feedback from millions of users, new social networks are often challenging for new users to navigate.\n\nThese platforms also pose new security threats. Decentralized networks generally allow anyone to join and don\u2019t link accounts to real-world identities like phone numbers. These systems often use public key cryptography to ensure account security. But managing public keys is hard for most users, and building software that is both cryptographically secure and easy to use is difficult.\n\nSocial media platforms are curators, not just publishers. Platforms like Facebook control not only what is acceptable to publish, but what posts we see, bringing the most interesting posts to one\u2019s attention. Platforms tend to optimize for advertising revenue, prioritizing attention-grabbing or feel-good content. Designing robust reward mechanisms to curate content that keeps people informed rather than entertained remains a problem. If distributed platforms could solve it, they could theoretically tackle media challenges like echo chambers and filter bubbles, but such dilemmas still present a serious challenge for new systems.\n\nFinally, platforms benefit from economies of scale \u2014 it\u2019s cheaper to acquire resources like storage and bandwidth in bulk. And with network effects, which make larger platforms more useful, you have a recipe for consolidation. Even in self-consciously decentralized systems like Bitcoin, there has been a natural consolidation toward super-participants like large mining pools and exchanges. Market consolidation is also driven by user-targeted advertising models, which encourage hoarding of user views and data, discourage interoperability, and drive platforms to become ever larger.\n\nOur research\u2014a combination of technical and historical analysis, and dozens of interviews with open web advocates\u2014indicates that there is no straightforward technical solution to the problem of platform monopolies . Moreover, it\u2019s not clear we can solve the nuanced issues of centralization by pushing for \u201cre-decentralization\u201d of publishing online. The reality is that most people do not want to run their own web servers or social network nodes. They want to engage with the web through friendlier platforms, and these platforms will be constrained by the same forces that drive consolidation today.\n\nA better strategy would be to pursue policies that strengthen the environment for decentralized platforms, including data portability, interoperability, and alternatives to advertising-based funding models. For instance, if users have more control of their data, including the right to export and reuse content they\u2019ve created and friends they follow, they\u2019ll be more willing to experiment with new platforms. Decentralized web advocates have good intentions, but there\u2019s no silver-bullet technical solution for the challenges that lie ahead.\n\nWIRED Opinion publishes pieces written by outside contributors and represents a wide range of viewpoints. Read more opinions here .", "sentiment": 0.10511664261664264},
{"link_title": "What are some cute things that beginner programmers do?", "url": "http://brianknapp.me/cute-things-that-beginner-programmers-do/", "text": "What are some cute things that beginner programmers do?\n\nThis is an oldie, but a goodie\u2026\n\nWhen I was a beginner programmer some 22 years ago, I knew very little about all kinds of things (I was maybe 11 at the time, so cut me some slack)\n\nAnyhow, I used to write all of my code in Notepad.exe. At the time I would have been coding in C or C++. Needless to say, my workflow wasn\u2019t very good and I didn\u2019t make progress very fast.\n\nInterestingly, looking back it\u2019s sort of bizarrely impressive. Notepad doesn\u2019t have line numbers, syntax highlighting, or really any features at all. It\u2019s black text on a white background. The end.\n\nOh and I didn\u2019t really understand Makefiles either, so compiling and running my programs was a complete chore.\n\nNowadays, there are a million great options out there, but back then I never had access to an IDE, and the ones that were available were pretty expensive, so I suffered through Notepad until I got to college.\n\nI don\u2019t see many beginner programmers doing that now, but I imagine I\u2019m not the only beginner who used Notepad to write code.\n\nP.S. Have you subscribed to Code Career Genius yet?", "sentiment": 0.21047916666666672},
{"link_title": "Show HN: Build and share curated collections of learning resources", "url": "https://www.promilio.com/c/hacker-news-favorite-cs-papers-august-2017-ghkcxhhvywsunmzdu", "text": "", "sentiment": 0.0},
{"link_title": "The Corpus Christi Prime", "url": "https://friendlyfieldsandopenmaps.com/2017/09/08/the-corpus-christi-prime/", "text": "This number is a prime, with 2688 digits. It also looks rather a lot like Corpus Christi College, Cambridge. The top left corner encodes my initials, JRH, in ASCII. The bottom right corner is my date of birth.\n\nI was inspired by Numberphile\u2019s most recent video, which demonstrates a prime number of 1350 digits, which looks like the coat of arms of Trinity Hall College, Cambridge.\n\nI created some pixel art, which looks a bit like the College, but slightly squished since pixels are square, but characters are rectangles.\n\nNext, I selected a font (Menlo) and counted the number of pixels used for each digit. I chose Menlo because it has very heavy zeros, which I thought might come in useful.\n\nThen I wrote a program which generated an \u201cideal\u201d number, based on these two pieces of information. I then manually made the two required modifications (which were not completely narcissistic \u2013 the number had to end with an odd digit, and numbers starting with 1 are ever so slightly more likely to be prime).\n\nFinally, I generated random fluctuations in the number and tested each with the Miller-Rabin primality test. This produced a shortlist of numbers which were very very likely to be prime. I used Dario Alpern\u2019s fantastic tool to determine whether any of them actually were prime. Of the 8 candidates I had generated overnight, all of them were prime, so I selected the nicest looking one, which you see above.\n\nThe prime number theorem tells us that there are approximately primes less than . So there are approximately\n\n2688-digit primes. So approximately one in every 6200 2688-digit numbers is prime. Now, I wasn\u2019t looking at even numbers, so that reduces that number by half, so things are looking quite good.\n\nI set my program running for a little bit and determined that on my hardware (a MacBook Air with a 1.7 GHz processor and 8GB of RAM) my program could determine whether about 30 2688-digit numbers are probably prime per minute. So I thought that it would take about 100 minutes to find a candidate. I had slightly overestimated this time: overnight (9 hours), I had checked 25750 numbers and found 8 probable primes. This does line up pretty well with the back of the envelope calculation above.\n\nFeel free to check whether it is prime:", "sentiment": 0.051663059163059154},
{"link_title": "The Nitrogen Problem: Global Warming Is Making It Worse", "url": "http://e360.yale.edu/features/the-nitrogen-problem-why-global-warming-is-making-it-worse", "text": "New research shows that increases in rainfall and extreme weather because of climate change will increase the amount of nitrogen polluting rivers and other waterways. The findings underscore the urgency of reforming agriculture to dramatically reduce the use of nitrogen fertilizers.\n\nIt is a painful lesson of our time that the things we depend on to make our lives more comfortable can also kill us. Our addiction to fossils fuels is the obvious example, as we come to terms with the slow motion catastrophe of climate change. But we are addicted to nitrogen, too, in the fertilizers that feed us, and it now appears that the combination of climate change and nitrogen pollution is multiplying the possibilities for wrecking the world around us. A new study in Science projects that climate change will increase the amount of nitrogen ending up in U.S. rivers and other waterways by 19 percent on average over the remainder of the century \u2014 and much more in hard-hit areas, notably the Mississippi-Atchafalaya River Basin (up 24 percent) and the Northeast (up 28 percent). That\u2019s not counting likely increases in nitrogen inputs from more intensive agriculture, or from increased human population. Instead, Stanford University researcher Eva Sinha and her co-authors simply took historical records of nitrogen runoff as a result of rainstorms over the past few decades, recorded by the U.S. Geological Survey. Then, assuming for the sake of argument that there will be no change in the amount of nitrogen being added to the environment, they calculated how much additional nitrogen would be leached out of farm fields and washed down rivers solely because of extreme weather events and increased total rainfall predicted in most climate change scenarios. The bottom line: \u201cAnticipated changes in future precipitation patterns alone will lead to large and robust increases in watershed-scale nitrogen fluxes by the end of the century for the business-as-usual scenario.\u201d\n\nBut the business-as-usual scenario is of course already in trouble, even without climate change. Headlines have tended to fixate on the Gulf of Mexico \u201cdead zone\u201d produced by nitrogen flushed down the Mississippi River from the cornfields of the upper Midwest. (This year\u2019s \u201cdead zone\u201d is the largest ever, the National Oceanic and Atmospheric Administration announced last week.) But the problem is already much broader than that, says senior author Anna M. Michalak, also of Stanford, citing a series of recent incidents caused by nitrogen pollution. Last summer, for instance, a 33-square-mile algae bloom caused Florida to declare a four-county state of emergency. Another closed the Dungeness crab fishery along half of the Washington State coast last year and affected other fisheries as far south as Mexico. The combined effect of climate change and nitrogen pollution is also evident on inland waterways, according to Hans Paerl, an aquatic ecologist at the University of North Carolina\u2019s Institute of Marine Sciences. In the past, cleanup efforts on lakes and other freshwater bodies could achieve major improvements just by targeting phosphorous pollution, also from fertilizer. But now they routinely face toxic blue-green algae (or cyanobacteria) blooms, fueled by nitrogen pollution. That problem is being exacerbated, Paerl and his co-authors argued in a study last year, by warmer temperatures and increased rainfall associated with climate change. Efforts by water quality managers to protect the water supply may not work in the future, they wrote, because climate change introduces so many new uncertainties about hydrology, stratification, and nutrient dynamics.\n\nThese toxic algae blooms have become alarmingly widespread in recent decades, according to Paerl. One such bloom in the western end of Lake Erie forced Toledo, Ohio, to cut off the water supply temporarily to 500,000 residents in 2014. The same thing happened in China\u2019s Lake Taihu in 2007, leaving 2.3 million people without water. The threat to human health was not hypothetical. Blue-green algae toxins in the drinking water at a dialysis center in Brazil caused 76 deaths from acute liver failure in a 1996 incident. Those toxins have also caused liver damage in children drinking from China\u2019s Three Gorges Reservoir. In the United States, a 2015 study found evidence of blue-green algae blooms in 62 percent of the 3,100 U.S. counties surveyed and concluded that these blooms were \u201csignificantly related to the risk of non-alcoholic liver disease death.\u201d The problem with nitrogen is evident, finally, even on land. Atmospheric nitrogen \u2013 from intensive farming and livestock operations, power plants, road traffic, and other sources \u2013 now gets deposited everywhere, making soils more fertile. That has the paradoxical effect of reducing plant diversity by displacing native species adapted to nutrient-poor soils. A study last year in Proceedings of the National Academy of Sciences (PNAS) examined more than 15,000 forest, woodland, grassland, and shrubland sites across the United States and found that a quarter of them have already exceeded the nitrogen levels associated with species loss. Researchers don\u2019t know yet how nitrogen and climate change together will affect plant diversity. But in an experiment in an arid southern California habitat, added nitrogen together with changing rainfall patterns caused a community of native shrubs to shift to non-native grasses.\n\nFarmers are acutely aware of their leading role in this unfolding disaster. In Europe, they have managed to reduce nitrogen use substantially without any decrease in productivity over the past quarter century because of mandatory European Union limits. The United States has so far relied on a voluntary approach, with mixed results. But when the city of Des Moines, Iowa sued upstream farm counties two years ago for the cost of equipment to remove nitrogen runoff from its drinking water supply, many farmers heard alarm bells. (A federal court ultimately dismissed the lawsuit early this year.) \u201cI haven\u2019t seen a willingness to engage in a conservation program like this in my lifetime,\u201d says Nick Goeser, a soil scientist and director of the Soil Health Partnership. The issue resonates with farmers in part because applying nitrogen fertilizer accounts for up to half the cost of running a farm, and they would naturally prefer the expenditure to pay off in increased yield rather than have it wash away down the river. They recognize that nitrogen runoff is contaminating their own drinking water, says Goeser, and they have also noticed the effects of climate change on their crops.\n\nThe Soil Health Partnership, which combines agribusiness funding with technical advice from the Environmental Defense Fund and the Nature Conservancy, works to scale up three solutions to the nitrogen problem\u2014use of off-season cover crops to reduce the runoff that inevitably occurs when fields remain bare through the winter, low- or no-till farming, and \u201cadvanced nutrient management,\u201d or what Goeser describes as \u201cspoon-feeding\u201d nitrogen in the precise amount and time that the plant needs it. None of that is as simple as it may sound. For instance, use of cover crops \u201cmakes an incredible difference, with a 60-80 percent improvement in runoff,\u201d says Goeser. It\u2019s expensive, however, and could actually decrease corn or soybean yield the following year if the farmer does it wrong. It only starts to improve resilience to extreme weather events like flooding or drought, and thus yield, after three to five years. But in the Midwest, says Goeser, 60 percent of the acreage is operated on a one-year rental basis, meaning farmers have no incentive to invest in the long-term health of the land. Fewer than 5 percent of them plant cover crops.\n\nThe combined threat of climate change and nitrogen pollution could soon mandate far more dramatic changes in agriculture. Among the long-term solutions put forward by University of Victoria researchers in a companion piece to the new study in Science: Genetically-engineered cereals to fix nitrogen from the atmosphere, and laboratory cultured meat, to reduce the global herd from 1.5 billion head of cattle to a population of just 30,000 that will be used as stem-cell donors. Climate change means that it will be necessary, the co-authors note, to cut agricultural nitrogen use in the Mississippi River Valley not by 32 percent, as the U.S. Environmental Protection Agency now proposes, but by almost double that amount. The challenge will be far greater in the developing world, particularly Asia. The Stanford-led research team identified three risk factors that make an area more vulnerable to the compounding effects of nitrogen pollution and climate change: heavy nitrogen inputs (mostly for agriculture), a high current rate of precipitation, and a large projected increase in precipitation because of climate change. East, South, and Southeast Asia face the greatest peril, with India especially vulnerable \u201cbecause it exhibits all three risk factors across more than two-thirds of its area \u2026 and has one of the fastest-growing populations.\u201d People throughout the region \u201care heavily dependent on surface water supplies,\u201d the researchers note. But as climate change multiplies the rate of nitrogen runoff, they may increasingly find their water undrinkable. Correction, August 8, 2017: An earlier version of this article incorrectly attributed to Nick Goeser the statement that farmers recognize that nitrogen runoff is poisoning their drinking water. Goeser did not use the word poisoning.", "sentiment": 0.04801002219069447},
{"link_title": "To understand rising inequality, consider two janitors", "url": "https://www.nytimes.com/2017/09/03/upshot/to-understand-rising-inequality-consider-the-janitors-at-two-top-companies-then-and-now.html?mcubz=1", "text": "Eastman Kodak was one of the technological giants of the 20th century, a dominant seller of film, cameras and other products. It made its founders unfathomably wealthy and created thousands of high-income jobs for executives, engineers and other white-collar professionals. The same is true of Apple today.\n\nBut Kodak also created enough working-class jobs to help create two generations of middle-class wealth in Rochester. The Harvard economist Larry Summers has often pointed at this difference, arguing that it helps explain rising inequality and declining social mobility.\n\n\u201cThink about the contrast between George Eastman, who pioneered fundamental innovations in photography, and Steve Jobs,\u201d Mr. Summers wrote in 2014. \u201cWhile Eastman\u2019s innovations and their dissemination through the Eastman Kodak Co. provided a foundation for a prosperous middle class in Rochester for generations, no comparable impact has been created by Jobs\u2019s innovations\u201d at Apple.\n\nMs. Evans\u2019s pathway was unusual: Few low-level workers, even in the heyday of postwar American industry, ever made it to the executive ranks of big companies. But when Kodak and similar companies were in their prime, tens of thousands of machine operators, warehouse workers, clerical assistants and the like could count on steady work and good benefits that are much rarer today.\n\nWhen Apple was seeking permission to build its new headquarters, its consultants projected the company would have 23,400 employees, with an average salary comfortably in the six figures. Thirty years ago, Kodak employed about 60,000 people in Rochester, with average pay and benefits companywide worth $79,000 in today\u2019s dollars.\n\nPart of the wild success of the Silicon Valley giants of today \u2014 and what makes their stocks so appealing to investors \u2014 has come from their ability to attain huge revenue and profits with relatively few workers.\n\nApple, Alphabet (parent of Google) and Facebook generated $333 billion of revenue combined last year with 205,000 employees worldwide. In 1993, three of the most successful, technologically oriented companies based in the Northeast \u2014 Kodak, IBM and AT&T \u2014 needed more than three times as many employees, 675,000, to generate 27 percent less in inflation-adjusted revenue.\n\nThe 10 most valuable tech companies have 1.5 million employees, according to calculations by Michael Mandel of the Progressive Policy Institute, compared with 2.2 million employed by the 10 biggest industrial companies in 1979. Mr. Mandel, however, notes that today\u2019s tech industry is adding jobs much faster than the industrial companies, which took many decades to reach that scale.\n\nMany of the professional jobs from those companies in the 1980s and \u201990s have close parallels today. The high-paying positions setting corporate strategy, developing experimental technologies and shaping marketing campaigns would look similar in either era.\n\nBut a generation ago, big companies also more often directly employed people who installed products, moved goods around warehouses, worked as security guards and performed many of the other jobs needed to get products into the hands of consumers.\n\nIn part, fewer of these kinds of workers are needed in an era when software plays such a big role. The lines of code that make an iPhone\u2019s camera work can be created once, then instantly transmitted across the globe, whereas each roll of film had to be manufactured and physically shipped. And companies face brutal global competition; if they don\u2019t keep their work force lean, they risk losing to a competitor that does.\n\nBut major companies have also chosen to bifurcate their work force, contracting out much of the labor that goes into their products to other companies, which compete by lowering costs. It\u2019s not just janitors and security guards. In Silicon Valley, the people who test operating systems for bugs, review social media posts that may violate guidelines, and screen thousands of job applications are unlikely to receive a paycheck directly from the company they are ultimately working for.\n\nAnd the phenomenon stretches far beyond Silicon Valley, where companies like Apple are just a particularly extreme example of achieving huge business success with a relatively small employee count. The Federal Express delivery person who brings you a package may well be an independent contractor; many of the people who help banks like Citigroup and JPMorgan service mortgage loans and collect delinquent payments work for contractors; and if you call your employer\u2019s computer help desk, there\u2019s a good chance it will be picked up by someone in another state, or country.\n\nApple emphasizes that its products generate many jobs beyond those who receive a paycheck from the company directly. The company estimates 1.5 million people work in the \u201capp economy,\u201d building and maintaining the mobile applications used on Apple products. Apple stores in 44 states tend to offer more generous pay and benefits than is standard in the retail industry. And it is growing quickly, including far from its California headquarters, such as with 6,000 jobs in Austin, Tex., at an average salary of $77,000 a year, and many more indirectly through what it says is $50 billion in annual spending to suppliers in the United States.\n\n\u201cWe\u2019re responsible for creating over two million jobs in the U.S., across all 50 states,\u201d said an Apple spokesman, \u201cranging from construction, customer care, retail and engineering to app development, manufacturing, operations and trucking. All Apple employees, whether full time or part time, are eligible for benefits and stock grants. We\u2019re also fortunate to have skilled contractors that contribute to our products and services daily, along with over 9,000 suppliers in every state.\u201d\n\nPerhaps the biggest indictment of the more paternalistic approach taken by an earlier generation of corporate behemoths is that, Kodak, despite having been an early innovator in digital photography, is a shell of its former self. After a bankruptcy and many years of layoffs, the company now has only 2,700 employees in the United States and 6,100 worldwide.\n\nBut it is also clear that, across a range of job functions, industries and countries, the shift to a contracting economy has put downward pressure on compensation. Pay for janitors fell by 4 to 7 percent, and for security guards by 8 to 24 percent, in American companies that outsourced, Arindrajit Dube of the University of Massachusetts-Amherst and Ethan Kaplan of Stockholm University found in a 2010 paper.\n\nThese pay cuts appear to be fueling overall inequality. J. Adam Cobb of the Wharton School at the University of Pennsylvania and Ken-Hou Lin at the University of Texas found that the drop in big companies\u2019 practice of paying relatively high wages to their low- and mid-level workers could have accounted for 20 percent of the wage inequality increase from 1989 to 2014.\n\nThe same forces that explain the difference between 1980s Kodak and today\u2019s Apple have big implications not just for every blue-collar employee who punches a timecard, but also for white-collar professionals who swipe a badge.\n\nPhil Harnden was coming out of the Navy in 1970 when he applied for a job at Kodak, and soon was operating a forklift in a warehouse. He made $3 an hour, equivalent to $20 an hour today adjusted for inflation. That is roughly what an entry-level contracting job testing software pays.\n\nThe difference between the two gigs, aside from the absence of heavy machinery in Apple\u2019s sleek offices, is the sense of permanence. Mr. Harnden put in 16 years operating forklifts before he left in 1986 to move to Florida. When he returned 10 years later, he was quickly rehired and even kept his seniority benefits.\n\nIn interviews, tech industry contractors in Silicon Valley describe a culture of transience. They can end up commuting to a different office park that houses a new company every few months; in many cases 18 months is the maximum a contractor is allowed to spend at one company.\n\n\u201cI would rather have stability,\u201d said Christopher Kohl, 29, who has worked as a contractor at several Silicon Valley companies, including a stint doing quality assurance on Apple Maps. \u201cIt\u2019s stressful to find a new job every 12 to 18 months.\u201d\n\nFor Silicon Valley\u2019s contracting class, there are reminders large and small of their second-class status. Contractors generally do not receive the stock options that have made some midlevel Silicon Valley workers wealthy over the years, nor the generous paid time off for vacation, illness or the birth of a child. The health insurance plans tend to be stingier than those that the tech giants they serve provide for their direct employees.\n\nThe smaller reminders can be just as telling. One former Apple contractor recalled spending months testing a new version of Apple\u2019s operating system. To celebrate the release, the Apple employees they\u2019d worked closely with on the project were invited to a splashy party in San Francisco, while the contractors had beers among themselves in a neighborhood pub.\n\nThe compensation these white-collar contractors receive puts them squarely in the middle rungs of workers in the United States, and the most skilled can make six figures (though that doesn\u2019t go far in the hyper-expensive Bay Area housing market). Apple, based on its consultants\u2019 report, expected to be indirectly responsible for nearly 18,000 jobs in Santa Clara County by now at an average pay of about $56,000 a year.\n\nThere are some advantages. If they work for one of the companies like Apple or Google that feature a subsidized, high-quality cafeteria, contractors can enjoy the food. They can tell their friends that they work at one of the world\u2019s most admired companies, and enjoy predictable, regular hours. Once in a while, a contractor will be hired into a staff position.\n\n\u201cIt\u2019s not evil,\u201d said Pradeep Chauhan, managing partner of OnContracting, a site to help people find tech contracting positions. \u201cThey have a job and they\u2019re getting paid. But it\u2019s not ideal. The problem with contracting is, you could walk in one day and they could say, \u2018You don\u2019t need to come in tomorrow.\u2019 There is no obligation from the companies.\u201d\n\nAnd that is the ultimate contrast with the middle-skill, middle-wage jobs of earlier generations of titans \u2014 a sense of permanence, of sharing in the long-term success of the company.\n\n\u201cThere were times I wasn\u2019t happy with the place,\u201d Mr. Harnden said of his Kodak years. \u201cBut it was a great company to work for and gave me a good living for a long time.\u201d\n\nWhen an automaker needs a supplier of transmissions for its cars, it doesn\u2019t just hold an auction and buy from the lowest bidder. It enters a long-term relationship with the supplier it believes will provide the best quality and price over time. The company\u2019s very future is at stake \u2014 nobody wants to buy a car that can\u2019t reliably shift into first gear.\n\nBut when that same automaker needs some staplers for the office supply cabinet, it is more likely to seek out the lowest price it can get, pretty much indifferent to the identity of the seller.\n\nThe right product engineer or marketing executive can mean the difference between success or failure, and companies tend to hire such people as full-time employees and as part of a long-term relationship \u2014 something like the transmission supplier. What has changed in the last generation is that companies today view more and more of the labor it takes to produce their goods and services as akin to staplers: something to be procured at the time and place needed for the lowest price possible.\n\nThere is plenty of logic behind the idea that companies should focus on their core competence and outsource the rest. By this logic, Apple executives should focus on building great phones and computers, not hiring and overseeing janitors. And companies should outsource work when the need for staff is lumpy, such as for software companies that may need dozens of quality-assurance testers ahead of a major release but not once the product is out.\n\nThere\u2019s no inherent reason that work done through a contractor should involve lower compensation than the same work done under direct employment. Sometimes it goes in the other direction; when a company hires a law firm, it is basically contracting out legal work, yet lawyers at a firm tend to be paid better than in-house counsel.\n\nBut as more companies have outsourced more functions over more time, a strong body of evidence is emerging that it\u2019s not just about efficiency. It seems to be a way for big companies to reduce compensation costs.\n\nFirms in the United States are legally required to offer the same health insurance options and 401(k) match to all employees \u2014 meaning if those programs are made extra generous to attract top engineers, a company that doesn\u2019t outsource will have to pay them for everyone.\n\nMore broadly, there are a whole set of social pressures and worries about morale that encourage companies to be more generous with pay and benefits for employees who are on the same payroll.\n\n\u201cThere was an overriding concern about equity,\u201d said Mr. Cobb, the Wharton professor. \u201cFirms would try to set pay so that the gap between the security guard or administrative assistant and senior V.P. wasn\u2019t as great as you might expect, essentially by paying lower- and middle-skill workers more than they were probably worth on the market.\u201d\n\nLinda DiStefano applied for a secretarial job at Kodak during Easter week of her senior year in high school in 1968, and was hired to start immediately after her graduation for $87.50 a week, today\u2019s equivalent of $32,000 a year. She put in four decades at the company, first as a secretary, then helped administer corporate travel and other projects.\n\n\u201cI helped put on the dinners for the board of directors, which in retrospect someone of my grade shouldn\u2019t have been doing,\u201d she said. \u201cBut I had a series of managers who trusted me.\u201d\n\nIt bought her a house off Lake Avenue, a new car every few years and occasional long-distance trips to Motown reunion concerts. When her department was abolished in 2008, the travel bookings contracted out, she was making $20 an hour. The best job she could find was as a pharmacy technician at a grocery store for $8.50 an hour.\n\nAs you drive around Rochester, the role of Eastman Kodak in the city is evident everywhere, in the Kodak Tower that looms over the center of town, in the Eastman Theater on Main Street, and in the hulking buildings and empty parking lots of the manufacturing center known as Kodak Park.\n\nIn reading the company\u2019s old annual reports, you get a sense that its executives thought of the jobs created and the wages paid as a source of pride and achievement. On the first page of most years\u2019 annual reports was an accounting of how many employees the company had in the United States and worldwide, and the total pay and benefits they received.\n\nApple, with a spaceship-like campus about to open, looms large over Cupertino in its own way, accounting for something like 40 percent of the jobs in the city, and investing $70 million in local environmental and infrastructure upgrades. It is no middle-class enclave; the median home price is $1.9 million.\n\n\u201cWe definitely feel a sense of pride to be the home of Apple,\u201d said Savita Vaidhyanathan, the mayor of Cupertino. \u201cBut they consider themselves a global company, not necessarily a Cupertino company.\u201d She said she has never met Tim Cook, Apple\u2019s chief executive. \u201cWe would have a hard time getting an audience with anybody beyond upper-middle management,\u201d she said.\n\nMs. Ramos, the Apple janitor, lives down the road in San Jose. She pays $2,300 monthly for a two-bedroom apartment where she and her four children live. Before overtime and taxes, her $16.60 an hour works out to $34,520 a year. Her rent alone is $27,600 a year, leaving less than $600 a month once the rent is paid. Overtime, in addition to the wages from one of her teenage children who works part time at a grocery store, helps make the math work, though always tenuously.\n\nShe works from 6 p.m. until 2 a.m. On days when one of the other cleaners doesn\u2019t show up, she may get a few extra hours, which is great for the overtime pay, but it means even less sleep before it is time to take her children to school.\n\nA 60-cent-an-hour raise this year negotiated by her union, the SEIU United Service Workers West, helped. Kodak in its prime was not unionized, though in that era when organized labor was more powerful over all, managers deliberately kept wages high to try to prevent the company\u2019s workers from forming one.\n\nThere is little chance for building connections at Apple. \u201cEveryone is doing their own thing and has their own assignment, and we don\u2019t see each other outside of work,\u201d said Ms. Ramos in Spanish.\n\nMs. Evans, who was a Kodak janitor in the early 1980s before her rise to executive there and at other leading firms like Microsoft and Hewlett-Packard, recalls a different experience.\n\n\u201cOne thing about Eastman Kodak is they believed in their people,\u201d said Ms. Evans, now chief information officer at Mercer, the human resources consulting giant. \u201cIt was like a family. You always had someone willing to help open a door if you demonstrated that you were willing to commit to growing your skills and become an asset that was valuable for the company.\u201d\n\nThe shift is profound. \u201cI look at the big tech companies, and they practice a 21st-century form of welfare capitalism, with foosball tables and free sushi and all that,\u201d Rick Wartzman, senior adviser at the Drucker Institute and author of \u201cThe End of Loyalty,\u201d said. \u201cBut it\u2019s for a relatively few folks. It\u2019s great if you\u2019re a software engineer. If you\u2019re educated, you\u2019re in command.\u201d\n\nBut in the 21st-century economy, many millions of workers find themselves excluded from that select group. Rather than being treated as assets that companies seek to invest in, they have become costs to be minimized.", "sentiment": 0.15036685085465573},
{"link_title": "The day Steve Jobs dissed me in a keynote (2010)", "url": "https://sivers.org/itunes", "text": "In May 2003, Apple invited me to their headquarters to discuss getting CD Baby\u2019s catalog into the iTunes Music Store.\n\niTunes had just launched two weeks before, with only some music from the major labels. Many of us in the music biz were not sure this idea was going to work. Especially those who had seen companies like eMusic do this exact same model for years without big success.\n\nI flew to Cupertino thinking I\u2019d be meeting with one of their marketing or tech people. When I arrived, I found out that about a hundred people from small record labels and distributors had also been invited.\n\nWe all went into a little presentation room, not knowing what to expect.\n\nThen out comes Steve Jobs. Whoa! Rock star.\n\nHe was in full persuasive presentation mode. Trying to convince all of us to give Apple our entire catalog of music. Talking about iTunes success so far, and all the reasons we should work with Apple.\n\nHe made a point of saying, \u201cWe want the iTunes Music Store to have every piece of music ever recorded. Even if it\u2019s discontinued or not selling much, we want it all.\u201d\n\nThis was huge, because until 2003, independent musicians were always denied access to the big outlets. For Apple to sell all music, not just music from artists who had signed their rights away to a corporation, this was amazing!\n\nThen they showed us the software we\u2019d all have to use to send them each album. The software required us to put the audio CD into a Mac CD-Rom drive, type in all of the album info, song titles and bio, then click [encode] for it to rip, and [upload] when done.\n\nI raised my hand and asked if it was required that we use their software. They said yes.\n\nI asked again, saying we had over 100,000 albums, already ripped as lossless WAV files, with all of the info carefully entered by the artist themselves, ready to send to their servers with their exact specifications.\n\nThe Apple guys said, \u201cSorry, you need to use this software; there is no other way.\u201d\n\nUgh. That means we have to pull each one of those CDs off of the shelf again, stick it in a Mac, then cut-and-paste every song title into that Mac software. But so be it. If that\u2019s what Apple needs, OK.\n\nThey said they\u2019d be ready for us to start uploading in the next couple weeks.\n\nI flew home that night, posted my meeting notes on my website, emailed all of my clients to announce the news, and went to sleep.\n\nWhen I woke, I had furious emails and voicemails from my contact at Apple.\n\n\u201cWhat the hell are you doing? That meeting was confidential! Take those notes off your site immediately! Our legal department is furious!\u201d\n\nThere was no mention of confidentiality at the meeting and no agreement to sign. But I removed my notes from my site immediately, to be nice. (You can still see a copy someone posted here.)\n\nAll was well, or so I thought.\n\nApple emailed us the iTunes Music Store contract. We immediately signed it and returned it the same day.\n\nI started building the system to deliver everyone\u2019s music to iTunes.\n\nI decided we\u2019d have to charge $40 for this service, to cover our bandwidth and payroll costs of pulling each CD out of the warehouse, entering all the info, digitizing, uploading, and putting it back in the warehouse.\n\n5000 musicians signed up in advance, each paying $40. That $200,000 helped pay for the extra equipment and people needed to make this happen.\n\nWithin two weeks, we got contacted by Rhapsody, Yahoo Music, Napster, eMusic, and more - each saying they wanted our entire catalog.\n\nMaybe you can\u2019t appreciate this now, but the summer of 2003 was the biggest turning point that independent music has ever had. Until that point, almost no big business would sell independent music.\n\nBy iTunes saying they wanted everything, then their competitors needing to keep up, we were in! Since the summer of 2003, all musicians everywhere can sell all their music in almost every outlet online. Do you realize how amazing that is?\n\nBut there was one problem.\n\niTunes wasn\u2019t getting back to us.\n\nYahoo, Rhapsody, Napster and the rest were all up and running. But iTunes wasn\u2019t returning our signed contract.\n\nWas it because I posted my meeting notes?\n\nNobody at Apple would say anything. It had been months. My musicians were getting impatient and angry.\n\nI gave optimistic apologies, but I was starting to get worried, too.\n\nA month later, Steve Jobs did a special worldwide simulcast keynote speech about iTunes.\n\nPeople had been criticizing iTunes for having less music than the competition. They had 400,000 songs while Rhapsody and Napster had over 2 million songs. (Over 500,000 of those were from CD Baby.)\n\nFour minutes in, he said something that made my pounding heart sink to my burning stomach:\n\n\u201cThis number could have easily been much higher, if we wanted to let in every song. But we realize that record companies do a great service. They edit! Did you know that if you and I record a song, for $40 we can pay a few of the services to get it on their site, through some intermediaries? We can be on Rhapsody and all these other guys for $40? Well we don\u2019t want to let that stuff on our site! So we\u2019ve had to edit it. And these are 400,000 quality songs.\u201d\n\nI\u2019m the only one charging $40. That was me he\u2019s referring to!\n\nShit. OK. That\u2019s that. Steve changed his mind. No independents on iTunes. You heard the man.\n\nI hated the position this put me in.\n\nEver since I started my company in 1998, I had been offering an excellent service. I could make promises and keep them, because I was in full control.\n\nNow, for the first time, I had promised something that was out of my control.\n\nSo it was time to do the right thing, no matter how much it hurt.\n\nI decided to refund everybody\u2019s $40, with my deepest apologies. With 5000 musicians signed up, that meant I was refunding $200,000.\n\nSince we couldn\u2019t promise anything, I couldn\u2019t charge money in good conscience.\n\nI decided to make it a free service from that point on.\n\nThe very next day, we got our signed contract back from Apple, along with upload instructions.\n\nWe asked, \u201cWhy now?\u201d, but got no answer.\n\nI quietly added iTunes back to the list of companies on our site.\n\nBut I never again promised a customer that I could do something beyond my full control.", "sentiment": 0.1312522210376688},
{"link_title": "A Failed Attempt to Use Logistic Regression to Distinguish Between Cats and Dogs", "url": "http://thevivekpandey.github.io/posts/2017-09-09-using-logistic-regression-to-distinguish-between-cat-and-dog-images.html", "text": "I recently tried to apply logistic regression to distinguish between images of a cat from the images of a dog. While I was unsuccessful in creating a good model, I am noting down the findings for future reference.\n\nIn the deep learning course on coursera, in week 2 assignment, we built a logistic regression model to distinguish cat images from non cat images. We trained our model on 209 images, each of which were 64x64. We obtained an accuracy of 99%+ on training set. Our test set consisted of 50 images, each of which were 64x64. We achieved an accuracy of 70% on the test set. Each image was either a cat image, or a non cat image (in which case it could be buildings, or a flower, or bottles, etc)\n\nI wanted to try out logistic regression to some other problem than the assignment problem. I found out this Kaggle Problem. The training set consists of 25000 images each of which is either a cat image or a dog image. The test set consists of 12,500 image which you need to classify as either cat image or dog image. I thought this is a problem very similar to what I have done in the assignment, and should be solvable by logistic regression.\n\nSince the input images were of various sizes, and I do not know how to deal with variable size images, I resized them all to 200x200 size, padding them with white strips if necessary. This is what a few images look like:\n\nI ran logistic regression model on 500 cat images and 500 dog images. To my surprise, all the weights came close to 1 (larger than 0.9999), and thus all predictions are that the the image is a cat.\n\nHere is the model that I use: \n\n $\\hat{y} = \\sigma(w^{T}x + b)$ \n\n where $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n\nHere $x$ is the feature vector. I use gradient descent to find weights $w$ and $b$. All the components of $w$ turn out to be close to 1.\n\nI need to figure out if I am commiting some error in coding, or is there some inherent limitation of 1 layer neural network here. Any points from readers would be appreciated. I'll upload a docker image if I find time. I have put my code here.", "sentiment": 0.03226495726495726},
{"link_title": "Startups are finally taking on food stamps", "url": "https://www.wired.com/story/startups-are-finally-taking-on-food-stamps", "text": "Felicia Graybill uses her smartphone for everything: sending email, checking Facebook, and even monitoring her bank account. But for years, when the 28-year-old Brooklyn mom went to check on her food stamps benefits she might as well have been using a landline. Reviewing her balance required dialing into a hotline and entering her entire card number. All she could access was the sum of her funds\u2014there was no way of breaking down how and when she\u2019d spent the money.\n\nThere is an endless variety of apps designed to manage life for the upper middle class, but low-income Americans\u2014a group that spends a disproportionate amount of its budget on basic necessities\u2014don\u2019t benefit from the same time-saving hacks. Registering for housing assistance or putting food on the table requires reams of paperwork, DMV-esque long waits, and thorough interviews\u2014many of which have to be completed in person. Apps have inarguably made life easier, but only for those who already have resources to spare.\n\nThanks to new trends in civic technology, that\u2019s beginning to change. Young tech workers have increasingly noticed the wide-open opportunity to bring disruption to outdated social programs. With a user base of nearly 43 million Americans, the Supplemental Nutrition Assistance Program (SNAP), which provides food stamps, is ripe for innovation.\n\nTake Graybill. Now she uses FreshEBT, an app launched by mobile software startup Propel. The app allows her to check her balance on her phone and organize her budget around local deals using an online shopping list. Keeping tabs on her account lets her know what she can and can\u2019t buy while she shops. \u201cIf I need to check my balance outside the grocery store, now I can just privately look on my phone,\u201d says Graybill.\n\nWith 250,000 active users, FreshEBT is primed for growth\u2014and in April, Propel announced $4 million in seed funding from big names like Andreessen Horowitz. Yet Propel is just one of several companies stepping in to up-level the technology for public sector services. Some states, such as Texas and New York, have managed to roll out new websites and apps that help welfare recipients manage their benefits. But not all of their peers have innovated as quickly. At the state level, where most social services (including SNAP) are implemented, there isn\u2019t enough money and technical know-how to keep digital communications up to date. Though organizations like the US Digital Service and 18F pioneered building technical infrastructure at the federal level, thanks to the hiring freezes, budget cuts, and high profile exits that have occurred during the Trump administration, they face an uncertain future.\n\nThis innovation is shifting to the private and nonprofit sectors, where companies are using both classic venture capital models and more traditional forms of state contracting to significantly improve civic technology. Code for America, a nonprofit founded in 2009 to connect developers and designers with local and state governments, had previously tackled community problems like building an app to identify the fire hydrants that needed shoveling after snowstorms. But its leaders turned their attention to the SNAP program after finding it in need of a revamp. California has one of the lowest SNAP enrollment numbers in the country (in 2014 nearly half of eligible residents weren\u2019t receiving benefits); through one of the organization\u2019s fellowships, Code for America developers started working on a mobile web application in 2013.\n\nThe early problems weren\u2019t just technical. For example, to get more people to apply, they cut the 200-question application to a simple 10-minute questionnaire. \u201cAt a high level, a lot of people in government are constantly looking at how to do things better,\u201d says Dave Guarino, senior software engineer at Code for America. \u201cWhat we bring is a way to test things on a smaller scale, rather than just plan, plan, plan, and try to take a year to come up with a perfect solution.\u201d\n\nWorking closely with local community organizations and actual clients using the app allowed the team to find other barriers quickly. Program administrators send out text reminders, in lieu of the paper mail that\u2019s often ignored, when it\u2019s time to send in renewal materials. An in-house bilingual team at Code for America also answers client questions via SMS.\n\nThe tech behind the new crop of food stamp apps may be simple, but the user testing was not. Code for America developers also had to deal with local government bureaucracies often laden with outdated or unnecessary barriers. For instance, Code for America employees convinced one county director to eliminate an additional form they hadn\u2019t seen any other county use. The director didn\u2019t even know the practice was still in place.\u201cWhen you\u2019re working with many different layers of government, you can often end up with friction,\u201d says Sarat Mayer, Chief Program Officer at Code for America.\n\nPropel founder Jimmy Chen, who cut his teeth as a product manager at Facebook before joining the social-impact incubator Blue Ridge Labs, initially set out to change the enrollment process. But when canvassing Philadelphia grocery stores, his team found that many of its potential clients\u2014around 80 percent, Chen says \u2014were already signed up for benefits. Like Graybill, the problem was in using the program: They didn\u2019t have a way to check their balance without a lengthy call process. So the company pivoted to its current form: apps that functions like mobile-banking for EBT benefits.\n\nLyndon Jackson founded Panacea Financial, a smartphone app, with the same purpose\u2014giving EBT users a way to monitor their benefits on their phones\u2014but his motivations are much more personal. Both he and his lead developer grew up on food stamps, so his initial product testing involved family and friends. \u201cWhen I was talking to some of these people, it was very exciting because basically they had been ignored,\u201d says Jackson. After receiving $25,000 in seed money from a Chicago pitch competition, the company is still testing. But Jackson is optimistic about its ability to grow quickly, citing relationships with other local civic ventures like MRelief, an app that connects users with community services via SMS.\n\nPropel\u2019s founder hopes to make money by selling advertising on the app to grocery stores. FreshEBT already boasts coupon and rewards partnerships with several major chains. In contrast, Code for America now supports its CalFresh work with a combination of government funding and philanthropic dollars. It recently signed a two-year contract with the California state government for $3.6 million to continue work on the program. Given the instability of both government funding and early startups, it\u2019s hard to say if either model is more tenable than the other.\n\nThe FreshEBT has also had a significant impact on its users. In a study with Duke University, Propel found that average SNAP recipients spent more than 80 percent of their SNAP benefits within the first nine days, completely exhausting the sum by day 21. But when given an in-app tool that showed them a weekly budget instead of the entire balance, users stretched their monthly balance by two days\u2014about six meals a month.\n\nMobile presents a big opportunity to innovate for low-income users. According to the Pew Research Center, 10 percent of Americans are \u201csmartphone dependent,\u201d meaning they have no other form of internet access at home. Americans making less than $30,000 are 13 times more likely to be smartphone-dependent than those making more than $75,000 a year. CalFresh developers found that one third to about one half of searches for the application came from mobile phones, a platform on which the old site was unusable.\n\n\u201cOne of the things we noticed is that the food stamp office is full of hundreds of people waiting in line and a majority of them have a smartphone in their hand,\u201d says Chen. \u201cSocial services had in some ways lagged behind what technology could do.\u201d\n\nCorrection: This article originally stated that Panacea Financial received $5,000 from a Chicago pitch competition. The company in fact received $25,000.", "sentiment": 0.054909812409812395},
{"link_title": "The (full) story of my first Burn", "url": "https://dlet.me/the-story-of-my-first-burn-80383d1c5ad5", "text": "I was hungry this morning. I took 3 breakfasts. We also have 2 new friends in our camp: Toru is from Japan and Marijn from the Netherlands. They already know each other. They tried to install a shaded area above all our tents but it failed.\n\nI felt dirty so I went to a camp to clean my foot. As most of the things in Black Rock City, when you receive, you are encouraged to give. It feels great. However, I don\u2019t feel as spontaneous as the days before. Tiredness maybe. When I go back, the shaded area is ready! These guys rock! It is really cool!! We spend some time to enjoy it.\n\nAfter a few hours of rest, I join Marijn at a Shibari class (Japanese bondage). The others could not make it. It is surprisingly a lot less sexually charged than what I imagined. Girls keep their clothes on and there is a lot of respect. It is body art. After this, Marijn invites me to a foam party where an artist she likes is supposed to paint. I don\u2019t know where I am going but well, it is Burning Man, so I follow. We wait in line at a non-binary camp and we enter a room with several hundred people, men and women, naked. Whaou, I was not exactly expecting that. Now that we are here, we get naked and climb some stairs to get our foam shower. It feels great! And the atmosphere is impressive too. Again, there is a ton of respect and kindness in the air. It is an amazing experience.\n\nI come back to the camp alone. On my way, I find a bar. Well, not really. The barman keeps yelling at everybody \u201cWe are not a bar, don\u2019t stop if you want a drink\u201d. I stop and begin to play his game. So no, I definitely don\u2019t want a drink, I don\u2019t have my ID and my cup and actually, I don\u2019t even drink alcohol. He gives me a drink and we have a lot of fun talking!\n\nI join Pauline at the camp. We decide to join Marijn again to watch a Shibari performance in the middle of the Playa. It is close to a net structure that you can climb. Again, amazing view and sunset from there. I will never have enough. The show is quite beautiful but disturbing. The traditional Japanese music is amazing. But the performance is a bit brutal and shows women as victims. I don\u2019t like this. Not my thing. I felt much more respect during the class.\n\nWhile coming back to sleep, we stop by the Temple and the Man. In the Temple, there are a lot of pictures and notes for deceased people. It is a powerful and peaceful place. The Man is beautiful too. The theme this year is Radical Ritual so he is inside his own temple. We get some hot dogs on our way back and chill a bit at our camp before sleeping.", "sentiment": 0.17758143939393936},
{"link_title": "ICO Wizard \u2013 open source tool to create token and crowdsale contracts in 5 steps", "url": "https://github.com/oraclesorg/ico-wizard", "text": "A quote: ICO tools should be available for non-coders for free. Raising funds from a crowd is our basic human right.\n\nICO wizard is a tool to create token and crowdsale contracts in five simple steps. Wizard is based on TokenMarket contracts. Wizard is baked how we like it: decentralized, client side, serverless, open source, free, awesome.\n\nICOs usually have two or more contracts. One token contract and one or more crowdsale contract plus supplemental contracts, e.g., safe math, pricing strategy, etc. Most token contracts are the same (ERC-20); most crowdsale contracts are different. Token implementation should be stable for compatibility, and it is crucial to connect token to exchanges and wallets. Crowdsale contracts on another side should follow fashion and differentiate a project from others, e.g., create a new type of FOMO, fear of missing out.\n\nYou can try it on https://wizard.oracles.org or watch a demo\n\nAt the moment, the Wizard supports one type of crowdsale contract, the mighty \"whitelisted with tiers and cap\". This strategy is popular in modern ICOs due to regulatory involvement in the process (September 2017). A native approach to comply with regulation is to perform KYC of buyers and restrict participants from democratic countries, e.g., the U.S. or PRC.\n\nFeatures of \"Whitelisted with tiers and cap\" strategy:\n\nFor dev purposes you can build and run ICO Wizard on your machine.\n\nGo to localhost:3000 and look around the app!\n\nNone! Send PR if you are the first.\n\nIssues which are looking for a handsome contributors are marked as LookingForContributor label in Issues section of the GitHub\n\nBrought to you by Oracles Network team.\n\nWe appreciate contributors from the community:", "sentiment": 0.24173205266955267},
{"link_title": "TeX Tables: How TeX Calculates Spanned Column Widths", "url": "https://www.overleaf.com/blog/527-tex-tables-how-tex-calculates-spanned-column-widths", "text": "This is a relatively lengthy post so we have included a table of contents to help you navigate it.\n\nIn this article we explore how \\(\\mathrm\\TeX\\) calculates table column widths when tables contain entries (e.g., table headings) that span multiple columns (e.g., using the \\(\\mathrm\\TeX\\) primitives and ). Using a basic \u201creference\u201d table as a starting point, we create a range of examples\u2014derived from that reference table\u2014by amending various entries to create spanned columns. By examining the effect of those alterations we can start to develop an understanding of the underlying algorithm that \\(\\mathrm\\TeX\\) uses to calculate the width of spanned columns.\n\nTo examine and explain how \\(\\mathrm\\TeX\\) itself decides the widths of spanned columns it is necessary to eschew any of the marvellous \\(\\mathrm\\LaTeX\\) table packages and revert back to fundamental, low-level (primitive), table-creation commands: in particular, , and . Existing \\(\\mathrm\\TeX\\)/\\(\\mathrm\\LaTeX\\) table packages are of course essential productivity tools and provide a wealth of extremely useful functionality which enables users to quickly produce a vast range of tabular material using \\(\\mathrm\\LaTeX\\). Those packages provide essential \u201cmacro scaffolding\u201d constructed around \\(\\mathrm\\TeX\\)\u2019s low-level behaviour and their developers provide very welcome abstractions and layers of insulation which take care of the underlying complexities. Many of those packages are truly incredible feats of complex \\(\\mathrm\\TeX\\) programming: we should all be grateful that they exist to shield us from having to use raw \\(\\mathrm\\TeX\\)!\n\nThe actual algorithm that \\(\\mathrm\\TeX\\) uses to calculate spanned column widths is explained on page 245 of the \\(\\mathrm\\TeX\\text{book}\\) and, with further details, in Section 801 (page 336) of the printed book containing \\(\\mathrm\\TeX\\)\u2019s source code \\(\\mathrm\\TeX\\text{: The Program}\\). However, for many people (myself included) Knuth\u2019s explanations are, on occasion, rather compact and succinct and, at times, they can be difficult to follow in detail: illustrated examples are always very helpful.\n\nIn section 768 (page 322) of the book \\(\\mathrm\\TeX\\text{: The Program}\\), Knuth makes an interesting comment:\n\nIn addition, Volume IV of the four-volume book series \\(\\mathrm\\TeX\\text{ in Practice}\\) dedicates no fewer than 180 pages (pp199\u2013379) to creating tables in \\(\\mathrm\\TeX\\)via and .\n\nSo, it is safe to observe that \\(\\mathrm\\TeX\\) tables are indeed \u201crather tricky\u201d.\n\nAs noted, to explore \\(\\mathrm\\TeX\\)\u2019s column-width calculations we need to use \u201craw\u201d \\(\\mathrm\\TeX\\); what this means is a combination of primitive commands and one \\(\\text{Plain }\\mathrm\\TeX\\) macro called . Although we won\u2019t be using these commands to directly illustrate our example tables (i.e, fully explaining all the \\(\\mathrm\\TeX\\) code), it is worth including a brief note explain them:\n\nIn essence, to span columns \\(\\mathrm\\TeX\\) ignores the appropriate number of table preamble templates and combines the required number of table entries into a single entry. works by expanding to the sequence of and tokens needed to span columns. For example, expands to .\n\nHere is our reference table followed by an annotated version which explains the elements used in its construction:\n\nBy amending our reference table we will observe what happens to the table\u2019s width, and the width of individual columns, as we add entries that span various columns. This reference table was produced in raw \\(\\mathrm\\TeX\\) using the primitive together with a number of custom macros required to typeset the tables\u2014we won\u2019t discuss those macros because they are not essential to understanding the examples and explanations.\n\nHere is an annotated version of our reference table to explain its features:\n\nOur first set of example tables, and initial reference table, have all set so that \\(\\mathrm\\TeX\\) does not add any space between our columns: in effect, they all touch each other. The reason for doing this is to simplify the initial discussion and ensuing calculations\u2014later in the article we re-introduce non-zero glue to examine its effect on calculating spanned column widths.\n\nAs noted in the annotations, we have added a small amount of white space (5pt) at the start of all non-spanning table entries (except the first row). That 5pt of white space forms part of the total width of all non-spanning entries (except the first row) and was added just to make the table look a little less cluttered.\n\nThe command has three forms:\n\nWhen \\(\\mathrm\\TeX\\) typesets a table usingit has to read the entire table into memory to perform the various calculations required to typeset it. Consequently, unless you have specified the width by usingyou cannot know the final width until \\(\\mathrm\\TeX\\) has finished processing (typesetting) it. One way to obtain the width of a table produced byis to first typeset the table inside a(e.g.,) and then, for example, useto obtain the width.\n\nIt is important to note that when \\(\\mathrm\\TeX\\) is typesetting a table created with any text within table entries is not automatically subjected to line-breaking: table entries are typeset in restricted horizontal mode\u2014just like an . To enable line-breaking, a table entry\u2019s text needs to be enclosed inside a together with using an appropriate value for within that . Note, however, that text within a command (a \\(\\mathrm\\TeX\\) primitive) used in an is subject to \\(\\mathrm\\TeX\\)\u2019s line-breaking. In effect, and as its name suggests, allows \\(\\mathrm\\TeX\\) to \u201cescape\u201d from the and place material between the rows of the table\u2014typically to produce horizontal rules between table rows.\n\nYou cannot directly typeset an inside an . Attempting to use will generate a rather confusing error:\n\nDue to the enclosing \\(\\mathrm\\TeX\\) is in restricted horizontal mode; it then detects which is a vertical mode command. For example, if you use within a paragraph, \\(\\mathrm\\TeX\\) will terminate the paragraph, process the and then carry on with the rest of the paragraph.\n\nWhen used inside an , the triggers \\(\\mathrm\\TeX\\) to try to escape back to vertical mode by attempting to force closure of the current group: \\(\\mathrm\\TeX\\) reports a \u201c \u201d and issues an error because it thinks you\u2019ve made a mistake in your use of grouping. Although a right-hand brace ( ) may not be missing from your \\(\\mathrm\\TeX\\) code, the error message is a symptom of the \u201cgetting in the way\u201d and \\(\\mathrm\\TeX\\) taking its \u201cbest guess\u201d at the appropriate course of action to resolve the problem.\n\nThe following sequence of table graphics provide a range of examples to demonstrate the effect of spanning table columns: indicating that lengthy table entries can have unexpected results on the width of certain columns\u2014and, consequently, on the width of the table itself. The question we are going to address is what does \\(\\mathrm\\TeX\\) do when a particular table entry spans a number of columns but it is \u201ctoo wide to fit\u201d. As noted above, \\(\\mathrm\\TeX\\) does indeed apply a specific algorithm to this problem of calculating column widths: the following examples are designed to help develop a \u201cfeel\u201d for the workings of that algorithm.\n\nIn this example we use to span columns 1 and 2 with an entry whose text is A table heading:\n\nAs in Example table 1, this example also uses to span columns 1 and 2 but here we use a longer entry whose text is A slightly longer table heading.\n\nIf you compare this example to our reference table we can see the following:\n\nIn this example we use to span columns 1 to 3 with an entry whose text is the same as Example table 2: A slightly longer table heading.\n\nAre you starting to see a pattern emerging?\n\nAs with Example table 3, here we use to span columns 1 to 3 but this time with an entry whose text is considerably longer: A considerably longer table heading that extends a long way.\n\nIf we look at Example table 2 and Example table 4 we can see that in both cases it is the last column in the span which had its width increased to make space for the long entry which spanned the columns:\n\nThe following calculations give a clearer indication of what \\(\\mathrm\\TeX\\) is doing. Here is what we know:\n\nWhat is the difference between those values? It is \\(306.91216\\text{pt}-109.63355\\text{pt} = 197.2786\\text{pt}\\) and this is the width used for column 3: it arises directly from the algorithm used by \\(\\mathrm\\TeX\\).\n\nBefore we get to a more complicated example, here is one more \u201csimple\u201d example. This table contains the same lengthy entry as Example table 4: A considerably longer table heading that extends a long way; however, this time we use which allows that entry to span the entire table. As you can see, the resulting table still has the same width as our reference table (\\(327.71722\\text{pt}\\)) meaning that no columns have been affected by this very long entry. Clearly, this is because the width of the entry (\\(306.91216\\text{pt}\\)) is less than the total width of all the entries it is spanning: \\(327.71722\\text{pt}\\); i.e., the width of the table.\n\nHere, we look at a series of three example tables (6(a)\u20136(c)) to show the effect of two different entries which both span into column 5. Example table 6(a) and Example table 6(b) each show a table containing a single entry that spans several columns up to column 5. Example table 6(c) combines both spanning entries into a single table and asks the question: which entry actually determines the width of column 5, and why? The answer takes us to the essence of the algorithm used by \\(\\mathrm\\TeX\\).\n\nHere, we combine the entries in example tables 6(a) and 6(b) into a single table: what happens?\n\nTo understand the results of \\(\\mathrm\\TeX\\)\u2019s algorithm and decision-making processes we note that this entry\n\nextends beyond the entries being spanned by \\(25.60608\\text{pt}\\); however, this entry\n\nextends even further beyond the entries being spanned: by \\(43.3943\\text{pt}\\). Hence that entry \u201cwins the race\u201d and column 5 has its width increased by the maximum of these two values (\\(43.3943\\text{pt}\\)). The width of column 5 now becomes \\(59.6501\\text{pt} + 43.3943\\text{pt} = 103.0444\\text{pt}\\) to accommodate the entry which spans columns 3 to 5. Our description of the exact \u201csequence of events\u201d is simplified slightly but the outcome is as we have described.\n\nTo minimize the complexity of our discussions (thus far) we\u2019ve used relatively simple examples to demonstrate the principles of \\(\\mathrm\\TeX\\)\u2019s algorithm; in particular, we set . In practice, \u201creal world\u201d tables are likely to have many entries that span a range of columns and, of course, will have non-zero values for the glue\u2014a topic we\u2019ll now revisit.\n\nTable design often requires the addition of white space between columns and, of course, \\(\\mathrm\\TeX\\) has this facility through a primitive command called . This command can be used to put fixed or flexible glue (spacing):\n\nHere an example to remind ourselves:\n\nThe presence of non-zero glue between columns provides additional space that spanned entries can \u201cabsorb\u201d before \\(\\mathrm\\TeX\\) needs to think about increasing the width of the last column in a span.\n\nIn our next example we will use two tables to compare the results of spanning two columns. The only difference between the tables is the use of glue.\n\nWithin the modified reference table the two spanned columns have no effect on the column widths (and table width) but they do affect the width of column 2 (and table width) in the original reference table.\n\nHere, we show our original reference table together with a second table (derived from our original reference table) which has an entry \u201cTest a longer table heading\u201d spanning columns 1 and 2, Quite clearly, column 2 (of the second table in the diagram) and thus the whole table, are both affected by the spanned columns.\n\nHere, we show our modified reference table together with a second table (derived from our modified reference table) which also has an entry \u201cTest a longer table heading\u201d spanning columns 1 and 2, Quite clearly, within the second table in the diagram, neither the width of column 2 or the table is affected by the spanned columns. In this case, the presence of glue ( ) between the columns has helped to \u201cabsorb\u201d the space required by the text in the entry spanning colums 1 and 2:\n\nHopefully, the range of examples provided above have helped develop a \u201csense\u201d of what \\(\\mathrm\\TeX\\) does to accommodate spanned entries and how \\(\\mathrm\\TeX\\) will, if necessary, adjust the width of the last column within each range of spanned columns. In addition to the width of entries within individual columns being spanned, the presence of non-zero glue is an important factor that \\(\\mathrm\\TeX\\) takes into consideration when deciding whether it needs to adjust any column widths. The key point to remember is that \\(\\mathrm\\TeX\\)\u2019s goal is to calculate a suitable width for the last column within each range of spanned columns.\n\nIn this final example we once again use our modified reference table (with glues values discussed above) to derive another table which contains various columns spanned by rules\u2014we have used rules to make the spans easier to see.\n\nThe two tables have been carefully aligned to show that, in the upper table, no columns prior to column 5 have been affected by the spanned columns. The darker green area to the left of the diagram shows that columns 1 to 4 of both tables still align perfectly. On the right is a lighter-green shaded area showing that only columns 5 and 6 have been affected by the spanned entries.\n\nIn the upper table, the spans are as follows:\n\nOnce again, the explanation is that within a series of spanned columns, only the width of the final column is adjusted (if required): intervening columns are not affected and here that means columns 1 to 4\u2014although, of course, the width of columns 1 to 4 (and intervening glue) is taken into consideration when calculating the adjusted widths of columns 5 and 6.\n\nWe\u2019ll conclude with a simplified walkthrough of \u201c\\(\\mathrm\\TeX\\)\u2019s thought processes\u201d as it works out the widths of columns in spanned entries. Describing \\(\\mathrm\\TeX\\)\u2019s algorithms is not always straightforward so we\u2019ll adopt some \u201csimplifying artistic licence\u201d to provide an overview of what is happening. Readers interested in all the messy details are referred to Section 801 (page 336) of the printed book containing \\(\\mathrm\\TeX\\)\u2019s source code \\(\\mathrm\\TeX\\text{: The Program}\\).\n\nReal-world tables are often created with many uses of the primitive (e.g., inside \\(\\mathrm\\LaTeX\\) packages) to construct multiple instances of spanned columns within the table. To manage this, the data structures (deep inside \\(\\mathrm\\TeX\\)) maintain information (so-called span nodes) which tell \\(\\mathrm\\TeX\\) about the links (spans) between table entries/columns. Clearly, \\(\\mathrm\\TeX\\) has to apply its algorithms in a systematic way and will need to process the entire table to make its final calculations\u2014to determine all column widths, the total width of the table and, if required, the amount by which flexible glues used in the table have to stretch or shrink. It is not really surprising that \\(\\mathrm\\TeX\\) cannot tell you the final table width until it has completely processed the command\u2014it really has a lot of work to do!\n\nThe starting point for column-width calculations is column 1 because, of course, nothing can span from the left of (and across/into) column 1. \\(\\mathrm\\TeX\\) starts out by determining the width of column 1 by determining which entry that has the maximum natural width. Let\u2019s call that maximum width \\(w_1\\)and if there are entries that span from column 1 to column 2 let\u2019s call the width of that entry \\(w_{12}\\) (width from 1 to 2). In addition, we\u2019ll denote the glue between columns 1 and 2 as \\(t_{1}\\)\u2014note that we are only considering the natural width of that glue and, for now, ignoring any stretch or shrink components it might possess. Also, let the maximum natural width of all non-spanning entries in column 2 be \\(w_2\\).\n\nThe key point to note is that \\(\\mathrm\\TeX\\) is trying to calculate the width of column 2 by considering only those entries where the span starts with column 1 and ends at column 2. The key consideration for \\(\\mathrm\\TeX\\) is the test \\(\\max(w_{2}, w_{12} - (w_1+ t_1))\\)\u2014there might be multiple entries spanning columns 1 and 2: some may be narrow (small \\(w_{12}\\)), others very wide (large \\(w_{12}\\)) so \\(\\mathrm\\TeX\\) is looking for the one that has the greatest effect (hence the \\(\\max(\\text{...})\\)). Here, the value of \\(w_{12} -(w_1+ t_1)\\) is the amount by which an entry spanning columns 1 and 2 \u201cspills over\u201d from column 1 into column 2: note that \\(\\mathrm\\TeX\\) is using the width of column 1 and the glue (\\(t_{1}\\)) between columns 1 and 2. Once \\(\\mathrm\\TeX\\) has determined whether any spans from column 1 to 2 do affect the width of column 2 it sets the width of column 2 to the maximum value it has determined (using the test described). \\(\\mathrm\\TeX\\) continues to work its way through all the other columns, performing similar tests.\n\nAnd finally, just for completeness, here we quote the essence of \\(\\mathrm\\TeX\\)\u2019s algorithm for calculating column widths (taken from Knuth\u2019s source code documentation of \\(\\mathrm\\TeX\\)):\n\nLet \\(w_{ij}\\) be the maximum of the natural widths of all entries that span columns \\(i\\) through \\(j\\), inclusive. The final column widths are defined by the formula\n\nwhere \\(t_k\\) is the natural width of the tabskip glue between columns \\(k\\) and \\(k+1\\).\n\nAll \\(\\mathrm\\TeX\\) tables presented in this article are Scalable Vector Graphics (SVG) files produced on the Overleaf platform. The annotations (arrows and green boxes) were added by opening the SVG graphic in Inkscape\u2014note, however, that the text of the annotations was typeset in \\(\\mathrm\\TeX\\) as additional text to accompany the table: only the arrows and green backgrounds were added in Inkscape. If you are interested to know how this was achieved, read on.\n\nOverleaf\u2019s servers use the \\(\\mathrm\\TeX \\text{ Live}\\) distribution which, in addition to \\(\\mathrm\\TeX\\)-based typesetting engines, provides a wealth of very useful \\(\\mathrm\\TeX\\)-related software tools and utilities. Among those is one called which, as its name suggests, converts \\(\\mathrm\\TeX\\)\u2019s traditional DVI (DeVice Independent) output file format into SVG. Among its many command-line options provides an option ( or ) that will instruct it to convert all text into paths which means that the text in SVG graphics is drawn using lines and curves rather than actual fonts and glyphs. This may increase the file size of the resulting SVG graphic but it ensures that the SVG graphics are extremely portable and almost certain to work well on any device.\n\nIn a previous article I discussed how you can use \\(\\text{Lua}\\mathrm\\TeX\\) to run the various software tools and utilities installed on Overleaf\u2019s servers\u2014it is an extremely easy and convenient technique. That technique was used to generate SVG graphics of typeset \\(\\mathrm\\TeX\\) tables, as follows. From within the main \\(\\mathrm\\TeX\\) document file, the code to typeset each table (created using using ) was written to a file. This was achieved by enclosing the table code within a pair of commands which I called and . There are likely to be many other ways to achieve the desired results but here are the macro definitions that I used:\n\nYou use them like this:\n\nNote that the token merely serves to delimit the parameter of the macro: \\(\\mathrm\\TeX\\) effectively discards the token so we don\u2019t actually need to define it (e.g., by ).\n\nThe \\(\\mathrm\\TeX\\) code contained in the is saved into a register called . One tricky point I came across (with \\(\\text{Lua}\\mathrm\\TeX\\)) was needing to prevent characters within the preamble from being \u201cdoubled\u201d to when written out to a file. To avoid this I had to temporarily set the of characters to 12 prior to saving the \\(\\mathrm\\TeX\\) code (tokens) in the token register.\n\nThe next step is to write the tokens contained in as a \\(\\mathrm\\TeX\\) file\u2014because I was using \\(\\text{Lua}\\mathrm\\TeX\\) this proved to be extremely easy thanks to \\(\\text{Lua}\\mathrm\\TeX\\)\u2019s wonderful Lua API. In brief, I wrote a macro called which takes as its parameter the name of a token register whose tokens you want to write out to a file (e.g., ). Within the macro I used the Lua API to get a textual representation of the token register:\n\nHere is a screenshot showing a little more of the command:\n\nThe Lua language and the Lua API provided by \\(\\text{Lua}\\mathrm\\TeX\\) can often simplify \\(\\mathrm\\TeX\\) programming tasks and it is because of these useful and powerful features that I have used \\(\\text{Lua}\\mathrm\\TeX\\) since ~2009\u2014and remain a huge fan of this truly marvellous \\(\\mathrm\\TeX\\) engine. OK, the \\(\\text{Lua}\\mathrm\\TeX\\) advert now concludes.\n\nHaving so easily obtained the \\(\\mathrm\\TeX\\) code stored in it is written out to a file together with some additional code to make it into a correctly formed \\(\\mathrm\\LaTeX\\) file. The next steps are:", "sentiment": 0.0897063847960709},
{"link_title": "How to Use Fio to Measure Disk Performance in Linux", "url": "https://dotlayer.com/how-to-use-fio-to-measure-disk-performance-in-linux/", "text": "Fio which stands for Flexible I/O Tester is a free and open source disk I/O tool used both for benchmark and stress/hardware verification developed by Jens Axboe.\n\nIt has support for 19 different types of I/O engines (sync, mmap, libaio, posixaio, SG v3, splice, null, network, syslet, guasi, solarisaio, and more), I/O priorities (for newer Linux kernels), rate I/O, forked or threaded jobs, and much more. It can work on block devices as well as files.\n\nFio accepts job descriptions in a simple-to-understand text format. Several example job files are included. Fio displays all sorts of I/O performance information, including complete IO latencies and percentiles.\n\nIt is in wide use in many places, for both benchmarking, QA, and verification purposes. It supports Linux, FreeBSD, NetBSD, OpenBSD, OS X, OpenSolaris, AIX, HP-UX, Android, and Windows.\n\nIn this tutorial, we will be using Ubuntu 16 and you are required to have sudo or root privileges to the computer. We will go over the installation and use of fio.\n\nWe are going to clone the repo on GitHub. Install the prerequisites, and then we will build the packages from the source code. Lets\u2019 start by making sure we have git installed.\n\nFor centOS users you can use:\n\nNow we change directory to /opt and clone the repo from Github:\n\nYou should see the output below:\n\nNow, we change directory into the fio codebase by typing the command below inside the opt folder:\n\nWe can finally build fio from source using the `make` build utility bu using the commands below:\n\nFor Ubuntu and Debian, fio is available on the main repository. You can easily install fio using the standard package managers such as yum and apt-get.\n\nFor Ubuntu and Debian you can simple use:\n\nFor CentOS/Redhat you can simple use:\n\n On CentOS, you might need to install EPEL repository to your system before you can have access to fio. You can install it by running the following command:\n\nYou can then install fio using the command below:\n\nWith Fio is installed on your system. It\u2019s time to see how to use Fio with some examples below. We are going to perform a random write, read and read and write test.\n\nLet\u2019s start by running the following command. This command will write a total 4GB file [4 jobs x 512 MB = 2GB] running 2 processes at a time:\n\nWe are going to perform a random read test now, we will be trying to read a random 2Gb file\n\nYou should see the output below:\n\nFinally, we want to show a sample read-write test to see how the kind out output that fio returns.\n\nThe command below will measure random read/write performance of USB Pen drive (/dev/sdc1):\n\nBelow is the outout we get from the command above.\n\nWe hope you enjoyed this tutorial and enjoyed following along, Fio is a very useful tool and we hope you can use it in your next debugging activity. If you enjoyed reading this post feel free to leave a comment of questions. Go ahead and clone the repo and play around with the code.", "sentiment": 0.12194444444444447},
{"link_title": "A guide to filtered permeability [pdf]", "url": "https://cityinfinity.files.wordpress.com/2017/09/draft-filtered-permeability-guidance2.pdf", "text": "", "sentiment": 0.0},
{"link_title": "And now the names of Apple\u2019s new iPhones look to have leaked\u2026", "url": "https://techcrunch.com/2017/09/09/and-now-the-names-of-apples-new-iphones-look-to-have-leaked", "text": "We are but days away from the event of the Apple calendar year when the company pulls the curtain up on new iPhone models. And yet the leaks keep coming.\n\nThe latest juicy tidbit is what looks to be the official names of the three models Apple is rumored to be announcing next Tuesday \u2014 and they suggest Cupertino is skipping its usual \u2018S\u2019 generation convention for this release, as well as throwing in a curve ball christening for the most expensive model of the trio.\n\nThe top-of-the-range iPhone, which is expected to ditch the home button to make way for more screen real-estate, replacing the fingerprint-powered Touch ID with a facial recognition alternative, has been referred to as \u2018D22\u2019 in a leaked iOS 11 firmware build.\n\nBut according to a developer who\u2019s being sifting through the firmware the official name will be the iPhone X.\n\nWhich is at least not iPhone Ferrari, as was another of its rumored codenames.\n\nWhile the other two iPhone models are set to be named the iPhone 8 and iPhone 8 Plus, according to the leak, meaning Apple is eschewing an iterative iPhone 7s/7s Plus release \u2014 underlining how the company wants this release to be interpreted; i.e. not at all iterative.\n\nAt this point you do have to wonder whether Apple is retiring the iPhone \u2018s\u2019 cycle for good \u2014 something TC\u2019s Editor in Chief Matthew Panzarino was calling for back in 2015 at the time of the iPhone 6s and 6s Plus.\n\nAfter all, when you\u2019re asking people to shell out as much as $1,000 or more for their next smartphone \u2014 as has been the rumored price-tag for the incoming iPhone X \u2014 there\u2019s rather a lot of perceived value resting on a name.", "sentiment": 0.11537433155080212},
{"link_title": "The Story of Quillette: Taking Risks to Move the Culture Forward", "url": "https://www.psychologytoday.com/blog/more-mortal/201709/taking-risks-move-the-culture-forward", "text": "An interview with Claire Lehmann, founder of an online magazine for free thought", "sentiment": 0.4},
{"link_title": "Thousands of Nazis Marched in New York in 1939", "url": "https://warisboring.com/thousands-of-nazis-marched-in-new-york-in-1939/", "text": "On Aug. 11 and 12, 2017, hundreds of self-identified white nationalists staged a \u201cUnite the Right\u201d rally in Charlottesville, Virginia, where they were taped chanting anti-Jewish and anti-Black slogans.\n\nThis chilling display of hatred was unfortunately not unique in American history. In 1939 around 20,000 Nazis attended a rally in Madison Square Garden in New York City. But what happened next is equally instructive. More than twice as many counterprotestors opposed the march. And in less than a year, the Nazi leader was in jail and the group was effectively shut down.\n\nU.S. president Franklin Roosevelt opposed the rise of Nazism from the very beginning, calling it a \u201ccancer.\u201d So when a German immigrant named Heinz Spankn\u00f6bel formed the pro-Nazi Friends of a New Germany in 1933 in order to coerce German-American newspapers and organizations to support the Nazi party, it attracted unfriendly attention from the U.S. government.\n\nThe House committee on Un-American Activities began investigating the group, which had the blessing of Rudolf Hess. In less than a year\u2019s time, Spankn\u00f6bel had been deported for failing to report his status as an agent of a foreign government. Berlin ordered FONG to disband in 1935.\n\nThe following year Fritz Julius Kuhn, an autoworker at a Ford factory, founded the German American Bund in Buffalo, New York to take FONG\u2019s place. Born in Bavaria, Kuhn had been awarded an Iron Cross while serving in the Germany Army during World War I. After a stint in Mexico he had emigrated to the United States in 1928 and become a citizen six years later.\n\nSupposedly a cultural organization for Americans of German descent, the German-American Bund\u2019s primary mission was to promote Nazi discourse and practices. Kuhn ranted that Jews and Communists were an insidious threat to social order and racial purity. The Bund\u2019s thousands of members wore Nazi uniforms and performed the Hitler salute with an extended right arm.\n\nSeeking to boost his organization, Kuhn and 50 followers traveled to Germany and had their photo taken with Hitler during the Munich Olympics in 1936. Failing to speak at length with the German leader or his cronies, Kuhn nonetheless returned to the United States falsely claiming to have been enthusiastically received by numerous Nazi officials.\n\nDespite its attachment to Hitler, the Bund draped itself heavily in the American flag. Kuhn extolled George Washington as \u201cthe first fascist,\u201d claiming the revolutionary general to be a military \u201cman of action\u201d who \u201cknew democracy could not work.\u201d The Nazis also admired white supremacist movements such as the KKK in the United States. Indeed, Hitler had expressed admiration for how the KKK had \u201cpoliced\u201d racial purity through terrorism and murder targeting Black Americans.\n\nThe Bund extracted tremendous revenue from its thousands of members by requiring them all to purchase Nazi uniforms, copies of Mein Kampf and subscriptions to a monthly fascist newspaper. In return, the Bund organized Nazi-themed training camps with names like Camp Nordland, Hindenberg and Siegrief in states with heavy German-American populations, including Pennsylvania, New York, New Jersey and Wisconsin. The camps not only indoctrinated adults, but also inducted children into its version of the Hitler Youth.\n\nThe Bund wasn\u2019t just summer camps and fascist children\u2019s activities. Thousands of members participated in a paramilitary wing known as the Ordnungsdienst modeled on the thuggish Brownshirts in Germany. These Storm Troopers perpetrated attacks on Jewish businesses, and when Jewish-American veterans of World War I confronted them in counter-protests, violent scuffles often broke out.\n\nKuhn wished to use the Bund to pressure America into adopting a more pro-Nazi foreign policy and push back against boycotts on German goods due to Hiter\u2019s anti-Semitic policies. He particularly denounced Roosevelt, implying he was s Jew by calling him a \u201cRosenfeld\u201d and describing the New Deal as the \u201cJew Deal.\u201d\n\nBut Kuhn\u2019s crudely direct politics were actually embarrassing to the German foreign ministry, as they agitated anti-Nazi sentiment at a time that German diplomacy focused on smoothly obfuscating the brutal nature of Nazism so as to minimize American willingness to intervene in a European conflict. Kuhn\u2019s events inspired tens of thousands of counter-protesters to mobilize against the Nazis. As a result, Berlin refused to provide direct material or rhetorical support to the Bund.\n\nThe Bund\u2019s activism reached its peak on Feb. 20, 1939, when Kuhn organized an enormous rally in Madison Square Garden in New York City which attracted 22,000 supporters. This \u201cMass Demonstration of True Americanism\u201d was supposedly in honor of the birthday of George Washington. The Bund draped a giant banner depicting the first U.S. president in between American flags and Nazi swastikas.\n\nBetween 50,000 to 100,000 counter-protestors confronted the Nazi rally, including Jewish war veterans and anti-Nazi Germans. It took 1,700 NYPD officers, many mounted on horseback, to keep the groups apart, pushing back several attempts by the anti-fascists to storm the gathering. When a Jewish hotel worker named Isadore Greenbaum charged Kuhn as he gave his speech, Kuhn\u2019s storm troopers beat him up badly and tore his pants off to the cheers of the rally-goers; he only escaped with his life due to intervention of the NYPD. Kuhn himself managed to dodge the punch.\n\nThe fascist demagogue proceeded to give his typical hate speech denouncing the \u201cJewish threat to the white race,\u201d leading attendees in a Hitler salute, and describing Roosevelt as a pawn of capitalist Jewish financiers who, of course, were also secretly Moscow-backed Communists. Tellingly, Kuhn defined who qualified as \u201creal Americans\u201d and who didn\u2019t based on race.\n\n\u201cThe Bund is fighting shoulder to shoulder with patriotic Americans to protect America from a race that is not the American race, that is not even a white race.\u201d\n\nKuhn must have thought the rally a tremendous success. But in reality, it had won him powerful enemies. The FBI had already led an investigation seeking to verify if Kuhn was a foreign agent\u2014the charge which had caused the dissolution of FONG. However, Berlin had issued a statement in March 1938 forbidding German nationals from participating in the Bund. Prior to the ban, around one quarter of its members were German citizens.\n\nWhen Kuhn traveled to Berlin to protest the ban, he was shown the door.\n\nAs Kuhn was genuinely not acting at Germany\u2019s behest, it was implausible to try him for doing so. However, New York City mayor Fiorello La Guardia went fishing for other grounds by which the Bund leader could be held culpable\u2014and an audit revealed that Kuhn had embezzled $14,000 \u2014 that\u2019s nearly $245,000 in 2017 dollars \u2014 out of his own organization, in part to pay for his multiple mistresses!\n\nIronically, the members of the Bund reportedly felt that the illicit skimming was Kuhn\u2019s right as the group\u2019s Bundfuehrer. But New York district attorney Thomas Dewey pressed charges anyway, and by December 1939 Kuhn was being packed away to serve two and a half years in Sing Sing prison for tax evasion and embezzlement. The Bund gradually disintegrated in his absence, and the state arrested Kuhn\u2019s successor Gerhard Kunze as well as 24 other members in 1941 for seeking to undermine U.S. military conscription.\n\nDespite the frightening peak size of the Bund, it failed in its mission to bully most German Americans into supporting Nazism. Indeed, polls suggested they were not especially more likely to be pro-Nazi than other Americans were. Many prominent German-Americans such as journalist Dorothy Thompson\u2014a heckler at the Madison Square Garden rally\u2014would prominently denounce Nazism in the 1930s and \u201940s.\n\nWhile the German American Bund served as the overt face of American Nazism in during the 1930s, there were other prominent U.S. citizens sympathetic to fascism that promoted anti-Semitism and aided Hitler\u2019s foreign policy objective of keeping the United States neutral.\n\nHenry Ford, the industrialist who sold the affordable Model T to the masses, was infamous for financing the Dearborn Independent, an anti-Semitic newspaper whose hateful screeds were later published internationally in book form. Ford\u2019s activities earned him praise from Hitler in Mein Kamp, who described Ford as an \u201cinspiration.\u201d\n\nLater in 1938, a German consul awarded the American automaker the Grand Cross of the German Eagle, a medal reserved for foreigners sympathetic to the Nazi party.\n\nFord later attempted to distance himself from the key role his publication played in popularizing and legitimizing anti-Semitism across the world. However, he campaign against U.S. involvement in World War II under the guise of pacifism coded in anti-Semitic language \u2014 the war was part of the agenda of greedy \u201cforeign financiers.\u201d He even claimed that American ships torpedoed by German submarines prior to the outbreak of hostilities were sabotaged by a pro-war conspiracy.\n\nDespite Ford\u2019s supposed anti-war stance, the Ford Werke branch in Germany continued production of vehicles under American management even after Hitler began his military conquests, and used French slave laborers captured after the fall of France.\n\nAnother sympathizer was the aviator Charles Lindbergh, legendary for being the first person to fly non-stop from America to Europe in 1927. In the 1930s, German aviation officials warmly received Lindbergh so he could evaluate their latest warplanes, which he proceeded to describe in glowing detail to the American military. Like Ford, Lindbergh was awarded the German Eagle medal\u2014just two weeks before the Kristallnacht riots that killed around 100 Jews. Lindbergh refused to disown the medal, however, arguing it wouldn\u2019t be \u201cconstructive.\u201d\n\nLindbergh\u2019s diary entries demonstrated that he accepted many Nazi ideas about preserving white \u201cracial strength\u201d against \u201cAsiatic\u201d races\u2014and agreed that there was a \u201cJewish problem.\u201d It is true that Lindbergh also disapproved of Nazi violence, and saw himself as promoting American interests, not Germany\u2019s. During World War II, Lindbergh reenlisted in the U.S. military and flew combat missions in the Pacific Theater.\n\nNonetheless, prior to American entry in World War II the famous aviator chose to use his star power to actively campaign against American support for the Allied powers. He did so as a leading speaker of the America First Committee which sought to cut off American aid to England during the early years of World War II, before the U.S. became involved in open hostilities.\n\nMembers of the America First Committee had a variety of motivations. Some were dedicated pacifists or isolationists that wished nothing to do with oversea wars, no matter the justification. Joseph Kennedy, the father of the future U.S. president, was notoriously antisemitic and was an isolationist convinced Hitler would defeat the United Kingdom. Inconveniently, for the Roosevelt administration, the Kennedy was also the U.S. ambassador in London!\n\nOther American Firster\u2019s held varying degrees of sympathy for fascism, including the pioneering female aviator Laura Ingalls, a Nazi true believer who was arrested in December 1941 for receiving money from the German embassy to promote non-interventionism.\n\nEven American Communists were under orders from Moscow to support the America First Committee because of the Molotov-Ribbentrop Pact allying the Soviet Union with Nazi Germany. Of course, official support changed to opposition after the German invasion of the Soviet Union in June 1941.\n\nAnother influential American extremist was Father Charles Coughlin, a popular Catholic radio preacher who attracted tens of millions of listeners to his fiery screeds denouncing Jewish financiers as Communist agents. The Irish Catholic expressed admiration for Hitler and Mussolini and vehemently criticized Roosevelt\u2019s efforts to support European democracies against Nazi Germany. Coughlin undermined his own occasional equivocations about fascism, such as when he justified Kristallnacht as an act of revenge by downtrodden Christians.\n\nAs the United States became increasingly involved in the Allied cause, government officials began restricting Coughlin\u2019s radio broadcasts until he lost his permit to broadcast entirely in 1940. Threats of a prosecution eventually pressured the priest into ceasing political activity entirely in 1942.\n\nFord, Lindbergh and Coughlin saw both sides as the problem. Yes, the Nazis were violent, they might concede, but hadn\u2019t they been provoked by the \u201cJewish problem\u201d, the wicked Communists and a belligerent France and United Kingdom?\n\nClearly there were plenty of Americans in the 1930s and 40s that either sympathized with the Nazis, or who were willing to make excuses for them and sabotage efforts to fight them, trying to cast the warring parties in an aura of false moral equivalency.\n\nHowever, those sympathizers and enablers proved to be a minority. Americans didn\u2019t fight Nazis by complacently standing by and imaging they would go away if they were simply ignored.\n\nTens of thousands of U.S. citizens protested and confronted Nazis on the streets. And politicians and security officials arrested and expelled foreign-sponsored agents seeking to subvert U.S. foreign policy, shut down fascist propaganda outlets, and targeted domestic leaders of the Nazi movement until it fell apart.", "sentiment": 0.04149906446557164},
{"link_title": "Instagram-py performs slick brute force attack on Instagram", "url": "https://github.com/deathsec/instagram-py", "text": "Instagram-Py is a simple python script to perform basic brute force attack against Instagram , this script can bypass login limiting on wrong passwords , so basically it can test infinite number of passwords. Instagram-Py is proved and can test over 6M passwords on a single instagram account with less resource as possible This script mimics the activities of the official instagram android app and sends request over tor so you are secure , but if your tor installation is misconfigured then the blame is on you.\n\nMake sure you installed the dependencies , and countinue with this section\n\nOpen your configuration file found in your home directory , this file is very important located at ~/instapy-config.json , do not change anything except tor configuration\n\nThe configuration file looks like this\n\napi-url : do not change this unless you know what you are doing\n\nuser-agent : do not change this unless you know your stuff\n\nig-sig_key : never change this unless new release, this is extracted from the instagram apk file\n\ntor : change everything according to your tor server configuration , do not mess up!\n\nopen your tor configuration file usually located at /etc/tor/torrc\n\nsearch for the file for this specific section\n\nuncomment 'ControlPort' by deleting the # before 'ControlPort' , now save the file and restart your tor server\n\nnow you are ready to crack any instagram account , make sure your tor configuration matched ~/instapy-config.json\n\nFinally , now you can use instagram-py!", "sentiment": 0.07653574434824435},
{"link_title": "A better Scala? Rust higher-kinded types RFC approved", "url": "https://github.com/rust-lang/rfcs/blob/master/text/1598-generic_associated_types.md", "text": "Allow type constructors to be associated with traits. This is an incremental step toward a more general feature commonly called \"higher-kinded types,\" which is often ranked highly as a requested feature by Rust users. This specific feature (associated type constructors) resolves one of the most common use cases for higher-kindedness, is a relatively simple extension to the type system compared to other forms of higher-kinded polymorphism, and is forward compatible with more complex forms of higher-kinded polymorphism that may be introduced in the future.\n\nConsider the following trait as a representative motivating example:\n\nThis trait is very useful - it allows for a kind of Iterator which yields values which have a lifetime tied to the lifetime of the reference passed to . A particular obvious use case for this trait would be an iterator over a vector which yields overlapping, mutable subslices with each iteration. Using the standard interface, such an implementation would be invalid, because each slice would be required to exist for as long as the iterator, rather than for as long as the borrow initiated by .\n\nThis trait cannot be expressed in Rust as it exists today, because it depends on a sort of higher-kinded polymorphism. This RFC would extend Rust to include that specific form of higher-kinded polymorphism, which is refered to here as associated type constructors. This feature has a number of applications, but the primary application is along the same lines as the trait: defining traits which yield types which have a lifetime tied to the local borrowing of the receiver type.\n\n\"Higher-kinded types\" is a vague term, conflating multiple language features under a single banner, which can be inaccurate. As background, this RFC includes a brief overview of the notion of kinds and kindedness. Kinds are often called 'the type of a type,' the exact sort of unhelpful description that only makes sense to someone who already understands what is being explained. Instead, let's try to understand kinds by analogy to types.\n\nIn a well-typed language, every expression has a type. Many expressions have what are sometimes called 'base types,' types which are primitive to the language and which cannot be described in terms of other types. In Rust, the types , , , and are all prominent examples of base types. In contrast, there are types which are formed by arranging other types - functions are a good example of this. Consider this simple function:\n\nhas the type (my apologies for using a syntax different from Rust's). Note that this is different from the type of , which is . This difference is important to understanding higher-kindedness.\n\nIn the analysis of kinds, all of these types - , , and so on - have the kind . Every type has the kind . However, is a base kind, just as is a base type, and there are terms with more complex kinds, such as . An example of a term of this kind is , which takes a type as a parameter and evaluates to a type. The difference between the kind of and the kind of (which is ) is analogous to the difference between the type of and . Note that has the kind , just like : even though is a type parameter, is still being applied to a type, just like still has the type even though is a variable.\n\nA relatively uncommon feature of Rust is that it has two base kinds, whereas many languages which deal with higher-kindedness only have the base kind . The other base kind of Rust is the lifetime parameter. If you have a type like , the kind of is .\n\nHigher-kinded terms can take multiple arguments as well, of course. has the kind . Given has the kind .\n\nTerms of a higher kind are often called 'type operators'; the type operators which evaluate to a type are called 'type constructors'. There are other type operators which evaluate to other type operators, and there are even higher order type operators, which take type operators as their argument (so they have a kind like ). This RFC doesn't deal with anything as exotic as that.\n\nSpecifically, the goal of this RFC is to allow type constructors to be associated with traits, just as you can currently associate functions, types, and consts with traits. There are other forms of polymorphism involving type constructors, such as implementing traits for a type constructor instead of a type, which are not a part of this RFC.\n\nThis RFC proposes a very simple syntax for defining an associated type constructor, which looks a lot like the syntax for creating aliases for type constructors. The goal of using this syntax is to avoid to creating roadblocks for users who do not already understand higher kindedness.\n\nIt is clear that the associated item is a type constructor, rather than a type, because it has a type parameter attached to it.\n\nAssociated type constructors can be bounded, just like associated types can be:\n\nThis bound is applied to the \"output\" of the type constructor, and the parameter is treated as a higher rank parameter. That is, the above bound is roughly equivalent to adding this bound to the trait:\n\nAssigning associated type constructors in impls is very similar to the syntax for assigning associated types:\n\nOnce a trait has an associated type constructor, it can be applied to any parameters or concrete term that are in scope. This can be done both inside the body of the trait and outside of it, using syntax which is analogous to the syntax for using associated types. Here are some examples:\n\nAssociated type constructors can also be used to construct other type constructors:\n\nLastly, lifetimes can be elided in associated type constructors in the same manner that they can be elided in other type constructors. Considering lifetime ellision, the full definition of is:\n\nUsers can bound parameters by the type constructed by that trait's associated type constructor of a trait using HRTB. Both type equality bounds and trait bounds of this kind are valid:\n\nThis RFC does not propose allowing any sort of bound by the type constructor itself, whether an equality bound or a trait bound (trait bounds of course are also impossible).\n\nAll of the examples in this RFC have focused on associated type constructors of lifetime arguments, however, this RFC proposes adding ATCs of types as well:\n\nThis RFC does not propose extending HRTBs to take type arguments, which makes these less expressive than they could be. Such an extension is desired, but out of scope for this RFC.\n\nType arguments can be used to encode other forms of higher kinded polymorphism using the \"family\" pattern. For example, Using the trait, you can abstract over Arc and Rc:\n\nBounds on associated type constructors are treated as higher rank bounds on the trait itself. This makes their behavior consistent with the behavior of bounds on regular associated types. For example:\n\nIn contrast, where clauses on associated types introduce constraints which must be proven each time the associated type is used. For example:\n\nEach invokation of will need to prove , as opposed to the impl needing to prove the bound as in other cases.\n\nThis feature is not full-blown higher-kinded polymorphism, and does not allow for the forms of abstraction that are so popular in Haskell, but it does provide most of the unique-to-Rust use cases for higher-kinded polymorphism, such as streaming iterators and collection traits. It is probably also the most accessible feature for most users, being somewhat easy to understand intuitively without understanding higher-kindedness.\n\nThis feature has several tricky implementation challenges, but avoids all of these features that other kinds of higher-kinded polymorphism require:\n\nThe advantage of the proposed syntax is that it leverages syntax that already exists. Type constructors can already be aliased in Rust using the same syntax that this used, and while type aliases play no polymorphic role in type resolution, to users they seem very similar to associated types. A goal of this syntax is that many users will be able to use types which have assocaited type constructors without even being aware that this has something to do with a type system feature called higher-kindedness.\n\nThis RFC uses the terminology \"associated type constructor,\" which has become the standard way to talk about this feature in the Rust community. This is not a very accessible framing of this concept; in particular the term \"type constructor\" is an obscure piece of jargon from type theory which most users cannot be expected to be familiar with.\n\nUpon accepting this RFC, we should begin (with haste) refering to this concept as simply \"generic associated types.\" Today, associated types cannot be generic; after this RFC, this will be possible. Rather than teaching this as a separate feature, it will be taught as an advanced use case for associated types.\n\nPatterns like \"family traits\" should also be taught in some way, possible in the book or possibly just through supplemental forms of documentation like blog posts.\n\nThis will also likely increase the frequency with which users have to employ higher rank trait bounds; we will want to put additional effort into teaching and making teachable HRTBs.\n\nThis would add a somewhat complex feature to the language, being able to polymorphically resolve type constructors, and requires several extensions to the type system which make the implementation more complicated.\n\nAdditionally, though the syntax is designed to make this feature easy to learn, it also makes it more plausible that a user may accidentally use it when they mean something else, similar to the confusion between and . For example:\n\nThis does not add all of the features people want when they talk about higher- kinded types. For example, it does not enable traits like . Some people may prefer to implement all of these features together at once. However, this feature is forward compatible with other kinds of higher-kinded polymorphism, and doesn't preclude implementing them in any way. In fact, it paves the way by solving some implementation details that will impact other kinds of higher- kindedness as well, such as partial application.\n\nThough the proposed syntax is very similar to the syntax for associated types and type aliases, it is probably not possible for other forms of higher-kinded polymorphism to use a syntax along the same lines. For this reason, the syntax used to define an associated type constructor will probably be very different from the syntax used to e.g. implement a trait for a type constructor.\n\nHowever, the syntax used for these other forms of higher-kinded polymorphism will depend on exactly what features they enable. It would be hard to design a syntax which is consistent with unknown features.\n\nAn alternative is to push harder on HRTBs, possibly introducing some elision that would make them easier to use.\n\nCurrently, an approximation of can be defined like this:\n\nYou can then bound types as to avoid the lifetime parameter infecting everything appears.\n\nHowever, this only partially prevents the infectiveness of , only allows for some of the types that associated type constructors can express, and is in generally a hacky attempt to work around the limitation rather than an equivalent alternative.\n\nWhat is often called \"full higher kinded polymorphism\" is allowing the use of type constructors as input parameters to other type constructors - higher order type constructors, in other words. Without any restrictions, multiparameter higher order type constructors present serious problems for type inference.\n\nFor example, if you are attempting to infer types, and you know you have a constructor of the form , without any restrictions it is difficult to determine if this constructor is or .\n\nBecause of this, languages with first class higher kinded polymorphism tend to impose restrictions on these higher kinded terms, such as Haskell's currying rules.\n\nIf Rust were to adopt higher order type constructors, it would need to impose similar restrictions on the kinds of type constructors they can receive. But associated type constructors, being a kind of alias, inherently mask the actual structure of the concrete type constructor. In other words, if we want to be able to use ATCs as arguments to higher order type constructors, we would need to impose those restrictions on all ATCs.\n\nWe have a list of restrictions we believe are necessary and sufficient; more background can be found in this blog post by nmatsakis:\n\nThese restrictions are quite constrictive; there are several applications of ATCs that we already know about that would be frustrated by this, such as the definition of for (for which the item , applying the lifetime twice).\n\nFor this reason we have decided not to apply these restrictions to all ATCs. This will mean that if higher order type constructors are ever added to the language, they will not be able to take an abstract ATC as an argument. However, this can be maneuvered around using newtypes which do meet the restrictions, for example:", "sentiment": 0.0880820888513196},
{"link_title": "Show HN: Coalesce \u2013 Where Research Lives (AMA and Comments)", "url": "http://www.coalescelabs.com", "text": "", "sentiment": 0.0},
{"link_title": "Death of NFL \u201cinevitable\u201d as middle class abandons the game", "url": "http://www.chicagotribune.com/news/columnists/kass/ct-football-concussions-youth-kass-met-0906-20170905-column.html", "text": "To witness the death of the multi-billion dollar National Football League, you really don't need to see sportswriters wringing their hands over the moral dilemma of covering America's Roman circus of brain trauma.\n\nAnd you don't need to watch multi-millionaire football stars, pampered for most of their lives, ostentatiously disrespecting the American national anthem, kneeling, their raised fists in the air.\n\nYou don't need to see the desperation in the NFL's television commercials: actresses in team gear, holding snack trays to feed their (virtual) extended team-gear-wearing families, as the NFL begs middle-class women to mother their game before it dies.\n\nYou don't have to do any of that to see how football is dying.\n\nAll you have to do is go out to a youth football field, as I did on Sunday morning, and talk to parents and coaches.\n\n\"Just four years ago, we had so many boys signing up for football, we had five teams at this fourth-grade level,\" says John Herrera, a dad, software engineer and football coach of the Wheaton Rams in the Bill George Youth Football League in the western suburbs of Chicago.\n\n\"And from five teams of fourth-graders four years ago, what do we have now? One team. Just one.\"\n\nOut on the field, the Wheaton Rams and the Lyons Tigers were going at it, having fun. Parents and grandparents watching, sipping lattes, a few dads nervously pacing the sidelines as dads always do, willing prowess on their sons.\n\nBut what do the numbers from the hometown of the \"Wheaton Ice Man,\" the great Red Grange, tell us about football in America?\n\n\"If dropping from five teams of fourth-graders to one doesn't tell you what's happening, nothing will,\" Herrera said. \"Football is such a great game, it teaches great lessons to young men. But I've got a sense of dread for this game of football that I love.\"\n\nHerrera cares about the lessons the game can teach. He and other coaches are deadly serious about instilling \"heads up\" tackling techniques to protect the heads of their players.\n\n\"But it's the parents,\" he said. \"They're worried about the brain.\"\n\nIt is all about the brain. The brains that are injured in the game, yes, but also about how the human mind works, as the American middle class withdraws from football, a cultural trend that will cut the NFL away from American virtue.\n\nWhat is virtuous about brain damage? I'd prefer to watch prizefighters. At least prizefighting is honest about its violence. It doesn't wrap itself up in mom and apple pie.\n\nFour years ago I wrote a column saying that football was dead in this country, as dead as the Marlboro Man, though it didn't know it yet.\n\nPutting your kids in football would be akin to giving them cigarettes, and leave you to face the withering judgment of your friends and neighbors.\n\nI was hated for it, accused of wussifying American boys. Some even called me a liberal. Now though, years later, the water is warm and others have jumped in, as the feeding frenzy around the NFL becomes undeniable.\n\nWithout that feeder system to provide fresh meat and fresh brains for the NFL meat grinder, the NFL as we know it is doomed.\n\nThere is still enough talent and size to fill the ranks. And gambling drives the game along. But without its connection to the middle class, the NFL loses what it can't afford to lose \u2014 market share.\n\nYou really think the NFL is worried about young athletes? If so, they'd have changed the rules years ago, abandoning face masks, enlarging the ball to make it difficult to throw, switching to one platoon football.\n\nBut they're not worried about players. They're worried about their money.\n\nParents read the news, they know about concussions and CTE, chronic traumatic encephalopathy. While a recent study wasn't random \u2014 brains were donated by concerned families \u2014 the analysis by Boston University of brains from dead players showed that of 111 brains from NFL players, 110 suffered CTE, a condition causing depression, psychosis, dementia, memory loss and death.\n\nAnd what does science tell us?\n\nIt's not the concussions that are killing football. Every sport has danger in it, and concussions can happen in basketball, soccer, perhaps even badminton, for all I know.\n\nAnd as a soccer dad \u2014 with two sons playing in college \u2014 I've spent my share of nights in emergency rooms. Concussions happen when brave athletes collide at speed, and mostly, it's the brave ones who get hurt.\n\nThere has been a pathetic and desperate spin by football to lump soccer and other contact sports into the discussion to save itself.\n\nBut it can't. Because what makes football different from the others is the design of the game \u2014 sending bodies crashing in high speed, high impact collisions. It is what makes it awesome and dangerous and fun to play.\n\nHeads get in the way. And football provides not only concussions, but by design, multiple hits to the head. There is no getting around this.\n\n\"Sure I'm concerned,\" said one of the moms at the game, a lawyer who is no stranger to courtroom debates about liability. \"But he loves the game so much. We haven't made a decision as to how long he'll play. At this level, they're just learning, they're not big enough to hurt each other. Later? I'm thinking about it.\"\n\nParents of youth football players are already feeling pressure and social stigma.\n\n\"It's not like smoking, yet,\" said a dad. \"But it's getting there.\"\n\nIt's already there, dad. It's there.\n\nListen to \"The Chicago Way\" podcast: http://wgnradio.com/category/wgn-plus/thechicagoway.", "sentiment": 0.01952702702702703},
{"link_title": "5 Live Web Cams of Hurricane Irma's Impact on FL Keys", "url": "https://zeryl.github.io/irma.html", "text": "", "sentiment": 0.0},
{"link_title": "Launch an idea in 12 weeks", "url": "http://launch.puricode.com", "text": "We develop from user stories which allow for an agile development process. Every part of development is tracked and reported on, to ensure we're efficient and delivering value with every commit. Maintenance and support are truly something we pride ourselves on being great at. We stick around after launch to make sure you understand your data and help to fix issues. \n\n", "sentiment": 0.6},
{"link_title": "Dependent Types", "url": "https://hydraz.club/posts/2017-09-08.html", "text": "Dependent types are pretty cool, yo. This post is a semi-structured ramble about dtt, a small dependently-typed \u201cprogramming language\u201d inspired by Thierry Coquand\u2019s Calculus of (inductive) Constructions (though, note that the induction part is still lacking: There is support for defining inductive data types, and destructuring them by pattern matching, but since there\u2019s no totality checker, recursion is disallowed).\n\nis written in Haskell, and served as a learning experience both in type theory and in writing programs using extensible effects. I do partly regret the implementation of effects I chose (the more popular did not build on the Nixpkgs channel I had, so I went with ; Refactoring between these should be easy enough, but I still haven\u2019t gotten around to it, yet)\n\nI originally intended for this post to be a Literate Haskell file, interleaving explanation with code. However, for a pet project, \u2019s code base quickly spiralled out of control, and is now over a thousand lines long: It\u2019s safe to say I did not expect this one bit.\n\nis a very standard \u03bb calculus. We have all 4 axes of Barendgret\u2019s lambda cube, in virtue of having types be first class values: Values depending on values (functions), values depending on types (polymorphism), types depending on types (type operators), and types depending on values (dependent types). This places dtt squarely at the top, along with other type theories such as the Calculus of Constructions (the theoretical basis for the Coq proof assistant) and TT (the type theory behind the Idris programming language).\n\nThe syntax is very simple. We have the standard lambda calculus constructs - \u03bb-abstraction, application and variables - along with -bindings, pattern matching expression, and the dependent type goodies: \u220f-abstraction and .\n\nAs an aside, pi types are called as so because the dependent function space may (if you follow the \u201ctypes are sets of values\u201d line of thinking) be viewed as the cartesian product of types. Consider a type with inhabitants , and a type with inhabitant . A dependent product \u220f , then, has inhabitants and .\n\nYou\u2019ll notice that dtt does not have a dedicated arrow type. Indeed, the dependent product subsumes both the \u2200 quantifier of System F, and the arrow type \u2192 of the simply-typed lambda calculus. Keep this in mind: It\u2019ll be important later.\n\nSince dtt\u2019s syntax is unified (i.e., there\u2019s no stratification of terms and types), the language can be - and is - entirely contained in a single algebraic data type. All binders are explicitly typed, seeing as inference for dependent types is undecidable (and, therefore, bad).\n\nThe term constructor, not mentioned before, is merely a convenience: It allows the programmer to check their assumptions and help the type checker by supplying a type (Note that we don\u2019t assume this type is correct, as you\u2019ll see later; It merely helps guide inference.)\n\nVariables aren\u2019t merely strings because of the large amount of substitutions we have to perform: For this, instead of generating a new name, we increment a counter attached to the variable - the pretty printer uses the original name to great effect, when unambiguous.\n\nThe variable constructor is used to support a \u2192 b as sugar for \u220f b when x does not appear free in b. As soon as the type checker encounters an variable, it is refreshed with a new name.\n\ndoes not have implicit support (as in Idris), so all parameters, including type parameters, must be bound explicitly. For this, we support several kinds of syntatic sugar. First, all abstractions support multiple variables in a binding group. This allows the programmer to write instead of . Furthermore, there is special syntax for single-parameter abstraction with type , and lambda abstractions support multiple binding groups.\n\nAs mentioned before, the language does not support recursion (either general or well-founded). Though I would like to, writing a totality checker is hard - way harder than type checking ambda i, in fact. However, an alternative way of inspecting inductive values does exist: eliminators. These are dependent versions of catamorphisms, and basically encode a proof by induction. An inductive data type as Nat gives rise to an eliminator much like it gives rise to a natural catamorphism.\n\nIf you squint, you\u2019ll see that the eliminator models a proof by induction (of the proposition P) on the natural number n: The type signature basically states \u201cGiven a proposition P on \u2115, a proof of P , a proof that P follows from P and a natural number n, I\u2019ll give you a proof of P .\u201d\n\nThis understanding of computations as proofs and types as propositions, by the way, is called the Curry-Howard Isomorphism. The regular, simply-typed lambda calculus corresponds to natural deduction, while ambda i corresponds to predicate logic.\n\nShould this be called the term system?\n\nOur type inference algorithm, contrary to what you might expect for such a complicated system, is actually quite simple. Unfortunately, the code isn\u2019t, and thus isn\u2019t reproduced in its entirety below.\n\nThe simplest case in any type system. The typing judgement that gives rise to this case is pretty much the identity: \u0393 \u22a2 \u03b1 : \u03c4\u2234 \u0393 \u22a2 \u03b1 : \u03c4. If, from the current typing context we know that \u03b1 has type \u03c4, then we know that \u03b1 has type \u03c4.\n\nSince dtt has a cummulative hierarchy of universes, : . This helps us avoid the logical inconsistency introduced by having type-in-type , i.e. : . We say that is the type of small types: in fact, is where most computation actually happens, seeing as for k \u2265 1 is reserved for \u220f-abstractions quantifying over such types.\n\nType hints are the first appearance of the unification engine, by far the most complex part of dtt\u2019s type checker. But for now, suffices to know that errors if the types t1 and t2 can\u2019t be made to line up, i.e., unify.\n\nFor type hints, we infer the type of given expression, and compare it against the user-provided type, raising an error if they don\u2019t match. Because of how the unification engine works, the given type may be more general (or specific) than the inferred one.\n\nThis is where it starts to get interesting. First, we mandate that the parameter type is inhabited (basically, that it is, in fact, a type). The dependent product \u220f \u03b1, while allowed by the language\u2019s grammar, is entirely meaningless: There\u2019s no way to construct an inhabitant of 0, and thus this function may never be applied.\n\nThen, in the context extended with (\u03b1 : \u03c4), we require that the consequent is also a type itself: The function \u220f 0, while again a valid parse, is also meaningless.\n\nThe type of the overall abstraction is, then, the maximum value of the indices of the universes of the parameter and the consequent.\n\nMuch like in the simply-typed lambda calculus, the type of a \u03bb-abstraction is an arrow between the type of its parameter and the type of its body. Of course, ambda i incurs the additional constraint that the type of the parameter is inhabited.\n\nAlas, we don\u2019t have arrows. So, we \u201clift\u201d the lambda\u2019s parameter to the type level, and bind it in a \u220f-abstraction.\n\nNote that, much like in the case, we type-check the body in a context extended with the parameter\u2019s type.\n\nApplication is the most interesting rule, as it has to not only handle inference, it also has to handle instantiation of \u220f-abstractions.\n\nInstantation is, much like application, handled by \u03b2-reduction, with the difference being that instantiation happens during type checking (applying a \u220f-abstraction is meaningless) and application happens during normalisation (instancing a \u03bb-abstraction is meaningless).\n\nThe type of the function being applied needs to be a \u220f-abstraction, while the type of the operand needs to be inhabited. Note that the second constraint is not written out explicitly: It\u2019s handled by the case above, and furthermore by the unification engine.\n\nYou\u2019ll notice that two typing rules are missing here: One for handling s, which was not included because it is entirely uninteresting, and one for expressions, which was redacted because it is entirely a mess.\n\nHopefully, in the future, the typing of expressions is simpler - if not, they\u2019ll probably be replaced by eliminators.\n\nThe unification engine is the man behind the curtain in type checking: We often don\u2019t pay attention to it, but it\u2019s the driving force behind it all. Fortunately, in our case, unification is entirely trivial: Solving is the hard bit.\n\nThe job of the unification engine is to produce a set of constraints that have to be satisfied in order for two types to be equal. Then, the solver is run on these constraints to assert that they are logically consistent, and potentially produce substitutions that reify those constraints.\n\n Our solver isn\u2019t that cool, though, so it just verifies consitency.\n\nThe kinds of constraints we can generate are as in the data type below.\n\nUnification of most terms is entirely uninteresting. Simply line up the structures and produce the appropriate equality (or instance) constraints.\n\nThose are all the boring cases, and I\u2019m not going to comment on them. Similarly boring are binders, which were abstracted out because hlint told me to.\n\nThere are two interesting cases: Unification between some term and a pi abstraction, and unification between two variables.\n\nIf the variables are syntactically the same, then we\u2019re done, and no constraints have to be generated (Technically you could generate an entirely trivial equality constraint, but this puts unnecessary pressure on the solver).\n\nIf either variable has a known type, then we generate an instance constraint between the unknown variable and the known one.\n\nIf both variables have a value, we equate their types\u2019 types and their types. This is done mostly for error messages\u2019 sakes, seeing as if two values are propositionally equal, so are their types.\n\nUnification between a term and a \u220f-abstraction is the most interesting case: We check that the \u220f type abstracts over a type (i.e., it corresponds to a System F \u2200 instead of a System F \u2192), and instance the \u220f with a fresh type variable.\n\nSolving is a recursive function of the list of constraints (a catamorphism!) with some additional state: Namely, a strict map of already-performed substitutions. Let\u2019s work through the cases in reverse order of complexity (and, interestingly, reverse order of how they\u2019re in the source code).\n\nSolving an empty list of constraints is entirely trivial.\n\nWe infer the index of the universe of the given type, much like in the inferrence case for \u220f-abstractions, and check the remaining constraints.\n\nWe infer the types of both provided values, and generate an equality constraint.\n\nWe merely have to check for syntactic equality of the (normal forms of) terms, because the hard lifting of destructuring and lining up was done by the unification engine.\n\nIf the variable we\u2019re instancing is already in the map, and the thing we\u2019re instancing it to now is not the same as before, we have an inconsistent set of substitutions and must error.\n\nOtherwise, if we have a coherent set of instances, we add the instance both to scope and to our local state map and continue checking.\n\nNow that we have both and , we can write : We unify the two types, and then try to solve the set of constraints.\n\nThe real implementation will catch and re-throw any errors raised by to add appropriate context, and that\u2019s not the only case where \u201creal implementation\u201d and \u201cblag implementation\u201d differ.\n\nWow, that was a lot of writing. This conclusion begins on exactly the 500th line of the Markdown source of this article, and this is the longest article on this blag (by far). However, that\u2019s not to say it\u2019s bad: It was amazing to write, and writing was also amazing. I am not good at conclusions.\n\nis available under the BSD 3-clause licence, though I must warn you that the source code hasn\u2019t many comments.\n\nI hope you learned nearly as much as I did writing this by reading it.", "sentiment": 0.061545027742210844},
{"link_title": "Why Paris will be the \ufb01rst post-car metropolis", "url": "https://www.ft.com/content/1b785f3e-9299-11e7-a9e6-11d2f0ebb7f0", "text": "You will be billed $66.30 per month after the trial ends", "sentiment": 0.0},
{"link_title": "Numbers and tagged pointers in early Lisp implementations", "url": "https://www.snellman.net/blog/archive/2017-09-04-lisp-numbers/", "text": "There was a bit of discussion on HN about data representations in dynamic languages, and specifically having values that are either pointers or immediate data, with the two cases being distinguished by use of tag bits in the pointer value:\n\nI was going to nitpick a bit with the following:\n\nBut the latter part of that was speculation, maybe I should try to check the facts first before being tediously pedantic? Good call, since that speculation was wrong. Let's take a tour through some early Lisp implementations, and look at how they represented data in general, and numbers in particular.\n\nBefore we get started, let's state the problem that tagged pointers solve. In a dynamically typed programming language, the language implementation must be able to distinguish between values of different types. The obvious implementation is boxing; all values are treated as blobs of memory allocated somewhere on the heap, with an envelope containing metadata such as the type and (maybe) the size of the object.\n\nBut this means that integers now have tons of overhead. They use up heap space, need to be garbage collected, and new memory needs to be constantly allocated for the results of arithmetic operations. Since integers are so critical to almost all kinds of computing, it would be great to minimize the overhead. And ultimately, to eliminate the overhead completely by encoding small integers as recognizably invalid pointers.\n\nI wasn't super hopeful about finding out exactly what numbers looked like in the original Lisp implementation. As far as I know, the source code hasn't been preserved. Now, the original paper describing Lisp ( Recursive Functions of Symbolic Expressions and their Computation by Machine, Part I ) isn't quite as theoretical as the title suggests. For example it describes the memory allocator and garbage collector on a reasonable systems level. But it doesn't mention numbers at all; this is a system for symbolic computation, so numbers might as well not exist.\n\nThe LISP I Programmer's Manual from 1960 is more illuminating, though not entirely consistent. In one place the manual claims that LISP I only supports floats, and you'll need to wait until LISP II to use integers. But the rest of the document happily describes the exact memory layout of integers, so who can tell.\n\nA floating point value looks like this:\n\nLet's say we have the value 1.0 in a LISP I program. This value is actually pointer to a word. How do we know what the type of the pointed to word is? If the upper half of that word is -1, it's a symbol. Otherwise it's a cons. (The use of -1.0 and 1.0 as the example floats in this picture is unfortunate, since it looks like the -1.0 and -1 are somehow related. That's not the case, -1 is the universal tag value for atoms, and independent of the exact floating point values.)\n\nSo the number 1.0 is a symbol? Technically yes, since at this stage of Lisp's evolution everything is either a symbol or a cons. There are no other atoms. We can find out if the symbol represents a number by following the linked list starting from the of the symbol (a pointer stored in the lower half of the word). If we find the symbol on the list, it's some kind of number. If we find the symbol , it's a floating point number, and the property list will be pointing to a word that contains the raw floating point value that this number represents.\n\nThere's a detail here that's kind of amazing. Notice that 1.0 and -1.0 share the same list structure. The only difference is that -1.0 has the symbol in the list, after which the list merges with the list of 1.0. What a fabulously inefficient representation! Not only do you have to do a bunch of pointer chasing just to find the actual value of a number, but then you'll get to do it again to find out the sign!\n\nThe question I can't answer just from reading this document is how exactly the raw floating point value is handled. Surely the garbage collector must know not to interpret those raw bits as pointer data? There is a very detailed example of the memory layout for an integer on pages 94-95, but even with that example I just don't see where the type information is stored. It's clearly not based on address ranges (the raw values are mixed in with the other words), nor the pointer value (all the pointers are stored as 2's complement), nor the 6 unused bits in the machine word.\n\nSuggestions welcome. My best guess is that the example is inaccurate.\n\nThe LISP 1.5 Programmer's Manual from 1962 explains in a very concise manner how numbers worked in that implementation:\n\nNumbers are still considered to be symbols, and symbols are still marked with -1 as the . But the standard symbol property list is now gone; instead the symbol is pointing directly to the memory that stores the raw integer value. How does the program know not to follow that pointer as a list? As the document says, that's specified by \"certain bits in the tag\".\n\nThe tag? What's the tag? The IBM 704 had a 36-bit word size but just a 15 bit address space. The words were split (on the ISA level) into a 3 bit \"prefix\", 15 bit \"address\", 3 bit \"tag\", and 15 bit \"decrement\". Since Lisp values are pointers, only the two 15 bit regions are useful for that. One of the 3 bit regions has been repurposed by the Lisp implementation to mark the pointers to raw data.\n\nThis is a clear improvement over LISP I, but a number is still represented as an untagged pointer to a tagged pointer to the raw value. Why is the intermediate word there at all, why not go directly with a tagged pointer to the raw value? Maybe code size?\n\nIn parallel to that, the address space has now been split into multiple separate pieces, with the cons cells being allocated from a different range of addresses than plain data like numbers and string segments. It could well be that the tagged pointer is irrelevant to the GC, which just makes its decisions on what's a pointer based on whether the pointer is contained in the \"full word space\" or the \"free space\". The tags would then be used just for implementing .\n\nFor a L. Peter Deutsch joint, The LISP implementation for the PDP-1 Computer proves to be a surprisingly unsatisfying document. It's almost exclusively user documentation, with no information on the systems architecture. Well, except a full source code listing. Guess we'll have to look at that, then. is the easiest starting point:\n\nThe main thing that need to be known from the rest of the code is that the interpreter stores a pointer to the Lisp value that's currently operated on value at address (octal).\n\nFirst follows the pointer to read the first data words of the value into the accumulator. The next line looks bizarre; due to the way the PDP-1 macro-assembler works, effectively means . So this instruction is masking away all but the top two bits of the accumulator, and is checking whether the result of the masking equals octal . It appears that there is nothing special about the pointer to a number, but numbers are identified by having the top two bits set in the pointed-to value.\n\nThe next step in understanding the layout is the code for reading the raw value of a number.\n\nloads the current Lisp value into the IO register. sets the accumulator to zero. then rotates the combination of the IO register and accumulator by 2 bits. The accumulator now contains as its low bits the previous high two bits of the IO register. compares the accumulator to 3; if they're not equal we jump to qi3 (the error routine for \"non-numeric arg for arith\"). moves the pointer to the next word of the value, and reads that word into the accumulator. And finally the combination of the two registers is rotated by 16 bits, so that we end up with the raw 18 bit value in the accumulator. Written out step by step the process looks like this:\n\nClearly an integer is now represented by a pointer to two words that has a special tag in the high bits of the first word. This implementation got rid of the extra layer of indirection in LISP 1.5; an integer is now just a pointer to tagged data. But we're still left with the storage of a one-word integer requiring three words.\n\nWhy use a layout that requires shuffling data around this much, instead of just having the tag in X and the raw value in X+1? It seems awfully inconvenient. My best guess is that the top 1-2 bits of the second word are reserved for the GC, e.g. for use as mark bits. But understanding exactly how the GC works is maybe a project for another day.\n\nBefore starting research for this article, I'd never heard of the early Lisp implementation for the Univac M 460. A description of the system can be found in the 1964 collection The programming language LISP: Its operation and applications .\n\nThis is another bit of progress! The key insight on the road to tagged pointers is that invalid parts of the address space can be used to distinguish between pointers and immediate data. Another important insight in this paper is that most numbers in a program are going to be small, so it might make sense to have variable representations for numbers of different magnitude. But it's not a full realization of the concept yet, immediate small numbers are not accessible directly by the user. They are internal to the implementation, used as a building block for boxed integers of various levels of inefficiency.\n\nThe paper gets even better once we get a few more pages in, since for characters M 460 Lisp does take that final step:\n\nThat's about as clear a case of using embedding immediate data in pointers as it gets. It's just that the tag is rather large (22 highest bits, rather than the 1-4 lowest bits you'd expect today). And it's also dealing with characters rather than numbers, so let's carry on with the investigation a bit longer.\n\nThe June 1966 report on PDP-6 LISP has the following to say on integers:\n\nThis is starting to get close to the modern fixnum, except for no facility for immediate negative numbers and a tiny range. (This is a machine with 36 bit words and 18 bit pointers; one would hope for a bit more than 12 bits for immediate integers).\n\nStructure of a LISP system using two-level storage is a wonderful systems design paper from November 1966, describing BBN LISP for a PDP-1 with 16K of core memory, 88K of absurdly slow drum memory, and no hardware paging support. How do you make efficient use of the drum memory? By some clever data layout, software-driven paging, and a locality-optimizing memory allocator.\n\nSo it's actually a paper I thought was totally worth reading just for its own sake. But for the purposes of this post, this is the money quote:\n\nThe paper describes a machine with both an 18-bit word size and address space, with 16-bit signed fixnums embedded in the pointers. That's about as good as it gets. (Though not quite optimal; they're using bit 17 as the integer tag, but what happened to bit 18? The paper doesn't say, but odds are that it's again a GC mark bit).\n\nThe particularly observant reader might have noticed that this machine had 104K words of physical memory, but the described tagging scheme only leaves 64K words addressable. What's up with that? On one level it's exactly what M 460 LISP and PDP-6 Lisp were doing: that 40K of address space stores things that can't be directly pointed to from another Lisp value. But those other implementations were just opportunistically reusing the parts of address space that contained native code.\n\nBy contrast, BBN LISP carefully arranged for there to exist as much of such storage as possible, and for it to be located above the address 200,000 (octal).\n\nThe most clever example of that is the representation of symbols. The first implementations we saw just implemented symbols as a list of properties indexed by name (e.g. name, value cell, function cell, etc). An obvious optimization is to allocate a symbol as a single larger block of memory with fixed slots for the most common properties, and a generic property list slot to contain anything else.\n\nWhat BBN Lisp does instead is allocate a symbol in multiple separate blocks rather than a single contiguous one. A pointer to the symbol will point to the block of value cells, so reading the value cell is trivial. What if you want to read another property, e.g. the function? We look at the offset of the value cell pointer to the start of the value cell block, and access the function cell block at the same offset. In modern parlance it ends up as an structure-of-arrays layout rather than an array-of-structures.\n\nIn addition to getting more address space for fixnums, they also got exactly the same kind of locality improvements that an structure-of-arrays would be used for today. So it was an all-around neat optimization.\n\nThere is also an early design document for BBN 940 LISP from almost the same time as the above paper. It appears to describe the kind of elaborate tagging scheme that a modern Lisp might use, and places the tags in the low bits where they're easier to test for/eliminate. And they even call heap-allocated numbers \"boxed\"! I had no idea this terminology was in use 50 years ago. The relevant section:\n\nIt looks like they ended up not using this design for BBN 940 LISP, and it instead uses an extended version of the segmented memory scheme from the PDP-1 implementation described earlier in this section. But even if these particular bits weren't practical to use with that hardware, at this point just about all the ideas for tagged pointers have definitely been invented.\n\nThe initial LISP I implementation in 1960 had the least efficient implementation of numbers this side of church numerals, where even just getting the value might imply chasing half a dozen pointers. But new implementations optimized that layout aggressively. By 1964, the M 460 LISP implementation had arrived at the general solution of using pointers to invalid parts of the address space for storing immediate data, but user-accessible integers were still boxed; the only use for the unboxed integers was as an internal building block. In 1966 PDP-6 LISP applied the idea of tagged immediate data to tiny positive integers, and the PDP-1 based BBN LISP took the idea to the logical conclusion, and allowed immediate storage of integers of almost the full machine word.\n\nI would not have guessed that these optimizations were discovered and applied so early and so aggressively. It's also noteworthy that this was independent of both the machine word size, address space size, and addressing mode of the machine. The first fully fledged implementation I found was on a machine with 18 bit words, 18 bits of address space, and word-addressing. That should have been just about the worst case!\n\nThere's an interesting tangent with how MacLISP ended up reversing this progress in the '70s and going back to boxed integers, since they wanted to have just a single integer representation. I won't go into the details since this post already grew longer than intended. But for those interested in the subject AI Memo 421 is a fun read.\n\nWas the technique definitely first used in Lisp? These implementations are early enough that there aren't a ton of other possibilities. The only ones I can think of would be APL and Dartmouth BASIC. If anyone can find documentation on earlier uses of storing immediate data in tagged pointers, please let me know and I'll edit the article.", "sentiment": 0.11020366300366304},
{"link_title": "Quantum computers are about to get real", "url": "https://www.sciencenews.org/article/quantum-computers-are-about-get-real", "text": "Although the term \u201cquantum computer\u201d might suggest a miniature, sleek device, the latest incarnations are a far cry from anything available in the Apple Store. In a laboratory just 60 kilometers north of New York City, scientists are running a fledgling quantum computer through its paces \u2014 and the whole package looks like something that might be found in a dark corner of a basement. The cooling system that envelops the computer is about the size and shape of a household water heater.\n\nBeneath that clunky exterior sits the heart of the computer, the quantum processor, a tiny, precisely engineered chip about a centimeter on each side. Chilled to temperatures just above absolute zero, the computer \u2014 made by IBM and housed at the company\u2019s Thomas J. Watson Research Center in Yorktown Heights, N.Y. \u2014 comprises 16 quantum bits, or qubits, enough for only simple calculations.\n\nIf this computer can be scaled up, though, it could transcend current limits of computation. Computers based on the physics of the super\u00adsmall can solve puzzles no other computer can \u2014 at least in theory \u2014 because quantum entities behave unlike anything in a larger realm.\n\nQuantum computers aren\u2019t putting standard computers to shame just yet. The most advanced computers are working with fewer than two dozen qubits. But teams from industry and academia are working on expanding their own versions of quantum computers to 50 or 100 qubits, enough to perform certain calculations that the most powerful supercomputers can\u2019t pull off.\n\nThe race is on to reach that milestone, known as \u201cquantum supremacy.\u201d Scientists should meet this goal within a couple of years, says quantum physicist David Schuster of the University of Chicago. \u201cThere\u2019s no reason that I see that it won\u2019t work.\u201d\n\nBut supremacy is only an initial step, a symbolic marker akin to sticking a flagpole into the ground of an unexplored landscape. The first tasks where quantum computers prevail will be contrived problems set up to be difficult for a standard computer but easy for a quantum one. Eventually, the hope is, the computers will become prized tools of scientists and businesses.\n\nSome of the first useful problems quantum computers will probably tackle will be to simulate small molecules or chemical reactions. From there, the computers could go on to speed the search for new drugs or kick-start the development of energy-saving catalysts to accelerate chemical reactions. To find the best material for a particular job, quantum computers could search through millions of possibilities to pinpoint the ideal choice, for example, ultrastrong polymers for use in airplane wings. Advertisers could use a quantum algorithm to improve their product recommendations \u2014 dishing out an ad for that new cell phone just when you\u2019re on the verge of purchasing one.\n\nQuantum computers could provide a boost to machine learning, too, allowing for nearly flawless handwriting recognition or helping self-driving cars assess the flood of data pouring in from their sensors to swerve away from a child running into the street. And scientists might use quantum computers to explore exotic realms of physics, simulating what might happen deep inside a black hole, for example.\n\nBut quantum computers won\u2019t reach their real potential \u2014 which will require harnessing the power of millions of qubits \u2014 for more than a decade. Exactly what possibilities exist for the long-term future of quantum computers is still up in the air.\n\nThe outlook is similar to the patchy vision that surrounded the development of standard computers \u2014 which quantum scientists refer to as \u201cclassical\u201d computers \u2014 in the middle of the 20th century. When they began to tinker with electronic computers, scientists couldn\u2019t fathom all of the eventual applications; they just knew the machines possessed great power. From that initial promise, classical computers have become indispensable in science and business, dominating daily life, with handheld smartphones becoming constant companions (SN: 4/1/17, p. 18).\n\nSince the 1980s, when the idea of a quantum computer first attracted interest, progress has come in fits and starts. Without the ability to create real quantum computers, the work remained theoretical, and it wasn\u2019t clear when \u2014 or if \u2014 quantum computations would be achievable. Now, with the small quantum computers at hand, and new developments coming swiftly, scientists and corporations are preparing for a new technology that finally seems within reach.\n\n\u201cCompanies are really paying attention,\u201d Microsoft\u2019s Krysta Svore said March 13 in New Orleans during a packed session at a meeting of the American Physical Society. Enthusiastic physicists filled the room and huddled at the doorways, straining to hear as she spoke. Svore and her team are exploring what these nascent quantum computers might eventually be capable of. \u201cWe\u2019re very excited about the potential to really revolutionize \u2026 what we can compute.\u201d\n\nQuantum computing\u2019s promise is rooted in quantum mechanics, the counterintuitive physics that governs tiny entities such as atoms, electrons and molecules. The basic element of a quantum computer is the qubit (pronounced \u201cCUE-bit\u201d). Unlike a standard computer bit, which can take on a value of 0 or 1, a qubit can be 0, 1 or a combination of the two \u2014 a sort of purgatory between 0 and 1 known as a quantum super\u00adposition. When a qubit is measured, there\u2019s some chance of getting 0 and some chance of getting 1. But before it\u2019s measured, it\u2019s both 0 and 1.\n\nBecause qubits can represent 0 and 1 simultaneously, they can encode a wealth of information. In computations, both possibilities \u2014 0 and 1 \u2014 are operated on at the same time, allowing for a sort of parallel computation that speeds up solutions.\n\nAnother qubit quirk: Their properties can be intertwined through the quantum phenomenon of entanglement (SN: 4/29/17, p. 8). A measurement of one qubit in an entangled pair instantly reveals the value of its partner, even if they are far apart \u2014 what Albert Einstein called \u201cspooky action at a distance.\u201d\n\nSuch weird quantum properties can make for superefficient calculations. But the approach won\u2019t speed up solutions for every problem thrown at it. Quantum calculators are particularly suited to certain types of puzzles, the kind for which correct answers can be selected by a process called quantum interference. Through quantum interference, the correct answer is amplified while others are canceled out, like sets of ripples meeting one another in a lake, causing some peaks to become larger and others to disappear.\n\nOne of the most famous potential uses for quantum computers is breaking up large integers into their prime factors. For classical computers, this task is so difficult that credit card data and other sensitive information are secured via encryption based on factoring numbers. Eventually, a large enough quantum computer could break this type of encryption, factoring numbers that would take millions of years for a classical computer to crack.\n\nQuantum computers also promise to speed up searches, using qubits to more efficiently pick out an information needle in a data haystack.\n\nQubits can be made using a variety of materials, including ions, silicon or superconductors, which conduct electricity without resistance. Unfortunately, none of these technologies allow for a computer that will fit easily on a desktop. Though the computer chips themselves are tiny, they depend on large cooling systems, vacuum chambers or other bulky equipment to maintain the delicate quantum properties of the qubits. Quantum computers will probably be confined to specialized laboratories for the foreseeable future, to be accessed remotely via the internet.\n\nThat vision of Web-connected quantum computers has already begun to Quantum computing is exciting. It\u2019s coming, and we want a lot more people to be well-versed in itmaterialize. In 2016, IBM unveiled the Quantum Experience, a quantum computer that anyone around the world can access online for free.\n\nWith only five qubits, the Quantum Experience is \u201climited in what you can do,\u201d says Jerry Chow, who manages IBM\u2019s experimental quantum computing group. (IBM\u2019s 16-qubit computer is in beta testing, so Quantum Experience users are just beginning to get their hands on it.) Despite its limitations, the Quantum Experience has allowed scientists, computer programmers and the public to become familiar with programming quantum computers \u2014 which follow different rules than standard computers and therefore require new ways of thinking about problems. \u201cQuantum computing is exciting. It\u2019s coming, and we want a lot more people to be well-versed in it,\u201d Chow says. \u201cThat\u2019ll make the development and the advancement even faster.\u201d\n\nBut to fully jump-start quantum computing, scientists will need to prove that their machines can outperform the best standard computers. \u201cThis step is important to convince the community that you\u2019re building an actual quantum computer,\u201d says quantum physicist Simon Devitt of Macquarie University in Sydney. A demonstration of such quantum supremacy could come by the end of the year or in 2018, Devitt predicts.\n\nResearchers from Google set out a strategy to demonstrate quantum supremacy, posted online at arXiv.org in 2016. They proposed an algorithm that, if run on a large enough quantum computer, would produce results that couldn\u2019t be replicated by the world\u2019s most powerful supercomputers.\n\nThe method involves performing random operations on the qubits, and measuring the distribution of answers that are spit out. Getting the same distribution on a classical supercomputer would require simulating the complex inner workings of a quantum computer. Simulating a quantum computer with more than about 45 qubits becomes unmanageable. Supercomputers haven\u2019t been able to reach these quantum wilds.\n\nTo enter this hinterland, Google, which has a nine-qubit computer, has aggressive plans to scale up to 49 qubits. \u201cWe\u2019re pretty optimistic,\u201d says Google\u2019s John Martinis, also a physicist at the University of California, Santa Barbara.\n\nMartinis and colleagues plan to proceed in stages, working out the kinks along the way. \u201cYou build something, and then if it\u2019s not working exquisitely well, then you don\u2019t do the next one \u2014 you fix what\u2019s going on,\u201d he says. The researchers are currently developing quantum computers of 15 and 22 qubits.\n\nIBM, like Google, also plans to go big. In March, the company announced it would build a 50-qubit computer in the next few years and make it available to businesses eager to be among the first adopters of the burgeoning technology. Just two months later, in May, IBM announced that its scientists had created the 16-qubit quantum computer, as well as a 17-qubit prototype that will be a technological jumping-off point for the company\u2019s future line of commercial computers.\n\nBut a quantum computer is much more than the sum of its qubits. \u201cOne of the real key aspects about scaling up is not simply \u2026 qubit number, but really improving the device performance,\u201d Chow says. So IBM researchers are focusing on a standard they call \u201cquantum volume,\u201d which takes into account several factors. These include the number of qubits, how each qubit is connected to its neighbors, how quickly errors slip into calculations and how many operations can be performed at once. \u201cThese are all factors that really give your quantum processor its power,\u201d Chow says.\n\nErrors are a major obstacle to boosting quantum volume. With their delicate quantum properties, qubits can accumulate glitches with each operation. Qubits must resist these errors or calculations quickly become unreliable. Eventually, quantum computers with many qubits will be able to fix errors that crop up, through a procedure known as error correction. Still, to boost the complexity of calculations quantum computers can take on, qubit reliability will need to keep improving.\n\nDifferent technologies for forming qubits have various strengths and weaknesses, which affect quantum volume. IBM and Google build their qubits out of superconducting materials, as do many academic scientists. In superconductors cooled to extremely low temperatures, electrons flow unimpeded. To fashion superconducting qubits, scientists form circuits in which current flows inside a loop of wire made of aluminum or another superconducting material.\n\nSeveral teams of academic researchers create qubits from single ions, trapped in place and probed with lasers. Intel and others are working with qubits fabricated from tiny bits of silicon known as quantum dots (SN: 7/11/15, p. 22). Microsoft is studying what are known as topological qubits, which would be extra-resistant to errors creeping into calculations. Qubits can even be forged from diamond, using defects in the crystal that isolate a single electron. Photonic quantum computers, meanwhile, make calculations using particles of light. A Chinese-led team demonstrated in a paper published May 1 in Nature Photonics that a light-based quantum computer could outperform the earliest electronic computers on a particular problem.\n\nOne company, D-Wave, claims to have a quantum computer that can perform serious calculations, albeit using a more limited strategy than other quantum computers (SN: 7/26/14, p. 6). But many scientists are skeptical about the approach. \u201cThe general consensus at the moment is that something quantum is happening, but it\u2019s still very unclear what it is,\u201d says Devitt.\n\nWhile superconducting qubits have received the most attention from giants like IBM and Google, underdogs taking different approaches could eventually pass these companies by. One potential upstart is Chris Monroe, who crafts ion-based quantum computers.\n\nOn a walkway near his office on the University of Maryland campus in College Park, a banner featuring a larger-than-life portrait of Monroe adorns a fence. The message: Monroe\u2019s quantum computers are a \u201cfearless idea.\u201d The banner is part of an advertising campaign featuring several of the university\u2019s researchers, but Monroe seems an apt choice, because his research bucks the trend of working with superconducting qubits.\n\nMonroe and his small army of researchers arrange ions in neat lines, manipulating them with lasers. In a paper published in Nature in 2016, Monroe and colleagues debuted a five-qubit quantum computer, made of ytterbium ions, allowing scientists to carry out various quantum computations. A 32-ion computer is in the works, he says.\n\nMonroe\u2019s labs \u2014 he has half a dozen of them on campus \u2014 don\u2019t resemble anything normally associated with computers. Tables hold an indecipherable mess of lenses and mirrors, surrounding a vacuum chamber that houses the ions. As with IBM\u2019s computer, although the full package is bulky, the quantum part is minuscule: The chain of ions spans just hundredths of a millimeter.\n\nScientists in laser goggles tend to the whole setup. The foreign nature of the equipment explains why ion technology for quantum computing hasn\u2019t taken off yet, Monroe says. So he and colleagues took matters into their own hands, creating a start-up called IonQ, which plans to refine ion computers to make them easier to work with.\n\nMonroe points out a few advantages of his technology. In particular, ions of the same type are identical. In other systems, tiny differences between qubits can muck up a quantum computer\u2019s operations. As quantum computers scale up, Monroe says, there will be a big price to pay for those small differences. \u201cHaving qubits that are identical, over millions of them, is going to be really important.\u201d\n\nIn a paper published in March in Proceedings of the National Academy of Sciences, Monroe and colleagues compared their quantum computer with IBM\u2019s Quantum Experience. The ion computer performed operations more slowly than IBM\u2019s superconducting one, but it benefited from being more interconnected \u2014 each ion can be entangled with any other ion, whereas IBM\u2019s qubits can be entangled only with adjacent qubits. That interconnectedness means that calculations can be performed in fewer steps, helping to make up for the slower operation speed, and minimizing the opportunity for errors.\n\nComputers like Monroe\u2019s are still far from unlocking the full power of quantum computing. To perform increasingly complex tasks, scientists will have to correct the errors that slip into calculations, fixing problems on the fly by spreading information out among many qubits. Unfortunately, such error correction multiplies the number of qubits required by a factor of 10, 100 or even thousands, depending on the quality of the qubits. Fully error-corrected quantum computers will require millions of qubits. That\u2019s still a long way off.\n\nSo scientists are sketching out some simple problems that quantum computers could dig into without error correction. One of the most important early applications will be to study the chemistry of small molecules or simple reactions, by using quantum computers to simulate the quantum mechanics of chemical systems. In 2016, scientists from Google, Harvard University and other institutions performed such a quantum simulation of a hydrogen molecule. Hydrogen has already been simulated with classical computers with similar results, but more complex molecules could follow as quantum computers scale up.\n\nOnce error-corrected quantum computers appear, many quantum physicists have their eye on one chemistry problem in particular: making fertilizer. Though it seems an unlikely mission for quantum physicists, the task illustrates the game-changing potential of quantum computers.\n\nThe Haber-Bosch process, which is used to create nitrogen-rich fertilizers, is hugely energy intensive, demanding high temperatures and pressures. The process, essential for modern farming, consumes around 1 percent of the world\u2019s energy supply. There may be a better way. Nitrogen-fixing bacteria easily extract nitrogen from the air, thanks to the enzyme nitrogenase. Quantum computers could help simulate this enzyme and reveal its properties, perhaps allowing scientists \u201cto design a catalyst to improve the nitrogen fixation reaction, make it more efficient, and save on the world\u2019s energy,\u201d says Microsoft\u2019s Svore. \u201cThat\u2019s the kind of thing we want to do on a quantum computer. And for that problem it looks like we\u2019ll need error correction.\u201d\n\nPinpointing applications that don\u2019t require error correction is difficult, and the possibilities are not fully mapped out. \u201cIt\u2019s not because they don\u2019t exist; I think it\u2019s because physicists are not the right people to be finding them,\u201d says Devitt, of Macquarie. Once the hardware is available, the thinking goes, computer scientists will come up with new ideas.\n\nThat\u2019s why companies like IBM are pushing their quantum computers to users via the Web. \u201cA lot of these companies are realizing that they need people to start playing around with these things,\u201d Devitt says.\n\nQuantum scientists are trekking into a new, uncharted realm of computation, bringing computer programmers along for the ride. The capabilities of these fledgling systems could reshape the way society uses computers.\n\nEventually, quantum computers may become part of the fabric of our technological society. Quantum computers could become integrated into a quantum internet, for example, which would be more secure than what exists today (SN: 10/15/16, p. 13).\n\n\u201cQuantum computers and quantum communication effectively allow you to do things in a much more private way,\u201d says physicist Seth Lloyd of MIT, who envisions Web searches that not even the search engine can spy on.\n\nThere are probably plenty more uses for quantum computers that nobody has thought up yet.\n\n\u201cWe\u2019re not sure exactly what these are going to be used for. That makes it a little weird,\u201d Monroe says. But, he maintains, the computers will find their niches. \u201cBuild it and they will come.\u201d\n\nThis story appears in the July 8, 2017, issue of Science News with the headline, \"Quantum Computers Get Real: As the first qubit-based machines come online, scientists are just beginning to imagine the possibilities.\"", "sentiment": 0.13116661926880901},
{"link_title": "CAHF: Ethereum miners to fork before scheduled fork reduces block reward by 40%", "url": "http://www.cahf.co", "text": "HF: \u2018HF\u2019 stands for Hardfork. Developers add a mandatory rule set to change the node software. These changes make previously invalid blocks become valid. Nodes/Miners with the rule set changes will follow this chain if they have a consensus.\n\nCAHF: \u2018CAHF\u2019 stands for Community Activated Hard Fork. Like HF, developers add a mandatory rule set to change the node software and these changes make previously invalid blocks become valid. The CAHF proposal is a peaceful and voluntary departure of different community members who have different opinions or visions, and it is not intended to make an attack against other blockchain(s), even if the CAHF chain has the higher hash rate or higher value.\n\nByzantium HF: \u2018Byzantium Hardfork\u2019 is a hard fork will be occurred to post-chain (called Main-net Homestead chain). It contains EIP649 and will be the bedrock of POS(Proof-of-Stake) change.\n\nPOW: \u2018POW\u2019 stands for Proof-of-work function. A chain or network using POW may consume a lot of energy resources but has an effective, sustainable security system.\n\nPOS: \u2018POS\u2019 stands for Proof-of-stake function. A chain or network using POS may consume less energy than POW but may have security risk and less circulation on tokens.\n\nEIP649 node: An Ethereum node that has implemented EIP649 consensus rule changes.\n\nEIP649 chain: A blockchain that is valid according to the EIP649 consensus rule changes.\n\nOriginal chain(Homestead): The blockchain that uses the same consensus rules in use today.\n\nCirculating Supply(Homestead): Circulating Supply (or total amount of newly issued ether by mining & pre-sale) of Ethereum Main-net. Now circulating supply is approx. 94 million ether.\n\nLike all Ethereum user knows, Ethereum has passed over several hard forks. Several HF that have initiated to Ethereum main-net caused several consensuses between Ethereum foundation, miners and community. Despite that changes of main-net consensus and protocol updates maintained Ethereum\u2019s inherent value, however, all hard forks weren\u2019t able to pull all user\u2019s agreement. For example, hardfork that contains refund of The DAO hack occurred chain split to the one\u2019s who did not updated their protocol (Known as Ethereum-Classic chain) and the hard forked chain (Now the main-net of Ethereum)\n\nAfter 1 year later of the chain split, Ethereum\u2019s main-net has been scheduled to change to Metropolis, therefore main-net chain may pass 2 Hardforks called Byzantium and Constantinople hard fork. Before the hard fork, main-net chain\u2019s mean block time has been increased due to an extreme difficulty increase by a code called Difficulty-Bomb code. It\u2019s code\u2019s purpose was to give an incentive when main-net hardforks, eventually abandon original chain.\n\nIn order to delay the difficulty bomb, Ethereum Foundation and their dev team has decided to change the codes on Byzantium(EIP649). Meanwhile, they are also changing mining issuance 5 to 3 as they are saying since the mean of the block time has decreased total issuance of mining may have kept the same until Ethereum\u2019s POS change.\n\nThe start of necessity of issuance reduction was started from single anonymous individuals, claiming that reduction is needed for reducing the incentives for miners and reducing inflation, eventually cause POS change and it would be more \u201cgreener\u201d Ethereum (see https://github.com/Ethereum/EIPs/issues/186).\n\nHowever, EIP649 including issuance reduction poses a significant risk for the Ethereum ecosystem, so we are preparing a contingency plan to protect the economic activity on the Ethereum blockchain from this threat.\n\nThe purpose of this blog post is to announce our CAHF contingency plan for EIP649/Metropolis.\n\nLate September, The Byzantium hard fork including EIP649 will trigger chain split and will be adopted at higher chances.\n\nEIP649 is very dangerous for Ethereum ecosystem including users and miners. Smaller incentives for mining is equal to smaller payment for security of main chain, and the risk of security caused from decrease of mining hashes may grow up higher. Since miners won\u2019t spend their energy or equipment to smaller intensive, they would likely move to another chain before POS change. Therefore, the mining activity behind a Byzantium chain may stop without notice, and investors who bought in the EIP649 propaganda may lose all their investment. Any exchanges that decide to support a Byzantium token after the forking point need to consider the stagnation risk attached to it.\n\nAlso decreasing supplement of ether without POS may cause serious economic problem based on \u201cunplanned\u201d supply such as deflation or other arguments from investors, eventually make investors reconsider their investing activity to Ethereum.\n\nFurthermore, We, the Ethereum holders or miners has a right and \u2018duty\u2019 to \u2018capture\u2019 or \u2018preserve\u2019 our investigation which is the main-net of Ethereum against the risk, we should build a contingency plan for our original chain being attack.\n\nThis plan is for Community Activated Hard Fork, or CAHF. You can find technical specs here: https://github.com/CAHF/Ethereum-CAHF\n\nCAHF will be activated \u201cright before\u201d Byzantium HF. We will update the specific time ASAP when the blockstamp of Byzantium hardfork is updated.\n\nCAHF chain will follow and fork existing rules of the Ethereum\u2019s main-chain, except for issuance reduction and POS. We will maintain our POW chain which will be the Ethereum-POW chain (when Ethereum hardforks to POS).\n\nDespite of maintaining \u201clarge\u201d network of cryptocurrency costs lots of human resources and other consumption, we will issue 10% of pre-mine inflation for \u201cDevelop and \u2018storing value\u2019\u201d stake. (This will be 1% inflation of total supply)\n\nOnce we mine our stake \u2018privately\u2019 and check that there is no further problem on the chain, we will make our repository \u2018public\u2019 and release our CAHF geth node. (Will be updated to our twitter account)\n\nOur CAHF developers are serious for monetary policy of CAHF chain therefore we decided CAHF\u2019s total supply approx. 1 billion. When CAHF chain issued total 5 hundred million, we will halve the issuance to 2.5. This decision is from that \u201cEthereum is not an asset, is a currency\u201d.\n\nWe share the same belief with some very early Ethereumers, that decentralization means that more than 1 billion people in 200 countries are using Ethereum as a saving currency and payment network, which comprises of hundreds of thousands of Ethereum services, traders, exchanges and software to be decentralized. We do not believe that decentralization means a decrease of issuance so there will be no more inflation before POS change or change to POS so some accounts that has larger volume of Ethereum takes most of issuance than who doesn\u2019t.\n\nCurrently, there are at least 4 client development teams working on the code of the spec. All of them want to stay quiet and away from the propaganda and troll army of certain companies. They will announce themselves when they feel ready for it. Users will be able to install the software and decide whether to join the CAHF.\n\nThe software is expected to be ready before late September, and it will be live on testnet by then.\n\nFor other parties in the ecosystem, we recommend detailed research on effects of the EIP649. All Ethereum businesses must be prepared on that day to mitigate or eliminate the risks that EIP649 carries.", "sentiment": 0.0020289234574948863},
{"link_title": "If You Think Basic Income Is \u201cFree Money\u201d or Socialism, Think Again", "url": "https://medium.com/basic-income/if-you-think-basic-income-is-free-money-or-socialism-think-again-4a17e8d15b1", "text": "First, saying basic income is socialism is as absurd as saying money is socialism. It\u2019s money. It\u2019s all it is. What do people do with money? They use it in markets. In other words, basic income is fuel for markets. Markets are a wonderful invention that serve to calculate via a massively distributed computer comprised of people, what goods and services should be made, using what, going where, by whom, of what quantity, etc. It\u2019s an incredible act of decentralization built upon supply and demand signaling.\n\nWhen someone has money and wants to buy something, that is a demand signal. Businesses meet this signal with supply. Basically, buying is like voting. We vote on what we want using money as our ballots, and we do this over and over and over again, every day. Now imagine someone has no money in a system built around markets. How do they vote? They can\u2019t. The market thus confuses this lack of a vote as a \u201cno\u201d vote. These two signals are of course very different. One is zero and one is null, but markets don\u2019t know that. They can\u2019t differentiate between them. This means markets containing people who don\u2019t have enough money to signal their demand can\u2019t function properly.\n\nHave you ever played the game Monopoly? I\u2019m sure you have. Is that a game about socialism? According to \u201cfree money is socialism\u201d logic it is, because everyone starts the game with free money and everyone gets free money for simply passing Go. But I\u2019m sure you know the game is actually about capitalism, right? It\u2019s just a smart enough game to know that a game involving money requires that all players get a minimum amount of money.\n\nHave you ever heard of the Alaska dividend? Every year since 1982, all residents of Alaska receive an equal dividend as their share of the dividend of the Alaska Permanent Fund (APF), now worth over $61 billion. Rich or poor, adult or child, everyone has received a Permanent Fund Dividend (PFD) of $1,000 per year on average for over 30 years. Is Alaska a socialist utopia? Do people move to Alaska to worship at the altar of Karl Marx? Nope. Alaska is a red state, but not that kind of red state. It\u2019s a conservative state where everyone gets what too many media outlets refer to as \u201cfree money\u201d.\n\nOf course, it\u2019s not actually free is it? It comes from someone. It comes from the oil companies. How? Well, Alaska owns the land and charges oil companies for the right to drill in it. They then put 25% of the earnings in the APF. Being the conservative state it is, they figure that the government should keep it\u2019s damn hands off that money. The APF is not to be touched, and the dividends go directly to Alaskans because Alaskans know far better what to do with that money than government does. As a result, the dividend boosts and stabilizes the economy, and every year come dividend time, local businesses compete against each other for the business of Alaskan consumers armed with dividend checks.\n\nDoes that sound like socialism to you?\n\nIt didn\u2019t to Milton Friedman or Friedrich Hayek, both of whom supported the idea of basic income, and both of whom can be considered fathers of free market economics. Hayek was even from the Austrian school. If you are confused why free market economists could like the idea of basic income, that\u2019s because you probably don\u2019t actually know what basic income is.\n\nBut that\u2019s not the only problem in such thinking. A bigger problem appears to be a preference of ideological thinking over scientific thinking. Here\u2019s what you need to do to avoid this mental trap. Go out and find experimental studies that show how given two populations, one a control group, and one a group where the experimental variable is cash without conditions, that the one given money ends up worse off. Also, good luck with that. Because I\u2019ve been looking for years now, and all I keep finding are study after study of people ending up better off.\n\nOne of the biggest problems with government, and believe me, I recognize its many problems, is that it\u2019s full of people who think they know what\u2019s best for other people. But the thing is, it\u2019s that way because that\u2019s how we are. We vote for people who think they know what\u2019s best for people because we think we know what\u2019s best for people. It\u2019s our problem. And it\u2019s a big problem. Why?\n\nBecause that\u2019s how places get to be what Americans think of when they think of the Soviet Union. Don\u2019t trust people by giving them money and letting them spend it as consumers in markets. Give them food. Don\u2019t let them choose what kind of food. They don\u2019t know what\u2019s best. Give them approved food as determined by a board of health specialists. Give them housing. Don\u2019t let them choose their housing. Build public housing that\u2019s best for them. Give them this. Give them that. Just don\u2019t give them money, because money enables too much freedom. Money can be spent on anything. And don\u2019t give money unconditionally, because money without conditions means lack of control. Money with conditions means people must do what we want them to do. No having kids out of wedlock. Two kids maximum. Fill out 10 job applications per day. No refusing a job no matter what it is or what it pays. Work eight hours a day. Go to school. Retrain. Do what we say, or else. That\u2019s control. Conditions are control. Lack of conditions is freedom.\n\nDo you think seniors receiving Social Security are under the control of government, because they\u2019re all so scared that it can be removed at any time? Hell no. The exact opposite is true. Seniors vote in large numbers to make damn sure government responds to their needs. Government is afraid of seniors. And seniors can use their Social Security checks on anything. It\u2019s cash. They can also work to the day they die if they so choose. No one is stopping seniors from earning additional income. Their Social Security checks are simply an absolute minimum. It\u2019s a monthly non-zero income floor from age 65 or so till death, possibly 50 years.\n\nDo you honestly think seniors are worse off with Social Security? That someone receiving it for 30 years is suffering under an oppressive socialist government that\u2019s controlling them, and if only they stopped receiving checks, they would be emancipated and private charity would step in and lift them up just as high or even higher? Because there\u2019s really clear evidence that before Social Security seniors were far worse off than after, and as Social Security spending per capita has risen, poverty rates have fallen in lock step.\n\nBut again, such understanding requires looking at actual evidence doesn\u2019t it? It requires comparing the map that\u2019s in our heads with the actual ground. Do they match? Always remember, the map is not the territory.\n\nHere\u2019s some more of the actual ground. UBI and UCT (unconditional cash transfer) experiments and programs have shown decreased hospitalization rates via reduced stress, lower crime rates, higher education rates, better education scores, healthier eating, reduced drug/alcohol use, increased entrepreneurship (3x more than control groups), economic multipliers ($1 grows the economy more than $1), healthier baby weights due to better maternal nutrition, higher rates of home ownership, reduced teen pregnancy rates, etc. The list goes on and on.\n\nIt\u2019s really hard to look at the evidence and say giving people money is wrong, because having enough money means being able to buy enough food. It means being able to have a roof over your head. For those without money, money means everything. It can even mean the difference between life and death.\n\nNow maybe you\u2019re so extreme in your ideology that you don\u2019t care about how basic income improves lives. Maybe you only care that the money had to come from someone in the top 20% (because the bottom 80% would all benefit from universal basic income), and that it\u2019s stealing and therefore wrong. It\u2019s wrong to eliminate poverty if it means yachts are a foot smaller, right? Okay, let\u2019s look at that.\n\nOnce upon a time, none of the land was owned. Sure, in the US, natives lived here, but screw them, right? We started to carve up the land with invisible lines. It was done all over the world. What once wasn\u2019t owned, became owned. That was a great deal for the new owners, not so much for those whose land was taken, or who lost access to the land or even just free passage through it. That land was passed down generation after generation and here we are now with everything owned where human beings are born onto a planet they aren\u2019t allowed to exist on unless they work for those who own the planet.\n\nNow that should be a problem to anyone who cares about voluntaryism. Should you be able to force someone to work by withholding the access they would have otherwise had to meeting their basic needs, had we not put walls up everywhere? I for one think the labor market should be fully voluntary.\n\nEveryone should get enough money to be able to refuse to work. That way, the incentive is shifted to employers, where it should be. Want someone to do something? Pay them enough to do it. If you refuse to pay anyone enough thanks to their ability to tell you to take your job and shove it, then either automate that job, or let it go undone. If the work really needs to be done, you\u2019ll sweeten the deal or hand it to a robot.\n\nBy the way, the best work is work done voluntarily. People are more engaged and productivity is higher. You should know that, but if you are fighting against a free labor market and for forcing people to do the bidding of those who own enough property to command people to do their bidding, maybe you don\u2019t. Or maybe you\u2019re the one enjoying the involuntary labor force?\n\nNow, one of America\u2019s founding fathers looked at this whole loss of common property situation and thought wait a second, when someone owns land, that land used to belong to everyone who no longer has access to it, and so they owe a rent to everyone to compensate everyone for their loss in exchange for the right of private ownership. That man was Founding Father Thomas Paine, and he thought the revenue from that rent should supply everyone an unconditional grant when reaching the age of adulthood and also pay for seniors and the disabled. Sound familiar?\n\nLet\u2019s also look at what\u2019s been happening in the economy. For decades, almost all the economic growth has gone to the top ONLY. Our productivity has been going up and up and up, but wages haven\u2019t.\n\nIt didn\u2019t used to be this way. It used to be that as your income went down, the faster your income grew. Now it\u2019s the opposite. Now, as your income goes up, the faster your income grows.\n\nEven weirder, why did hours worked stop going down as they had for centuries, and instead start creeping back up starting in 1980? Does this make any sense in a country where productivity is increasing, that people are working more hours and earning less money?\n\nSo then let\u2019s take a quick look at the technology picture where so many appear to subscribe to the silly notion that technology is nothing to worry about because jobs are always created. Yes, we\u2019ve been creating jobs as we\u2019ve lost jobs, but let\u2019s look closer. We\u2019ve been automating mid-skill, fairly high productivity jobs because the tech capability was there and the price point was right. Those unemployed were then put into new low-skill low productivity jobs, because the tech ability to automate those jobs wasn\u2019t there (yet), and the price point wasn\u2019t right (yet).\n\nIn other words, here many \u201cexperts\u201d are saying everything is just fine and dandy when someone loses their $60,000 full-time 40-hour per week career job to a machine and gets handed two part-time $20,000 60-hour per week shit jobs. Notice how this explains the rise in hours worked? People need to work more hours in order to try to prevent their incomes from falling.\n\nMeanwhile, what happens to all these newly created low-skill jobs once technology is cheap enough to even handle them too? Which jobs are left when technology is not only doing all the routine labor, but also the non-routine thanks to artificial intelligence breakthroughs like deep learning neural nets? There\u2019s nowhere else to go except for the lucky few. Game over.\n\nYes, it really is different this time.\n\nIt\u2019s also not only about how much people earn, but when they earn it, so let\u2019s also make sure and notice that income variability has increased as well so even if someone is making the same amount of income per year working in a new job, their pay may vary a lot more month to month, which introduces the ability to fall behind on bills and fall into downward spirals of debt.\n\nSuffice to say, our entire safety net is built around an antiquated notion of jobs, and universal basic income is a key component of a 21st century system built as a solid foundation instead of a hole-filled net.\n\nBy the way, about those holes, in the US about 1 in 5 people living under the federal poverty line get TANF (our cash welfare program). About 1 in 4 people who qualify for housing assistance get it. About 1 in 5 people with a disability get disability income. Basically, the way the system works right now is to exclude far more people in need than it includes. Even worse, due to targeting of benefits, those who receive them see the highest marginal tax rates of all. No billionaire sees as high a tax rate.\n\nHow does that work you ask? Well, by providing a targeted benefit, it gets pulled away when they earn income. Essentially, welfare punishes work. It\u2019s built as a ceiling instead of a floor. Would you work a shit job if you were promised 5 cents on the dollar? Of course not. Why would you? Because any job, even a shit job that effectively pays 36 cents an hour provides the meaning of existence?\n\nOne final thing. Universal basic income need not be done via taxation, just as the Alaska dividend isn\u2019t via taxation. It can be done in many ways including by non-debt-based money creation, which by the way is mostly done by private banks right now, not the government. And yes, even cryptocurrencies could be designed to provide UBI. So an issue against taxes is not necessarily an issue with UBI itself, at least it shouldn\u2019t be, but with how it\u2019s funded. In that case, be a part of the solution instead of part of the problem by talking about the best way to go about basic income instead of lying to people purposely or out of ignorance about what basic income actually is. Basic income is not left or right. It\u2019s forward.\n\nBasic income is also not charity. It\u2019s not a handout. It is owed to you. It\u2019s your dividend. It\u2019s your compensation for your loss of access to this rock in space called Earth. It\u2019s your return on investment for the tax dollars invested in Level 1 research and development and thus the technology unemploying you. It\u2019s your royalties for the big data which everything you now do (and even don\u2019t do) is increasingly generating. It\u2019s your share of rising productivity that used to go to you before it was stolen from you. It\u2019s your right to life, and just as no one has the right to take your life, no one has the right to force you to do anything for them in order to stay alive.\n\nBasic income is not free money. It\u2019s freedom. And freedom belongs to you.\n\nI like markets. I want to reduce the size of government. I want less bureaucracy. I want less administration. I want fewer government jobs. I want lower taxes for everyone who hasn\u2019t been benefiting from our economic growth for decades. I want a simpler tax code. I want fewer subsidies. I want less market distortion. I want a voluntary labor market. I want more freedom. If we appear to want many of the same things, I urge you to question what you think you know, and spend some real time looking into basic income.\n\nWith that said, here\u2019s how I recommend we go about doing universal basic income in the US in a way that doesn\u2019t raise anyone\u2019s income tax rates.", "sentiment": 0.12795036687893824},
{"link_title": "Deep AutoEncoders for Collaborative Filtering", "url": "https://github.com/NVIDIA/DeepRecommender", "text": "This is not an official NVIDIA product. It is a research project described in: \"Training Deep AutoEncoders for Collaborative Filtering\"(https://arxiv.org/abs/1708.01715)\n\nThe model is based on deep AutoEncoders.\n\nThe code is intended to run on GPU. Last test can take a minute or two.\n\nIn this example, the model will be trained for 12 epochs. In paper we train for 102.\n\nNote that you can run Tensorboard in parallel\n\nAfter 12 epochs you should get RMSE around 0.927. Train longer to get below 0.92\n\nIt should be possible to achieve the following results. Iterative output re-feeding should be applied once during each iteration.", "sentiment": 0.0},
{"link_title": "How Hurricanes Turn Nature Upside Down", "url": "http://nautil.us/blog/how-hurricanes-turn-nature-upside-down", "text": "Alligators wandering through inundated streets, snakes hiding on porch doors, deer careening across neighborhoods, and other wild sights emerged in the aftermath of Hurricane Harvey. What else would you expect? Hurricanes can shift ecology in strange ways.\n\nThe Hawaiian island of Kauai gets overrun with feral chickens after hurricanes. In 1982 and 1992, after Hurricanes Iwa and Inika, respectively, chickens escaped coops and interbred across the island. Hurricane Georges, in 1998, swept the red fruit bat (Stenoderma rufum) onto islands east of Puerto Rico, where they were thought to be extinct. Hurricanes have even caused large, flightless animals, like howler monkeys, to split into other species.\n\nHurricanes also tweak the natural rhythms of organisms in human habitats in disturbing, impressive ways.\n\nAfter Hurricane Sandy, pest controllers received an overwhelming amount of calls concerning rat infestations near storm shelters. In the wake of Harvey, entire colonies of fire ants, expelled from soil, linked their water-repellent bodies together into a raft. Encounters between one floating mound and another produced fights.\n\nNon-native and invasive species of plants also break new ground in a hurricane. Ivan, in 2004, brought a wave of torpedograss onto the Gulf coast, and rattlebox rode in on Hurricane Opal\u2019s coattails in 1995. In the Caribbean Sea, hurricanes can pound coral reefs, decreasing coral cover by 15 to 20 percent.\n\nHurricanes also loose harmful microbes. Following Hurricane Matthew in 2016, people feared the dispersal of the coastal water-loving Vibrio cholerae bacterium, which causes cholera. Hurricane Sandy, in 2012, also threatened a surge in E. coli populations, as raw sewage filled the streets.\n\nWhen hurricanes tear apart cityscapes and shorelines, and humans rebuild them, the biosphere twists and turns, shuffling the ecological deck.\n\n\n\nSilvia Golumbeanu is an editorial intern at Nautilus.", "sentiment": 0.06871224014081155},
{"link_title": "The Sun has produced a whole bunch of solar flares this week \u2013 The Verge", "url": "https://www.theverge.com/2017/9/9/16277434/nasa-sunspot-solar-flare-cycle-dynamics-observatory", "text": "While the hurricanes popping up in the Atlantic has captured everyone\u2019s attention, the Sun has had its own active week as well. A total of six solar flares have erupted from the same active sunspot since Monday \u2014 including the largest flare the Sun has produced in its current cycle.\n\nSunspots are cool, dark regions on the Sun\u2019s surface with strong magnetic fields. These solar phenomena pop up on the Sun from time to time, sometimes relatively frequently. In fact, the Sun has a roughly 11-year sunspot cycle \u2014 the result of the its changing magnetic field. Magnetic material inside the Sun is always moving and rising to the surface, and it eventually causes the Sun\u2019s north and south poles to flip. Because of this, the Sun alternates between two periods: solar maximum \u2014 when sunspots are much more frequent on the Sun\u2019s surface \u2014 and solar minimum \u2014 when the Sun\u2019s surface is relatively sunspot free.\n\nthe largest flare the Sun has produced in its current cycle\n\nRight now the Sun is actually on the downswing, heading toward solar minimum. But NASA says sunspots can still form on the Sun, and they can sill lead to powerful solar flare eruptions. These flares don\u2019t pose any significant threats to Earth, but they can mess with our power systems. Flares are associated with something called coronal mass ejections, when high-energy plasma is sent out into space. When this reaches Earth, the plasma interacts with the magnetic field and particles surrounding our planet, causing geomagnetic storms that can impact satellites and even electronics on the ground.\n\nThe biggest flare to erupt from this active sunspot occurred on September 6th, and apparently the radio blackout that it caused has already passed. Check out the flares below, thanks to observations from NASA\u2019s Solar Dynamics Observatory \u2014 a satellite that has been monitoring the Sun since 2010.", "sentiment": 0.1195436507936508},
{"link_title": "How Our Obsession with the Future Is Destroying the Web with Maciej Ceglowski", "url": "https://dev.to/niko/how-our-obsession-with-the-future-is-destroying-the-web-w-maciej-ceglowski-", "text": "Maciej contrasts web developers to our game developer siblings, that often have 5 years between consoles. He cites this, as why you can track massive improvements throughout the years as they familiarize themselves with and master the current technology set. Maciej argues, that today web designers and developers aren't given any time to get used to the current set of devices and capabilities, before we\u2019re already being sent to work on the next set, so what is passed off as web technology improvement is just bloat.\n\nMaciej's slide (shown in this article's cover image), shows the old technology stack the internet is built on as an example of how things fossilize in technology when they are useful.\n\nThe talk's concluding questions and statements rocked me:\n\nWhat are we personally building the web for (to connect the world, eat the world, or end the world)? Also what would the internet look like/how would we change our development practices, if we treated it as a medium that was meant to last?", "sentiment": -6.1679056923619804e-18},
{"link_title": "In Cloud Software Wars, Mesosphere Bows to Kubernetes \u2013 The Information", "url": "https://www.theinformation.com/in-cloud-software-wars-mesosphere-bows-to-kubernetes?shared=b05a3c", "text": "Mesosphere has been trying to counter the Kubernetes trend for the past few years. Its first attempt to respond came in 2015 when it built custom software that made it possible to run Kubernetes on top of DC/OS. But the product didn\u2019t take off because it created additional complexity for customers, said a person with knowledge of the product. This time, Mesosphere is integrating Kubernetes more deeply in order to address these issues, said the person. Last year Mesosphere began offering a free version of DC/OS as part of its effort to fight Kubernetes. It primarily generates revenue from offering customer support to people who use DC/OS, as well as from paid tools that run on top of DC/OS.", "sentiment": 0.19090909090909092},
{"link_title": "Baby clothes that grow with your ballooning baby are now a thing UK", "url": "http://www.wired.co.uk/article/james-dyson-winner-petit-pli-children-clothing#_=_", "text": "What if you didn't have to buy new clothes for a growing child every couple of months? A new fabric, inspired by solar panels and satellites, means baby clothes grow as a baby does.\n\n\"We have limited resources on Earth so we need to be clever about how we use them,\" says engineer Ryan Yasin, 24, the UK winner of this year's James Dyson Award. \"Because this is seven sizes in one, you're not buying, manufacturing and transporting seven times as many garments.\"\n\nYasin's Petit Pli clothing expands to fit children from six to 36 months. This is possible, he says, because the material uses the Negative Poisson\u2019s ratio. If you pull the pleated garment along it's length, it will grow along its width simultaneously.\n\nEstablished in 2007, the James Dyson Award is an international student design competition with a brief to create something that solves a problem. Yasin will receive \u00a32,000 for winning the UK round of the competition. The international winner, who will receive \u00a330,00, will be announced on October 26.\n\nYasin was inspired by his two-year-old nephew, who kept growing out of the clothes he bought him. \"Usually in children's wear brochures you see these little angels, but we are designing clothes for little rugrats who are running around, exploring,\" he says.\n\nHe also believes his garments could challenge a reliance on fast fashion. \"If you start raising children with this clothing, they they won't have that readiness and willingness to keep buying new clothes,\" Yasin says. \"Children are used to outgrowing their clothes but it doesn't have to be like that. We can be more resourceful.\"\n\nYasin has a degree in aeronautical engineering and is a graduate from the Royal College of Art. This interdisciplinary background, he says, helped to inspire the project \"It gave me different insights into the fashion industry; people aren't used to engineers coming into this quite archaic industry.\"\n\nThe fabric is also heat-treated to make it last longer, hydro-phobic and windproof. Petit Pli was the result of over 500 prototypes, but Yasin isn't stopping there. His team are working on making a garment out of a single material, so that it can be as recycled more easily.\n\nYasin will use his winnings to expand the business and help bring a small range of products to the market. And, he says, a lot of those sales could happen online. \"Typically, only 30 per cent of children's clothes are bought online. That's because of concerns about buying the right size, so as my product is good for seven sizes, this worry is removed.\"", "sentiment": 0.1267913121361397},
{"link_title": "Food Stamps Are Finally Being Disrupted", "url": "https://www.wired.com/story/startups-are-finally-taking-on-food-stamps/?mbid=social_tw_backchannel", "text": "Felicia Graybill uses her smartphone for everything: sending email, checking Facebook, and even monitoring her bank account. But for years, when the 28-year-old Brooklyn mom went to check on her food stamps benefits she might as well have been using a landline. Reviewing her balance required dialing into a hotline and entering her entire card number. All she could access was the sum of her funds\u2014there was no way of breaking down how and when she\u2019d spent the money.\n\nThere is an endless variety of apps designed to manage life for the upper middle class, but low-income Americans\u2014a group that spends a disproportionate amount of its budget on basic necessities\u2014don\u2019t benefit from the same time-saving hacks. Registering for housing assistance or putting food on the table requires reams of paperwork, DMV-esque long waits, and thorough interviews\u2014many of which have to be completed in person. Apps have inarguably made life easier, but only for those who already have resources to spare.\n\nThanks to new trends in civic technology, that\u2019s beginning to change. Young tech workers have increasingly noticed the wide-open opportunity to bring disruption to outdated social programs. With a user base of nearly 43 million Americans, the Supplemental Nutrition Assistance Program (SNAP), which provides food stamps, is ripe for innovation.\n\nTake Graybill. Now she uses FreshEBT, an app launched by mobile software startup Propel. The app allows her to check her balance on her phone and organize her budget around local deals using an online shopping list. Keeping tabs on her account lets her know what she can and can\u2019t buy while she shops. \u201cIf I need to check my balance outside the grocery store, now I can just privately look on my phone,\u201d says Graybill.\n\nWith 250,000 active users, FreshEBT is primed for growth\u2014and in April, Propel announced $4 million in seed funding from big names like Andreessen Horowitz. Yet Propel is just one of several companies stepping in to up-level the technology for public sector services. Some states, such as Texas and New York, have managed to roll out new websites and apps that help welfare recipients manage their benefits. But not all of their peers have innovated as quickly. At the state level, where most social services (including SNAP) are implemented, there isn\u2019t enough money and technical know-how to keep digital communications up to date. Though organizations like the US Digital Service and 18F pioneered building technical infrastructure at the federal level, thanks to the hiring freezes, budget cuts, and high profile exits that have occurred during the Trump administration, they face an uncertain future.\n\nThis innovation is shifting to the private and nonprofit sectors, where companies are using both classic venture capital models and more traditional forms of state contracting to significantly improve civic technology. Code for America, a nonprofit founded in 2009 to connect developers and designers with local and state governments, had previously tackled community problems like building an app to identify the fire hydrants that needed shoveling after snowstorms. But its leaders turned their attention to the SNAP program after finding it in need of a revamp. California has one of the lowest SNAP enrollment numbers in the country (in 2014 nearly half of eligible residents weren\u2019t receiving benefits); through one of the organization\u2019s fellowships, Code for America developers started working on a mobile web application in 2013.\n\nThe early problems weren\u2019t just technical. For example, to get more people to apply, they cut the 200-question application to a simple 10-minute questionnaire. \u201cAt a high level, a lot of people in government are constantly looking at how to do things better,\u201d says Dave Guarino, senior software engineer at Code for America. \u201cWhat we bring is a way to test things on a smaller scale, rather than just plan, plan, plan, and try to take a year to come up with a perfect solution.\u201d\n\nWorking closely with local community organizations and actual clients using the app allowed the team to find other barriers quickly. Program administrators send out text reminders, in lieu of the paper mail that\u2019s often ignored, when it\u2019s time to send in renewal materials. An in-house bilingual team at Code for America also answers client questions via SMS.\n\nThe tech behind the new crop of food stamp apps may be simple, but the user testing was not. Code for America developers also had to deal with local government bureaucracies often laden with outdated or unnecessary barriers. For instance, Code for America employees convinced one county director to eliminate an additional form they hadn\u2019t seen any other county use. The director didn\u2019t even know the practice was still in place.\u201cWhen you\u2019re working with many different layers of government, you can often end up with friction,\u201d says Sarat Mayer, Chief Program Officer at Code for America.\n\nPropel founder Jimmy Chen, who cut his teeth as a product manager at Facebook before joining the social-impact incubator Blue Ridge Labs, initially set out to change the enrollment process. But when canvassing Philadelphia grocery stores, his team found that many of its potential clients\u2014around 80 percent, Chen says \u2014were already signed up for benefits. Like Graybill, the problem was in using the program: They didn\u2019t have a way to check their balance without a lengthy call process. So the company pivoted to its current form: apps that functions like mobile-banking for EBT benefits.\n\nLyndon Jackson founded Panacea Financial, a smartphone app, with the same purpose\u2014giving EBT users a way to monitor their benefits on their phones\u2014but his motivations are much more personal. Both he and his lead developer grew up on food stamps, so his initial product testing involved family and friends. \u201cWhen I was talking to some of these people, it was very exciting because basically they had been ignored,\u201d says Jackson. After receiving $25,000 in seed money from a Chicago pitch competition, the company is still testing. But Jackson is optimistic about its ability to grow quickly, citing relationships with other local civic ventures like MRelief, an app that connects users with community services via SMS.\n\nPropel\u2019s founder hopes to make money by selling advertising on the app to grocery stores. FreshEBT already boasts coupon and rewards partnerships with several major chains. In contrast, Code for America now supports its CalFresh work with a combination of government funding and philanthropic dollars. It recently signed a two-year contract with the California state government for $3.6 million to continue work on the program. Given the instability of both government funding and early startups, it\u2019s hard to say if either model is more tenable than the other.\n\nThe FreshEBT has also had a significant impact on its users. In a study with Duke University, Propel found that average SNAP recipients spent more than 80 percent of their SNAP benefits within the first nine days, completely exhausting the sum by day 21. But when given an in-app tool that showed them a weekly budget instead of the entire balance, users stretched their monthly balance by two days\u2014about six meals a month.\n\nMobile presents a big opportunity to innovate for low-income users. According to the Pew Research Center, 10 percent of Americans are \u201csmartphone dependent,\u201d meaning they have no other form of internet access at home. Americans making less than $30,000 are 13 times more likely to be smartphone-dependent than those making more than $75,000 a year. CalFresh developers found that one third to about one half of searches for the application came from mobile phones, a platform on which the old site was unusable.\n\n\u201cOne of the things we noticed is that the food stamp office is full of hundreds of people waiting in line and a majority of them have a smartphone in their hand,\u201d says Chen. \u201cSocial services had in some ways lagged behind what technology could do.\u201d\n\nCorrection: This article originally stated that Panacea Financial received $5,000 from a Chicago pitch competition. The company in fact received $25,000.", "sentiment": 0.054909812409812395},
{"link_title": "We Rebuilt Evernote for iOS in Swift", "url": "https://blog.evernote.com/tech/2017/09/08/we-rebuilt-evernote-for-ios-in-swift/", "text": "By 2016, the developers of Evernote for iOS were struggling to add new features to the app, keep it working with new versions of iOS, and untangle technical debt in the code. To fix this, we began an effort that ultimately produced an app with a new design, a new user interface, and a new software architecture\u2014all delivered by a mostly new team.\n\nThe previous version of Evernote for iOS (7.x) was designed, built, and released alongside Apple\u2019s iOS 7 in the fall of 2013. In the following years, it acquired millions of users and a lot of new features, but also a lot of technical debt. This burden was manageable until three things led us to reconsider the product from the ground up: we had a new design to implement, a new editor to integrate, and a newly staffed team ready to build a more sustainable app.\n\nOur iOS team had an engineering manager, a product manager, two designers, several QA people, and four developers. Although we knew rebuilding our app would be a big effort, we couldn\u2019t have everyone on the team start working on it immediately. We had to continue to support our current version, so we began with one person exploring the rebuild and bringing issues back to the team for discussion.\n\nThe developers began with a retrospective look at the existing code and architecture. It can be difficult to step back from an app that you are intimately familiar with and emotionally invested in and ask \u201cWhat would I do if I started it all over again?\u201d But it can also be liberating, and after a few sessions where we reexamined the origin, design, and purpose of each piece, we were gleefully throwing stuff out and rethinking how to rebuild the app.\n\nWe also knew that how we made decisions as a team was going to be very important, as we were embarking on a rewrite with many thousands of important decisions to be made over the coming months.\n\nRather than have a single technical lead dictating strategy, the four developers decided to work together and found that they were in broad agreement about the outline, if not all the details. Where they disagreed or were unsure, we encouraged them to experiment and demonstrate the results. To move ahead, we had to tolerate some uncertainty but remained confident we could work our way out of trouble. We were collaborative, often passionate, yet always professional.\n\nHaving a rough plan also let us focus on the skills we wanted as we began interviewing and hiring additional people. We decided that we needed three additional iOS developers with experience working with UIKit to build native apps.\n\nArmed with a new design, a clear business case for rebuilding the app, solid support from within the company, a code name (\u201cLightning\u201d), and a team ready to go, we started defining some of the key technical requirements and constraints.\n\nModern UIKit: Since nearly all of our users run a recent version of iOS, we would be able to build our new user interface code using the latest UIKit features. This would include using auto-layout everywhere, using size classes, and support for split-screen multitasking on the iPad. To help ditch some old habits, we rewatched all the UIKit sessions from the previous couple of years at WWDC. We worked with our design team to think about iOS size classes and orientation instead of specific devices or screen sizes; we decided to define all of our auto-layout constraints directly in code for precise clarity and control; and we looked carefully at how massive ViewControllers tend to grow out of control and developed strategies to keep our new ones a reasonable size.\n\nSwift 3.0: We wanted the new code for Lightning to be modern and sustainable, so we decided to write it with Swift 3.0. We had been writing small pieces of code in Swift since its introduction, and had transitioned them with each evolution of the language, so bringing our existing Swift 2.x code up to 3.0 only took a few days of fixing build errors and dealing with all the API renaming. As we wrote new code, we also absorbed many of the changes in the standard library, API design, and naming conventions so we could make things as \u201cSwift-like\u201d as possible.\n\nA fresh start: While we had planned to write a lot of the new UI from scratch in Swift, we also wanted to preserve large chunks of the existing app. To do this, we decided to create a new Lightning project in Xcode and then determine what to bring into it from the existing project. We wanted to bring over code and assets from the previous app only after thoughtful consideration, so that the new app would only include things we were certain to use. Almost all of the old code was in Objective-C and so, with the new code in Swift, we would have a clear delineation between the two bodies of work.\n\nClean, clear, simple, bold: Instead of attempting to plan in detail all the pieces of software architecture we would need for Lightning, we embraced some values to guide their creation. We wanted classes that were clean, self-contained, single-purpose, and with few dependencies; classes, structs, enums, protocols, etc., would all have a clearly named and well-defined purpose\u2014each piece would be simple and focused on doing one thing well. We wanted to create code that worked exactly the way we intended, without premature optimization or compromise. And so we began with these goals in mind while acknowledging that the real work would end up being messy, compromised, and complicated.\n\nWe started by standing up an early build of the new app that would simply connect to the Evernote service, get some data, and display a list of notes. Our objective was to quickly create a working, but very limited, application that we could then rapidly iterate on. To do this, we had to pull some basic pieces into place.\n\nEvernote accounts: The first step when using Evernote is to sign into your account. We reused a lot of existing, well tested, Objective-C code for authentication and logging into accounts, but built new Swift code around it to have a clean presentation of an Evernote account for the rest of the app.\n\nData persistence and synchronization: We already had a successful Objective-C framework (CoreNote) for data persistence and synchronization with the Evernote service. CoreNote maintains the user\u2019s data in a CoreData database and uses the Evernote service API to keep the data in sync with the user\u2019s account. After some configuration work, we were able to connect the framework to an account and have it sync its data.\n\nData model: We built a data model with clean, simple representations of all the things in an Evernote account: notes, notebooks, resources (i.e., data attached to notes), etc. This data model would isolate and manage interactions with the persistence and synchronization framework, be independently testable, and present an API tailored to the needs of the view models.\n\nNote list: The centerpiece of the user interface in Evernote is the list of notes in an account. Later we would be putting a lot of effort into the note list, but to start we simply created a UITableViewController with generic cells that displayed the titles for each note. The data for the table came from a note list View Model that handled taking info from the Data Model and formatting it correctly for the table view.\n\nPutting these pieces in place demonstrated basic functionality and helped identify which portions of the existing code we could reuse. We also used them to outline an app architecture with simple but firm boundaries.\n\nWe knew that those boundary lines would be tested when we began adding more code. Whether by accident or temptation, it would be easy to have code in Lightning Data trying to reference things in the UI, or UI elements attempting to know about things in CoreNote\u2019s persistence scheme. These groups of code were in the same target in the same Xcode project and could easily become tangled.\n\nTo combat this, we paused development on features and began organizing the code into a few frameworks: one each for CoreNote (our persistence and synchronization library), Lightning Data (which clearly separated our data model from the rest of the app), and Lightning UI (for some common UI pieces). The CoreNote library was already a framework in Evernote for Mac, so making it one for iOS was fairly simple, but the rest of this effort was tedious, spanning several weeks, and overlapped our conversion from Swift 2.3 to 3.0. The existing code had to be untangled, with the occasional puzzle over \u201cWhich framework does this piece belong in?\u201d While creating the public API for the frameworks, we were also adjusting to Swift 3.0 and \u201cthe great renaming\u201d that came along with it. For added excitement, most weeks also brought a new beta of Xcode and the iOS 10 SDK.\n\nWhen the dust had settled and our build system could churn out the app reliably, we were then able to start building the pieces we wanted for our new app design.\n\nWe started work with a team of seven developers: four at our headquarters in Redwood City and three in Austin. A few had been tinkering with Swift since 1.0, but most were fairly new to the language. Because of the proliferation of resources for learning Swift, however, and the fact that we had a team of smart, curious people, we were confident we could start producing code without a steep learning curve.\n\nAs we began working together, we discovered that projects like this often progress through several phases: one is where you write all your Swift code much like you would write it in Objective-C. Another is where you watch the \u201cProtocol Oriented Programming\u201d video, try to use Swift protocols everywhere, then learn when to dial it back a bit. Finally, the ideas behind the \u201cgreat renaming\u201d that came with Swift 3.0 will settle in and you\u2019ll start to name things in your code the same way; you\u2019ll explore different ways to define and access properties and discover all their tradeoffs; you\u2019ll find interesting edge cases and behavior with \u201cbridging\u201d and \u201cunsafe\u201d types; and you\u2019ll start using unique and interesting features in the Swift standard library.\n\nWe asked each other, \u201cIs there a style guide?\u201d and settled on the common Swift community style used in most of Apple\u2019s code, encouraged by Xcode, and loosely documented several places online; but we didn\u2019t pour much passion into this\u2014working code was more important. Eventually, we discovered the Swift-Lint tool and ran that over the code occasionally to clean things up.\n\nThere is much to explore with a new language, but we needed to be focused on our work so, everywhere we could, we borrowed existing tools and practices from the Swift community and resisted the urge to invent our own.\n\nWe were all learning together and used several tools to collaborate: Slack, meetings, in-person visits, Jira & Bitbucket, and various flawed flavors of video conferencing. We adapted the elements of Agile and Scrum that suited us: two-week sprints, team planning sessions, daily virtual stand-ups in Slack, and frequent longer sessions where developers could discuss issues. It was not a model process, and we have since become more disciplined, but it was enough structure to keep the team focused and provide a rough idea of our progress.\n\nBitbucket was especially important, not only for code review but also for the window it provided into each others\u2019 developing style and technique\u2014more than once, we reworked code after seeing what others had created in another part of the app. Review comments were professional, specific, and constructive. Occasionally something brewed up into a bigger issue to be taken up at our next developer meeting.\n\nAllowing everyone space to blow off steam was very important. People who may be confident with Objective-C can feel unsettled when they find themselves in a new environment. Not only is Swift a different language, but many of the surrounding tools in Xcode don\u2019t work as well\u2014and sometimes not at all. Everyone has their own threshold where \u201cinconvenient\u201d becomes \u201cintolerable,\u201d so it\u2019s good to be able to stop and vent to your colleagues from time to time. Ranting about developer tools is a fixture of the profession and a normal thing to do on your way to building something great.\n\nThe note list, a vertically scrolling collection of notes, is the centerpiece of the app\u2019s UI. The list may show all your notes, notes from a specific notebook, or notes from a set of search results. Each element in the list is a \u201cnote preview,\u201d with the note\u2019s title, date, and a sample of text and images.\n\nLike much of our user interface, it shows you a list of things and lets you act on them\u2014this fits well with the classic model-view-controller scheme. Previously, our view controllers often became bloated and disorganized, so we added a view model to help organize and format data before handing it off to the views for display (Data Model -> ViewModel -> ViewController -> View). Here\u2019s how this works in practice with our note list as an example:\n\nWhen our NoteListViewController\u2019s UITableView needs to produce a cell to display a note, it gets an instance of a Note from the data model. Then it creates a view model for that note called a NotePreview. The NotePreview looks at the properties of the note, in this case the date the note was last modified. Next, it uses some predefined rules and preference settings to format the data. Since the last modified date property is from the day before the current one, it formats it as \u201cYesterday\u201d instead of a standard formatted date string.\n\nThe NotePreview and the destination NotePreviewView are unacquainted, so the NoteListViewController\u2019s configurePreviewView method takes the formatted date string from the NotePreview and drops it in the NotePreviewView\u2019s noteDataLabel. Similar operations fill in the rest of the note\u2019s title and content.\n\nThis is a simple, slightly boring, standard way to organize iOS code and that\u2019s exactly what we wanted. No one is going to write books or give conference talks about it, but we wanted our current people to be able to focus on building out the app\u2019s features, and wanted future developers to find something simple and familiar.\n\nAn exciting part of writing all the new Swift code was bringing to life a great new design for our app. When working with our designers, we started with the idea that design and development is a conversation; there isn\u2019t usually a point where the design is \u201cdone\u201d and then some coding happens and then everyone is happy. Instead, developers write some code and show the designer the result\u2014often in a working build but sometimes just a screenshot. Great work happens through iteration. Feedback ensues, then more coding, more feedback, etc.\n\nNew designs would often provoke a series of questions from developers: \u201cCan we change this a bit, there is a built-in UIKit thing I can use,\u201d \u201cI think the touch-target for this thing is too small,\u201d \u201cI\u2019ve never seen this before, it isn\u2019t copied from our Android app is it!?!\u201d \u201cWhat happens when I rotate the device?\u201d \u201cThere is another thing in the app that looks like this, can we make them the same?\u201d \u201cWhat does it do when I\u2019m offline?\u201d \u201cWhat does it look like on larger screens?\u201d\n\nAs a side note, this last question was originally, \u201cWhat does it look like on iPad?\u201d When talking with the designers, we had become (perhaps annoyingly) fanatical about referring to regular and compact screen sizes instead of \u201ciPhone\u201d or \u201ciPad.\u201d Partly this came from the trauma of attempting to get our old code to work nicely on the big iPad Pro and to support a split screen.\n\nWhile you can test a lot of things by showing people pictures and prototypes, you can\u2019t know if you have it right until you have a version of it working in the app. The work we put into having clear boundaries and narrow dependencies in the code paid off when we had to rewrite portions of the UI in response to user testing and other feedback. Because changing the app was now easier to do, it reduced the penalty if we didn\u2019t get it exactly right the first time.\n\nWhile we had regular sessions where developers could discuss whatever they liked, a lot of important feedback happened in code review\u2014most days there were six to ten new pull requests to review. We tried to keep the scope of these manageable and most were reviewed within a day, and when one would linger or seem too large, the author would usually back up and rework their PR. The most common review comments had to do with the names of things and requests for more clarity, readability, comments, and other documentation. Frequently, duplication was avoided when someone would point out similar code in another part of the app. Comments were professional, not overly pedantic, and focused on values (readability, testability, maintainability) rather than personal style. Occasionally, feedback and disagreement would expose larger issues with goals or architecture\u2014those we would pull out of the code review process and sort out in our next developer team meeting.\n\nWhile it was exciting to have lofty goals when rebuilding our app, we also knew that in the interests of time, compromise would occasionally be required. We had intended to rewrite all of the app\u2019s user interface in Swift, but paused when we looked at our app\u2019s settings:\n\nThere were many more screens of settings like these, all connected to the app in specific ways and written in fairly standard UIKit. While some minor redesign was needed, we couldn\u2019t really think of a good reason not to reuse the existing code. Rewriting was high-cost and low-value so the old code stayed in. We had many good reasons to be doing all of our shiny new architecture and code but they were our own reasons. While rebuilding the app, we reminded ourselves that the only thing our users would care about would be their experience using it.\n\nWe wanted automated testing to be an integral part of our development process, but pausing development while we created a good framework for testing was too much of a short-term penalty. We had an excellent team doing manual testing and we relied on them for much of our first release. Now we\u2019re working on expanding the scope of our tests and building them into the development of new features.\n\nOften it\u2019s when you\u2019ve just finished writing some code that you realize how you might have done it better if you started over again. In those cases, we usually only stopped to rewrite something if it was foundational and would make other things better too. Because each developer was usually focused on adding a particular new feature, we sometimes missed opportunities for building common code. When we discovered these, we would have to add a \u201crefactor and cleanup\u201d story to our backlog and move on.\n\nIn an attempt to move development along faster, we added more developers to the team. Although often disastrous, this move actually worked well for us. The foundations of the app had already been built, we had several isolated components, and the new people were highly skilled, so this made a significant impact on our backlog.\n\nAnother (and usually more successful) way to speed things along is to reduce the scope of the work. Evernote for iOS had grown a lot of features over the years so we took a critical look at each one again. Anything that no longer made sense, was infrequently used, or didn\u2019t fit the new design was cut. This allowed the team to focus on core features, quality, and polish.\n\nThere is a mythology about creating software that says it gets made by eccentric rock-star geniuses who madly code through long caffeine-fueled \u201cdeath marches\u201d and \u201ccrunch times.\u201d To build Evernote 8.0, we instead used a team of skilled, experienced, and diverse professionals. We planned and estimated together, learned together, wrote a lot of code together, and stayed focused. There was serious pressure to deliver the app and debut our new design but, while we sustained several delays, everyone kept their cool.\n\nYou can test an app as much as you like but some things you won\u2019t discover until you deliver it to millions of people. Bugs that seem rare and unreproducible in testing scale up to affect large numbers of users. Lower-priority issues turn out to have buried side-effects that are much more serious. After Evernote 8.0 appeared in the App Store, we started monitoring automated crash reports, app store reviews, social media, our support forums, and feedback to our customer service team.\n\nWe had some serious issues to address and immediately began a series of app updates. We had to build these updates quickly but with great care and focus so we didn\u2019t inadvertently introduce new problems. Some issues we could fix right away but others required more investigation.\n\nFor example, some people were reporting a crash when starting the app\u2014a problem we hadn\u2019t seen in our testing and none of our beta testers had reported. We really needed access to someone\u2019s device so we could reproduce and fix it, but finding someone who had this problem and would let us run a debug session on their phone was difficult. Luckily, we were discussing this when my brother called me to complain that our app update was crashing his phone, so I spent the afternoon in his office debugging our app startup code. The crash turned out to be a problem with interpreting some login data saved in a format that was used by an old version of the app. That version was only around for a couple months over two years ago and was a problem only if the user had remained signed in since then.\n\nOur stream of minor updates continued as we found and fixed issues. The App Store\u2019s dramatically improved app review time really helped us get these fixes in people\u2019s hands quickly.\n\nAs things settled down, we were able to start addressing some issues people had complained about but which weren\u2019t critical bug fixes. We reworked the note list to improve performance and included some minor UI changes that were nearly ready for the initial release, but hadn\u2019t made the cut.\n\nWe next took a pause to get the team together to talk about what we had done and how we could improve things in the future. As a result, we refined our process for planning, estimating, communicating, and developing. Part of looking back was making an inventory of any technical debt we had created while getting the release out the door. We added that to our backlog as well so we could begin to address it in balance with new feature development.\n\nSince we shipped, we\u2019ve built a suite of automated tests that exercise the code both at the unit level and by driving the user interface; a growing number check performance as well as correctness. Rather than have someone on the test team in charge of test automation, we moved that person to the development team and made it everyone\u2019s responsibility. The scope of our tests is still more shallow than we would like, but we have a framework to build upon and have started changing our habits, making sure the tests pass before merging code and building test creation into the scope of new work.\n\nWWDC 2017 brought a new version of Swift and new iOS 11 betas to test against. By beta 3, Xcode 9 was able to build our existing Swift 3.2 code without modification. We created a Swift 4 branch, and after one developer spent a couple of days making adjustments, that was working too. Getting our tests working took longer because they use third-party code that also needed an update.\n\nNow we have a slew of work lined up supporting new features we have planned for the app and new technology in iOS 11. We\u2019re starting with an app that has a solid architecture, malleable features, and accessible code. All ready for a long game of supporting a modern, sustainable Evernote for iOS.", "sentiment": 0.13619294121544814},
{"link_title": "Does Your Genome Predict Your Face? Not Quite Yet", "url": "https://www.technologyreview.com/s/608813/does-your-genome-predict-your-face-not-quite-yet/", "text": "On Monday, the California gene-hunting company Human Longevity published a paper making the bold claim that it can identify individuals using their genomes to predict what their faces looks like.\n\nThe assertion\u2014that your DNA can be used to create a photo-like reconstruction of you\u2014has potentially big implications. It would allow police to pick suspects out of a lineup using a blood spot and it would mean no genome collected for research is truly private.\n\nBut a withering reaction to the face-prediction paper by scientists on social media is probably not what Human Longevity\u2019s founder, the famed genomics expert J. Craig Venter, had in mind.\n\nAccording to two experts who reviewed the paper\u2014and one former employee\u2014Venter can\u2019t actually pick a person out of a crowd using a genome, and his report had difficulty finding a publisher.\n\n\u201cCraig Venter cannot predict faces,\u201d Yaniv Erlich, the chief scientific officer of MyHeritage.com, a genealogy website, said bluntly on Twitter. To make the point, Erlich posted Venter\u2019s prediction of his own face, noting that it looked more like actor Bradley Cooper than the biologist-turned-entrepreneur.\n\nVenter\u2019s team used genome data to predict face shape, eye and hair color, and even what your voice sounds like and melded them into images they said were accurate enough to re-identify whose genome it was.\n\nBut skeptics say Human Longevity actually uses a person\u2019s race and sex\u2014easily measured from simpler DNA tests and not a new idea\u2014to create pictures that portray average faces, not specific ones, as the company said.\n\nUsing its method, the company reported it could pick the right person from a lineup of 20 photographs about 70 percent of the time. But after discarding people of a different sex and race than the subject, accuracy dropped drastically. Used to pick a specific European man out of a lineup of 20 other European men, it was right 11 percent of the time.\n\n\u201cThe face prediction is just predicting the average face for your race. You will always say, \u2018Wow, that kind of looks like me,\u2019\u201d says Jason Piper, a genetics expert who worked at Human Longevity. Piper, who is listed as an author of the paper but is now employed by Apple, also took to Twitter to criticize its conclusions as unjustified.\n\nThe face study was meant to showboat the capabilities of Human Longevity, which has raised about $300 million to sequence one million human genomes. The business plan: create the largest DNA database on the planet and unleash the power of the genome to make health predictions.\n\nThe effort to privately amass gene information echoed Venter\u2019s controversial role in the original Human Genome Project, when he raced public sector scientists to complete a first draft. His effort at that time to create a Bloomberg of DNA data, through his former company Celera, didn\u2019t pan out.\n\nWith his new venture, Venter\u2019s big name and ambition again helped him attract stars, such as machine-learning specialist Franz Och, who joined from Google to crunch data. At a \u201cHealth Nucleus\u201d in San Diego, Venter started offering ultra-detailed medical workups costing as much as $25,000. Included are a genome readout and a personalized scientific poster depicting a customer's genes.\n\nThe face prediction project was undertaken to demonstrate the value of those offerings. After all, if Venter can print your face from your DNA, then Human Longevity\u2019s medical workups must be worth it. \u201cIf I tell you that you are at risk of heart disease and you need to take statins, how can you be confident?\u201d says Piper. \u201cIf [the company] can predict what the customer looks like, it\u2019s a ground truth for confidence.\u201d\n\nIn genetics, traits like the color of your eyes or cholesterol levels are called phenotypes. These phenotypes are determined, to a greater or lesser degree, by your particular DNA, or genotype.\n\nThat\u2019s why identifying someone's face from their DNA isn\u2019t just theoretically possible, but likely to be possible a few years, says Mark Shriver, who works on genes-to-face prediction at the department of anthropology at Penn State. \u201cI think it\u2019s in our future for sure,\u201d he says.\n\nHowever, scientists still have work to do to know exactly how, say, your DNA influences the length of your nose or the width of your mouth, or the timbre of your voice. Shriver thinks Venter\u2019s paper didn\u2019t add much to what can already be predicted using existing DNA tests that measure your ethnic makeup and sex.\n\n\u201cCalling it predicting from the genome is what\u2019s wrong,\u201d says Shriver. \u201cThe main message is way overstated. They just didn\u2019t have enough people to find the genes that distinguish people. This is not the paper that is going to convince people that this is going to affect privacy or help forensics.\u201d\n\nSince it was started in 2013, Human Longevity has lost or fired several employees. Half the authors of the new paper, for instance, no longer work at the company. Och, the Google star, decamped after a short time for blood-testing startup GrailBio. Also gone are its former chief scientist and chief information officer.\n\nA spokesperson for Human Longevity said such turnover was normal. In response to questions about its paper, the company on Tuesday released a statement noting that its data set, based on 1,061 people, was not extremely large and that \u201clarger cohorts would be needed to make much more precise predictions and identifications.\u201d\n\nIn its statement, however, the company reasserted its contested claim: \u201cIf you have a genome from the public domain, researchers can generate a picture of that individual, thus identifying that person.\u201d\n\nVenter\u2019s paper did not have an easy time finding a publisher. Shriver says he acted as a reviewer of the paper for Science, the leading American scientific journal, where it was twice reviewed and ultimately rejected. It was instead published on Monday in the Proceedings of the National Academy of Sciences.\n\nIn press coverage of his result, Venter also grated on some scientists by using face prediction to argue that genome data\u2014particularly public data\u2014holds a serious new privacy risk. Namely, if someone had access to your de-identified genome, they might be able to link it to you by generating a photograph.\n\n\u201cI've always said to people that your genome is more than all the other numbers in your life that identify you: your credit card number, your date of birth, your address, your Social Security number,\u201d Venter told the San Diego Union-Tribune.\n\nThat privacy argument could be seen as self-serving, however, since Venter's point is that private databases, such as his, are more likely to offer needed protections.\n\nPiper, the former Human Longevity employee, says he decided to speak up about the paper because he disagrees with Venter\u2019s privacy argument. \u201cGenomics privacy is important. But this is not how to approach it. It\u2019s not that we can\u2019t share. It\u2019s how to share without fear,\u201d says Piper. \u201cThe overall message of the paper is not something I agree with.\u201d", "sentiment": 0.08976765422077926},
{"link_title": "A taxonomy and analysis for finding good restaurants", "url": "https://thezvi.wordpress.com/2017/03/05/restaurant-guide-1-restaurants-should-not-look-like-most-restaurants/", "text": "Related: Surgeons Should Not Look Like Surgeons, An Economist Gets Lunch\n\nConceptually Related More Than You Might Think: Short-Termism\n\nI have spent a lot of time optimizing restaurant choice, and what to do when you get there. A lot of this is based on my personal preferences, but a lot also generalizes to others. A lot also generalizes to things that are not restaurants.\n\nThe basic thesis is that a restaurant will choose what signals to send based on what type of place they want to be and what they care about, so those signals are remarkably reliable. It would not benefit a place to send a misleading signal to someone using my criteria, because it would hurt them otherwise.\n\nI can\u2019t find his old posts on this, but Scott Adams had experience with it when he owned a restaurant. He didn\u2019t know the rules, so he had a signal mismatch. The most stark example was when he made his place look \u2018too nice.\u2019 People would walk in, decide the place was not right for them, and leave, so he got better results by making the place superficially appear worse. A signal mismatch drives people away. Even when people don\u2019t consciously understand what is going on, they can feel it.\n\nThe other basic principle is that the more a place is optimizing for things you don\u2019t want, the less they are optimizing for the things you do want. Attention and focus are limiting factors, and (except perhaps on the very high end) if the place is focusing on the symbolic representation of the thing, it won\u2019t get you the thing.\n\nYou want the thing. Restaurants tell you what they are. Believe them. The trick is to know how to read them.\n\nThis post covers finding candidate restaurants, and considerations for evaluating those restaurants that are mostly independent of cuisine. Intent is for part 2+, to the extent I do finish this series, is to cover different cuisine types, chains, how to order, how to balance exploration/exploitation, min/max considerations, group dynamics, delivery, and location recommendations in New York City.\n\nWhere to Look\n\nThere are seven procedures I know of to decide what places to consider. Each has its place.\n\nThis is the default method in New York, since walking around is a free action. You can get better data from looking in person than you can any other way.\n\nIf you zoom in close, it will show you most but not all the places in the area. You can then click on them to get more information.\n\nI find searching unhelpful in areas you know well, but useful while travelling if you have strong preferences on the type of food you want. Avoid this when you can.\n\nZagat cares about signals I do not care about, but incentive to send those signals correlates with the food, so the numbers are useful. Michelin ratings, on the other hand, seem to convey no additional information and provide no useful search.\n\nThis works quite well. Even if the person is random, what they choose for you is better than the average choice they make for themselves. You are already ahead of the curve. Where you live, ask for favorite places and for types of cuisine you can\u2019t find good options for. Where you are visiting, keep it as general as you can given your group, and ask what is good.\n\nOption 6: Keep an Eye Out\n\nWhenever people mention a restaurant in another context, that is a sign you should investigate. The context does not have to be praise, it only has to not be them complaining about the place.\n\nIf I find a review of a place that sounds interesting, I find it to be worth looking. The details they give you are often great. If you know what details you value, you can see if those are described. Their advice on what to order is trustworthy. Trust the details, distrust the rating.\n\nI try to get as much as I can from #1 (where practical) and #6, and opportunistic #5 and #7. #2 is worth doing in your area and in any place you will stay for more than a day or two. If you know you are short on options for a trip, you should first put marginal work into #2. #3 and #4 should be kept to a minimum, as should additional #5 and #7 that require non-trivial work.\n\nHow to Use Google/Yelp Style Reviews\n\nGoogle ratings are noisy but useful. If a rating is below par on 20+ reviews, unless you have a good story why, avoid. Full stop. Exceptional ratings on large reviews counts are also reliable. Yelp ratings are more noisy, but still have value, and are good mostly for negative selection.\n\nDifferent areas have different average ratings for a place of similar quality, so look for the local baseline, especially in a similar price range, before making judgments. In New York City, a (20+ review) 4.4 or higher is reliably good, 3.9-4.3 is still promising, and anything at 3.8 or lower is probably bad. Also, high volume of ratings is itself an endorsement. Exceptional places often have very high rating counts.\n\nReading the reviews themselves is worthwhile if you are considering going.\n\nIf the place is great, a large percentage of the reviews will use words like \u2018best\u2019 and \u2018amazing\u2019 and use exclamation points. Reviews will be enthusiastic endorsements. Here is an example of what you want to see.\n\nLook for specific praise. Watch for dishes that are mentioned as very good, which is a good sign in general and a guide on what to order. The more detail about the food you see from positive reviews the better.\n\nBad reviews require careful attention. What was wrong? If staff was rude, delivery was poor and you are not ordering delivery, or other trivialities, disregard that. Overpriced can mean different things. Sometimes it is code for small portions. Sometimes it is code for \u2018not fancy enough for the price point.\u2019 Sometimes, however, it is a warning they are trying to signal a tier or two better food than they serve, and if this is clearly what the person means, that should be taken seriously. Also worry about unambiguous bad experiences. If there are a lot of unjustified bad reviews, the review average is lower than it \u2018should\u2019 be.\n\nBad reviews focused on the food signal trouble. Look at what they ordered. Some places do multiple things and are only good at one of them (e.g. many Italian places have great pasta or great main dishes, but not both, and if you order the wrong way you will be sad). So if all the bad reviews mention pasta, place could still be good. Just avoid the pasta.\n\nOnce a place has your attention, start with the menu. Find cuisine type and where it stands on the low-end to high-end scale, and look for what you might order. If this combination does not appeal to you, or there is no dish that appeals to you, there is no reason to continue.\n\nCents are a key story. If prices mostly end in .95 (or worse .99) this is very bad. It means they are trying to look cheaper than they are. If the prices below $20 end in .50 sometimes, that is acceptable, or .25 and .75 below $10, but less is better. If prices are bizarre, e.g. $6.38, they are round post-tax numbers and these are the resulting pre-tax numbers, and that is fine.\n\nDifferent things should cost different prices, and those prices should be a bit \u2018off\u2019. If everything costs the same amount, or the differences are exactly the differences you expect, everything is generic. Look for things that are more expensive than they \u2018should\u2019 be. If a few dishes are relatively expensive, that is a good sign for the restaurant and that those dishes are special.\n\nOverall price level is similar. Ideal is either somewhat more, or somewhat less, expensive than you expect after controlling for location, setting/decor and cuisine. Unusually high prices means higher upside. Places at a small discount are paying attention to detail and often doing simple things well, so they too can be quite good, but be wary when prices go too low. Half-price offerings are optimizing for price first, using fast procedures and sub-par ingredients, and will likely be bad to inedible.\n\nThe smaller the menu, the better. Duplicate or unnecessary offerings are very bad, as is a lack of unique offerings. Every item should have a reason to exist, filling a role nothing else fills. There is some slack for things that it would be expensive to not have (e.g. the standard cuts of steak at a steakhouse) but it would still be a good sign if some of them were missing. If there is something that is conspicuously missing (e.g. a curry they don\u2019t offer with a meat they do offer elsewhere) that is also great.\n\nThe menu should tell you what to do. It should highlight their best and most unique dishes. Having a signature dish is a big plus. The first time you go you should probably order it.\n\nThe typical \u2018generic Chinese\u2019 menu is the prototype of what to avoid. On a typical \u2018generic Chinese\u2019 menu there are 80+ items, so no one will look for something and fail, but has zero items that surprise. The majority of the 80+ items have no reason to exist, and take focus away from quality and originality. If something was special, how would you know? If your answer was \u2018Chef\u2019s Specials\u2019 then I say that this is another terrible sign. They are always the same. You specialize in General Tso\u2019s Chicken? What a coincidence! If the specials were genuinely different from typical specials, then that would be very good. They are not.\n\nThe flip side of the bad menu is something like this that indicates that the restaurant is going something different, but has decided it must also include the standard menu. Note the section title \u201cAmerican Chinese Food.\u201d It would be better if they did not feel this compromise was necessary, but you can be confident real things are happening in that kitchen. Even if you do order off the generic menu, things will likely go well (and, in fact, the place is quite good).\n\nThe \u2018great middle\u2019 inoffensive menu, and the huge diner menu, are also places to avoid. They focus on being inoffensive, and giving everyone something they can order, which prevents the focus necessary for quality offerings.\n\nDetailed descriptions of dishes are a plus, especially descriptions vary. Same principle. The exception is if they are telling the story that they are healthy or locally sourced or fresh. This is signaling, protests too much, and is bad. Damn hipsters.\n\nPictures of the food are helpful, but correlate very badly with restaurant quality, which is unfortunate. Huge negative. I wish it was otherwise. Putting the menu in plastic is slightly bad.\n\nOnce you control for other factors, not offering lunch is only a small positive. What is a strong positive is being closed for entire days of the week, or hours that make no sense. Something other than profit is being maximized.\n\nThe more obscure the location, the better. Side streets offer better value. Quiet signs are better than loud. Even better is almost no sign at all. If it is hard to notice a restaurant without looking closely, that is almost a guarantee of quality.\n\nBefore looking, listen. Loud music means bad food. Quiet music is good, give or take whether you want to listen to it.\n\nOnce you are looking inside, the first question for low and medium-end places is how full they are. More people is better. The lower end a place is, the more this matters; a strangely empty low-end restaurant is almost always awful. This goes double if you can contrast with a full similar place nearby. At busy times, the best low-end places are packed.\n\nAt high-end places popularity means less. Being full often means you are \u2018trendy\u2019 which is net negative. Many of my favorites are empty at the times I go.\n\nSecond, consider what types of people are inside. You want people who match the restaurant. Every group knows their own ethnic food best.\n\nThird, look at the dishes. See if they look good, and what people actually order. One thing you can measure is the bread basket, which matters a lot to me. Good free additions are great value and indicate great other things.\n\nSee how many people have food so you know if service is slow.\n\nFourth, look at presentation and overall vibe. As with everything else, you want something unique. Almost anything that you don\u2019t normally see is a plus. What matters most depends on the cuisine type. One good universal: offering higher quality cutlery than you would expect from other signals (non-disposable chopsticks, real forks/plates/cups and so forth) is a great sign. Offering disposables when you would have expected non-disposables is neutral to small positive. Treys are a remarkably bad sign.", "sentiment": 0.12600250746905675},
{"link_title": "This Is Exciting \u2013 Event Ticketing on Ethereum", "url": "https://icowatchlist.com/blog/ico-focus-interview-ashton-addison-event-chain/", "text": "Event Chain is a blockchain project that has been established to help solve ticketing issues that the entertainment industry is confronted with. The use cases for the platform is however not limited to the entertainment industry alone, it can be extrapolated to fit other industries as well. The ICOWatchlist team had a chit-chat with Mr. Ashton Addison who is the Chief Executive Officer and this is how the conversation unfolded;\n\nHello Ashton. Kindly tell us briefly what the Event Chain project is about?\n\nHey, Thanks for having me. Eventchain is a blockchain Smart Ticketing system for fans to purchase tickets directly from artists using blockchain technology and smart contracts. The project aims to solve many of the ticketing issues in the event ticketing industry today including counterfeit ticketing, ticket scalping, excessive ticketing prices.\n\nHow did you come by the idea of using blockchain to solve the ticketing issues plaguing the events industry? Was it by work experience?\n\nI understood that Blockchain has the power to connect people directly and fans understand that the goal is to pay back the artist for their efforts and not have all of these financial intermediaries taking a large cut of those funds. In 2012 I created Shed Show Productions, an event production company, and then realized first-hand the excessive ticketing fees charged by third-party processors. I was always following distributed technology and cryptocurrencies during my degree in Finance and Accounting, and after graduation decided to combine these two interests to achieve a common goal, making it cheaper and more accessible for everyday people to attend events.\n\nIn which countries will the Event Chain project be accessible and are there legal implications per jurisdiction?\n\nThe EventChain system will be accessible by all countries where artists or promoters seek to create an event. Barring any future acts on using cryptocurrency to access token utility, the EventChain tokens used within the system will provide benefits to users in each country where these events are hosted.\n\nWhat can you tell us about your team? How does the team compliment the execution of the project?\n\nThe EventChain team includes a lot of like-minded individuals poised to take decentralized event ticketing mainstream. Our team consists of blockchain developers, entrepreneurs and software developers with over seven years of software development experience in booking applications. The Board of Advisors is stacked with expert talent and is the perfect combination of blockchain and business techies including Dr. Steven Funk, Silicon Valley Billionaire, Piotr Piasecki, Core Developer of Factom, Jeans Tang, President of Blockchain Research Institute in China, Dror Medalion, CEO of Bitjob.io in Israel and more.\n\nWhere do you see the value of Event Chain Tokens in the medium to Long term and the ultimate benefit for token holders?\n\nThe EventChain Tokens were designed as fuel to run and manage the SmartTicketing system. The Smart Ticketing system incentivizes fans to use EventChain tokens as a primary means of purchasing tickets to waive all processing fees will bring demand and liquidity to the EVC market. In the long term many participants in the system will hold EVC tokens to access VIP benefits, resale market features and API access for distributed sales, and the combination of holding incentives and need for liquidity will benefit token holders in the long run.\n\nBriefly run us through how the process of Token crowdsale will be executed?\n\nThe ICO starts on September 13th 17:00 UTC and will be launched at tokensale.eventchain.io. It is not whitelisted so everyone has an equal chance to participate. I will contact the participants signed up to the EventChain newsletter prior to the ICO start. The initial ICO will last until all the EVC is sold or when Phase-2 begins on October 1st. The EVC tokens are received in your ERC20 wallet immediately but are locked until the ICO is over, and then after that the tokens will be listed. As a reminder, the sale will available at tokensale.eventchain.io.\n\nThank you so much Ashton for taking time to make this interview happen and good luck with your ICO\n\nThank you too ICOWatchlist for your support thus far.\n\nFor more details, follow the EventChain ICO here.", "sentiment": 0.16984126984126988},
{"link_title": "You Don\u2019t Need to Do a Prolonged Fast Before Surgery", "url": "http://www.slate.com/articles/health_and_science/medical_examiner/2017/09/you_shouldn_t_fast_before_surgery.html", "text": "In 1946, obstetrician and cardiologist Curtis Lester Mendelson discovered a disturbing phenomenon: He found that some women who had anesthesia in labor were vomiting and aspirating on their stomach contents during delivery. As reported in his landmark study on surgical aspiration, Mendelson discovered 66 such cases in more than 44,000 pregnancies. This condition, aspiration pneumonitis, occurs because the laryngeal reflexes do not work under general anesthetic, making it possible to draw the contents of the stomach into the lungs. In his study, he wrote about two women who had died from the condition, their airways obstructed by undigested solid food. (Those who had aspirated liquid suffered from shortness of breath, blue discoloration [cyanosis], and a faster heartbeat than normal.) It was coined Mendelson\u2019s syndrome after its author, and patients were advised to fast for prolonged periods prior to operations to avoid contracting the asthmalike syndrome.\n\nBy the 1960s the term nil by mouth (or its Latin variant NPO, nil per os) after midnight had become the widely accepted guideline for all surgical patients. If you recently had an elective procedure, you might know that it has not changed much since\u2014fasting before surgery, meaning no food and no water, is still advice routinely given to preoperative patients. Yet the evidence\u2014and medical practice, and even the recommendations\u2014have evolved since Mendelson. Medical practice has yet to catch up.\n\nFor one thing, anesthesiologists no longer use ether, a substance known to make patients nauseated. They also employ endotracheal tubes, which protect the airways from the aspiration of stomach contents. Knowledge about digestion has increased to the point where the rate of calories leaving the stomach is predictable: A spate of studies on gastric emptying found that patients who consume clear fluids two hours prior to an operation do not have higher gastric volumes than those who fast for longer. In 1999, the tide of mounting evidence pushed the American Society of Anesthesiologists to amend its preoperative fasting guidelines: Patients are now instructed to have a light meal six hours before a procedure and clear fluids\u2014drinks that you can see through, such as pulp-free juices, black coffee, or tea without milk and cream\u2014until two hours prior to the operation. Guidelines in other countries were similarly amended.\n\nNevertheless, most patients appear to still be getting outdated advice and arrive to surgery thirsty and irritable. A presentation at the 2016 Anesthesiology Annual Meeting found that only 25 percent of hospitals in Michigan adhered to the new guidelines. A 2016 study of oral and maxillofacial surgeons found that 99.1 percent of them did not adopt ASA guidelines, and a worrying analysis of pediatric practices discovered that most children were fasting longer than necessary before their medical procedures, leading to negative experiences. Prolonged fasting can be associated with dehydration, hypoglycemia, and electrolyte imbalance. Some patients experience headaches and nausea before surgery.\n\nStill the NPO-after-midnight approach seems to be ossified in surgical practice. \u201cIt\u2019s still a fallback position for a lot of people. \u2018Let\u2019s be on the safe side and make sure we do this,\u2019 so they stick with that old standby routine,\u201d said Craig Palmer, a professor of anesthesiology at the University of Arizona College of Medicine. One of the barriers preventing practice change is the structure of surgical lists. Surgeries are scheduled throughout the day but are sometimes changed or adjusted. It\u2019s easier to make scheduling shifts if all patients have received the same blanket advice to fast from midnight onward. \u201cThere\u2019s an efficiency imperative at work with a lot of hospitals,\u201d Palmer said.\n\nBut this reasoning overestimates how frequently patients are taken into surgery early. Anesthesiologist Joyce Wahr, the director of the University of Minnesota Health Preoperative Assessment Center, researched this phenomenon and found that patients were taken in ahead of time in just 5 to 6 percent of cases. In her own hospital she tracked data and discovered that even fewer patients\u2014just 1 percent\u2014went to surgery earlier than expected.\n\nEven hospitals that did adopt the new guidelines had problems. At Uppsala University\u2019s Children\u2019s Hospital in Sweden, children would stop drinking liquids two hours before their scheduled surgeries, but then the surgeries might be delayed, causing further dehydration, said Peter Frykholm, an associate professor at the hospital. \u201cWe didn\u2019t want children going to surgery thirsty or hypoglycemic,\u201d he said.\n\nSo, in 2000, the hospital decided to do away with the fluid fast altogether. Frykholm\u2019s team analyzed physiology and gastric emptying rates and found that a half-hour safety margin after drinking clear fluids was enough to circumvent rates of aspiration. Conveniently, this was exactly how much time elapses between being summoned for an operation and being anesthetized. \u201cWe just let children drink water, apple juice, lemonade, and clear ice blocks until they are called to theater,\u201d Frykholm says.\n\nAnother reason why hospitals might not be implementing the new guidelines is that they don\u2019t see extra fasting as harmful, Palmer said. Instead, they just think of it as taking an extra precaution. But research says that there are definite clinical consequences. For one thing, from the perspective of staff, patients are more irritable after fasting. \u201cThirst is such a primitive drive. When you aren\u2019t allowed to have a drink of water, you are really miserable,\u201d Wahr said. Anesthetists also find that dehydrated patients can present to surgery more anxious and in some cases require more drugs. And putting an IV into a patient is much easier when he or she is hydrated. The physical pressure a surgery puts on a body is much like a marathon, so it\u2019s baffling to think that patients\u2014especially elderly ones\u2014would commence such an event by dehydrating themselves.\n\nIn 2014, the Annals of the Royal College of Surgeons of England investigated the effects of administering a complex carbohydrate drink two hours before an operation. Carbohydrate-loading was found to significantly improve insulin resistance and several categories of patient comfort, \u201cespecially hunger, thirst, malaise, anxiety and nausea.\u201d No adverse effects to do with aspiration were reported. A recent article in the British Journal of Anesthesia touted the benefits of preoperative carb-loading and credited it for reducing length of hospital stays and improving postoperative muscle function.\n\nWahr said she\u2019d be interested in examining the evidence and costs and benefits of complex carbohydrate drinks, but that at this stage she would be happy if patients did the bare minimum and drank water rather than fasting in the two hours before their operation times. If you have an operation scheduled and a physician advises you to fast for longer than two hours beforehand, refer her to the ASA guidelines and ask if she really feels a prolonged fast is necessary. As Wahr said, \u201cIt\u2019s an education issue. Today the data is so good. Not following the guidelines is just cruel.\u201d", "sentiment": 0.06676881171263195},
{"link_title": "Using QL to find a remote code execution vulnerability in Apache Struts", "url": "https://lgtm.com/blog/apache_struts_CVE-2017-9805", "text": "", "sentiment": 0.0},
{"link_title": "\u201cGoogling for \u201dvisual studio application insights win7 diagtrack\u201c reveals...\u201d", "url": "https://twitter.com/yuhong2/status/906272630033924097", "text": "", "sentiment": 0.0},
{"link_title": "Show HN: The Free Chess Club \u2013 Play Chess on the Free Internet Chess Server", "url": "https://www.freechess.club", "text": "\ud83c\udf10 Access the Free Internet Chess Server (FICS) without the hassle of downloading and installing a chess client. The Free Chess Club's web-based client lets you play a quick game of online chess anywhere.\n\n\ud83d\udc7e Learn to play chess, solve chess puzzles, participate in daily tournaments, play different variants at all levels, play with a computer and watch relayed games. Maximum chess\u2014guaranteed!\n\n\ud83d\udc68\u200d\ud83d\udc68\u200d\ud83d\udc67\u200d\ud83d\udc67 At The Free Chess Club, you are a part of a vast community of chess experts and enthusiasts, with free access to a host of channels involving engaging discussions on a variety of topics.", "sentiment": 0.1933333333333333},
{"link_title": "Gt \u2013 go test with caching", "url": "https://godoc.org/rsc.io/gt", "text": "Gt is a wrapper for \u201cgo test\u201d that caches test results.\n\nThe usage of \u201cgt\u201d is nearly identical to that of \u201cgo test.\u201d\n\nThe difference between \u201cgt\u201d and \u201cgo test\u201d is that when testing a list of packages, if a package and its dependencies have not changed since the last run, \u201cgt\u201d reuses the previous result.\n\nThe -f flag causes gt to treat all test results as uncached, as does the use of any \u201cgo test\u201d flag other than -short and -v.\n\nThe -l flag causes gt to list the uncached tests it would run.\n\nCached test results are saved in $CACHE/go-test-cache if $CACHE is set, or else $HOME/Library/Caches/go-test-cache on OS X and $HOME/.cache/go-test-cache on other systems. It is always safe to delete these directories if they become too large.\n\nGt is an experiment in what it would mean and how well it would work to cache test results. If the experiment proves successful, the functionality may move into the standard go command.", "sentiment": 0.09459325396825397},
{"link_title": "iOS 11 GM leak/\u2018iPhone 8\u2019 Features: Portrait Lighting, True Tone, New AirPods", "url": "https://9to5mac.com/2017/09/08/ios-11-gm-d22-iphone-8-details/", "text": "Here we go. We\u2019re digging through the iOS 11 GM we received this evening to unpack what we can learn about the D22 \u2018iPhone 8\u2019 and the rest of the lineup ahead of Apple\u2019s big unveiling on Tuesday. It looks like the infamous HomePod leak left a few surprises for us after all.\n\nThe first discovery is a stunning set of new wallpapers coming with iOS 11 and the first look at the LTE Apple Watch. Next up: new and confirmed features coming to the OLED iPhone\u2026\n\nStarting with what might be the most interesting new discovery: Portrait Lighting. This feature appears to be an enhancement on the iPhone\u2019s Portrait mode effect for creating dSLR-like shots using depth by simulating different lighting effects.\n\nPortrait Lighting, which we believe will launch in beta similar to Portrait mode last year, supports Contour Light, Natural Light, Stage Light, Stage Light Mono, and Studio Light. It\u2019s possible this Portrait mode enhancement could be related to the flash when shooting.\n\nAlso present is new video recording resolution and capture speeds:\n\n1080p HD at 240 fps 480 MB with 1080p HD at 240 fps\n\n4K at 24 fps (Footer) 270 MB with 4K at 24 fps (film style) (HEVC Footer) 135 MB with 4K at 24 fps (film style)\n\n4K at 60 fps (Footer) 450 MB with 4K at 60 fps (higher resolution, smoother) (HEVC Footer) 400 MB with 4K at 60 fps (higher resolution, smoother)\n\nWe\u2019re also seeing evidence that the OLED iPhone will feature a True Tone Display for white balancing like the iPad Pro lineup. As for the new resolution, we believe we\u2019ll see 1125\u00d72436 based on this firmware.\n\nFace ID appears to be the official marketing name for what\u2019s been referenced as Pearl ID, the facial recognition features that will likely replace the Touch ID fingerprint recognition feature. There\u2019s also this video tutorial for setting up the Face ID feature:\n\nAnd there are more references to the new screen design first leaked by the HomePod firmware which continue to use the notch as an icon:\n\nIncluding instructions on enabling SOS mode by pressing the power and power switch:\n\nThere even appears to be some more fun in iOS 11 that we haven\u2019t seen yet. Something referenced as Jellyfish that appears to be 3D, animated versions of emoji characters for iMessage:\n\n[Update: These are called Animoji and will use facial tracking and your voice to create expressive animated messages. And yes, there\u2019s a smiling pile of poop.]\n\niOS 11 also references a revised version of AirPods which can be seen below:\n\nThis is likely a minor revision and not a major upgrade. Based on the internal name \u2014 AirPods1,1 to AirPods1,2 \u2014 and the video above, I\u2019m guessing the charging indicator light is relocated from inside the case to the front of the case for checking charging status without opening the lid.\n\nThe OLED iPhone is also expected to totally ditch the concept of a Home button if favor of a full screen experience and a dynamic area denoted by a line at the bottom of each app.\n\nAnd there\u2019s also clues about how the power switch (called the side button and not lock button) will be used to interact with iOS without a Home button. For example, double-clicking the side button will show Apple Pay cards and passes just like on the Apple Watch. You can also press and hold the side button to invoke Siri.\n\nThis new side button method is also tied to new Accessibility settings for adjusting the speed of the double and triple click, enabling Type to Siri, and even using Face ID to \u201ccheck to see if you are looking at your iPhone before dimming the display, unlocking, or lowering the volume of alerts.\u201d\n\nEarlier tonight we saw what appears to be the LTE Apple Watch with a new watch face and Digital Crown, and now we have a watchOS screenshot of Control Center with a cellular signal indicator and the \u2018notch\u2019 OLED iPhone.\n\nWe\u2019re still digging through the unreleased iOS 11 firmware and will continue reporting what we discover so stay tuned for updates!", "sentiment": 0.1469664869029276},
{"link_title": "Bad code isn\u2019t technical debt, it\u2019s an unhedged call option (2014)", "url": "http://www.ontechnicaldebt.com/blog/bad-code-isnt-technical-debt-its-an-unhedged-call-option/", "text": "This is a post that discusses an alternative to the metaphor of technical debt for \u2018bad code\u2019. The problem with technical debt as a metaphor is that for managers debt can be a good thing \u2013 it can required for financial needs or encouraged by tax breaks in certain financial situations. However, the debt that is often spoken about in codebases does not fully capture the risk that comes along with writing short cuts.\n\nMaybe the better metaphor would then be a call option. A call option is when the right, not the obligation, is sold to someone to buy an agreed on quantity of something at a price that is fixed now. From the seller perspective, if the price stays to low they get to keep the payment and are ahead but they also run the risk of still having to provide the good even when the price has increased greatly. This is can be referred to as an unhedged call \u2013 where no matter what the a product costs to the seller they have to supply it.\n\nCall options is a better model for code issues because it captures the unpredictability of what can happen when writing in certain features and code. For example, a feature can be added to a system without cleaning it up and the benefit is collected immediately (the premium is collected). If that code is never called upon again than it would have been foolish to clean it up in the first place. However, lets say a radical new feature is going to added \u2013 these quick fixes and become very expensive to deal with. Time has to be taken to fix these issues right before deadline or no one on the team remembers pertinent information need for the code to make sense in the first place \u2013 the market by then has moved away from where it was expected to be and the option has been called. No matter what the cost to input this feature it must be delivered.\n\nTherefore, even if it is more expensive to do thing clean from the start, it would also be less risky. A messy system is full of unhedged calls that can be called upon at an unpredictable cost to the organization. Technical debt does not communicated the risk of writing sloppy or quick fixes into code \u2013 debt is something to be managed and just another tool. When talking about implausible delivery dates it may make more sense to talk about unhedged call options.\n\nTo read the full post go to: http://nblo.gs/12c3vP", "sentiment": 0.14267676767676768},
{"link_title": "Minimal Scala multi-module project using Bazel (Google's next-gen build tool)", "url": "https://github.com/chrisbeach/bazel-scala-example", "text": "A simple example of a Scala multi-module project configured to build using Bazel.\n\nOr, once built using Bazel, call the generated run-script:", "sentiment": 0.0},
{"link_title": "Show HN: I made a website with 12,000+ textless movie posters", "url": "https://www.moviemania.io", "text": "Welcome to the largest textless high-resolution movie wallpapers database on the Internet. \n\n", "sentiment": 0.8},
{"link_title": "On the discussion of security vulnerabilities. (1853)", "url": "http://www.crypto.com/hobbs.html", "text": "The debate over the open discussion of security vulnerabilities long predates the Internet and computers. The recent reaction of some locksmiths to my master keying research paper heightened my interest in this subject. (January '05 update: See also the reaction to my paper on safe locks.) Here's what one of the 19th century's foremost inventors of mechanical locks had to say 150 years ago:\n\nA commercial, and in some respects a social doubt has been started within the last year or two, whether or not it is right to discuss so openly the security or insecurity of locks. Many well-meaning persons suppose that the discussion respecting the means for baffling the supposed safety of locks offers a premium for dishonesty, by showing others how to be dishonest. This is a fallacy. Rogues are very keen in their profession, and know already much more than we can teach them respecting their several kinds of roguery. Rogues knew a good deal about lock-picking long before locksmiths discussed it among themselves, as they have lately done. If a lock, let it have been made in whatever country, or by whatever maker, is not so inviolable as it has hitherto been deemed to be, surely it is to the interest of honest persons to know this fact, because the dishonest are tolerably certain to apply the knowledge practically; and the spread of the knowledge is necessary to give fair play to those who might suffer by ignorance. It cannot be too earnestly urged that an acquaintance with real facts will, in the end, be better for all parties. Some time ago, when the reading public was alarmed at being told how London milk is adulterated, timid persons deprecated the exposure, on the plea that it would give instructions in the art of adulterating milk; a vain fear, milkmen knew all about it before, whether they practiced it or not; and the exposure only taught purchasers the necessity of a little scrutiny and caution, leaving them to obey this necessity or not, as they pleased. -- From A.C Hobbs (Charles Tomlinson, ed.), Locks and Safes: The Construction of Locks. Published by Virtue & Co., London, 1853 (revised 1868). (My thanks are due to Steve Bellovin for having first brought this text to my attention almost ten years ago.)\n\nContrast this with a view from 100 years later, when the authors of the standard textbook on safes and safe lock manipulation included this warning in their own book:\n\nIronically, while Hobbs' reasoning today enjoys wide acceptance among practitioners of computer security and cryptology, the locksmith world seems to have at least partially reverted to embracing the security through obscurity advocated by Lentz and Kenton. Predictably, the dearth of open literature in that field now makes it quite difficult for a potential user to be sure whether or not a given lock suffers from known attacks. One can only wonder whether the locksmiths' reticence is more for the benefit of the security of their profession than for that of their clients.\n\nClick here to return to the crypto.com home page.", "sentiment": 0.10608333333333336},
{"link_title": "The Treacherous Optimization (2006)", "url": "http://ridiculousfish.com/blog/posts/old-age-and-treachery.html", "text": "\"I'm going to beat grep by thirty percent!\" I confidently crow to anyone who would listen, those foolish enough to enter my office. And my girlfriend too, who's contractually obligated to pay attention to everything I say.\n\nSee, I was working on Hex Fiend, and searching was dog slow. But Hex Fiend is supposed to be fast, and I want blazingly quick search that leaves the bewildered competition coughing in trails of dust. And, as everyone knows, the best way to get amazing results is to set arbitrary goals without any basis for believing they can be reached. So I set out to search faster than grep by thirty percent.\n\nThe first step in any potentially impossible project is, of course, to announce that you are on the verge of succeeding.\n\nI imagine the author of grep, Ultimate Unix Geek, squinting at vi; the glow of a dozen xterms is the only light to fall on his ample frame covered by overalls, cheese doodles, and a tangle of beard. Discarded crushed Mountain Dew cans litter the floor. I look straight into the back of his head, covered by a snarl of greasy locks, and reply with a snarl of my own: You're mine. The aphorism at the top, like the ex girlfriend who first told it to me, is dim in my recollection.\n\nHaving exhausted all my trash-talking avenues, it's time to get to work. Now, everyone knows that without some sort of preflighting, the fastest string search you can do still takes linear time. Since my program is supposed to work on dozens of gigabytes, preflighting is impossible - there's no place to put all the data that preflighting generates, and nobody wants to sit around while I generate it. So I am resigned to the linear algorithms. The best known is Boyer-Moore (I won't insult your intelligence with a Wikipedia link, but the article there gives a good overview).\n\nBoyer-Moore works like this: you have some string you're looking for, which we'll call the needle, and some string you want to find it in, which we'll call the haystack. Instead of starting the search at the beginning of needle, you start at the end. If your needle character doesn't match the character you're looking at in haystack, you can move needle forwards in haystack until haystack's mismatched character lines up with the same character in needle. If haystack's mismatch isn't in needle at all, then you can skip ahead a whole needle's length.\n\nFor example, if you're searching for a string of 100 'a's (needle), you look at the 100th character in haystack. If it's an 'x', well, 'x' doesn't appear anywhere in needle, so you can skip ahead all of needle and look at the 200th character in haystack. A single mismatch allowed us to skip 100 characters!\n\nFor performance, the number of characters you can skip on a mismatch is usually stored in an array indexed by the character value. So the first part of my Boyer-Moore string searching algorithm looked like this:\n\nSo we look at the character in haystack and if it's not what we're looking for, we jump ahead by the right distance for that character, which is in jump_table.\n\n\"There,\" I sigh, finishing and sitting back. It may not be faster than grep, but it should be at least as fast, because this is the fastest algorithm known. This should be a good start. So I confidently ran my benchmark, for a 1 gigabyte file...\n\nOuch. I'm slower, more than 50% slower. grep is leaving me sucking dust. Ultimate Unix Geek chuckles into his xterms.\n\nMy eyes darken, my vision tunnels. I break out the big guns. My efforts to vectorize are fruitless (I'm not clever enough to vectorize Boyer-Moore because it has very linear data dependencies.) Shark shows a lot of branching, suggesting I can do better by unrolling the loop. Indeed:\n\nBut I was still more than 6% slower, and that's as fast as I got. Exhausted, stymied at every turn, I throw up my hands. grep has won.\n\n\"How do you do it, Ultimate Unix Geek? How is grep so fast?\" I moan at last, crawling forwards into the pale light of his CRT.\n\n\"Hmmm,\" he mumbles. \"I suppose you have earned a villian's exposition. Behold!\" A blaze of keyboard strokes later and grep's source code is smeared in green-on-black across the screen.\n\n\"You bastard!\" I shriek, amazed at what I see. \"You sold them out!\"\n\nSee all those d = d1[U(tp[-1])], tp += d; lines? Well, d1 is the jump table, and it so happens that grep puts 0 in the jump table for the last character in needle. So when grep looks up the jump distance for the character, via haystack_index += jump_table[haystack_char], well, if haystack_char is the last character in needle (meaning we have a potential match), then jump_table[haystack_char] is 0, so that line doesn't actually increase haystack_index.\n\nAll that is fine and noble. But do not be fooled! If the characters match, the search location doesn't change - so grep assumes there is no match, up to three times in a row, before checking to see if it actually found a match.\n\nPut another way, grep sells out its worst case (lots of partial matches) to make the best case (few partial matches) go faster. How treacherous! As this realization dawns on me, the room seemed to grow dim and slip sideways. I look up at the Ultimate Unix Geek, spinning slowly in his padded chair, and I hear his cackle \"old age and treachery...\", and in his flickering CRT there is a face reflected, but it's my ex girlfriend, and the last thing I see before I black out is a patch of yellow cheese powder inside her long tangled beard.\n\n\"Damn you,\" I mumble at last, rising from my prostrate position. Chagrined and humbled, I copy the technique.\n\nCopying that trick brought me from six percent slower to two percent faster, but at what cost? What penalty has grep paid for this treachery? Let us check - we shall make a one gigabyte file with one thousand x's per line, and time grep searching for \"yy\" (a two character best case) and \"yx\" (a two character worst case). Then we'll send grep to Over-Optimizers Anonymous and compare how a reformed grep (one that checks for a match after every character) performs.\n\nInnnnteresting. The treacherous optimization does indeed squeeze out almost 8% faster searching in the best case, at a cost of nearly 70% slower searching in the worst case. Worth it? You decide! Let me know what you think.\n\nResolved and refreshed, I plan my next entry. This isn't over, Ultimate Unix Geek.", "sentiment": 0.10857433217189315},
{"link_title": "Where Is the App for Escaping a Hurricane?", "url": "https://www.citylab.com/transportation/2017/09/where-is-the-app-for-escaping-a-hurricane/539184/", "text": "As Irma descends on Florida, ride-seekers confront a problem Silicon Valley has neglected to solve.\n\nThere are apps for massages, dog walkers, and anti-hangover IV drips. There are apps for car rides, shuttle buses, helicopters, and scooter bikes. But there is no app for Adrienne Beauchamp, who is sitting in her home in West Palm Beach, waiting for Hurricane Irma to hit. Beauchamp is one of countless Floridians who took to local rideshare boards in search of an escape from the Category 4 storm predicted to wreak unprecedented damage on the Sunshine State this weekend. She\u2019d like to be in Atlanta right now, like thousands who\u2019ve evacuated by car, plane, bus and train. But she\u2019s not\u2014and it isn\u2019t for lack of trying. \u201cWe\u2019re looking for a ride, anywhere,\u201d read the ad she posted to Craigslist on Wednesday. Hers was one among a great many requests that have popped up since Tuesday on Craigslist, Carpoolworld, and in Facebook groups. \u201cI've been packed, been ready. Help me get out of the eye of the storm please,\u201d reads one. \u201cPlease if you have anything message me,\u201d reads another. Most were requests for a seat headed north, but there were also plenty of offers of vehicle space. Good intentions seemed pervasive, but some were looking to capitalize: \u201cExpenses need to be covered, the seats/ride will be offered to the highest bidders.\u201d A few were perfectly Craigslist in character, flagging preferences for certain ethnicities, gender, and chill factors: Some posters have made successful matches, like one man driving to Orlando who picked up a friend-of-a-friend at a bus stop. Two French citizens in Miami Beach found a ride with a family headed to Atlanta\u2014a match I happened to make through reporting.\n\nOthers, like Beauchamp, have not been so lucky. She and her husband have two dogs, which was a deal-breaker when they put up a Craigslist ad in search of a ride. So on Wednesday night, Beauchamp booked a rental car and put up another ad offering two extra seats to Atlanta. An avalanche of strangers\u2019 texts and emails hit her inbox. \u201cHi my name is patty im looking for a ride out poss up to GA\u2026 or as far away as poss plz call me,\u201d one woman begged. From a mother in another state: \u201cI need a ride for my son out of delray to melbourne today\u2026 I will pay cash just to drop him off.\u201d Another was calm and collected: \u201cHi Adrienne. Saw your ad. Have you left South Florida yet?\u201d Most requests were to get to other parts of Florida, which put Beauchamp in a bind: \u201cIf you\u2019re running for your own life, you can\u2019t take detours,\u201d she tells me. Mismatched geographies, price gouging, wasted seats, lost time, and the wrong cars: these are the inconveniences the Uber-for-everything universe was supposed to solve. They are why the world\u2019s largest ride-hailing app all but destroyed the taxi industry when it came onto the scene in 2010: They connect riders, drivers, and vehicles cheaply, quickly, and effectively. Peer-to-peer carpooling is happening every day of the week in major U.S. metros on Waze, Carma, and Getaround. Anyone can get on their phones and deliver groceries for a buck, or fix somebody\u2019s leaking toilet.\n\nYet in this so-called \u201csharing economy,\u201d the best available tools for Floridians looking to escape a catastrophic storm are luck, diligence, and prayer. And this is in a state with regularly occurring hurricanes and an enormous population of non-drivers\u2014some elderly, others non-car-owners, many undocumented and fearful of roadside checkpoints. That\u2019s why Florida, famously transit-poor, has a robust networks of carpool boards and one of the most established jitney networks in the U.S. Where is the on-demand app for emergency rides? Unlike so many button-tap \u201csolutions,\u201d this is a problem for which technology would be an obvious win. FEMA could pay for it. State or local governments could advertise and distribute it. Imagine if Uber or Lyft (and local taxi companies, too) switched into an emergency mode in the event of a hurricane\u2014and transit agencies made custom apps free to download ahead of storms. With a GPS-enabled tool, \u201cyou could have coordination on both sides of the evacuation,\u201d says David King, a transportation researcher at Arizona State University who specializes in taxi and ride-hailing services. Imagine all the empty car seats that just escaped South Florida in the last few hours. Each extra spot could mean a saved life. Riders in one direction needn\u2019t be stranded on the way home. King also points out that an Uber-esque app capable of scanning traffic data would be able to suggest the best routes and openings for good samaritans headed in from safe zones to help evacuation efforts. \u201cAs we\u2019ve seen with Harvey and every natural disaster, people are primed to pitch in,\u201d King says. It seems Uber-for-hurricanes is just another example of the real-world problems Silicon Valley has neglected to solve. King has heard developers floating the idea of evacuation apps, but says has never seen them come to fruition. Reached for comment about such a feature, an Uber representative pointed to the company\u2019s assistance with post-hurricane recovery efforts. Lyft did not respond.\n\nOf all the woeful gaps in disaster preparedness and planning that Americans are now being confronted with lately, this could be the lowest-hanging fruit. As Hurricane Harvey descended on Texas two weeks ago, Houston Mayor Sylvester Turner decided against issuing an evacuation order\u2014largely because of highway capacity. \u201cThat seems like a failure of emergency evacuation,\u201d says Daniel B. Hess, a professor of urban planning at the University at Buffalo who has studied volunteer-led evacuation efforts in the face of disaster. \u201cSurely we can do a better job of coordinating people who are ready and willing to evacuate, but can\u2019t, because they don\u2019t have a car or anyone to take them.\u201d It\u2019s impossible to say how much extra capacity such an app would add in a huge metro area like Miami, but imagine all the empty car seats that just escaped South Florida in the last few hours. Each extra spot could mean a saved life. \u201cAnything we can do to help people get out of harm's way, we should be doing,\u201d says Hess. Clearly, there are no easy answers when it comes to moving millions in the face of a storm. Private vehicles are never going to be a complete answer to emergency mobility in dense areas when disaster strikes\u2014we don\u2019t all get our own lifeboats on this ship. And, amid rain and wind and rising waters, it\u2019s often more dangerous to be on the road than not. Coordinated mass transit is essential; so is coordinated planning with fuel companies, so that drivers aren\u2019t stranded for fear of lacking a refill. But being stranded because you couldn\u2019t match with an empty seat, as Beauchamp is, is plainly unnecessary. Which is exactly where Beauchamp still is. On Thursday morning, as she sifted through texts searching for a match, she got wind that the rental agency had rejected her reservation. By then, it was too late to book a train or plane. So she\u2019s at home, trying to stay calm, and still receiving texts for her original post. Texts like: I\u2019ll pay for gas, my name is Anthony, please let me know My dad is down there in Deerfield Beach, Century Village. am looking to get him up Atlanta or somewhere in GA Looking to get out of FL. \u201cAn app that worked like Uber would have been awesome,\u201d says Beauchamp. \u201cMaybe next time Florida will get this right.\u201d", "sentiment": 0.17079766363474227},
{"link_title": "Show HN: Procedural Hex Tile Planets with WebGL", "url": "http://jarettgross.me/Procedural-Planet-Generation/planets.html", "text": "", "sentiment": 0.0},
{"link_title": "A Puzzle about Ruby Constants", "url": "https://engineering.thriveglobal.com/a-puzzle-about-ruby-constants-e958d15dbada", "text": "The other day, I was doing some refactoring in the Rails CMS that serves thriveglobal.com. We allow our editors to toggle several boolean attributes on stories\u2014starring them, flagging them, etc.\u2014and we\u2019d DRY\u2019d up the controllers for these attributes by subclassing them to an abstract controller, :\n\nThe child controllers implemented the method to return the appropriate attribute, like this:\n\nNoticing that this method always returned a static value, I wondered if it should be a constant instead of a method. So I refactored like this:\n\nI ran the tests, and to my surprise I hit an error: . The code that was calling the constant couldn\u2019t find it even though it was able to find a method in the same context. Huh! What is going on here?\n\nThe method I changed into a constant is defined in one class, but is called from within its superclass. Here\u2019s an example that distills the structure of the code I was working with:\n\nThe original code was analogous to calling . After refactoring, the new code was analogous to calling . We can recreate my failed refactor by noticing that the method version works but the constant version fails:\n\nThe version that refers to a method in the subclass returns the desired value, but the version that refers to a constant in subclass throws an error. So the puzzle is this: Why does the version that uses a method work but the version that uses the constant fail?\n\nLet\u2019s look more closely at what\u2019s going on when we call the successful . Here\u2019s a crude, high-level description of how Ruby evaluates that method call:\n\nSteps (1) and (2) exemplify Ruby\u2019s method lookup algorithm. Ruby looks in the receiver\u2019s class for the method definition, and if it can\u2019t find it, Ruby iterates up the superclass chain (also known as the \u201cancestor chain\u201d) until it finds a class that implements the method. (If it gets all the way to and strikes out, it invokes .)\n\nSo that\u2019s the deal with method lookup. This is all pretty ho-hum for those with experience working in Ruby, or really any language that supports class inheritance. But why don\u2019t constants behave the same way?\n\nIf you suspect that constant lookup does work similarly, you are largely correct. As with methods, Ruby is also able to look through the superclass chain to find constants. Here\u2019s a demonstration:\n\nRuby is able to resolve even though that constant is not defined in , but rather in its superclass, . This is the same lookup behavior that caused to succeed.\n\nSo why does fail?\n\nThe answer is that while both lookup algorithms look for definitions in the ancestors of the current class, they differ in how they determine which class counts as \u201cthe current class\u201d. For method lookup the current class\u2014the first class in the ancestor chain Ruby will traverse in search of a definition\u2014is the receiver\u2019s class. When we call on , the value of within the body of this method is our receiver, . So the current class for the purposes of looking up is .\n\nConstant lookup determines the \u201ccurrent class\u201d differently. Rather than relying on the receiver to determine the current class, constant lookup starts with the class containing the method. To be more precise, constant lookup begins its superclass chain search using the class containing the current lexical scope. (If no class is open in the current scope, Ruby starts with the class.)\n\nLexical scope is the context defined by where you are in the code. It\u2019s what allows local variables to be defined within a block without affecting variables outside of the block, for example. So, while method lookup is relative to the receiver on which the method was called, constant lookup is relative only to the place in the code where Ruby encounters the constant.\n\nTo substantiate my claims about how Ruby finds constant definitions, let\u2019s dive into the Ruby source code that actually implements this algorithm. (Note: For this discussion, I will be focusing only on the YARV implementation of Ruby, version 2.4.1.)\n\nLet\u2019s investigate what Ruby does when we call and it comes across the constant in the body of that method. When we access a constant, the Ruby virtual machine, YARV, calls the instruction, defined here. Let\u2019s take a look at the comment on this function:\n\nand are YARV\u2019s way of referring to Ruby and . The parameter refers to the explicit scope we apply to a constant when we call it. E.g. if we called , would be . In our puzzle we have a \u201cnaked\u201d invocation of , for which is . So what the comment is telling us is that when we encounter a naked constant, it will be \u201csearched in the current scope\u201d. That sounds promising! Let\u2019s travel down the call stack to see what this really means in practice.\n\ncalls , a friendly 75-line function that actually implements constant lookup. This function gets passed four parameters: a thread; the explicit class context, ; an identifier for the constant; and a caching parameter, , with the value . We are interested in a naked constant call, so will be . Looking within the block, then, the first thing the function does is initialize a local variable, (short for \u201ccode reference\u201d). This variable holds the root of the lexical scope chain, which represents the place in the code at which the constant was encountered.\n\nNext, the long block iterates up the lexical scope chain, checking at each step along the way to see if the constant is defined in that context. The line is where we take a step up the chain. The routine keeps climbing the chain until it finds a scope in which the constant is defined or until finally there is no next , in which case we exit the block. It\u2019s the latter that will occur in our puzzle when we call ; The constant is not defined in the root lexical scope, that within , and so lexical scope search will come up empty.\n\nBut YARV doesn\u2019t stop its search there. It\u2019s the next bit of code that is most crucial to our puzzle:\n\nYARV will now attempt to resolve the constant by looking through the superclass hierarchy. YARV\u2019s first order of business is to identify the class it will use as the root of this hierarchy. How it determines that class is exactly what we were hoping to learn.\n\nThe root lexical scope is within the context of a class ( ), so the condition\n\nis satisfied, and is initialized as follows:\n\nThe function just returns the class it\u2019s passed as a second argument (see definition here), so gets assigned . was passed into as , which is falsey, so the return value for the constant lookup we care about will be where is .\n\nOk, we\u2019re almost home! calls , which in turn tries to look up the constant via a call to . This search function looks through the superclass hierarchy starting with the class it gets passed\u2014in our case, . To see that this is what is doing, note that just before the retry block, is set to , and we move up the hierarchy every time we hit here. The aptly named takes a Ruby class and returns its superclass.\n\nSo there we have it! When we call , Ruby searches for in and its superclasses. It never looks in , and so it can\u2019t find a definition for the constant.\n\nIn the last section, we established that Ruby\u2019s constant lookup algorithm works as follows:\n\nThe bolded text in step (4) illustrates the difference between Ruby\u2019s superclass chain search algorithm for constants and its similar algorithm for methods. For methods, the search starts with the receiver\u2019s class. For constants, the search starts with the class you are in at the code location where the constant is called, also known as the lexical scope.\n\nWhy doesn\u2019t Ruby use the same superclass chain lookup algorithm for constants that it uses for methods? And was I unreasonable to expect that it might?\n\nWhen we invoke a Ruby method, there is always an object we are calling it on. Method calls are messages that are passed to an object, the \u201creceiver\u201d of the message. This object might be explicitly mentioned in the code ( ), or it might be left implicit ( ), in which case the receiver defaults to the current value of . In either case, anywhere it finds a method call, Ruby has a unique object to work with. All Ruby objects have a class, and Ruby can\u2014and does\u2014look for the method\u2019s definition in this class and its ancestors.\n\nConstants, on the other hand, are not messages passed to objects. When we access a constant, that action is not relative to any particular object. So unless we provide Ruby with an explicit class to look in by prefixing our constant with , the only scope available to search in is the lexical scope. So that\u2019s where Ruby searches.\n\nWhy did I ever expect otherwise? In the case of my puzzle, it just so happened that my constant invocation occurred within the context of a method call. So in my puzzle there was a reference object, , that in theory could be used to determine a class scope. But this isn\u2019t generally true for constant calls, because constants can be accessed outside of the context of method calls. Nevertheless, the apparent existence of a reference object tempted me to into thinking that Ruby would use it to resolve .\n\nMatz, Ruby\u2019s creator, has said he is \u201ctrying to make Ruby natural, not simple\u201d. He is willing to make Ruby\u2019s implementation more complex in order to make its interface organic and frictionless. Ruby users embrace this design principle and have come to expect their language to \u201cjust work\u201d. That is what I was doing when I refactored my controller. Sure, it would be more complex for Ruby to make an exception for constants encountered in the context of method calls. But if it did, we\u2019d get to do things like , which feel natural. Given that Ruby goes out of its way to be natural, I don\u2019t think expecting Ruby to behave this way was unreasonable.\n\nThis all started because I found it surprising that code like the following doesn\u2019t work:\n\nIf I really wanted to use a constant here, and if I wanted to continue to support many subclasses of , what could I do?\n\nOne solution would be to provide an explicit class scope when we call the constant, like so:\n\nWith this change, will be invoked with set to the subclassed controller\u2014 in our example\u2014and will immediately find the constant defined there. Whether or not this code is better than what I started with I leave to the reader\u2019s judgement, but at least now we understand how and why it works.", "sentiment": 0.07933632024541114},
{"link_title": "Videos from ElixirConf 2017", "url": "https://www.youtube.com/channel/UC0l2QTnO1P2iph-86HHilMQ/videos", "text": "", "sentiment": 0.0},
{"link_title": "Are we being watched? Tens of other worlds could spot the Earth", "url": "https://www.eurekalert.org/pub_releases/2017-09/ras-awb090817.php", "text": "A group of scientists from Queen's University Belfast and the Max Planck Institute for Solar System Research in Germany have turned exoplanet-hunting on its head, in a study that instead looks at how an alien observer might be able to detect Earth using our own methods. They find that at least nine exoplanets are ideally placed to observe transits of Earth, in a new work published in the journal Monthly Notices of the Royal Astronomical Society.\n\nThanks to facilities and missions such as SuperWASP and Kepler, we have now discovered thousands of planets orbiting stars other than our Sun, worlds known as 'exoplanets'. The vast majority of these are found when the planets cross in front of their host stars in what are known as 'transits', which allow astronomers to see light from the host star dim slightly at regular intervals every time the planet passes between us and the distant star.\n\nIn the new study, the authors reverse this concept and ask, \"How would an alien observer see the Solar System?\" They identified parts of the distant sky from where various planets in our Solar System could be seen to pass in front of the Sun - so-called 'transit zones' -- concluding that the terrestrial planets (Mercury, Venus, Earth, and Mars) are actually much more likely to be spotted than the more distant 'Jovian' planets (Jupiter, Saturn, Uranus, and Neptune), despite their much larger size.\n\n\"Larger planets would naturally block out more light as they pass in front of their star\", commented lead author Robert Wells, a PhD student at Queen's University Belfast. \"However the more important factor is actually how close the planet is to its parent star - since the terrestrial planets are much closer to the Sun than the gas giants, they'll be more likely to be seen in transit.\"\n\nTo look for worlds where civilisations would have the best chance of spotting our Solar System, the astronomers looked for parts of the sky from which more than one planet could be seen crossing the face of the Sun. They found that three planets at most could be observed from anywhere outside of the Solar System, and that not all combinations of three planets are possible.\n\nKatja Poppenhaeger, a co-author of the study, adds, \"We estimate that a randomly positioned observer would have roughly a 1 in 40 chance of observing at least one planet. The probability of detecting at least two planets would be about ten times lower, and to detect three would be a further ten times smaller than this.\"\n\nOf the thousands of known exoplanets, the team identified sixty-eight worlds where observers would see one or more of the planets in our Solar System transit the Sun. Nine of these planets are ideally placed to observe transits of Earth, although none of the worlds are deemed to be habitable.\n\nIn addition, the team estimate that there should be approximately ten (currently undiscovered) worlds which are favourably located to detect the Earth and are capable of sustaining life as we know it. To date however, no habitable planets have been discovered from which a civilisation could detect the Earth with our current level of technology.\n\nThe ongoing K2 mission of NASA's Kepler spacecraft is to continue to hunt for exoplanets in different regions of the sky for a few months at a time. These regions are centred close to the plane of Earth's orbit, which means that there are many target stars located in the transit zones of the Solar System planets. The team's plans for future work include targeting these transit zones to search for exoplanets, hopefully finding some which could be habitable.", "sentiment": 0.1364021571648691},
{"link_title": "The Crypto Currency Debate: Future of Money or Speculative Hype?", "url": "http://aswathdamodaran.blogspot.com/2017/08/the-crypto-currency-debate-future-of.html", "text": "When it comes to any finance-related questions, I am fair game, and those questions usually span the spectrum, from what I think about Warren Buffett (or why I don't agree with everything he says) to whether tech stocks are in a bubble (a perennial question for worry warts). In the last few months, though, I have noticed that I have been getting more and more questions about crypto currencies, especially Bitcoin and Ether, and whether the price surges we have seen in these currencies are merited. While I have an old post on bitcoin , I have generally held back from talking about crypto currencies in this blog or in my other teaching for two reasons. First, I find that any conversation about bitcoin quickly devolves into an argument rather than a discussion, since both proponents and critics tend to hold strong views on its use (or uselessness). Second, I find that some of the technical underpinnings of bitcoin, ether and other cryptocurrencies are beyond my limited understanding of block chains and technology and I risk saying something incredibly ill informed. While both reasons still persist, I am going to throw caution to the winds and put down my thoughts about the rise, the mechanics and the future, at least as I see it, of crypto currencies in this post.\n\nAny discussion of crypto currencies has to start with the recognition that the experiment is still young. Satoshi Nakamoto's paper on bitcoin was made public in October 2008 and implemented as open source in January 2009. Less than ten years later, the market capitalization of bitcoin alone is in excess of $40 billion and the success story, at least in terms of bitcoin as an investment, can be seen in the graph below:\n\nWhile the crypto currencies emphasize their differences, the most successful ones share a base architecture, the block chain. A block chain is a shared digital ledger of transactions in an asset where the validation of transactions is decentralized. I know that sounds mystical, but the picture below (using bitcoin to illustrate) should provide a better sense of what's involved:\n\nIn effect, a block chain is a digital intermediation process where transactions are checked by members of the network, and recorded, and once that is done, cannot be altered fraudulently. As you can see from its description, the block chain technology is about far more than crypto currencies. It can be used to record transactions in any asset, from securities in financial markets to physical assets like houses, and do so in a way that replaces the existing intermediaries with decentralized models. It should come as no surprise that banks and stock exchanges, which make the bulk of their money from intermediation, not only see block chains as a threat to their existence but have been early investors in the technology, hoping to co-opt it to their own needs.\n\nIf you define success as a rise in market capitalization and popular interest, crypto currencies have clearly succeeded, perhaps more quickly than its original proponents ever expected it to. But the long term success of any crypto currency has to answer a different question, which is whether it is a \"good\" currency. Harking back to Money 101, you measure a currency's standing by looking at how well it delivers on its three purposes:\n\nonly with an intermediary who converts the bitcoin into US dollars for them . I certainly would not embark on a long or short trip away from home today, with just bitcoins in my pocket, nor would I be willing to convert all of my liquid savings into bitcoin or any other crypto currency. Would you?\n\n \n\n So, why has crypto currency not seen wider acceptance in transactions? There are a few reasons, some of which are more benign than others:\n\n : Fiat currencies have a had a long run, and it is not surprising that for many people, currency is physical and takes the form of government issued paper and coins. While people may use credit cards and Apple Pay, their thinking is still framed by the past, and it may take a while, especially for older consumers and retailers, to accept a digital currency. That said, the speed with which consumers have adapted to ride sharing services and taken to social media suggests that inertia cannot be the dominant reason holding back the acceptance of crypto currencies. : Crypto currencies have seen and continue to see wild swings in prices, not a bad characteristic in a traded asset but definitely not a good one in a currency. A retailer or service provider who prices his or her goods and services in bitcoin will constantly have to reset the price and consumers have little certitude of how much the bitcoin in their wallers will buy a few hours from now. : The crypto currency game is still young and the competing players each claim to have found the \"magic bullet\" for eventual acceptance. As technologies and tastes evolve, you will see a thinning of the herd, where buyers and sellers will pick winners, perhaps from the current list or maybe something new. It is possible that until this happens, transactors will hold up, for fear of backing the wrong horse in the race. Given these requirements, you can see why there are no perfect currencies and why every currency has to measured on a continuum from good to bad. Broadly speaking, currencies can take one of three forms, a physical asset (gold, silver, diamonds, shells), a fiat currency (usually taking the form of paper and coins, backed by a government) and crypto currencies. Gold's long tenure as a currency can be attributed to its strength as a store of value, arising from its natural scarcity and durability, though it falls short of fiat currencies, in terms of convenience and acceptance, both as a unit of account and as medium of exchanges. Fiat currencies are backed by sovereign governments and consequently can vary in quality as currencies, depending upon the trust that we have in the issuing governments. Without trust, fiat currency is just paper, and there are some fiat currencies where that paper can become close to worthless. For crypto currencies, the question then becomes how well they deliver on each of the purposes. As units of account, there is no reason to doubt that they can function, since they are fungible, divisible and countable. The weakest link in crypto currencies has been their failure to make deeper inroads as mediums of exchange or as stores of value. Using Bitcoin, to illustrate, it is disappointing that so few retailers still accept it as payment for goods and services . Even the much hyped successes, such as Overstock and Microsoft accepting Bitcoin is illusory, since they do so on limited items, and. I certainly would not embark on a long or short trip away from home today, with just bitcoins in my pocket, nor would I be willing to convert all of my liquid savings into bitcoin or any other crypto currency. Would you?So, why has crypto currency not seen wider acceptance in transactions? There are a few reasons, some of which are more benign than others:\n\nUltimately, though, I lay some of the blame on the creators of the crypto currencies, for their failure, at least so far, on the transactions front. As I look at the design and listen to the debate about the future of crypto currencies, it seems to me that the focus on marketing crypto currencies has not been on transactors, but on traders in the currency, and it remains an unpleasant reality that what makes crypto currencies so attractive to traders (the wild swings in price, the unpredictability, the excitement) make them unacceptable to transactors.\n\nYou can see the disconnect in how crypto currencies have been greeted, by contrasting the rousing reception that markets have given them with the arms length at which they have been held by merchandisers and consumers. In the graph below, I focus on the divergence between the market price rise of bitcoin and the increase in the number of transactions involving bitcoin:\n\nThe analogy between gold and crypto currency has one weak link. Gold has held its value through the centuries and is a physical asset. For better or worse, it is unlikely that we will decide a few years from now that gold is worthless. A crypto currency that few people use as currency ultimately will not be able to sustain itself, as shiner and newer versions of it pop up. Ironically, if traders in bitcoin and ether want their investments in the crypto currencies to hold their value, the currencies have to become less exciting and lucrative as investments, and become more accepted as currencies. Since that will not happen by accident, I would suggest that the winning crypto currency or currencies will share the following characteristics;\n\n: From creators and proponents of the currency, you will hear less talk about how much money you would make by buying and selling the currency and more on its efficacy in transactions. The design of the crypto currency will focus on creating features that make it attractive as a currency (for transactions), not as investments. Thus, if you are going to impose a cap (either rigid like Bitcoin or more flexible, as with other currencies), you need to explain to transactors, not traders, why the cap makes sense. : I know that we live in an age where trust is a scarce resource and I argued that that the growth in crypto currencies can be attributed, at least partly, to this loss of trust. That said, to be effective as a currency, you do need to be able to trust in something and perhaps accept compromises on privacy and centralized authority (at least on some dimensions of the currency). It is also worth noting that the real tests for crypto currencies will occur when they reach their caps (fixed or flexible). After all, bitcoin and ether miners have been willing to put in the effort to validate transactions because they are rewarded with issues of the currency, feasible now because there is slack in the currency (the current number is below the cap). As the cap becomes a binding constraint, the rewards from miners have to come from transactions costs and serious thought has to go into currency design to keep these costs low. Hand waving and claiming that technological advances will allow this happen are not enough. I know that there are many in the crypto currency world who recognize this challenge, but for the moment, their voices are being drowned out by traders in the currency and that is not a good sign.\n\nIf you expected a valuation of bitcoin or ether in this post, you are probably disappointed by it, but here is a simple metric that you could use to determine whether the prices for crypto currencies are \"fair\". Currencies are priced relative to each other (exchange rates) and there is no reason why the rules that apply to fiat currencies cannot be extended to crypto currencies. A fair exchange rate between two fiat currencies will be on that equalizes their purchasing power , an old, imperfect and powerful theorem. Consequently, the question that you would need to address, if you are paying $2,775 for a bitcoin on August 1, 2017, is whether you can (or even will be able to) but $2,775 worth of goods and services with that bitcoin. If you believe that bitcoin will eventually get wide acceptance as a digital currency, you may be able to justify that price, especially because there is a hard cap on bitcoin, but if you don't believe that bitcoin will ever acquire wide acceptance in transactions, it is time that you were honest with yourself and recognized that is just a lucrative, but dangerous, pricing game with no good ending.\n\n\n\n \n\n YouTube Video\n\n Crypto currencies, with bitcoin and ether leading the pack, have succeeded in financial markets by attracting investors, and in the public discourse by garnering attention, but they have not succeeded (yet) as currencies. I believe that there will be one or more digital currencies competing with fiat currencies for transactions, sooner rather than later, but I am hard pressed to find a winner on the current list, right now, but that could change if the proponents and designers of one of the currencies starts thinking less about it as a speculative asset and more as a transaction medium, and acting accordingly. If that does not happen, we will have to wait for a fresh entrant and the most enduring part of this phase in markets may be the block chain and not the currencies themselves.\n\nThe initial rise could have been a flash in the pan, a fad attracting speculators, but in the last two years, Bitcoin seems to have found new fans, as can be seen below:Bitcoin's success, at least in the financial markets, has attracted a host of competitors, with Ethereum (Ether) being the most successful. Ether's rise in market price, since its introduction in 2015 has been even more precipitous that Bitcoin's, though it has pulled back in recent weeks:The key features of a block chain are:While the price of bitcoin has increase more than a thousand fold, since the start of 2012, the number of transactions involving bitcoin was only about thirty two times larger in July 2017 than what it was at the start of 2012. In my view, there are three possible explanations for the divergence, and they are not mutually exclusive:", "sentiment": 0.05636053425908501},
{"link_title": "How to Sue Equifax", "url": "http://howtosueequifax.com/index.html", "text": "On September 7, 2017, Equifax announced a cybersecurity incident potentially impacting approximately 143 million U.S. consumers. The breach was originally discovered on July 29. The information accessed primarily includes names, Social Security numbers, birth dates, addresses and, in some instances, driver\u2019s license numbers. In addition, credit card numbers for approximately 209,000 U.S. consumers, and certain dispute documents with personal identifying information for approximately 182,000 U.S. consumers, were accessed. Equifax did not disclose if PIN numbers and other sensitive information was compromised as well, nor did it explain its delay in reporting the breach (which it discovered in July) to the public. It was furthermore revealed that three Equifax executives had sold their company shares before the incident was made public.\n\nDisclaimer: I am not a lawyer. This is a description of the steps I took. The information is provided \u201cas is\u201d, without warranty of any kind, expressed or implied. I am not responsible for your actions.\n\nHere are the steps I took to sue Equifax in the Magistrate Court of Fulton County in Georgia. Equifax\u2019s headquarters is in Atlanta, Fulton County, Georgia.\n\nI went to the Magistrate Courts Free Forms Generator Website. The website is listed on georgiacourts.gov, here. Then, I did the following:\n\nBy making use of any information on this website, you agree to the following:\n\nNO WARRANTIES: All of the information provided on this website is provided \u201cAS-IS\u201d and with NO WARRANTIES. No express or implied warranties of any type, including for example implied warranties of merchantability or fitness for a particular purpose, are made with respect to the information, or any use of the information, on this site. The website creator makes no representations and extends no warranties of any type as to the accuracy or completeness of any information or content on this website.\n\nDISCLAIMER OF LIABILITY: The website creator specifically DISCLAIMS LIABILITY FOR INCIDENTAL OR CONSEQUENTIAL DAMAGES and assumes no responsibility or liability for any loss or damage suffered by any person as a result of the use or misuse of any of the information or content on this website. The website creator assumes or undertakes NO LIABILITY for any loss or damage suffered as a result of the use, misuse or reliance on the information and content on this website.\n\nUSE AT YOUR OWN RISK: This website is for informational purposes only. Consult a lawyer for legal advice. It is your responsibility to evaluate your own legal situation, or that of your clients, and to independently determine whether to perform, use or adapt any of the information or content on this website. By voluntarily using any information displayed on this website, you assume the risk of any resulting consequences.", "sentiment": 0.09857142857142857},
{"link_title": "ServiceStack .NET Core 2.0 Web Apps", "url": "http://templates.servicestack.net/docs/web-apps", "text": "Web Apps are a new approach to dramatically simplify .NET Wep App development and provide the most productive development experience possible whilst maximizing reuse and component sharing. They also open up a number of new use-cases for maintaining clean isolation between front-end and back-end development with front-end developers not needing any knowledge of C#/.NET to be able to develop UIs for high-performance .NET Web Apps. Web Apps also make it easy to establish and share an approved suite of functionality amongst multiple websites all consuming the same back-end systems and data stores.\n\nWeb Apps leverages Templates to develop entire content-rich, data-driven websites without needing to write any C#, compile projects or manually refresh pages - resulting in the easiest and fastest way to develop Web Apps in .NET!\n\nNot having to write any C# code or perform any app builds dramatically reduces the cognitive overhead and conceptual knowledge required for development where the only thing front-end Web developers need to know is Template's syntax and what filters are available to call. Because of Template's high-fidelity with JavaScript, developing a Website with Templates will be instantly familiar to JavaScript devs despite calling and binding directly to .NET APIs behind the scenes.\n\nAll complexity with C#, .NET, namespaces, references, .dlls, strong naming, packages, MVC, Razor, build tools, IDE environments, etc has been eliminated leaving all Web Developers needing to do is run a cross-platform web/app.dll .NET Core 2.0 executable and configure a simple web.settings text file to specify which website folder to use, which ServiceStack features to enable, which db or redis providers to connect to, etc. Not needing to build also greatly simplifies deployments where multiple websites can be deployed with a single rsync or xcopy command or if deploying your App in a Docker Container, you just need to copy your website files, or just the web.settings if you're using an S3 or Azure Virtual File System.\n\nThe iterative development experience is also unparalleled for a .NET App, no compilation is required so you can just leave the web/app.dll running whilst you add the template .html files needed to build your App and thanks to the built-in Hot Reloading support, pages will refresh automatically as you save. You'll just need to do a full page refresh when modifying external .css/.js files to bypass the browser cache and you'll need to restart web/app.dll to pick up any changes to your web.settings or added any .dlls to your /plugins folder.\n\nThe easiest way to get started to either clone the WebAppStarter GitHub project:\n\nOr download a zip of it from github.com/ServiceStack/WebAppStarter/archive/master.zip.\n\nThis contains 2 folders, /app contains all source code and assets for your Web App and /web contains a copy of the WebApp binaries from ServiceStack/WebApp. There's also start-app.bat and start-app.sh scripts for running your app in Windows, OSX and Linux. Both scripts run the same command:\n\nWhich can be run instead of the script, it also means cat start-app.bat | sh will also run the app on OSX and Linux.\n\nOnce running you can view your App on http://localhost:5000/ which you can keep running whilst developing your app which will reload the page you're currently viewing on each file save using the built-in Hot Reloading.\n\n/app is an example of a single Web App, you can have multiple Web Apps in different folders and run any of them with:\n\nIf you intend to deploy your Web App on AWS or Azure you may prefer to start with one of the example Cloud Apps below which come pre-configured with deployment scripts for deploying with Travis CI and Docker:\n\nWe've developed a number of Web Apps to illustrate the various features available and to showcase the different kind of Web Apps that can easily be developed. The source code for each app is available either individually from [github.com/NetCoreWebApps](https://github.com/NetCoreWebApps) or all combined in the same repo at NetCoreWebApps/WebApp/apps. Each app runs the same unmodified [Web App Binary](https://github.com/NetCoreWebApps/Web) that's also used in the WebAppStarter project above.\n\nEach of these apps can be run locally by cloning NetCoreWebApps/WebApp and running the start.bat scripts in the /apps folder. If you want to use your local RDBMS you can use /support/northwind-data to quickly populate it with the Northwind database and /support/copy-files to populate your own S3 Bucket or Azure Blob Container with the /rockwind-vfs Web App's files. As each app runs on http://localhost:5000 you'll need to do a hard refresh with Ctrl+Shift+F5 after launching each App to tell the browser to ignore the cached .css and .js from the previous App.\n\nThe Getting Started project contains a copy of the bare.web-app.io project below which is representative of a typical Company splash Website:\n\nThe benefits over using a static website is improved maintenance as you can extract and use its common _layout.html instead of having it duplicated in each page. The menu.html partial also makes menu items easier to maintain by just adding an entry in the JavaScript object literal. The dynamic menu also takes care of highlighting the active menu item.\n\nThe other primary benefit is that this is an example of a website that can be maintained by employees who don't have any programming experience as Templates in their basic form are intuitive and approachable to non-developers, e.g: The title of each page is maintained as metadata HTML comments:\n\nTemplate's syntax is also the ideal way to convey variable substitution, e.g: <title>{{ title }}</title> and even embedding a partial reads like english {{ 'menu' | partial }} which is both intuitive and works well with GUI HTML designers.\n\nBelow is the web.settings for a Basic App, with contentRoot being the only setting required as the rest can be inferred but including the other relevant settings is both more descriptive to other developers as well making it easier to use tools like sed or powershell to replace them during deployment.\n\nFor the Redis Browser Web App, we wanted to implement an App that was an ideal candidate for a Single Page App but constrain ourselves to do all HTML rendering on the server and have each interaction request a full-page reload to see how a traditional server-generated Web App feels like with the performance of .NET Core 2.0 and Templates. We're pleasantly surprised with the result as when the App is run locally the responsiveness is effectively indistinguishable from an Ajax App. When hosted on the Internet there is a sub-second delay which causes a noticeable flicker but it still retains a pleasant UX that's faster than most websites.\n\nThe benefits of a traditional website is that it doesn't break the web where the back button and deep linking work without effort and you get to avoid the complexity train of adopting a premier JavaScript SPA Framework's configuration, dependencies, workflow and build system which has become overkill for small projects.\n\nWe've had a sordid history developing Redis UI's which we're built using the popular JavaScript frameworks that appeared dominant at the time but have since seen their ecosystem decline, starting with the Redis Admin UI (src) built using Google's Closure Library that as it works different to everything else needed a complete rewrite when creating redisreact.servicestack.net (src) using the hot new React framework, unfortunately it uses React's old deprecated ES5 syntax and Reflux which is sufficiently different from our current recommended TypeScript + React + Redux + WebPack JavaScript SPA Stack, that is going to require a significant refactor to adopt our preferred SPA tech stack.\n\nThe nice thing about generating HTML is that it's the one true constant in Web development that will always be there. The entire functionality for the Redis Web App is contained in a single /redis/index.html which includes all Template and JavaScript Source Code in < 200 lines which also includes all as server logic as it doesn't rely on any back-end Services and just uses the Redis Filters to interface with Redis directly. The source code also serves as a good demonstration of the declarative coding style that Templates encourages that in addition to being highly-readable requires orders of magnitude less code than our previous Redis JavaScript SPA's with a comparable feature-set.\n\nHaving a much smaller code-base makes it much easier to maintain and enhance whilst being less susceptible to becoming obsolete by the next new JavaScript framework as it would only require rewriting 75 lines of JavaScript instead of the complete rewrite that would be required to convert the existing JavaScript Apps to a use different JavaScript fx.\n\nThe web.settings for Redis is similar to Web App Starter above except it adds a redis.connection to configure a RedisManagerPool at the connection string provided as well as Redis Filters to give Templates access to the Redis instance.\n\nThe Rockwind website shows an example of combining multiple websites in a single Web App - a Rockstars Content Website and a dynamic data-driven UI for the Northwind database which can run against either SQL Server, MySql or SQLite database using just configuration. It also includes API Pages examples for rapidly developing Web APIs.\n\n/rockstars is an example of a Content Website that itself maintains multiple sub sections with their own layouts - /rockstars/alive for living Rockstars and /rockstars/dead for the ones that have died. Each Rockstar maintains their own encapsulated mix of HTML, markdown content and splash image that intuitively uses the closest _layout.html, content.md and splash.jpg from the page they're referenced from. This approach makes it easy to move entire sub sections over by just moving a folder and it will automatically use the relevant layout and partials of its parent.\n\n/northwind is an example of a dynamic UI for a database containing a form to filter results, multi-nested detail pages and deep-linking for quickly navigating between referenced data. Templates is also a great solution for rapidly developing Web APIs where the /api/customers.html API Page below:\n\nIs all the code needed to generate the following API endpoints:\n\nIn addition to being a .NET Core 2.0 App that runs flawlessly cross-platform on Windows, Linux and OSX, Web Apps can also support multiple RDBMS's and Virtual File Systems using just configuration.\n\nSQLite uses a file system database letting you bundle your database with your App. So we can share the northwind.sqlite database across multiple Apps, the contentRoot is set to the /apps directory which can only be accessed by your App, whilst the webRoot is configured to use the Web Apps folder that hosts all the publicly accessible files of your App.\n\nTo run the Rockwind app using the northwind.sqlite database, run the command below on Windows, Linux or OSX:\n\nTo switch to use the Northwind database in SQL Server we just need to update the configuration to point to a SQL Server database instance. Since the App no longer need access to the northwind.sqlite database, the contentRoot can be reverted back to the Web Apps folder:\n\nThe /support/northwind-data project lets you quickly try out Rockwind against your local RDBMS by populating it with a copy of the Northwind database using the same sqlserver identifier and connection string from the App, e.g:\n\nYou can run against a MySql database in the same way as SQL Server above but using a MySql db connection string:\n\nThe example Azure configuration is also configured to use a different Virtual File System where instead of sourcing Web App files from the filesystem they're sourced from an Azure Blob Container. In this case we're not using any files from the App so we don't need to set a contentRoot or webRoot path. This also means that for deployment we're just deploying the WebApp binary with just this web.settings since both the Web App files and database are sourced remotely.\n\nThe /support/copy-files project lets you run Rockwind against your own Azure Blob Container by populating it with a copy of the /rockwind App's files using the same configuration above:\n\nAs Templates is unable to use a Typed ORM like OrmLite to hide the nuances of each database, we need to be a bit more diligent in Templates to use parameterized SQL that works across multiple databases by using the sql* DB Filters to avoid using RDBMS-specific SQL syntax. The /northwind/customer.html contains a good example containing a number of things to watch out for:\n\nUse sqlConcat to concatenate strings using the RDBMS-specific SQL for the configured database. Likewise sqlCurrency utilizes RDBMS-specific SQL functions to return monetary values in a currency format, whilst sqlQuote is used for quoting tables named after a reserved word.\n\n/rockwind-vfs is a clone of the Rockwind Web App with 3 differences: It uses the resolveAsset filter for each .js, .css and image web asset so that it's able to generate external URLs directly to the S3 Bucket, Azure Blob Container or CDN hosting a copy of your files to both reduce the load on your Web App and maximize the responsiveness to the end user.\n\nTo maximize responsiveness when using remote storage, all embedded files utilize caching:\n\nThe other difference is that each table and column has been quoted in \"double-quotes\" so that it works in PostgreSQL which otherwise treats unquoted symbols as lowercase. This version of Rockwind also works with SQL Server and SQLite as they also support \"Table\" quotes but not MySql which uses `BackTicks` or [SquareBrackets]. It's therefore infeasible to develop Apps that support both PostgreSQL and MySql unless you're willing to use all lowercase, snake_case or the sqlQuote filter for every table and column.\n\nIf using a remote file storage like AWS S3 or Azure Blob Storage it's a good idea to use the resolveAsset filter for each external file reference. By default it returns the same path it was called with so it will continue to work locally but then ServiceStack effectively becomes a proxy where it has to call the remote Storage Service for each requested download.\n\nServiceStack asynchronously writes each file to the Response Stream with the last Last-Modified HTTP Header to enable browser caching so it's still a workable solution but for optimal performance you can specify an args.assetsBase in your web.settings to populate the assetsBase TemplateContext Argument the resolveAsset filter uses to generate an external URL reference to the file on the remote storage service, reducing the load and improving the performance of your App, especially if it's configured to use a CDN.\n\nThe AWS settings shows an example of this where every external resource rockwind-aws.web-app.io has been replaced with a direct reference to the asset on the S3 bucket:\n\nWith all files being sourced from S3 and the App configured to use AWS RDS PostgreSQL, the AWS settings is an example of a Pure Cloud App where the entire App is hosted on managed cloud services that's decoupled from the .NET Core 2.0 binary that runs it that for the most part won't require redeploying the Web App binary unless making configuration changes or upgrading the web/app.dll as any App changes can just be uploaded straight to S3 which changes reflected within the checkForModifiedPagesAfterSecs setting, which tells the Web App how long to wait before checking for file changes whilst defaultFileCacheExpirySecs specifies how long to cache files like content.md for.\n\nDeployments are also greatly simplified as all that's needed is to deploy the WebApp binary and web.settings of your Cloud App, e.g. here's the DockerFile for rockwind-aws.web-app.io - deployed to AWS ECS using the deployment scripts in Rockwind.Aws and following our .NET Core Docker Deployment Guideline:\n\nWe can also create Azure Cloud Apps in the same we've done for AWS above, which runs the same /rockwind-vfs Web App but using an Azure hosted SQL Server database and its files hosted on Azure Blob Storage:\n\nUp till now the Apps above only have only used functionality built into ServiceStack, to enable even greater functionality but still retain all the benefits of developing Web Apps you can drop .dll with custom functionality into your Web App's /plugins folder. The plugins support in Web Apps is as friction-less as we could make it, there's no configuration to maintain or special interfaces to implement, you're able to drop your existing implementation .dll's as-is into the App's `/plugins` folder.\n\nPlugins allow \"no touch\" sharing of ServiceStack Plugins, Services, Template Filters Template Code Pages, Validators, etc. contained within .dll's or .exe's dropped in a Web App's /plugins folder which are auto-registered on startup. The source code for all plugins used in this App were built from the .NET Core 2.0 projects in the /example-plugins folder. The plugins.web-app.io Web App below walks through examples of using Custom Filters, Services and Validators:\n\nServiceStack Plugins can be added to your App by listing it's Type Name in the features config entry in web.settings:\n\nAll plugins listed in features will be added to your Web App's AppHost in the order they're specified. They can further customized by adding a separate config entry with the Plugin Name and a JavaScript Object literal to populate the Plugin at registration, e.g the config above is equivalent to:\n\nIn this case it tells our CustomPlugin from /plugins/ServerInfo.dll to also show Process Links in its /metadata Page:\n\nWhere as it was first registered in the list will appear before any links registered by other plugins:\n\nIt also tells the ValidationFeature to scan all Service Assemblies for Validators and to automatically register them which is how ServiceStack was able to find the ContactValidator used to validate the StoreContact request.\n\nOther optional plugins registered in this Web App is the metadata Services required for Open API, Postman as well as support for CORS. You can check the /metadata/debug Template for all Plugins loaded in your AppHost.\n\nAll Services loaded by plugins continue to benefit from ServiceStack's rich metadata services, including being listed in the /metadata page, being able to explore and interact with Services using /swagger-ui/ as well as being able to generate Typed APIs for the most popular Mobile, Web and Desktop platforms.\n\n/chat is an example of the ultimate form of extensibility where instead of just being able to add Services, Filters and Plugins, etc. You can add your entire AppHost which Web Apps will use instead of its own. This vastly expands the use-cases that can be built with Web Apps as it gives you complete fine-grained control over how your App is configured.\n\nFor chat.web-app.io we've taken a copy of the existing .NET Core 2.0 Chat App and moved its C# code to /example-plugins/Chat and its files to /apps/chat where it can be developed like any other Web App except it utilizes the Chat AppHost and implementation in the SelfHost Chat App.\n\nCustomizations from the original .NET Core Chat implementation includes removing MVC and Razor dependencies and configuration, extracting its _layout.html and converting index.html to use Templates from its original default.cshtml. It's also been enhanced with the ability to evaluate Templates from the Chat window, as seen in the screenshot above.\n\nOne nice thing from being able to reuse existing AppHost's is being able to develop all back-end C# Services and Custom Filters as a stand-alone .NET Core Project where it's more productive with access to .NET IDE tooling and debugging.\n\nTo account for these 2 modes we use AddIfNotExists to only register the TemplatePagesFeature plugin when running as a stand-alone App and add an additional constructor so it reuses the existing web.settings as its IAppSettings provider for is custom App configuration like OAuth App keys required for enabling Sign-In's via with Twitter, Facebook and GitHub when running on http://localhost:5000:\n\nAfter the back-end has been implemented we can build and copy the compiled Chat.dll into the Chat's /plugins folder where we can take advantage of the improved development experience for rapidly developing its UI.", "sentiment": 0.15230148273626534},
{"link_title": "I worked at Bell Pottinger. The South Africa scandal reflects its toxic culture", "url": "https://www.theguardian.com/commentisfree/2017/sep/07/bell-pottinger-south-africa-scandal-toxic-pr-race-relations?CMP=Share_iOSApp_Other", "text": "It was lunchtime on 31 July 2013, and I had suddenly realised how toxic the corporate culture at Bell Pottinger \u2013 the disgraced PR firm currently at the centre of a race relations scandal in South Africa \u2013 really was.\n\nOne of the many TVs that dotted the office had announced that Doreen Lawrence, mother of the murdered teenager Stephen Lawrence, was to be made a peer. A male director spluttered and choked out his outrage to all within earshot. \u201cWhat\u2019s she ever done to deserve it?\u201d he jeered, contemptuous that a black, progressive woman had been given such an honour.\n\n\u201cSo have I!\u201d he responded with obnoxious self-belief.\n\nIn another life I was a corporate lobbyist, and I started my career in the most notorious of PR houses \u2013 Bell Pottinger. The company now looks to be in its death throes, after being expelled from the Public Relations and Communications Association, the industry regulatory body, for whipping up racial hatred in South Africa on behalf of its client, the Gupta family. The scandal has forced Bell Pottinger\u2019s chief executive, James Henderson, to resign, and there is a mass exodus of clients.\n\nNo one could have foreseen what would be happening now in 2012, when I joined the company. But even then the writing was on the wall, if you cared to look.\n\nBell Pottinger was, and is, a company full of smart, kind, decent people \u2013 some of the best in the industry. It is unfair that many of them will lose their jobs, through no fault of their own. For a young graduate, it offered an unparalleled introduction to the world of corporate lobbying, and many used the skills they learned there for good \u2013 entering the charity or political worlds, as I\u2019d intended to do.\n\nBut underneath it all, there was a toxic, Machiavellian working culture. The company had a history of taking on the clients no one else would want: in fact, it cultivated an image of sailing close to the wind in order to attract them. I worked for a tobacco company (other firms won\u2019t touch tobacco), a \u201clegal loan shark\u201d that charged impoverished clients obscene interest rates, a commodities house linked to a toxic spill, and an oil company trying to drill in a world heritage site.\n\nThe work we were doing was immoral, though not illegal. It\u2019s in this context of accepting the clients that other firms wouldn\u2019t touch that the Gupta scandal should properly be viewed. But my main criticisms weren\u2019t with the clients, but the culture. Sure, I didn\u2019t like representing a tobacco firm; but I smoked, didn\u2019t I? It was the atmosphere in the office that ground me down, day after day.\n\nAs a young, minority woman who also voted Labour in a firm with close links to the Tories, working there was often unbearable. Male colleagues would sneer at me in meetings, calling me \u201cPolly Toynbee\u201d \u2013 the ultimate term of contempt to Tory activists looking for safe seats, who spent their free time drinking in the Strangers\u2019 Bar with the backbenchers of the 1922 Committee. One morning I woke up to find that members of the committee had followed me \u2013 a young graduate with a tiny following \u2013 on Twitter in the middle of the night. It didn\u2019t take a genius to work out I had been a figure of fun during a late-night conversation.\n\nIt was a boys\u2019 club, and we never forgot that. An office party was held at the Playboy Club, and female attendees were told to wear high heels. Female partners were rare, and didn\u2019t last long \u2013 I saw two senior women pushed out within months of joining. A male partner took me to lunch and kindly explained that I needed to be better at hiding my emotions, because being emotional would hold me back. Another told me to let it go when one of his newly hired ex-army pals cheerfully assumed I was an intern.\n\nHiring practices looked nepotistic. I\u2019d entered through the now-defunct graduate scheme, which offered no promise of a job at the end of a year-long contract, despite being hugely competitive. Multiple other people were hired while I was there into permanent, entry-level roles \u2013 bypassing the scheme \u2013 seemingly thanks to family connections.\n\nThere were very few fellow minorities, and all the ones I worked with subsequently left. I watched a male director scream in outrage at one woman of colour because she had dared to query his expenses. A year after I left, I went for coffee with my former director, a man of south Asian descent, one of the few nonwhite directors. Looking morose, he told me about a high-level meeting in which a racially offensive jibe had been made about him by a colleague. He had also now left.\n\nTowards the end of my time at the company, I was asked to assist on a pitch led by the Doreen Lawrence critic. I overprepared: I didn\u2019t trust him, and I\u2019d never worked for him before. I was right to do so.\n\nDuring the pitch, the client team tossed us a difficult question. With a smile, the director \u2013 who was clearly winging it \u2013 passed the grenade to me, despite it being on a topic I\u2019d been told not to research. I answered, raging internally at his attempt to set me up to fail. Afterwards he emailed me, congratulating me on my performance. Perhaps I had won him round, I wondered.\n\nWeeks later, the same director pushed me out. I received a bungled phone call one morning at my desk from the HR manager, asking about my leave date. It was the first I\u2019d heard of it. Blinking back tears, I asked my ally, the South Asian director, what had happened.\n\nHe had vouched for me, he explained gently, but the other director said my work was substandard and I didn\u2019t fit the values of the company. \u201cBut the only piece of work I ever did for him he congratulated me on!\u201d I responded, throat gravel-coarse. The implication felt clear: I wasn\u2019t right for Bell Pottinger, because of who I was, not how hard I worked or what I\u2019d done.\n\nBell Pottinger was, and is, a place where good people struggled to have their voices heard about the din or were pushed out. But what today\u2019s scandal shows is that what is rotten will eventually come unstuck.", "sentiment": 0.10262524311437356},
{"link_title": "The Future of Luxe", "url": "https://medium.com/@curtisylee/the-future-of-luxe-49ce60d5249d", "text": "Curtis Lee, CEO of Luxe & VP of Digital for Volvo Cars\n\nLuxe\u2019s assets have been acquired by Volvo Cars.\n\nWe started Luxe four years ago with the idea that owning a car shouldn\u2019t be so difficult and focused our initial efforts around the worst of all problems for drivers \u2014 parking. Equipped with a smartphone and thousands of eager blue jacketed valets, we transformed how commuters parked every day. We reduced traffic from drivers who would have otherwise circled aimlessly looking for parking, we saved customers valuable time, and helped transform cities by reducing the dependency on having parking lots in central city locations by storing cars in the fringe. Thank you to all of our customers. Your love, passion and feedback became the fuel that inspired us to make Luxe more useful and delightful every day.\n\nA few months ago, we were approached by Volvo about teaming up. Many employees at Volvo were avid Luxe customers and were impressed by what we built. And as we learned more about Volvo, it was clear they were one of the most progressive automotive companies we had met. Their recent bold commitment to be the first traditional automaker to only launch electric or hybrid new models starting in 2019, resonated strongly with our mission of transforming transportation. As we engaged in many conversations, it was clear there was also a shared vision on the future of automotive. A belief that we are on the precipice of a technological revolution where electrification, autonomous drive, connectivity and digital experiences among others will transform our lives like nothing we\u2019ve ever seen.\n\nWorking at Volvo gives us scale, resources and access to a global company that is committed to the future. Moreover, Volvo is committed to allowing us to fulfill our vision. With the acquisition, Luxe will continue to operate as its own unit working on some of Volvo\u2019s most meaningful initiatives in mobility and digital customer experiences. I look forward to continuing to lead the Luxe team at Volvo.\n\nLastly, I want to take the time to thank all of our great employees (past and present), including your families. Thank you for the late nights, weekends, blown vacations and other hardships we endured running a live operating business moving vehicles nonetheless. It definitely was not easy, but you made it all work through sheer grit and smarts and I\u2019m forever grateful and honored to have worked alongside you.", "sentiment": 0.15755928853754944},
{"link_title": "Facebook will spend as much as $1B on original TV in the next year", "url": "https://www.theverge.com/2017/9/8/16273406/facebook-original-video-content-tv-shows-billion-netflix-2018", "text": "The Wall Street Journal reports that Facebook is going into the next year \u201cwilling to spend as much as $1 billion\u201d on original video content, building out the roster of exclusive TV on its revamped video tab Watch.\n\nIn June, the WSJ reported that Facebook was willing to pay up to $3 million per episode for centerpiece shows, and was also interested in original sitcoms with episodic budgets in the six-figures. At the time, VP of media partnerships Nick Grudin told The Verge in an email, \u201cWe're funding these shows directly now, but over time we want to help lots of creators make videos funded through revenue sharing products like Ad Break.\u201d Eventually, Facebook wants to pay nothing for the original shows, instead offering the creators a 45 percent share of their ad revenue.\n\nEven a $1 billion investment is still a fraction of what Facebook\u2019s digital competitors, like Netflix ($6 billion in 2017) and Amazon ($4.5 billion) shell out, and WSJ reports that the number could fluctuate based on the success of the first crop of shows. However, it\u2019s an aggressive hike from previous investments.\n\nLast year, Facebook put up $50 million to pay celebrities and brands (including The Verge\u2019s parent company Vox Media) to make content for its then-new Live platform. As of April, the company has been touting this as a success, announcing that one in five videos shared on the platform are live-streamed. In May, Facebook announced a new partnership with several media brands \u2014 including Vox Media and BuzzFeed \u2014 in which it would pay up to $35,000 for episodes of 5- to 10-minute shows that the companies would own and Facebook would get a 45 percent cut of the ad revenue from. The same week, Facebook announced a deal to stream one Major League Baseball game per month. Earlier this week, Bloomberg reported that the company was offering \u201chundreds of millions\u201d to music publishers in exchange for the rights to songs in the background of user-generated content.\n\nFacebook\u2019s plan is that Watch, and all of the original content it buys for it, will make TV, like everything on its platform, a social experience. Product management leader Daniel Danker said last month, \u201cYou discover videos through your friends. You often find yourself discussing videos with friends. Video has this amazing power to bring people together and build community.\u201d All of Watch\u2019s shows have comments enabled and some are incorporated into Facebook Groups.\n\nThis news comes a month after WSJ reported that Apple would spend $1 billion on original content in the next year.", "sentiment": 0.16589646464646463},
{"link_title": "Lunirite \u2013 library for working with data in JavaScript world", "url": "https://pxyup.github.io/lanurite/", "text": "Library for Models and Collection use in JS worlds\n\nFor use in browser\n\nFor use in TypeScript\n\nFor use in NodeJS", "sentiment": 0.0},
{"link_title": "No matter what, Equifax may tell you you\u2019ve been impacted by the hack", "url": "https://techcrunch.com/2017/09/08/psa-no-matter-what-you-write-equifax-may-tell-you-youve-been-impacted-by-the-hack/?ncid=mobilenavtrend", "text": "Those hoping to find out if their Social Security number and other identifying info was stolen, along with a potential 143 million other American\u2019s data won\u2019t find answers from Equifax.\n\nIn what is an unconscionable move by the credit report company, the checker site, hosted by Equifax product TrustID, seems to be telling people at random they may have been affected by the data breach.\n\nI started noticing most people who\u2019d tested out the site in my Facebook and Twitter feeds had been given the message that they may have been part of the millions who\u2019s information was affected. It stood to reason that was likely, given the scope of the leak would affect possibly one out every two people I know in the country.\n\nHowever, I then decided to try it out for myself. First, I entered my real information\u2026and received the bad news.\n\n\u201cBased on the information provided, we believe that your personal information may have been impacted by this incident,\u201d the site said.\n\nI was then encouraged on the next line to continue my enrollment in TrustedID Premier. I was not aware I was enrolling in anything simply by giving my information. I had been instructed to add my last name and the last six digits of my Social Security number only to find out if I\u2019d been impacted.\n\nSo then I decided to test the system with a different last name and six random numbers. I used the more popular English spelling of my last name for this purpose, entering \u201cBurr\u201d instead of \u201cBuhr\u201d and entered six random numbers I don\u2019t even remember now.\n\nSure enough, this made-up person had also been impacted. I tried it over and over again and got the same message. The only time I did not get the message I\u2019d been impacted was when I entered \u201cElmo\u201d as the last name and \u201c123456\u201d as my Social Security number.\n\nSome of my colleagues also tried to fool the system and came up with different outcomes. Sometimes, after entering a made-up name, the site said they had been impacted. A few times it said they were not.\n\nOthers have tweeted they received different answers after entering the same information.\n\nThe assignment seems random. But, nevertheless, they were still asked to continue enrolling in TrustID.\n\nWhat this means is not only are none of the last names tied to your Social Security number, but there\u2019s no way to tell if you were really impacted.\n\nIt\u2019s clear Equifax\u2019s goal isn\u2019t to protect the consumer or bring them vital information. It\u2019s to get you to sign up for its revenue-generating product TrustID.\n\nEarlier it was revealed executives had sold stock in the company before going public with the leak. We also found TrustID\u2019s Terms of Service to be disturbing. The wording is such that anyone signing up for the product is barred from suing the company after.\n\nNew York attorney general Eric Schneiderman has hammered Equifax for using language meant to discourage arbitration and is asking Equifax for answers over the data breach. The company has stated since it would not bar consumers from joining breach-related lawsuits.\n\nNo doubt, those who sold company stock before publicly admitting the issues are going to face some legal trouble of their own as well.\n\nI\u2019ve since reached out to the company but so far for this story and inquiries I\u2019ve made in the last two days, I have yet to hear back.\n\nThese actions, and many others, are disgraceful, especially for a company of this size and responsibility and I truly hope Equifax feels the heat they are under for mishandling what is the largest data breach in the history of the U.S.", "sentiment": 0.011578282828282832},
{"link_title": "Introduction to Redux and Mobx", "url": "https://medium.com/@guptagaruda/introduction-to-redux-and-mobx-e6fa98b6479", "text": "In my earlier post, I compared the performance and memory profiles of a benchmark application written in AngularJS, React/Redux and React/Mobx. It\u2019s quite obvious from the metrics that React with Redux or Mobx gives significant performance gains compared to AngularJS. In this post, I will go over the core concepts, benefits and gotchas with both the libraries. All the below code snippets are from the ticker dashboard application from my earlier post.\n\nThe state of your whole application is stored in an object tree within a single store. Here is the UI state tree for the stock ticker dashboard application. The only way to change the state is to emit an action, an object describing what happened. Action creator is a function returning the action object. In the below example, each Action is represented with a \u2018type\u2019 and \u2018payload\u2019. To specify how the state tree is transformed by actions, you write pure reducers. Reducers are pure functions that modify the state in immutable way (returns new objects instead of mutating the object). Reducers shouldn\u2019t cause side effects like making external api calls, triggering route changes. If we design the action interfaces to have a common property (\u2018type\u2019 in this example), we can leverage TypeScript\u2019s discriminated unions to get type checking as well as the intellisense support in each case statement. Usage with React \u00b7 Redux\n\nTechnically you could write the container components by hand using store.subscribe() . We don't advise you to do this\u2026redux.js.org React-Redux library provides a Higher order React component which automatically listens for events. When an action is dispatched to the store, Redux will notify the state changed event to all subscribers. All \u201cReact-Redux\u201d connected components are subscribers to the Redux store and every connected component\u2019s mapStateToProps method is executed with every state change. This method helps you to slice the UI state tree and pick the data needed for the specific component tree. Pure components are normal React components. They read data from props to render and execute the callbacks sent via props. They are not aware of Redux or UI state tree. Here is how the component hierarchy would look: Here is an example of the connected component. TickerTile component renders stock ticker details (ticker, company name, price, sma, volume etc) and depends on the tickerData as a prop. The parent component just sends the tickerId (e.g. MSFT) as prop (termed as ownprops). The Higher Order connect component takes mapStateToProps mapper function which takes the tickerId and returns tickerData from UI state\u2019s tickerHash and tickerId. Here is an example of the pure component:\n\nEverything with Redux is very explicit without any magic. React\u2019s deterministic state changes and view renders are great for testability and easy to debug any issues. Redux patterns force developers to think hard on the schema of the UI state tree. Patterns will be consistent across the codebase (at the cost of verboseness). UI would simply become the representation of the state tree. Changes to the State tree can be easily traced/monitored as the actions play out. Few things to watch out for Make sure your UI state tree is normalized and try to keep it as flat as possible (instead of deeply nested structures). At times, deciding whether a property should go into the UI state or not can be confusing. Using local component state is fine as long as other parts of the application do not care about it. Connected Components (containers) \n\nmapStateToProps method of all connected components will be executed with every state changed event of the Redux store. Optimizing this method is one of the critical steps to get optimal performance in complex applications. If you need to execute expensive operations like deriving computed data from the normalized state tree, use Redux reselect library. It memoizes the result and skips recalculating unless input references change (there is a gotcha if you\u2019re trying to reuse the selectors in multiple components). If there are minimal connected components, props need to be propagated several levels down of the component hierarchy which is clearly not ideal in the large applications. Redux used to recommend connecting few components to the Redux store. Now, the recommendation is to use as many as you need. In the stock ticker dashboard application from my previous post, In the updates test scenario, with 1500 tickerTile (connected) components in the view, Redux refreshed the price/volume changes within 6ms. These two PRs (authored by Dan Abramov, creator of Redux) achieved substantial performance gains by connecting several components to Redux store and by optimizing mapStateToProps. Batch dispatch calls\n\nAnytime Redux\u2019s dispatch method is called with an action, Redux executes all reducers, updates the state and notifies all the subscribers synchronously. React-Redux will then trigger mapDispathToProps on all the connected components. If you\u2019re updating different sections of the state tree through multiple actions at the same time, try to batch them to trigger only one notification. Redux Libraries\n\nRedux is a tiny state management library (with minimal API) which acts as a building block for the higher level constructs. You need to bring in a lot of libraries to put together any real world application. It might be overwhelming at first (esp. for folks coming from AngularJS) but most of these libraries are small and come with good documentation. Functional Programming\n\nRedux uses several functional programming patterns (currying, higher order functions, composition etc.). The patterns and code might look strange for folks coming from object oriented design background. But Redux comes with a great documentation and there are a ton of great articles/videos out there to gain familiarity with the patterns.", "sentiment": 0.10073431685273791},
{"link_title": "Introducing Pytorch for fast.ai", "url": "http://www.fast.ai/2017/09/08/introducing-pytorch-for-fastai/", "text": "The next fast.ai courses will be based nearly entirely on a new framework we have developed, built on Pytorch. Pytorch is a different kind of deep learning library (dynamic, rather than static), which has been adopted by many (if not most) of the researchers that we most respect, and in a recent Kaggle competition was used by nearly all of the top 10 finishers.\n\nWe have spent around a thousand hours this year working with Pytorch to get to this point, and we are very excited about what it is allowing us to do. We will be writing a number of articles in the coming weeks talking about each aspect of this. First, we will start with a quick summary of the background to, and implications of, this decision. Perhaps the best summary, however, is this snippet from the start of our first lesson:\n\nOur goal at fast.ai is for there to be nothing to teach. We believe that the fact that we currently require high school math, one year of coding experience, and seven weeks of study to become a world-class deep learning practitioner, is not an acceptable state of affairs (even although this is less prerequisites for any other course of a similar level). Everybody should be able to use deep learning to solve their problems with no more education than it takes to use a smart phone. Therefore, each year our main research goal is to be able to teach a wider range of deep learning applications, that run faster, and are more accurate, to people with less prerequisites.\n\nWe want our students to be able to solve their most challenging and important problems, to transform their industries and organisations, which we believe is the potential of deep learning. We are not just trying to teach people how to get existing jobs in the field \u2014 but to go far beyond that.\n\nTherefore, since we first ran our deep learning course, we have been constantly curating best practices, and benchmarking and developing many techniques, trialling them against Kaggle leaderboards and academic state-of-the-art results.\n\nAs we developed our second course, Cutting-Edge Deep Learning for Coders, we started to hit the limits of the libraries we had chosen: Keras and Tensorflow. For example, perhaps the most important technique in natural language processing today is the use of attentional models. We discovered that there was no effective implementation of attentional models for Keras at the time, and the Tensorflow implementations were not documented, rapidly changing, and unnecessarily complex. We ended up writing our own in Keras, which turned out to take a long time, and be very hard to debug. We then turned our attention to implementing dynamic teacher forcing, for which we could find no implementation in either Keras or Tensorflow, but is a critical technique for accurate neural translation models. Again, we tried to write our own, but this time we just weren\u2019t able to make anything work.\n\nAt that point the first pre-release of Pytorch had just been released. The promise of Pytorch was that it was built as a dynamic, rather than static computation graph, framework (more on this in a later post). Dynamic frameworks, it was claimed, would allow us to write regular Python code, and use regular python debugging, to develop our neural network logic. The claims, it turned out, were totally accurate. We had implemented attentional models and dynamic teacher forcing from scratch in Pytorch within a few hours of first using it.\n\nThe focus of our second course is to allow students to be able to read and implement recent research papers. This is important because the range of deep learning applications studied so far has been extremely limited, in a few areas that the academic community happens to be interested in. Therefore, solving many real-world problems with deep learning requires an understanding of the underlying techniques in depth, and the ability to implement customised versions of them appropriate for your particular problem, and data. Because Pytorch allowed us, and our students, to use all of the flexibility and capability of regular python code to build and train neural networks, we were able to tackle a much wider range of problems.\n\nAn additional benefit of Pytorch is that it allowed us to give our students a much more in-depth understanding of what was going on in each algorithm that we covered. With a static computation graph library like Tensorflow, once you have declaratively expressed your computation, you send it off to the GPU where it gets handled like a black box. But with a dynamic approach, you can fully dive into every level of the computation, and see exactly what is going on. We believe that the best way to learn deep learning is through coding and experiments, so the dynamic approach is exactly what we need for our students.\n\nMuch to our surprise, we also found that many models trained quite a lot faster on pytorch than they had on Tensorflow. This was quite against the prevailing wisdom, that said that static computation graphs should allow for more optimization to be done, which should have resulted in higher performance in Tensorflow. In practice, we\u2019re seeing some models are a bit faster, some a bit slower, and things change in this respect every month. The key issues seem to be that:\n\nUnfortunately, Pytorch was a long way from being a good option for part one of the course, which is designed to be accessible to people with no machine learning background. It did not have anything like the clear simple API of Keras for training models. Every project required dozens of lines of code just to implement the basics of training a neural network. Unlike Keras, where the defaults are thoughtfully chosen to be as useful as possible, Pytorch required everything to be specified in detail. However, we also realised that Keras could be even better. We noticed that we kept on making the same mistakes in Keras, such as failing to shuffle our data when we needed to, or vice versa. Also, many recent best practices were not being incorporated into Keras, particularly in the rapidly developing field of natural language processing. We wondered if we could build something that could be even better than Keras for rapidly training world-class deep learning models.\n\nAfter a lot of research and development it turned out that the answer was yes, we could (in our biased opinion). We built models that are faster, more accurate, and more complex than those using Keras, yet were written with much less code. We\u2019ve implemented recent papers that allow much more reliable training of more accurate models, across a number of fields.\n\nThe key was to create an OO class which encapsulated all of the important data choices (such as preprocessing, augmentation, test, training, and validation sets, multiclass versus single class classification versus regression, et cetera) along with the choice of model architecture. Once we did that, we were able to largely automatically figure out the best architecture, preprocessing, and training parameters for that model, for that data. Suddenly, we were dramatically more productive, and made far less errors, because everything that could be automated, was automated. But we also provided the ability to customise every stage, so we could easily experiment with different approaches.\n\nWith the increased productivity this enabled, we were able to try far more techniques, and in the process we discovered a number of current standard practices that are actually extremely poor approaches. For example, we found that the combination of batch normalisation (which nearly all modern CNN architectures use) and model pretraining and fine-tuning (which you should use in every project if possible) can result in a 500% decrease in accuracy using standard training approaches. (We will be discussing this issue in-depth in a future post.) The results of this research are being incorporated directly into our framework.\n\nThere will be a limited release for our in person students at USF first, at the end of October, and a public release towards the end of the year. (By which time we\u2019ll need to pick a name! Suggestions welcome\u2026) (If you want to join the in-person course, there\u2019s still room in the International Fellowship program.)\n\nIf it feels like new deep learning libraries are appearing at a rapid pace nowadays, then you need to be prepared for a much faster rate of change in the coming months and years. As more people enter the field, they will bring more skills and ideas, and try more things. You should assume that whatever specific libraries and software you learn today will be obsolete in a year or two. Just think about the number of changes of libraries and technology stacks that occur all the time in the world of web programming \u2014 and yet this is a much more mature and slow-growing area than deep learning. So we strongly believe that the focus in learning needs to be on understanding the underlying techniques and how to apply them in practice, and how to quickly build expertise in new tools and techniques as they are released.\n\nBy the end of the course, you\u2019ll understand nearly all of the code that\u2019s inside the framework, because each lesson we\u2019ll be digging a level deeper to understand exactly what\u2019s going on as we build and train our models. This means that you\u2019ll have learnt the most important best practices used in modern deep learning\u2014not just how to use them, but how they really work and are implemented. If you want to use those approaches in another framework, you\u2019ll have the knowledge you need to develop it if needed.\n\nTo help students learn new frameworks as they need them, we will be spending one lesson learning to use Tensorflow, MXNet, CNTK, and Keras. We will work with our students to port our framework to these other libraries, which will make for some great class projects.\n\nWe will also spend some time looking at how to productionize deep learning models. Unless you are working at Google-scale, your best approach will probably be to create a simple REST interface on top of your Pytorch model, running inference on the CPU. If you need to scale up to very high volume, you can export your model (as long as it does not use certain kinds of customisations) to Caffe2 or CNTK. If you need computation to be done on a mobile device, you can either export as mentioned above, or use an on-device library.\n\nWe still really like Keras. It\u2019s a great library and is far better for fairly simple models than anything that came before. It\u2019s very easy to move between Keras and our new framework, at least for the subset of tasks and architectures that Keras supports. Keras supports lots of backend libraries which means you can run Keras code in many places.\n\nIt has a unique (to our knowledge) approach to defining architectures where authors of custom layers are required to create a method which tells Keras what shape output it creates for a given input. This allows users to more easily create simple architectures because they almost never have to specify the number of input channels for a layer. For architectures like Densenet which concatenate layers it can make the code quite a bit simpler.\n\nOn the other hand, it tends to make it harder to customize models, especially during training. More importantly, the static computation graph on the backend, along with Keras\u2019 need for an extra phase, means that it\u2019s hard to customize a model\u2019s behaviour once it\u2019s built.\n\nWe expect to see our framework and how we teach Pytorch develop a lot as we teach the course and get feedback and ideas from our students. In past courses students have developed a lot of interesting projects, many of which have helped other students\u2014we expect that to continue. Given the accelerating progress in deep learning, it\u2019s quite possible that by this time next year, there will be very different hardware or software options that will make todays\u2019 technology quite obsolete. Although based on the quick adoption of new technologies we\u2019ve seen from the Pytorch developers, we suspect that they might stay ahead of the curve for a while at least\u2026\n\nIn our next post, we\u2019ll be talking more about some of the standout features of Pytorch, and dynamic libraries more generally.\n\nTo discuss this post at Hacker News, click here", "sentiment": 0.20260178686561664},
{"link_title": "Compliance code exposed on GitHub causing $3 Trillion loss in Health Care", "url": "https://slashdot.org/submission/7405451/hipaa-compliance-code-exposed-causing-3-trillion-decline-in-health-care", "text": "An anonymous reader writes: According to reports, the HIPAA code is used everywhere in the $3 Trillion Health Care industry. The following link shows a guide that the HIPAA code should be based around and security rules ( HIPAA Developer Guide )(2017). Our researchers searched all over the Web however we finally found HIPAA compliance code used in the $3 Trillion Health Care Industry that will allow Malware creators to exploit the $3 Trillion industry ( HIPAA code exposed at Github)(2017). Later this year, CNBC reported Tech set to transform $3 trillion health care industry, but the HIPAA code was sitting for 7 Months, which is enough time to spread the code and mix it with malware ( $3 Trillion Health Care Industry)(2017). The media previously reported numerous health care breaches due to malware and other hacking attacks (Health Care Databreaches caused for various reasons)(2017). Firmware is everywhere; from the largest data center to the smallest networked LED light bulb. It is the most powerful code on any system because it controls how devices operate. Compromised firmware can be used to corrupt or steal data, spy on your environment or even destroy the system it is controlling. My reasoning is that the HIPAA Security Rule standard for implementing a security management process requires that Covered Entities (CEs) and Business Associates (BAs): (i) conduct a risk analysis to identify threats and vulnerabilities to electronic protected health information (ePHI); and (ii) adopt security measures to reduce or remediate risks that are identified. These steps are designed to appropriately mitigate and respond to cybersecurity incidents impacting ePHI. All Health Care providers are required to follow HIPPA\u2019s strict security guidelines to protect their medical and financial information. The above exposed HIPAA code focuses on the dangers of HTTPS interception products designed to inspect network traffic for malware. The mechanics of the products\u2019 functionality might actually make network connections less secure, and vulnerable to Man in the middle attacks. MITM attacks take many forms and are difficult to detect because there\u2019s usually no trace left to indicate that information has been exposed. MITM attacks consist of a third party intercepting the communications between two parties\u2014say, a mobile health app and a database full of PHI. A malicious individual may execute an MITM attack to eavesdrop on or manipulate those communications to cause harm or bypass other security measures on either side of the connection.\n\nThese attacks can be particularly devastating for health care entities and the $3 Trillion Health Care Industry who need to comply with HIPAA, but the code can be planted on the server in (DIAGRAM 1 below). Yes, there\u2019s the potential for PHI exposure. But in addition, such a breach is difficult to detect, and so the entity may only become aware of the breach after it\u2019s too late\u2014after pilfered data is published on the internet.\n\nDIAGRAM\u20141\n\n\n\nMobile apps often are the front-end for much deeper enterprise architecture, making the MITM potential impact even greater. A compromised connection between a mobile app and an enterprise\u2019s network infrastructure can affect not only the compromised user, but the entire IT organization that supports the app. Recently Quest Diagnostics found this out the hard way when vulnerability in their mobile app exposed over 34,000 patient records. Attackers leveraged the mobile app to expose an entire database.\n\nIn addition to large-scale data compromise, a successful MITM attack could allow an adversary to read and modify the data sent between the parties, potentially affecting every user connected to the app. For example, the data flowing to doctors, hospitals, and medical devices could be altered in transit. And worst of all, the MITM attack could occur without either party\u2019s consent or knowledge, meaning the breach might not be discovered until it is too late, but the question arises to how many servers this exposed HIPAA code is planted on ?. The companies that have the most servers are Microsoft, Google, Amazon, Facebook and each are estimated to have over 1 million servers and some may have over 2 million, which these servers and companies are connected to the $3 Trillion Health Care Industry. According to this source, as of 2014 there were an estimated 75 million server\u2019s powering the internet, with Microsoft having the most number of servers at 1 million, while Google having 900,000 servers.\n\nHow many servers exist in the world?\u2014Quora\n\n", "sentiment": 0.007456140350877194},
{"link_title": "New Security Measures in iOS 11 and Their Forensic Implications", "url": "https://blog.elcomsoft.com/2017/09/new-security-measures-in-ios-11-and-their-forensic-implications/", "text": "Apple is about to launch its next-generation iOS in just a few days. Researching developer betas, we discovered that iOS 11 implements a number of new security measures. The purpose of these measures is better protecting the privacy of Apple customers and once again increasing security of device data. While some measures (such as the new S.O.S. sequence) are widely advertised, some other security improvements went unnoticed by the public. Let us have a look at the changes and any forensic implications they have.\n\nFor the mobile forensic specialist, one of the most compelling changes in iOS 11 is the new way to establish trust relationship between the iOS device and the computer. In previous versions of the system (which includes iOS 8.x through iOS 10.x), establishing trusted relationship only required confirming the \u201cTrust this computer?\u201d prompt on the device screen. Notably, one still had to unlock the device in order to access the prompt; however, fingerprint unlock would work perfectly for this purpose. iOS 11 modifies this behaviour by requiring an additional second step after the initial \u201cTrust this computer?\u201d prompt has been confirmed. During the second step, the device will ask to enter the passcode in order to complete pairing. This in turn requires forensic experts to know the passcode; Touch ID alone can no longer be used to unlock the device and perform logical acquisition.\n\nThe trust relationship and why it is needed\n\nEstablishing a trust relationship between an iOS device and the PC is required in order to perform logical acquisition. Without pairing the device to the PC, experts will be unable to make a local backup of the device. Considering current situation with iOS 11 jailbreak, physical acquisition is not (yet) an option, so logical (and cloud) acquisition is currently the only way to go.\n\nIn order to establish a trusted relationship, users would perform the following sequence:\n\niOS 11 introduces an extra step when establishing trusted relationship. The new pairing sequence:\n\nPrior to iOS 11, it was possible to perform logical acquisition of an iOS device by unlocking the device with Touch ID. The new pairing procedure requires the use of device passcode in order to establish trust between the device and the computer, thus making logical acquisition possible only if you know the passcode.\n\nThis change is very important from the legal standpoint. While in certain cases the user may be compelled to unlock their device using their fingerprint, obtaining the passcode from the user may be challenging and, in many jurisdictions, not legally possible.\n\nIn particular, Apple protects their customers\u2019 data in cases of mass device seizures with dubious warrants like the one mentioned in this Forbes article. If the user owns a device running iOS 11, forcing a fingerprint unlock will no longer allow investigators to gain access to information other than what can be manually accessed on device screen.\n\nIn iOS 11, Apple has added an new emergency feature designed to give users an intuitive way to call emergency by simply pressing the Power button five times in rapid succession. As it turns out, this SOS mode not only allows quickly calling an emergency number, but also disables Touch ID.\n\nOnce the Power button (Sleep/Wake) is pressed five times in rapid succession, the iPhone displays a menu presenting various options including an option to cancel. Regardless of the option chosen (including the Cancel button), iOS will temporarily disable Touch ID and require the user to enter a passcode in order to unlock the device.\n\nThis feature can be used to discretely disable Touch ID in situations where the user might be compelled to unlock their phone with a fingerprint. Once Touch ID is disabled, there is no other way to unlock the device but using the passcode or making use of an existing pairing record.\n\nThere is no way for to tell that Touch ID has been disabled by using the SOS feature. Once the sequence is completed and the user cancels the menu, the iPhone prompts for a passcode in the same manner it uses after Touch ID naturally times out.\n\nEven if the iPhone has been locked using the emergency feature, it may still be unlocked for logical acquisition using a valid pairing record extracted from the user\u2019s computer. It is essential that the iPhone in question remains powered on and is not allowed to shut down or reboot before the unlock is attempted.\n\nIf the user has engaged the SOS mode, or if Touch ID has expired according to Apple\u2019s rules, a valid pairing record will still allow you to get in and to produce a backup. You will need to use the last version of Elcomsoft iOS Forensic Toolkit for that. In order to unlock the device with pairing record, launch iOS Forensic Toolkit and choose option \u201cB\u201d for \u201cBackup\u201d. You will be prompted for a file containing the lockdown (pairing) record.\n\nLockdown records are saved in the following locations:\n\nWhile not directly related to iOS 11, it is important to note that macOS 10.2 and newer implement access control to restrict access to pairing records. This can be fixed by running the following command in console: There is also another issue with pairing records, and this time it is directly related to iOS 11. This issue apples to iOS Forensic Toolkit in macOS and Windows. Even as the connected device is already paired with the current system, EIFT does not recognize the device and asks for a pairing record. A workaround is simple; just enter the path to the pairing record when requested. In Windows, do not forget using the quotes: We are going to address the second issue in EIFT 2.31 that will be released shortly. As for the lockdown folder access problem, we decided to address it in the user\u2019s manual instead of changing permissions automatically due to potential security drawbacks.\n\nOnce again, for successfully unlocking the device with a pairing record it is essential that the iPhone in question remains powered on and is not allowed to shut down or reboot before the unlock is attempted.\n\nMore information about extracting and using pairing records in this article:\n\nOne relative weakness related to pairing records is carried over from previous versions of iOS. Namely, if the user changes their passcode, all existing pairing records are not revoked. As a result, all existing pairing records remain valid and are not automatically invalidated after the user adds, removes or changes device passcode, adds or removes fingerprints. Moreover, in iOS 11, pairing records still do not have a set expiry date.\n\nYour ability to unlock devices with pairing records extracted from the user\u2019s computer(s) remains unaffected even if the user adds, removes or changes authentication methods (passcode, fingerprint). However, in order to successfully unlock the device with a pairing record it is still essential that the iPhone in question remains powered on and is not allowed to shut down or reboot before the unlock is attempted.\n\nThis is one of the few obvious weaknesses still remaining in the iOS 11 security system. If Apple decides to automatically invalidate already issued pairing records on changing authentication methods (fingerprint/passcode or even just the passcode), or simply makes cryptographic keys in pairing records passcode-dependent, a major acquisition possibility may be locked.\n\nAt this time, pairing records do not have a set expiry date. They survive through reboots (providing that the phone has been unlocked with a passcode at least once after a reboot) and changes of authentication methods. It is known that Apple has full control over lifespan of the pairing records. The company may or may not change existing behaviour in the future.\n\nIn March, 2017, we discovered a way to extract undismissed notifications from iOS backups. Notifications are pushed by pretty much every app of forensic significance. Email clients and instant messengers, Uber and taxi apps, booking and travel services, online shopping and delivery services, social networks and banking apps are just a few things to mention. Unless read or dismissed, these notifications were stored in local and cloud backups.\n\nMore importantly, these notifications were kept in the backups forever. The user only has access to notifications from the last 7 days. Older notifications automatically disappear from the device notifications shade. For some reason these old notifications were still kept on the device; they were backed up and restored using both local and cloud backups.\n\nThis is no longer the case. Notifications are no longer part of any backups, local or iCloud. With no iOS 11 jailbreak (yet), we have no way to verify whether notifications older than 7 days are still stored on the device or not.\n\nYou now have one less piece of information available via logical or cloud-based acquisition process. Access to undismissed notifications with ElcomSoft tools lasted for less than 7 months.\n\nTwo-factor authentication has been around for a while. First introduced in iOS 9 as a successor to the old and insecure Two-Step Verification, the new 2FA method has proved to offer a reasonable balance between security and convenience. In iOS 11, Apple starts pushing two-factor authentication much harder, up to the point of displaying a prominent pending notification dot over the Settings icon. By opening Settings, the user will see a pending notification reminding to enable two-factor authentication.\n\nNotably, two-factor authentication is not yet universally available. The up to date list of regions where 2FA is already available at Apple\u2019s Web site: https://support.apple.com/en-us/HT205075\n\nElcomSoft products support most Two-Factor Authentication methods including codes pushed to trusted devices as well as offline codes generated on trusted devices. We don\u2019t currently support codes delivered as text messages.\n\niCloud tokens can still be used to bypass two-factor authentication in iOS 11. By extracting an authentication token from the user\u2019s i-Device, Mac or PC (the latter must have iCloud for Windows installed), experts can sign in to the user\u2019s iCloud account without knowing the user\u2019s Apple ID or password and without having to go through the second authentication step. Do note that iCloud tokens expire. More on the expiration of iCloud tokens as well as additional details on how to extract them in our blog.\n\nOne more thing. If Two-Factor Authentication is active on the user\u2019s account, gaining access to the user\u2019s iCloud Keychain is somewhat easier as one only needs to have the user\u2019s i-Device and does not need an iCloud Security Code.\n\nFor now, this was everything we wanted to share about the upcoming features of iOS 11. There are many more low-level and invisible changes to both the operating system and iCloud. There are changes in communication protocols, data formats and encryption. We kept an eye on the situation through all developer betas, and have already implemented support for most of them. An updated version of Elcomsoft Phone Breaker is just around the corner with support for iOS 11 local and cloud backups, the ability to download media, files, synced data and keychain produced by devices running the new OS. We are also updating Elcomsoft Phone Viewer to allow exploring local and cloud iOS 11 backups. Stay tuned for further announcements!", "sentiment": 0.10784813726419566},
{"link_title": "Why you should hang onto your ideas", "url": "https://medium.com/created-with-writing-ai/why-you-should-hang-onto-your-ideas-3f1a34989f7e", "text": "People sometimes ask how I came up with the idea for Writing.AI.\n\nA decade ago, I was an exhausted grad student trying to finish a ridiculous amount of writing in far too little time.\n\nFacing a blank page, I couldn\u2019t move forward until each sentence was a perfect piece of prose, flowing beautifully into the next. My internal editor Would. Not. Shut. Up. when I was this sleep-deprived and under the gun.\n\nThen, as I rewrote the same paragraph for the 10th time, a thought occurred to me: If I were chatting with a friend via instant message, I could easily explain what I was trying to say.\n\nBeing a computer science student with some experience in natural language processing, this gave me an idea.\n\nGiven how well ELIZA systems worked at driving psychiatric conversation, I should be able to get similar results for arbitrary writing topics. Then I could take whatever I\u2019d written, and use it as a rough draft.\n\nAnd so, Writing.AI became a thing.\n\nOver the years, I kicked the idea around and made some half-hearted attempts at working on it in my spare time.\n\nThere were domain names (draaft.com, convolis.com, scrawl.io), two different prototypes (one that worked), and ten pages of handwritten notes I recently unearthed.\n\nHonestly, I never really expected that this is what I\u2019d turn into a company some day. It was something that I\u2019d mostly wanted for myself, and the first few people I mentioned it to weren\u2019t particularly excited.\n\nBesides, it didn\u2019t really seem like a product anyway. In 2007, chatbots were novelties, not something you paid money for.\n\nSo back on the shelf it went, along with the dozens of other next-big-things I\u2019ve cooked up.\n\nLast August, my wife and I went on a long vacation that we both badly needed. We were both working long hours, exhausted, and hadn\u2019t taken nearly enough time off. So we boarded our dog, hopped on a plane, and flew to Africa.\n\nFor three amazing weeks, cut off from everything, I finally had a chance to step back and think about my priorities without the gnawing sense that I should be focusing on my job.\n\nI had been working like crazy for over two years, putting everything I had into my professional work. And that\u2019s when it hit me \u2014 I hadn\u2019t worked on my own stuff at all for the past two years.\n\nThis was crazy, I\u2019d never gone through a period like this before. Creating new things in my spare time was what I did. But somewhere along the line, working head down, month after month, I\u2019d just sort of lost track of a key piece of my identity.", "sentiment": 0.034295735129068465},
{"link_title": "Flying into the eye of Hurricane Irma", "url": "http://www.reuters.com/article/us-storm-irma-eye-exclusive/exclusive-flying-into-the-eye-of-hurricane-irma-with-u-s-hurricane-hunters-idUSKCN1BK0KD", "text": "THE EYE OF HURRICANE IRMA (Reuters) - The sky darkened, lightning flashed and a jolt of turbulence shook the cabin of the hulking Air Force turbo-prop aircraft as it plied its way toward the eye of Hurricane Irma, one of the strongest Atlantic storms ever recorded.\n\nPiloting the four-engine, WC-130J aircraft was Air Force Reserve Lieutenant Colonel Jim Hitterman, who over the past 22 years has flown into 40 to 50 hurricanes.\n\nEvery storm is different but he likens the experience to driving through a car wash - with one big difference.\n\n\u201cAs you\u2019re driving through that car wash, a bunch of gorillas start jumping on top of your car,\u201d Hitterman said, adding that sometimes shaking gets so bad, he cannot see his instruments.\n\nOn Friday and Saturday, Reuters accompanied the Air Force Reserves\u2019 \u201cHurricane Hunters,\u201d whose hard-won data taken directly from the center of storms like Hurricane Irma are critical to U.S. forecasts that save lives.\n\nExperts say U.S. satellite data simply cannot do the job.\n\n\u201cWe can estimate by satellite what the strength and size of a hurricane is. But only if you go into the hurricane can you really get an accurate measure of its exact center location, the structure, the maximum winds,\u201d said Rick Knabb, a hurricane expert at the Weather Channel and a former director the National Hurricane Center.\n\nThe 53rd Weather Reconnaissance Squadron\u2019s \u201cHurricane Hunters\u201d are based at Keesler Air Force Base in Biloxi, Mississippi. Its members trace the origin of hurricane hunting to a 1943 barroom dare by two then-Army Air Corps pilots to fly through a hurricane off Texas.\n\nToday, the missions are carried out largely by Air Force reservists who, after a few days or weeks of chasing storms, return to their jobs in the civilian world.\n\nHitterman, 49, flies for Delta Airlines most of the time and, as a hobby, races motorcycles.\n\nThe flight meteorologist, Major Nicole Mitchell, is an experienced television news meteorologist and mother of an eight-month-old baby boy. She normally lives in Minnesota.\n\nThe way Mitchell sees it, the more accurate her data is, the more accurate the forecasts can be that tell U.S. citizens whether to evacuate their homes as Irma or other storms advance.\n\n\u201cIt\u2019s a fact that we make a difference,\u201d she said.\n\nMitchell\u2019s plane would make four passes in total through Irma\u2019s eye during that mission, some entries and exits more turbulent than others. Its final pass came on Saturday, as Hurricane Irma walloped Cuba\u2019s northern coast.\n\nIrma\u2019s interaction with Cuba\u2019s terrain weakened the storm from a Category 5 to a Category 4 hurricane but U.S. National Hurricane Center warned the storm was anticipated to strengthen again.\n\nIrma was expected to hit Florida on Sunday morning, bringing massive damage from wind and flooding to America\u2019s fourth-largest state by population. Millions of Florida residents have been ordered to evacuate.\n\nDespite the severity of storms like Irma and the undeniable danger on the ground, these U.S. flights into hurricanes have an incredible safety record - not one aircraft has been lost in more than four decades. The last time was in 1974.\n\nBut they are not without risk. Some six hurricane or typhoon hunting aircraft have been lost in total, costing 53 lives, according to the Weather Underground website.\n\nJeff Masters, director of meteorology of The Weather Underground, recalled an extremely close call during a flight into Hurricane Hugo in 1989 organized by the National Oceanic and Atmospheric Administration (NOAA), which also fields its own turbo-prop aircraft.\n\nThe pilot lost control of the aircraft, one of the engines caught on fire, and the aircraft descended rapidly, all because satellite data had given his crew the sense they were flying into a Category 3 storm. It turned out to be a Category 5.\n\nThey were flying much too low for a storm that potent.\n\n\u201cWe went in at 1,500 feet, which is a no-no in a Category 5 and we got clobbered,\u201d recounted Masters. The pilot was able to recover control after entering Hugo\u2019s eye.\n\nOn the mission into Irma, jolts of turbulence also shook the equipment in the cabin as it neared the eye of the hurricane. Emergency parachutes swayed.\n\nBut then, suddenly, everything in the plane settled down.\n\nIt was safe enough to take off seat belts. The flying was smooth.\n\nInside the eye, the sky opened up. The dark \u201ceyewall\u201d - the surrounding ring of clouds - could be seen outside the cockpit window.\n\nMasters says someday drones might be able to do the risky job now done by experienced air crews.\n\nBut, from the cockpit of this Hurricane Hunter flight, that possibility still seems distant.\n\nThis aircraft, like all of the 53rd\u2019s 10 WC-130J planes, are specially equipped to gather meteorological data and send it to the U.S. National Hurricane Center. Some of that equipment is operated manually.\n\nThat includes releasing sensors through the belly of the aircraft that, as they fall, transmit storm data including Irma\u2019s pressure, wind speed and direction.\n\nAs the mission got underway, the sensors - known as dropsondes - appeared to be malfunctioning.\n\nTechnical Sergeant Karen Moore, the loadmaster who releases the dropsondes from the aircraft, among many other duties, said she could not get its GPS signal as it fell into Irma\u2019s winds.\n\nSo, Moore took out a screwdriver and literally started fixing them on the fly, one by one. That is something a drone would not be able to do.\n\nHitterman said he also could see a future where pilotless planes fly into hurricanes to get the data Americans need.\n\n\u201cBut I think it\u2019s a ways off,\u201d he said.", "sentiment": 0.17389804639804646},
{"link_title": "The sleuth tracking down the poetry cheats", "url": "https://www.theguardian.com/books/2017/sep/09/poetry-plagiarism-copying-maya-angelou-ira-lightman-will-storr", "text": "The poet Ira Lightman stared at his laptop screen in disbelief. Could it be true? He was sitting on the sofa in his terrace house in Rowlands Gill, five miles south-west of Newcastle, a narrow man with a curly mess of dark red hair. He\u2019d just made a routine visit to the Facebook group Plagiarism Alerts. There, a woman named Kathy Figueroa had posted something extraordinary: \u201cIt appears that one of Canada\u2019s former poet laureates has plagiarised a poem by Maya Angelou.\u201d\n\nLightman clicked the link. It led to a Canadian government webpage where a poem had been chosen to honour the memory of Pierre DesRuisseaux, Canada\u2019s fourth parliamentary poet laureate, who died in early 2016. The poem, it said, had been translated from DesRuisseaux\u2019s French original. Lightman read the opening lines: \u201cYou can wipe me from the pages of history/with your twisted falsehoods/you can drag me through the mud/but like the wind, I rise.\u201d The poem was called I Rise. Next, Lightman looked up the Maya Angelou. \u201cYou may write me down in history/With your bitter, twisted lies/You may trod me in the very dirt/But still, like dust, I\u2019ll rise.\u201d The poem was called Still I Rise.\n\nIncredible. But could it really be plagiarism? How likely was it that a poet laureate would steal anything at all, let alone a keystone work by a modern legend? How could he think he\u2019d get away with that? Then again, DesRuisseaux was a French speaker, writing for a French-speaking audience. Would his readers necessarily recognise a well-known English-language poem of the 20th century? After all, this had been spotted only because it had been translated into English.\n\nLightman found the website of DesRuisseaux\u2019s publisher and downloaded a free sample of Tranches De Vie, the book the poem had come from. He scanned the PDF for telling lines that he roughly translated into English, then popped into Google, in quotation marks, alongside the word \u201cpoem\u201d. It didn\u2019t take long. There was In The Beginning by Dylan Thomas. There was Prayer Before Birth by Louis MacNeice. There was Cut While Shaving by Charles Bukowski. Unaccredited. Stolen.\n\nIn May last year, Lightman contacted the publisher, noting his concerns and requesting a full book be sent for further investigation. The response, from \u00c9ditions du Noiro\u00eet, one of Canada\u2019s most prestigious publishing houses, was swift. His accusation was \u201cincredible\u201d. Lightman replied, \u201cFor me, it\u2019s not so incredible. I have studied numerous plagiarists.\u201d And this was true, for Ira Lightman was no ordinary poet \u2013 he was also the poetry sleuth. And it looked as if this might be his biggest catch yet.\n\nLightman is feared, reviled and lauded in the poetry world. For some, he\u2019s a tireless vigilante, bravely aiming his chin at his enemies. For those enemies, he\u2019s a bully and a witch-finder with an unnatural obsession. For others still, he\u2019s in error; some of his targets aren\u2019t plagiarists at all, they argue, just sloppy note-keepers. Moreover, Lightman makes no allowances for the practice of \u201cintertextuality\u201d: when you take someone else\u2019s poem and use its structure, mood or language as a foundation for something new. You might use intertextuality to comment on the original poem, for example, or alter it in such a way that it moves into your own lived experience. Perhaps a late-life experiment could explain the Canadian laureate\u2019s apparent thieving. It might just turn out to be DesRuisseaux\u2019s last opportunity to have his reputation restored in the pages of history and to rise (rise, rise) like the dust/wind.\n\nIt was an insult on Facebook that triggered Lightman\u2019s first investigation. It was around 8am on a January morning in 2013 when he came across a tense discussion concerning a poet named Christian Ward, who\u2019d had the Exmoor Society\u2019s Hope Bourne prize removed because of his winning entry\u2019s similarities to another poem, Deer by Helen Mort. \u201cThere was bucketloads of speculation,\u201d Lightman says. We\u2019re in his lounge, the litter of his creative life scattered about us: a ukulele, an enormous dictionary, posters with verse and his name printed at the bottom. He sits, suited and crane-like, on the sofa. \u201cEveryone was arguing about it: maybe it was an accident, maybe the judges weren\u2019t poetry people and don\u2019t understand intertextuality.\u201d Lightman was erring on the side of cock-up. Only the previous day, he\u2019d come across a poem of his own that he had no memory of writing. Perhaps Ward had found Deer in his files, assumed it was his, given it a polish and submitted it. \u201cI could just about accept that,\u201d he says. \u201cYou can be very prolific and amnesiac.\u201d He remembers joining the debate as a \u201cpeacemaker\u201d. But then a commenter called Sadie Fisher said something that annoyed him.\n\nWhile the Mort poem was available online, nobody on the Facebook group had actually seen Ward\u2019s, she pointed out. \u201cSadie Fisher was saying, \u2018You guys are all hacks. A proper journalist would look into it and say, \u2018Is this a spoof story? Have they got the facts wrong?\u2019\u201d\n\nLightman felt piqued. \u201cI\u2019ve always been interested in journalism,\u201d he says. \u201cMy grandfather was a subeditor for an Edinburgh paper. So I thought, I\u2019m going to find out.\u201d He phoned Bridgwater library and asked if they had the Exmoor Society quarterly that published the winners. \u201cThey said no, but Porlock might, and they\u2019re opening in 10 minutes.\u201d An anxious 10 minutes passed. \u201cI rang Porlock and they said, \u2018We\u2019ve got it.\u2019 I said, \u2018OK\u2019, heart beating.\u201d As the librarian fetched the journal, Lightman Googled the Mort poem. When she came back to the phone, he asked her to read it out, ready to scribble it down so he could compare them. That turned out to be unnecessary. \u201cIt was totally identical, except for about 5%.\u201d\n\nHow did that feel?\n\n\u201cIt was amazing. Just, oh, wow.\u201d\n\nLightman typed up the poems and posted them both on Facebook, telling everyone they could stop speculating. But a couple of days later, a friend sent him another suspicious Ward poem. \u201cThis felt like a different ballgame,\u201d he says. For the first time, the poetry sleuth felt overcome by the distant whiff of blood. \u201cI really wanted to get to the bottom of it. And then I was just really thorough. I looked at absolutely everything.\u201d\n\nHe developed a technique. He\u2019d try to spot breaks in the natural pattern of Ward\u2019s poems \u2013 jarring lines that felt, in some way, different. If Lightman has a secret superpower, it\u2019s that, beneath his own instinct for poetry, he has a mathematician\u2019s pattern-sensitive brain. \u201cIt surprises me, because people say, \u2018I looked through this person\u2019s work and I didn\u2019t see anything\u2019, then I find something in two minutes. It\u2019s because I\u2019m not reading it for affect, I\u2019m reading it for patterns.\u201d He went through all the Ward poems he could find. \u201cI looked through 300 or 400,\u201d he said. \u201cI found about 15 that were dodgy.\u201d It was this that taught him his golden rule: \u201cPlagiarists never do it once.\u201d\n\nLightman posted the poems on Facebook. For Ward, this was a catastrophe. His cheating became national news, and was reported in the New York Times (headline: Nice Poem; I\u2019ll Take It). He gave a statement to the Western Morning News, apologising to Mort and \u201cthe poetry community\u201d and admitting he had been \u201ccareless\u201d. Ward had already, he said, found another suspect poem of his own: \u201cI have discovered a 2009 poem called The Neighbour is very similar to Tim Dooley\u2019s After Neruda... I am still digging.\u201d\n\nWard did not respond to my requests for an interview, but someone who appears to have been him left a lengthy comment beneath the Guardian\u2019s news story in 2013. \u201cChristianWard99\u201d said it was \u201cone of the most uncomfortable and distressing experiences of my life\u201d, admitting, \u201cI have made several stupid mistakes during my time as a poet and there is simply no excuse for plagiarism.\u201d But he also pushed back: \u201cI have been bullied, victimised and abused by a number of \u2018poets\u2019 who thought it necessary to act like a lynch mob.\u201d\n\nOf all the plagiarists he ended up netting, Lightman says he retains most respect for Ward. \u201cHe\u2019d had a poem published in the Poetry Review and it was perfectly legitimate, written in his own voice, own style. I think he was on the verge of making something. He just messed up.\u201d He also admires the way Ward dealt with it: \u201cHe never tried to shrug it off.\u201d\n\nWhat makes a poetry sleuth? In the case of Lightman, it seems to be an unusual combination of anger, vulnerability and an intoxicating desire to feel powerful. Born to middle-class parents in 1967, Lightman was an unusual and sometimes difficult child. When, at the age of three, he was told the family were moving from Buckinghamshire to Kent, he pulled down his trousers and refused to pull them up again until they changed their minds. (\u201cIt didn\u2019t work.\u201d) At school, he wore his hair like Rowan Atkinson\u2019s character in Blackadder I. \u201cThe defining thing for me, as a child, was, I was not very good at making friends. I was very good at getting bullied, but I was really good at having some power.\u201d He won a public speaking competition and kept on winning it, year after year, revelling in the glory of witnessing everyone sing a new verse in the school hymn, which he\u2019d composed. \u201cI was a walking liability.\u201d\n\nDespite his talent for maths, Lightman pursued the arts, studying English language and literature at University College London. He\u2019d written bits of poetry as a teenager, but embraced the form seriously as a student, and quickly began to get Larkin-like work published in titles such as the London Magazine and the New Statesman. After university, he spent time in New Zealand, returning to the UK in 1991. In 2000, he moved to County Durham, got married and had two children. Lightman\u2019s marriage formally ended last year, but had been failing for some time. What buoyed him through the breakup, he says, was the community he found online. He was liked there. And when he became known for his poetry sleuthing, he was also powerful. \u201cI needed Facebook desperately. It was a total godsend.\u201d\n\nIn retrospect, he thinks this dependency might have interfered with his judgment during his investigation of Christian Ward. \u201cMy procedure was far too engaged with my own excitement about Facebook and getting notifications,\u201d he says. \u201cI was posting every single finding: here\u2019s number seven, here\u2019s number eight.\u201d\n\nDid his investigations also give him status? \u201cI think I was angry,\u201d he says. \u201cNot at the plagiarists. I felt like I was drowning. And you\u2019re right, there\u2019s an element that makes you feel good.\u201d\n\nWhat was he angry about?\n\nIn the spring of 2015, a friend tipped Lightman off about a potential new case. This one would become ugly and difficult in ways that none of the others had been, not least because this much-admired poet lived in Tynemouth, just a few miles down the road.\n\nThe first person to smell something suspicious about the popular north-east poet and tutor Dr Sheree Mack was another local poet, Ellen Phethean. She\u2019d been to the launch of Mack\u2019s book Laventille, and had noticed work that was uncomfortably similar to her own. She contacted the publisher, Andy Croft at Smokestack Books, who contacted Mack. She told him she\u2019d made a mistake: she had used a number of poems as scaffolding upon which to build her own work and, due to poor record-keeping, had failed to make the appropriate attribution. In her checking, she\u2019d also discovered the initials \u201cJJ\u201d next to her poem A Different Shade Of Red. That, she now realised, was originally based upon A Particular Blue by another local poet, Joan Johnston.\n\n\u201cAndy Croft emailed me a copy of Sheree\u2019s poem and said, \u2018What do you think?\u2019\u201d Johnston tells me when we meet, at Lightman\u2019s house. \u201cI told him, \u2018It\u2019s my poem.\u2019 Then I went to walk the dogs, very quickly, very furiously, around a couple of fields. When I got back, I emailed him again and said, \u2018I\u2019m absolutely furious about this.\u2019\u201d She sent Croft a copy of A Particular Blue, so he could see for himself. \u201cHe returned both poems to me, having highlighted the changes Sheree had made, saying, \u2018I don\u2019t think it\u2019s plagiarism. I think she\u2019s taken your poem and made something new out of it.\u2019 At which point, I went for another walk.\u201d\n\nWere the similarities down to Mack\u2019s sloppy note-keeping? Was it intertextuality? Or a bit of both? If anyone was going to find out, it was the poetry sleuth. Lightman decided to approach this case as he had all the others, contacting the accused via email, offering support and inviting confession, while commencing an investigation. Lightman found 12 poems in Laventille he thought extremely similar to other work. Checking Mack\u2019s previous book, Family Album, he found another \u201csix or seven\u201d problematic poems. More seriously, he thought he\u2019d read Mack saying that Family Album comprised the creative element of her PhD. He contacted the University of Newcastle, which had supervised it, explaining the potential issues. When there was no response, he decided to find a copy of the doctoral thesis himself, only to discover it had mysteriously disappeared from the university\u2019s catalogue. \u201cThey pulled it off for a year, so I couldn\u2019t look at it,\u201d he says.\n\nWhen it returned, \u201cI expected to see something that had been retroactively altered, but the poems from Family Album were still there, uncredited.\u201d The body of the thesis raised further issues. \u201cThe PhD is beyond the pale,\u201d Lightman says. \u201cThere were around 100 things I found problematic.\u201d (A university spokesperson told the Guardian, \u201cThe thesis was taken from the university library to be read following the allegation and was subsequently returned. A formal investigation is still ongoing.\u201d)\n\nMeanwhile, news of Mack\u2019s difficulties spread. A debate raged in blogs and specialist poetry publications and, of course, on Facebook. \u201cI was really surprised at the vitriol directed at Ira,\u201d Johnston tells me. \u201cAnd also the vitriol towards any of us who were saying, \u2018This is wrong\u2019, as though we were the problem.\u201d\n\nLightman felt there was a level of hypocrisy in all this, with friends and associates making allowances for Mack that they wouldn\u2019t make for strangers. \u201cAll the buggers who\u2019d called Christian Ward a scumbag and said, \u2018There\u2019s no excuse for this\u2019 were saying, \u2018I\u2019m sure Sheree has a reason.\u2019 It was galling.\u201d\n\nPerhaps inevitably, one of Lightman\u2019s fiercest critics was Mack\u2019s publisher, Andy Croft. On 7 May 2015, he emailed Lightman: \u201cAlthough I do not know you or your work, it has been explained to me that you are currently trying to make a career as a witch-finder general in the world of poetry. But this feels less like a witch-hunt than a lynch mob. As I am sure you are aware, your accusations have caused Sheree considerable distress.\u201d Croft explained that he planned to pulp the remaining copies of Laventille and reprint a corrected version. \u201cI do not believe that your accusations of plagiarism regarding Laventille are justified. But I am not prepared to have this beautiful and important book dirtied by the grubby little fingers of Pooterish readers.\u201d Croft then posted his email as an open letter on Facebook.\n\nMack herself posted a semi-apology on Facebook, admitting to \u201cslackness and carelessness\u201d, while insisting: \u201cNever, never, have I set out to deceive, mislead, or appropriate the work of others.\u201d She refused media requests (and declined to speak to me for this story).\n\nHowever, in 2016, Mack published a memoir, Rubedo, that recounted what she described as \u201ca public lynching of me the writer, poet and person\u201d. She put the problems down to a \u201clack of necessary diligence\u201d in keeping track of her sources, and her practice of intertextuality. \u201cWhere I have carried this out, I have created a whole new piece of writing that feeds from my own experiences, interests and heritage.\u201d She denied any wilful sin. Although she didn\u2019t mention Lightman by name, she didn\u2019t have to. The moment \u201ca certain poet\u201d posted his allegations on Facebook, she wrote, \u201che sealed my fate\u2026 He was two-faced and backstabbing.\u201d When she discovered her employment as a lecturer at the Open University had been terminated, she writes, she considered jumping off a bridge.\n\nThis made me wonder if Lightman had ever considered the question of balance. Mack\u2019s poems weren\u2019t all questionable; everyone agrees she was an inspiring teacher; her book Laventille had sold only 114 copies. His investigation left her suffering something approaching an annihilation of the self. Did he wrestle with that?\n\n\u201cYes,\u201d Lightman says, \u201cbut you can\u2019t undo what you\u2019ve done.\u201d\n\nHe doesn\u2019t think he went too far?\n\nI wonder what he and Johnston ultimately wanted. What would leave them satisfied?\n\n\u201cI don\u2019t want a public flogging or anything,\u201d Johnston says, \u201cbut if quietly her doctorate was taken away, that would be fair.\u201d\n\nA few weeks later, I meet Andy Croft at an anarchist book fair at Goldsmiths, University of London. In black jeans and a white T-shirt, his spectacles hooked over the neck and his grey hair brushed back, he is behind a stall, selling copies of Smokestack\u2019s books. Laventille is not among them.\n\nI ask what he thinks motivates Lightman. \u201cI honestly don\u2019t know,\u201d he says. \u201cMy only contact with his career is with someone who lacks proportion and lacks humility and lacks generosity.\u201d He characterises the fuss as nothing more than \u201ca series of low-level petty jealousies\u201d. Mack, who is of Trinidadian/Ghanaian/Bajan ancestry, \u201cis one of the tallest, most striking women you\u2019ve ever come across\u201d, Croft says. \u201cA lot of the original animosity was from white women poets in Newcastle. I don\u2019t even want to know how to unpick that. To begin with, it felt like some girls in a catfight, picking on the most glamorous and the most beautiful girl, because they\u2019re not as glamorous and beautiful.\u201d I put this accusation to Johnston, but she declined to comment.\n\nMack was, Croft insists, mostly guilty of sloppiness. She\u2019d been practising intertextuality and had forgotten to add the appropriate attributions. Mack, of course, claimed she always \u201ccreated a whole new piece of writing\u201d, but a comparison between her A Different Shade Of Red and Johnston\u2019s A Particular Blue, for example, strains that argument. Johnston\u2019s begins: \u201cThis afternoon the weather broke/and changing light/brought back morning.\u201d Mack\u2019s: \u201cThis evening the weather broke/and threatening light/brought into the long night.\u201d\n\nHow close would two poems have to be for it to be plagiarism, I ask Croft.\n\n\u201cUm,\u201d he says, \u201cit would only arise if I noticed. I\u2019m very widely read, but the chances are I\u2019d miss it.\u201d\n\nWould it have to be a facsimile?\n\n\u201cI suppose if someone typed up a poem as it was originally and just put their name at the bottom, I\u2019d say, why are you doing that?\u201d\n\nAnd would it be plagiarism?\n\n\u201cIt would be\u2026\u201d He thinks for a moment. \u201cPointless. It\u2019s like me saying my name is Will Storr. No, it\u2019s not! You\u2019d say that was stupid. You wouldn\u2019t say I was plagiarising you.\u201d\n\nThe second time I visit Ira Lightman, I press him on his claim to have wrestled with the question of moral balance between Mack\u2019s crime and punishment. Since his last communication with the University of Newcastle, he has halted his attempt at getting her doctorate removed, partly because he is worried about damage to his own reputation. But he says he might begin again if he finds she\u2019s using her doctorate to gain employment. He tells me he has read Rubedo, with its account of Mack\u2019s lowest moments. Did it change anything?\n\n\u201cI can completely imagine that was an awful time for her,\u201d he says. \u201cBut I don\u2019t think I\u2019d behave in a different way.\u201d\n\nMeanwhile, what of the Canadian mystery? Could former laureate DesRuisseaux really have blatantly plagiarised all those canonical poets? It seemed too mad to be true. When Lightman got hold of DesRuisseaux\u2019s book Tranches De Vie, he found even more apparent thefts. Two days of sleuthing found 30 out of 47 poems that were heavily based on the work of others. There were two more by Angelou, an Anna Akhmatova, a Federico Garc\u00eda Lorca, a Ted Kooser. There was even a Tupac Shakur. When Lightman told me he\u2019d failed to find any problems in other DesRuisseaux books he\u2019d got hold of, I recalled his \u201cgolden rule\u201d, that plagiarists never do it only once. It seemed to me that Tranches De Vie must have been an attempt to honour the greats by producing intertextual reinterpretations of their finest moments.\n\nUntil, that is, Lightman shows me the original source of DesRuisseaux\u2019s Curieux. \u201cIt\u2019s based on a poem by Nicole Renwick,\u201d Lightman tells me. \u201cI\u2019d never heard of her, but that does happen.\u201d\n\nHe taps her name into his search engine. We\u2019re sitting next to each other and I lean over, squinting at the screen. Some examples of Renwick\u2019s work appear on a site called allpoetry.com. There is the original poem, Funny\u2026 But Not. DesRuisseaux had cut it down from 13 lines to nine, and added his own closer. And then there\u2019s Nicole Renwick herself. She looks barely out of her teens. Her bio reads: \u201cHey everyone, I\u2019m hoping to become a writer one day, so I\u2019d appreciate every comment I get thanks.\u201d\n\nLightman scrolls down. The poem that follows the one DesRuisseaux had taken is called My Xbox. I read its opening stanza: \u201cXbox, Xbox/You\u2019re the one for me/I also love my 3DS/And my Nintendo Wii.\u201d\n\nI stare at the screen in mute astonishment.\n\n\u201cWhat was he doing?\u201d He shakes his head. \u201cWhat was he doing?\u201d\n\nLater, I contact Professor Thierry Bissonnette of Laurentian University\u2019s department of French studies in Ontario, who had not only read DesRuisseaux widely but knew him late in life. When he tells me he enjoyed Tranches De Vie \u2013 \u201cThat\u2019s a good one\u201d \u2013 I share Lightman\u2019s accusations.\n\n\u201cAre you familiar with intertextuality?\u201d I ask\n\n\u201cIs there a chance he was doing this in Tranches De Vie?\u201d\n\n\u201cNo,\u201d he says. \u201cNot at all.\u201d\n\nLightman completed his investigation into Tranches De Vie in May 2016, but speaking to me is the first time he has gone public. He emailed his findings to \u00c9ditions du Noiro\u00eet, who appeared to accept his verdict; in emails Lightman showed me, DesRuisseaux\u2019s editor wrote that it was his first experience of plagiarism, and expressed regret at having to tell the poet\u2019s family that he would have to remove the title from circulation.\n\nTranches De Vie is no longer on sale. Lightman advised the editor to make a public statement, but at the time of writing, nearly 18 months later, none has been made. I email the poetry sleuth to ask if this surprises him. His reply comes as one word: \u201cNope.\u201d\n\n\u2022 Commenting on this piece? If you would like your comment to be considered for inclusion on Weekend magazine\u2019s letters page in print, please email weekend@theguardian.com, including your name and address (not for publication).", "sentiment": 0.0940127853627854},
{"link_title": "Another Approach to Proof of Work: PoW2 [pdf]", "url": "https://github.com/Gulden/gulden-official/raw/master/technical_documentation/Gulden_PoW2.pdf", "text": "", "sentiment": 0.0},
{"link_title": "If Mark Zuckerberg runs for president, will Facebook help him win?", "url": "https://www.theguardian.com/commentisfree/2017/sep/09/mark-zuckerberg-president-facebook-algorithm", "text": "Despite his protestations to the contrary, Facebook founder Mark Zuckerberg has been acting like someone planning to run for office. He hired a pollster, visited a Detroit auto plant and other swing-state locations, and gave a high-profile commencement speech.\n\nMeanwhile, Facebook has been under intense criticism for its role as a vector of misinformation in recent elections. This week, Facebook admitted that Russian accounts purchased $100,000 in political ads in 2015 and 2016. This disclosure comes only two months after the platform refused to disclose who is paying for advertising on the platform and where they\u2019re running.\n\nThis confluence of events demonstrates the urgent need for greater transparency about how Facebook is already being used for electoral influence, particularly its algorithms and advertising features. Facebook must be regulated like the broadcast medium that it has become. And if Zuckerberg wants to run for office, he should be leading the charge for meaningful transparency.\n\nThe Facebook platform already has the ability to shift the outcome election if it so chooses. Setting aside the issue of so-called \u2018fake news\u2019 and its spread on the platform during 2016, Facebook itself is a powerful tool of voter mobilization and information.\n\nMy own research demonstrates that seeing one\u2019s Facebook friends praise others for voting increases turnout; other work finds that exposure to voting-related posts on Facebook increases turnout.\n\nMeanwhile, Facebook\u2019s internal research shows that exposing users to voting reminders that include pictures or names of friends also makes them more likely to vote. Algorithmic shifts that prioritize showing these kinds of messages to certain groups of voters and suppressing them from others could theoretically be used to help a particular candidate.\n\nCurrently, however, candidates can\u2019t manipulate the algorithm \u2013 they only have access to Facebook\u2019s public organizing and advertising tools. They can create fan pages, pay to promote posts, use Facebook ads to recruit fans, recruit members for their email lists, and deploy apps that supporters can use to engage their friends.\n\nAnd while shifts in Facebook\u2019s algorithms can hurt the organic reach of posts by candidate pages, theoretically these changes are deployed without the intent to harm or benefit specific campaigns.\n\nThe algorithm makes Facebook different from other media \u2013 we don\u2019t see a customized TV feed or hear different radio songs based on what we\u2019ve liked before. This algorithm has long been a black box for people outside of the company. Now, it\u2019s time for the platform to explain how it influences what political content gets spread on the network.\n\nAdvertising is another area where Facebook needs to radically increase transparency. While candidates must disclose their spending on required campaign finance filings, they can avoid reporting their exact Facebook spending amounts by hiring a firm to run the ads and paying for them as part of their overall consulting fee.\n\nOther entities don\u2019t have to disclose their Facebook spending at all, such as business entities, social welfare organizations, or so-called \u201cdark money\u201d groups. Furthermore, there\u2019s nothing to stop the administrator of a Facebook page from paying to promote misleading or false content into millions of newsfeeds \u2013 and it\u2019s not clear that they would have to disclose it to anyone.\n\nThat\u2019s why Facebook needs to pro-actively disclose the political advertisements made on its platform: the amount, spender, content, and targeting. There is precedent for this kind of disclosure: TV and radio stations have been providing so-called public inspection files.\n\nThis reform would also result in the disclosure of spending by any autocratic states or regimes in weak democracies that want to promote propaganda on Facebook. Facebook should also block the use of their paid features to promote links to websites and pages that knowingly publish political misinformation.\n\nThat would limit the spread of fake news on the platform going forward. And while Facebook claims to have blocked ads from running on fake news sites, they have not yet taken the step to block ads on Facebook by fake news sites \u2013 or haven\u2019t disclosed it, if they have. \n\n\n\nDisclosure of the algorithm and ad spending are big steps \u2013 and would generate an equally large increase in transparency regarding Facebook\u2019s role and use in spreading political content. While these disclosures would not necessarily stop outside interference or bad actors, at the very least voters would have the chance to know how the platform was being used to influence them.\n\nFacebook\u2019s report on \u201cinformation operations\u201d in the 2016 election was a good first step in identifying organized efforts to use Facebook to influence political opinion. However, their analysis is primarily focused on fake accounts and coordinated efforts to boost the organic spread of misinformation. Facebook\u2019s paid features and algorithm are arguably much more powerful. \n\n\n\nWith two billion global users, Facebook has been adopted faster than any technology or service in human history, and due to its widespread use, people deserve more information regarding its influence in elections. We need to know who is paying to put political content in our newsfeeds, and how the Facebook platform itself determines who sees what.\n\nIt\u2019s in the best interests of democratic society to do so. And if this appeal to virtue isn\u2019t convincing enough, Zuckerberg should do so out of self-interest. All of these concerns will reach a fever pitch if Facebook\u2019s CEO runs for office, given the unique ability of the platform to mobilize voters for the boss. Rather than wait for the next crisis, Facebook needs to increase its transparency now.", "sentiment": 0.05083646616541354},
{"link_title": "Airbnb vows to be first company to defy Trump and keep employing Dreamers", "url": "https://www.theguardian.com/us-news/2017/sep/07/silicon-valley-executives-dreamers-daca-trump", "text": "Airbnb has become the first major company to pledge to keep employing undocumented immigrants known as \u201cDreamers\u201d after their work permits expire, defying the Trump administration in what would potentially be a breach of employment law.\n\nThe plan, revealed in a statement to the Guardian, distinguishes the company from others in Silicon Valley, where the chief executives of corporations like Facebook and Microsoft have denounced Trump\u2019s decision to rescind a program protecting undocumented immigrants from deportation, but declined to offer specifics of how they will resist.\n\nThe Guardian asked 19 major tech corporations if they would lay off their employees who are Dreamers \u2013 immigrants brought to the US as children without documentation \u2013 if they lose their work permits because of Trump\u2019s elimination of their legal status.\n\nAirbnb spokesman Nick Papas replied in an email: \u201cNo. We are 100% committed to protecting Dreamers.\u201d\n\nDespite other CEOs\u2019 public promises to support employees targeted by the president, all the other tech companies refused to say if they would allow Dreamers to continue working for them, or did not respond to inquiries.\n\nIt\u2019s unclear how American companies could employ immigrants if their work authorization documents became invalid. Papas did not respond to follow-up questions and declined to comment on how many Dreamers Airbnb currently employs.\n\nImmigrants\u2019 rights advocates in Silicon Valley argued that the tech executives \u2013 who have earned widespread media coverage and praise from liberals for opposing the White House \u2013 should take a more meaningful stand and pledge to continue employing Dreamers regardless of Trump\u2019s repeal of their rights.\n\nThe president faced international backlash across the political spectrum when he announced his decision to phase out the Obama-era program known as Deferred Action for Childhood Arrivals (Daca) by March 2018. Daca, which protected immigrants known as Dreamers, granted temporary status to roughly 800,000 people, allowing them the right to live, study and work legally in the US.\n\nIf the Republican-controlled Congress does not, in the next six months, pass legislation to protect Dreamers, they could face deportation to the countries where they were born, even though for many, America is the only home they\u2019ve ever known. Dreamers will also lose their right to work in the US under Trump\u2019s rules.\n\nIn response, leaders in the tech industry \u2013 which often supports conservative policies despite its liberal reputation \u2013 issued perhaps their most vehement rebuke of Trump since his inauguration. Apple\u2019s chief executive, Tim Cook, tweeted the company would \u201cfight for [Dreamers] to be treated as equals\u201d and revealed that more than 250 employees are protected by Daca.\n\n\n\nMicrosoft\u2019s president said if the government tries to deport an employee, \u201cit\u2019s going to have to go through us to get that person.\u201d Facebook CEO Mark Zuckerberg doubled down on criticisms of Trump\u2019s decision with a live broadcast interview featuring Dreamers.\n\nBut all of the major tech firms refused to say if they would defy Trump and continue to employ Dreamers in the long-term.\n\nDavid Leopold, an immigration attorney and former president of the American Immigration Lawyers Association, noted that employers are obligated by law to check the immigration status of workers and cannot knowingly employ people who lack authorization, putting the companies in a difficult position.\n\nA Facebook spokesperson said the company was not \u201cspeculating about the future\u201d. A Microsoft representative pointed to its blogpost saying it would pay for lawyers if the US seeks to deport their employees, but said, \u201cWe are unable to provide any further information.\u201d\n\nDespite signing a letter urging Trump to preserve Daca, Google also declined to respond to questions about the fate of its own Dreamers. Companies that either ignored queries or declined to answer questions about its Dreamer workforce include Apple, Twitter, Uber, Lyft, Salesforce, Netflix, eBay, Box, Intel, Oracle, Cisco, LinkedIn, HP and IBM.\n\nAmazon also did not respond to specific questions, but a spokesperson pointed to a declaration the company submitted in support of a new lawsuit challenging Trump, which said at least nine employees are Daca recipients and that the company \u201cwill suffer injury\u201d if they lose their status and are deported.\n\nMadhuri Nemali, a Silicon Valley lawyer who represents immigrants in tech, said the big firms should promise to continue employing Dreamers.\n\n\u201cThat would be a powerful pledge,\u201d she said. \u201cIt\u2019s wonderful all the companies have been coming out with statements \u2026 but it\u2019s a lot easier to say you\u2019re going to stand by them when there are no consequences.\u201d\n\nThe impact would be more significant if the firms said they would allow Dreamers to stay after Daca expires, she said, sending the message that \u201cit\u2019s not just your lives on the line. We\u2019re standing by you and also willing to go against the law to take a moral stance.\u201d\n\nIf the firms were to employ Dreamers without permits, Leopold said, \u201cit would be an incredible statement. It would be unprecedented. The government would have to make a huge decision \u2013 are they going to go after major companies or are they going to do the right thing and reinstate Daca?\u201d\n\nLeopold said it was difficult for him to advocate that companies break the law, noting that they could theoretically face sanctions or even criminal charges.\n\nSarahi Espinoza Salamanca, a 27-year-old Dreamer based in Silicon Valley, who created an app that helps undocumented students with college applications, said she wanted to see tech firms make more specific promises to workers and hoped they would hire Dreamers as independent contractors if they lose authorization.\n\nAlthough companies aren\u2019t required to check the work status of contractors, they could still be liable for hiring people they know lack permits.\n\n\u201cI hope the tech industry steps up to the plate,\u201d said Salamanca, adding, \u201cA lot of the tech companies were founded by immigrants. They should definitely understand the situation we\u2019re going through.\u201d", "sentiment": 0.1336399711399712},
{"link_title": "HTTP-monitoring: Complete solution for prob monitoring HTTP and reporting", "url": "https://caripson.github.io/HTTP-monitoring/", "text": "Complete solution for prob monitoring HTTP services with several prob agents with a central reporting portal. The project includes realtime monitoring and SLA reporting with Kibana.\n\nTime Connect The time, in seconds, it took from the start until the TCP connect to the remote host (or proxy) was completed.\n\n\n\nTime Namelookup The time, in seconds, it took from the start until the name resolving was completed.\n\n\n\nTime Pretransfer The time, in seconds, it took from the start until the file transfer was just about to begin. This includes all pre-transfer commands and negotiations that are specific to the particular protocol(s) involved.\n\n\n\nTime starttransfer The time, in seconds, it took from the start until the first byte was just about to be transferred. This includes Time pretransfer and also the time the server needed to calculate the result.\n\n\n\nTime redirect The time, in seconds, it took for all redirection steps including name lookup, connect, pretransfer and transfer before the final transaction was started. time_redirect shows the complete execution time for multiple redirections.\n\n\n\nTime Total The total time, in seconds, that the full operation lasted.\n\n\n\nUpload the file to http://transfer.sh so that you can easly install it later on monitor probes\n\nYou may need to install the apt-transport-https package on Debian before proceeding:\n\nRun sudo apt-get update and the repository is ready for use. You can install it with:\n\nUse apt to install Nginx and Apache2-utils\n\nFirst, download the sample dashboards archive to your home directory:\n\nInstall the unzip package with this command:\n\nNext, extract the contents of the archive:\n\nAnd load the sample dashboards, visualizations and Beats index patterns into Elasticsearch with these commands:\n\nFirst, download the Filebeat index template to your home directory:\n\nThen load the template with this command:\n\nIf the template loaded properly, you should see a message like this:\n\nIt also uses the same GPG key as Elasticsearch, which can be installed with this command:\n\nNow restart Filebeat to put our changes into place:\n\nUnless stated otherwise all works are licensed under: MIT License\n\nBecome a contributor to this project!\n\nNo sponsors yet! Will you be the first?\n\n Become a contributor to this project!\n\n\n\n", "sentiment": 0.10778985507246376},
{"link_title": "Apache Struts Statement on Equifax Security Breach", "url": "https://blogs.apache.org/foundation/entry/apache-struts-statement-on-equifax", "text": "The Apache Struts Project Management Committee (PMC) would like to comment on the Equifax security breach, its relation to the Apache Struts Web Framework and associated media coverage.\n\nWe are sorry to hear news that Equifax suffered from a security breach and information disclosure incident that was potentially carried out by exploiting a vulnerability in the Apache Struts Web Framework. At this point in time it is not clear which Struts vulnerability would have been utilized, if any. In an online article published on Quartz.com [ 1 ], the assumption was made that the breach could be related to CVE-2017-9805, which was publicly announced on 2017-09-04 [ 2 ] along with new Struts Framework software releases to patch this and other vulnerabilities [ 3 ][ 4 ]. However, the security breach was already detected in July [ 5 ], which means that the attackers either used an earlier announced vulnerability on an unpatched Equifax server or exploited a vulnerability not known at this point in time --a so-called Zero-Day-Exploit. If the breach was caused by exploiting CVE-2017-9805, it would have been a Zero-Day-Exploit by that time. The article also states that the CVE-2017-9805 vulnerability exists for nine years now.\n\nWe as the Apache Struts PMC want to make clear that the development team puts enormous efforts in securing and hardening the software we produce, and fixing problems whenever they come to our attention. In alignment with the Apache security policies, once we get notified of a possible security issue, we privately work with the reporting entity to reproduce and fix the problem and roll out a new release hardened against the found vulnerability. We then publicly announce the problem description and how to fix it. Even if exploit code is known to us, we try to hold back this information for several weeks to give Struts Framework users as much time as possible to patch their software products before exploits will pop up in the wild. However, since vulnerability detection and exploitation has become a professional business, it is and always will be likely that attacks will occur even before we fully disclose the attack vectors, by reverse engineering the code that fixes the vulnerability in question or by scanning for yet unknown vulnerabilities.\n\nRegarding the assertion that especially CVE-2017-9805 is a nine year old security flaw, one has to understand that there is a huge difference between detecting a flaw after nine years and knowing about a flaw for several years. If the latter was the case, the team would have had a hard time to provide a good answer why they did not fix this earlier. But this was actually not the case here --we were notified just recently on how a certain piece of code can be misused, and we fixed this ASAP. What we saw here is common software engineering business --people write code for achieving a desired function, but may not be aware of undesired side-effects. Once this awareness is reached, we as well as hopefully all other library and framework maintainers put high efforts into removing the side-effects as soon as possible. It's probably fair to say that we met this goal pretty well in case of CVE-2017-9805.\n\nOur general advice to businesses and individuals utilizing Apache Struts as well as any other open or closed source supporting library in their software products and services is as follows: 1. Understand which supporting frameworks and libraries are used in your software products and in which versions. Keep track of security announcements affecting this products and versions.\n\n2. Establish a process to quickly roll out a security fix release of your software product once supporting frameworks or libraries needs to be updated for security reasons. Best is to think in terms of hours or a few days, not weeks or months. Most breaches we become aware of are caused by failure to update software components that are known to be vulnerable for months or even years.\n\n4. Establish security layers. It is good software engineering practice to have individually secured layers behind a public-facing presentation layer such as the Apache Struts framework. A breach into the presentation layer should never empower access to significant or even all back-end information resources.\n\nOnce followed, these recommendations help to prevent breaches such as unfortunately experienced by Equifax.\n\nPosted at 03:30PM Sep 09, 2017 by Sally in General | |", "sentiment": 0.08447639661925374},
{"link_title": "Telemarketing of Movie Leads to $32M Punishment in Class Action", "url": "http://www.hollywoodreporter.com/thr-esq/telemarketing-movie-leads-32-million-punishment-class-action-1036961", "text": "More than a quarter century after Congress passed the Telephone Consumer Protection Act with the intention of cracking down on unsolicited telephone solicitations, one movie's promotional campaign has proven quite costly. On Thursday, a Missouri federal judge awarded $32.4 million over calls made for Last Ounce of Courage.\n\nProducers of Last Ounce of Courage, featuring a man's stand for religious freedom in the face of government attack, hired AIC Communications to telemarket the film. In turn, former Arkansas Governor Mike Huckabee became the movie's pitchman in a pre-recorded robocall. Huckabee delivered an anti-Hollywood message to get audiences to see it.\n\n\"Do you agree that traditional American values are under attack in mainstream media and by our government?\" asked Huckabee in the call that went out in 2012. \"Would you, like me, Mike Huckabee, like to see Hollywood respect and promote traditional American values?\n\n\"I am an enthusiastic supporter of a new movie called Last Ounce of Courage,\" Huckabee continued. \"It is a film about faith, freedom and taking a stand for American values...\"\n\nThat spurred a class action lawsuit from recipients. In 2015, the 8th Circuit Court of Appeals gave the case a green light, although some of the co-defendants like Huckabee were later dismissed from the case.\n\nIn August, a jury trial was held, and after more than a week of testimony, U.S. District Court Judge E. Richard Webber granted plaintiffs' motion for judgment as a matter of law.\n\nThe real question centered on damages. The defendants in the case argued that statutory damages were unconstitutionally excessive.\n\nIn his decision, Webber rules that the TCPA's statutory damages clause is constitutional, but nevertheless disproportionate to the offense and unreasonable.\n\n\"There were 3,242,493 calls placed in violation of the TCPA in this case,\" he writes in the order (read here). \"At $500 per violation, the TCPA would require a damages award of $1,621,246,500. This is obviously unreasonable and wholly disproportionate to the offense. The Court will award $32,424,930. This amounts to $10.00 per call. This reflects the severity of the offense, a six-day telemarketing campaign which placed 3.2 million telephone calls, as well as respecting the purposes of the TCPA to have a deterrent effect and to account for unquantifiable losses including the invasions of privacy, unwanted interruptions and disruptions at home, and the wasted time spent answering unwanted solicitation calls or unwanted voice messages.\"\n\nAn attorney for the plaintiffs told the St. Louis Post-Dispatch he was \"pleased\" with the result, but would nevertheless appeal on the basis that the judge didn't have discretion to reduct the full amount.\n\nFor those who think that major Hollywood studios would never get caught up in such a fight, guess again. For instance, Universal Pictures and Legendary Pictures are currently involved in a pending case in Florida alleging unsolicited text messages were sent in violation of the TCPA to promote the 2016 film, Warcraft.", "sentiment": 0.11107445421961554},
{"link_title": "Getting started with Rust", "url": "https://dev.to/mnivoliez/getting-started-with-rust-presentation-and-installation", "text": "Hello, today we are going to talk about Rust.\n\n \"You are speaking about it quite often, aren't you?\"\n\nIndeed, more reason to introduce it properly, right?\n\n \"OK, we are listening.\"\n\nFirst thing first. Rust is a compiled programming language. Which means that the code you write will be translated to another one that the computer actually understand. This translation will be called executable or library (or, more generically, the artifact). Rust is also a strongly typed language. In other words, variable are of a certain type. We'll get back to it another time. Finally, on the official site, Rust is described as:\n\nIn case you have never practiced any \"low-level\" language such as C or C++, those terms are certainly new for you. I'll try to rapidly define them here, but we will speak more in detail in some other posts.\n\nSegfault, or segmentation fault, is when a program tries to access memory on which it has no right. Just like at your work, you got a desk to work on but you try to use the one of your colleague, and of the same as your colleague doesn't like that, the computer doesn't either. And then the program crash.\n\nAnd threads... We also call them \"lightweight process\". They can work in parallel of others. You can think of it as a pile of dishes to wash. You can use more people to wash all the dishes. The risk is in the case where two or more persons try to wash the same plate at the same time.\n\nRust prevent those issues and you'll see how in the course of these articles.\n\n \"OK, but it is still gloomy....\"\n\nNothing more normal, those notions are quite difficult to apprehend, and it will come in time. So don't worry, be happy.\n\n \"Fine, do we start?\"\n\nOf course! And the first thing we are going to do is to install Rust. To do so, we are going to use the tool Rustup. You will need a terminal from this point on.\n\nIf you are using Linux or Mac, this command should be enough:\n\nIf you are using Windows, go to the site of Rustup to download the utility and follow its instructions.\n\nNow, time to use Cargo!\n\n \"Cargo?\"\n\nIt's a tool to manage Rust projects. Go into your favourite directory (not the big one little filth, the one for your projects):\n\nThe flag tells to Cargo to make an executable. Without this option, the command will create the configuration for a library.\n\nInside the folder newly created, you can see a directory named which will hold up our sources, and a file named which contains project information:\n\nNow, take a look to the file inside :\n\n\"How are we supposed to read that?\"\n\nI'll help you, do not worry. First, we got a function called . When called, it will execute the macro with \"Hello, world!\" as parameters, which will print \"Hello, world!\" in the console.\n\nTo compile and execute the program, run cargo run.\n\nFunction and macro are big topics, so we will be talking about them later.\n\nIn the end, it doesn't .... sorry, in the end we have installed Rust and create a hello world project using Cargo.\n\nSee you later in another post on how to getting started with Rust.", "sentiment": 0.1534290449134199},
{"link_title": "The Ambition Collision", "url": "https://www.thecut.com/2017/09/what-happens-to-ambition-in-your-30s.html", "text": "What is this midlife crisis among the 30-year-olds I know? Millennial women \u2014 at least those who reside in professional bubbles \u2014 seem to have it all. They are better educated, more prosperous, less encumbered by cultural expectations than any previous generation of women. They delay marriage (if they marry at all) and children (if they choose to conceive). They can own or rent. They can save or spend. These women have been on familiar terms with their ambitions all their lives \u2014 raised by careful parents to aim high (millennial women are likelier than their male peers to have professional jobs, to be managers, and to work in finance), and tutored by their cultural icons to perform their empowerment, and never submit. You know, \u201cBow down, bitches,\u201d as they say.\n\nSo why are the well-employed, ambitious 30-year-olds of my acquaintance feeling so adrift, as discontented as the balding midlife sad sacks whose clich\u00e9 dissatisfactions made Updike rich? The women complain of the enervating psychic effects of the professional treadmill as white-collar piecework and describe their dread as they contemplate bleak futures \u2014 decade after decade, they imagine, unfulfilled. After a lifetime of saying \u2018yes\u2019 to their professional hunger \u2014 these are the opportunity-seizers, the list-makers, the ascendant females, weaned on Lean In \u2014 they\u2019ve lost it, like a child losing grasp of a helium balloon. Grief-stricken, they are baffled too, for they have always been propelled by their drive. They were the ones who were supposed to run stuff \u2014 who as girls imagined themselves leaving the airport in stylish trench coats, hailing a taxi with one hand while holding their cell in the other.\n\nNow, \u201cthere\u2019s no vision,\u201d one woman said to me. \u201cNothing solid,\u201d said another. Limp, desperate, they fantasize about quitting their good jobs and moving home to Michigan. They murmur about purpose, about the concrete satisfactions of baking a loaf of bread or watching a garden grow. One young woman I know dreams about leaving her consulting job, which takes her to Dubai and Prague, to move back home and raise a bunch of kids. Another, an accountant with corner-office aspirations, has decided to \u201cphone it in\u201d for a few years while she figures out what she wants to do. Mostly, though, these women don\u2019t bail out. They are too responsible, and too devoted to their wavering dreams. They stay put, diligently working, ordering Seamless and waiting for something \u2014 anything \u2014 to reignite them, to convince them that their wanting hasn\u2019t abandoned them for good. Any goal would do, one woman told me: a child, a dog \u2014 \u201ceven a refrigerator.\u201d People have been motivated by less.\n\nGet a grip, I want to tell them, for I am old enough to be, if not their mother then their world-weary aunt. Who ever said that work should be the be-all? You work for money. The money you earn pays the rent. You are the very, very lucky few, in possession of the jobs and apartments that every tier-one college student wants. But the more I listen, the more I think I hear in these young women\u2019s voices the echo of something familiar \u2014 the complaints of a long-ago generation but in reverse. The female dissatisfaction chronicled by Betty Friedan in The Feminine Mystique was prompted by a widespread awakening to the bullshit promises of domestic happiness, manufactured by culture to make female containment look good. Now another bullshit promise has taken its place, and another generation is waking up. The men in charge are still in charge. It is impossible for women to continue to have faith in a vision of their own empowerment, when that empowerment is, in fact, a pose. It is not true that a gleaming kitchen floor is the key to female satisfaction. And \u201cBow down, bitches\u201d is a lie.\n\nThe myth of female empowerment has always been on a collision course with the reality, but until relatively recently working females took this understanding for granted. We knew that we were tokens. We laughed as we made vagina jokes when the bosses weren\u2019t listening, for we could count: one or two of us in the top tier, compared to ten or twelve of them. Our music was rough and cynical and filled with longing. But millennial women made the mistake of dutifully believing what they were taught. They presumed their power: everything they read or watched, everyone suggested to them that the path ahead was clear. They got more degrees, they entered law in greater numbers, they knew they could support themselves and had no gendered expectations around eventual family.\n\nWhat does it mean to grow up listening to \u201cRoar\u201d when female achievement has flatlined? The wage gap is about the same as it has been for a decade, which is to say since these women were in college, buckled down tending to their GPAs and loading up their resumes with extracurriculars. There is still no occupation in which a woman who works full time earns a lot more than a man, and few in which women have parity. Women have less savings than men, and are less likely to qualify for a mortgage. The cost of living, for everyone, has risen in urban areas. These are the parameters of the psychic vise, for growing numbers of women are the main or sole breadwinners for their families. When a woman delays children and partnership into her 30s to earn money and establish independence and then sees how her paths are blocked, it is perhaps no wonder that something like anguish is the result. (I was furious at Rory Gilmore for getting accidentally pregnant in the end \u2014 all that wasted potential! \u2014 but maybe Rory saw better than I did the corner she was in.)\n\nThe professional stagnation my millennial friends report as an existential cloud is real. According to a new study by the Population Reference Bureau, fewer millennial women are working in STEM jobs than women in Generation X, unemployment among millennial women is higher than in Generation X, and overall well being (measured by rising suicide rates, poverty, maternal death) is on the decline. \u201cThere\u2019s this sense of not being able to make progress,\u201d says Beth Jarosz, an author of the PRB study. She started the research last summer, she said, when there was a female candidate running for president, and her colleagues were hopeful that they would find significant measures of millennial progress. Instead, \u201cthe more we dug, the more we were like, \u2018Oh, God, this is awful. There\u2019s really, really stalled momentum from Gen X to millennial.\u2019\u201d\n\nThe awakening occurs slowly, it seems.\n\nWomen enter workplaces filled with ambition and optimism and then, by 30 or so, become wise to the ways in which they are stuck. According to a 2015 study of female millennials by PriceWaterhouseCoopers, women\u2019s awakening to workplace sexism is a slow, inexorable evolution. As they age, their dissatisfactions increase so that by the time they\u2019re 30 or 34, two-thirds say their employer doesn\u2019t do enough to promote and encourage diverse hiring; more than a third say they don\u2019t have female role models; and just 39 percent say they believe they can rise to the top of the organization they\u2019re in (down from 49 percent of younger millennial women). At all ages, millennial women say they feel that men get the plum international assignments \u2014 even though they also believe that the plum international assignments are crucial to advancing their careers.\n\nIn general, young millennial men feel more bought into work than young women \u2014 more supported and more contented at their jobs, according to data provided by the Families and Work Institute, even though the young women are likelier to report that they put their jobs first, over family. It\u2019s as if the women have cleared spaces in their lives for meteoric careers, and then those careers have been less gratifying, or harder won, or more shrunken than they\u2019d imagined. And what\u2019s there to fill the space, except more Insta images of female gratification \u2014 vacations! cocktails! \u2014 that inadequately reflect the lives they lead?\n\nA dose of perspective is, perhaps, required here. The lesson of The Feminine Mystique was not that every woman should quit the burbs and go to work, but that no woman should be expected to find all her happiness in one place \u2014 in kitchen appliances, for example. And the lesson for my discontented friends is not that they should ditch their professional responsibilities but that they should stop looking to work, as their mothers looked to husbands, as the answer to the big questions they have about their lives. \u201cI think possibly work has replaced \u2018and they got married and lived happily ever after,\u2019 and that is a false promise,\u201d says Ellen Galinsky, co-founder of the Families and Work Institute. \u201cEveryone needs to have more than one thing in their life. We find people who are dual-centric to be most satisfied. If people put an equivalent stress on their life outside of their job they get further ahead and are more satisfied at their job.\u201d\n\nTo be clear: This is not about settling, about making peace with the humdrum sexism of traditional workplaces. Rage and revolution are called for, and such upheaval requires more professional investment by more females, not less. Instead, this is about a shift in perspective \u2014 an appreciation for imperfect circumstances and unmet yearnings as facts of life, and a willingness to seek gratifications and inspirations outside the boundaries of a job. Dogs are helpful in this regard. So are children and friends and sports and museums and live music and sex and activism and charity. The other day, I saw a 6-year-old girl wearing a T-shirt that said \u201cUndefeatable.\u201d She was skipping down the street and holding her father\u2019s hand. And I thought, That\u2019s the problem right there. Surely, that girl is as defeatable \u2014 or as undefeatable \u2014 as anyone. But that doesn\u2019t mean she shouldn\u2019t grow up to fight.", "sentiment": 0.12832686310105662},
{"link_title": "Game writers to be honored with Nebula Award", "url": "https://www.geekwire.com/2017/game-writers-honored-nebula-award-first-professional-science-fiction-fantasy-org/", "text": "Ever since the success of Valve\u2019s Half-Life nearly 20 years ago, video games of all types have been increasingly expected to have a detailed narrative. Now, writers of those games will be eligible to receive one of science fiction\u2019s highest honors \u2014 the Nebula Award.\n\nThat\u2019s one of the changes reflecting the evolution of science fiction\u2019s long-standing professional organization, the Science Fiction and Fantasy Writers of America (SFWA), as fantastic storytelling has become more mainstream. Founded in 1965 to protect the interests of professional writers, SFWA has nearly 2,000 members today and \u2014 despite its name \u2014 is global in scope. Readers know it mostly for the Nebulas, which vie for recognition with the fan-voted Hugo Awards.\n\nSFWA President Cat Rambo says the organization began admitting game writers as members last year, and announced a Best Game Writing award category for 2018 to cover works published this year.\n\n\u201cI would think that one of the things a Nebula imprimatur would mean for a game is that it is a game that really has some story to it,\u201d Rambo said. \u201cThat it\u2019s a game that can achieve that sort of immersive wonderful experience that only text can bring.\u201d\n\nRambo, a Seattle writer who is in her second term as SFWA president, sat down with GeekWire for this episode of our new podcast series on science fiction, pop culture, and the arts. Rambo has written more than 200 short stories and been nominated for the Nebula and World Fantasy Awards. Her stories are most recently collected in Neither Here Nor There (Hydra House) and Altered America: Steampunk Stories (Plunkett Press).\n\n[Listen to the podcast below, or download the MP3 here.]\n\nThe game writer award wasn\u2019t an easy addition, Rambo said, recalling that simply admitting game writers to the organization took three tries in three different decades. Her hope is that the new Nebula for game writers will both help game buyers recognize quality and writers raise their profile and exposure.\n\nThe new award joins honors for long-standing Nebula categories of novel, novella, novelette, and short story, plus awards for dramatic presentation and young adult book. And yes, in an era when a crowd-sourced recommendation is only a click away, Rambo thinks a Nebula still has cachet because, \u201cit\u2019s a peer-given award, as opposed to sort of everybody voting \u2026 this is a very special and significant one.\u201d\n\nAs to the Hugos, which are chosen by fans at the annual World Science Fiction Convention, Rambo sees them as complementary, not competitive (despite the fact the Hugos have announced that they, too, will add a young adult honor). Still, even science fiction awards are not immune from controversy.\n\nThis year, the Hugo categories were largely swept by women when the winners were announced in August. That\u2019s a dramatic turnaround from a 2015 dispute in which \u201cNo Award\u201d was a common winner after two conservative/libertarian-leaning \u201cpuppies\u201d groups (\u201csad\u201d and \u201crabid\u201d) tried to take over the ballot with bloc nominations, objecting to what some pups called \u201csocial justice warriors\u201d who wrote what they thought was boring stuff.\n\n\u201cIt\u2019s been very interesting because of course it\u2019s been sort of the larger culture wars getting played out in that particular arena,\u201d Rambo said. \u201cAnd it has been I will say extraordinarily ugly at times and to a point where I find myself very grateful that SFWA is not the body that administers the Hugo Awards.\u201d However, she said the process has been self-correcting, and thinks the latest Hugo winners \u201care much more representative \u2026 of the best and brightest.\u201d\n\nThese larger societal and political tensions, especially those in the past year, do take their toll on writers. \u201cI know that a number of the folks that I have spoken to have expressed a great deal of discouragement and a great deal of difficulty in writing near-future science fiction,\u201d Rambo said. \u201cWe are all somewhat despairing \u2026 and it\u2019s kind of hard to know where we\u2019re going to go next.\u201d\n\nYet Rambo said that\u2019s why it\u2019s important for writers to keep at it, \u201cto keep pushing for truth and civility because that\u2019s one of the things stories do.\u201d\n\nThen, there are the cats.\n\nCats have a long association with science fiction and fantasy. And naturally, no discussion with a writer named Cat Rambo can avoid them. Not only have cats figured prominently in Rambo\u2019s own work (for example, in the short story \u201cTortoiseshell Cats Are Not Refundable\u201c), but there have been fictional feline fixtures for decades (one of Rambo\u2019s favorites, in the classic 1958 Fritz Lieber story \u201cSpace-Time for Springers\u201c).\n\nWhy the affinity? \u201cI think because writers are solitary creatures and often drawn to cats,\u201d Rambo explained. \u201cAnd when you are sitting at home and the only living creature you are interacting with is a cat, every once in a while the cat is going to creep into your work.\u201d\n\nOverall, the move of science fiction and fantasy into the mainstream is good for writers, and good for SFWA, Rambo said. \u201cI think it affects us in a very important way in that we are eager to hit those new young readers those new young fans and say here\u2019s some good stuff, here\u2019s the stuff that will hook you, here\u2019s the stuff that you will love all your life, and that will lead you down new and interesting paths of reading.\u201d", "sentiment": 0.11393413619477451},
{"link_title": "Flatten an array of arrays with TypeScript", "url": "https://gist.github.com/maroun-baydoun/ec07608580113d132434698430e61db3", "text": "What would you like to do?", "sentiment": 0.0},
{"link_title": "More specialized matrix data structures in Clojure on CPU and GPU", "url": "http://dragan.rocks/articles/17/Neanderthal-015-Many-more-specialized-matrix-data-structures-in-Clojure", "text": "The new release of Neanderthal, the fast Clojure one stop shop for linear algebra and matrix computations at top speed on Intel & AMD CPU's, and both Nvidia and AMD GPU's has just been released to Clojars.\n\nIn addition to further internal refinements of existing features, there's now many specialized matrix types to choose from to speed up computations even more by exploiting the additional knowledge of matrices at hand:\n\nDense matrices now offer the choice of:\n\nWhen we know that all non-zero data in our matrix are concentrated close to the diagonal, we might want to choose banded matrices, that come in three flavors, too:\n\nBut that's not all. We can also opt for packed storage:\n\nWhen appropriate, Neanderthal can also do efficient polymorphic conversions between those.\n\nWhy all this variety? Because, in lots of cases, we know that the matrix we work with have some properties. By choosing the appropriate representation, we can get help from Neanderthal, who can now use that information to automatically select the best algorithm for the operation that we want to invoke! And expect more! There is going to be support for huge sparse matrices, and for tensors. All at high speed, and with a nice Clojure API that does almost all automatically for us!\n\nTo start discovering how linear algebra can help you in your Clojure programming, read my Clojure Linear Algebra Refresher series. In addition, expect more detailed tutorials about how to effectively use Neanderthal for certain numerical tasks. There is also an extensive test suite with lots of examples: check out Neanderthal GitHub repository. Last, but not least, each public function comes with the documentation, so don't forget to check that out.", "sentiment": 0.33198475437605873},
{"link_title": "Restoring a Soviet-era analog synthesizer", "url": "http://imgur.com/a/8plOW", "text": "TAKE ME UP", "sentiment": 0.0},
{"link_title": "Hurricane Irma", "url": "https://blog.cloudflare.com/irma/", "text": "Yesterday, we described how Hurricane Irma impacted several Caribbean islands, with the damage including a significant disruption to Internet access.\n\nAs Irma is now forecast to hit southern Florida as category 5 this weekend with gusty winds reaching up to 155mph, it is also expected that Internet infrastructure in the region will suffer.\n\nAt the time of writing, we haven\u2019t noticed any decrease in traffic in the region of Miami despite calls to evacuate.\n\nContrary to popular belief, Internet wasn't built for the purpose of resisting a nuclear attack. That doesn't mean that datacenters aren't built to resist catastrophic events.\n\nThe Miami datacenter housing servers for Cloudflare and other Internet operators is classified as Tier IV. What does this tiering mean? As defined by the ANSI (American National Standards Institute), a Tier IV datacenter is the stringent classification in term of redundancy of the critical components of a datacenter: power and cooling. It guarantees 99.995% uptime per year, that is only 26 minutes of unavailability. Tier IV datacenters provide this level of uptime by being connected to separate power grids, allowing their customers to connect their devices to both of these grids. They also provide fuel-powered backup generators, which can themselves be made redundant, for up to 96 hours of autonomy.\n\nData center facilities have already taken precautionary measures during these last days, one of them contacting their customers with the following (excerpt):\n\nDue to their importance in Internet infrastructure, Tier IV datacenters also have the most available connectivity to the Internet. This is the case for our Miami data center, which is connected to multiple Tier 1 transit providers and Internet Exchanges and will provide backup routes in case of an outage with a particular infrastructure.\n\nAs a last resort, in the event our Miami datacenter would be taken offline, our Anycast routing will smoothly reroute packets to our nearest data centers in the United States: Tampa, Atlanta and Ashburn (Washington DC).\n\nOur technical teams will take all the necessary steps to ensure our services stay online during these unfortunate events. We\u2019d like to remind our users to follow all precautions, and evacuate the regions as advised by the local authorities.", "sentiment": 0.020987654320987655},
{"link_title": "Equifax's impact checker site is reporting false outcomes", "url": "https://techcrunch.com/2017/09/08/psa-no-matter-what-you-write-equifax-may-tell-you-youve-been-impacted-by-the-hack/", "text": "Those hoping to find out if their Social Security number and other identifying info was stolen, along with a potential 143 million other American\u2019s data won\u2019t find answers from Equifax.\n\nIn what is an unconscionable move by the credit report company, the checker site, hosted by Equifax product TrustID, seems to be telling people at random they may have been affected by the data breach.\n\nI started noticing most people who\u2019d tested out the site in my Facebook and Twitter feeds had been given the message that they may have been part of the millions who\u2019s information was affected. It stood to reason that was likely, given the scope of the leak would affect possibly one out every two people I know in the country.\n\nHowever, I then decided to try it out for myself. First, I entered my real information\u2026and received the bad news.\n\n\u201cBased on the information provided, we believe that your personal information may have been impacted by this incident,\u201d the site said.\n\nI was then encouraged on the next line to continue my enrollment in TrustedID Premier. I was not aware I was enrolling in anything simply by giving my information. I had been instructed to add my last name and the last six digits of my Social Security number only to find out if I\u2019d been impacted.\n\nSo then I decided to test the system with a different last name and six random numbers. I used the more popular English spelling of my last name for this purpose, entering \u201cBurr\u201d instead of \u201cBuhr\u201d and entered six random numbers I don\u2019t even remember now.\n\nSure enough, this made-up person had also been impacted. I tried it over and over again and got the same message. The only time I did not get the message I\u2019d been impacted was when I entered \u201cElmo\u201d as the last name and \u201c123456\u201d as my Social Security number.\n\nSome of my colleagues also tried to fool the system and came up with different outcomes. Sometimes, after entering a made-up name, the site said they had been impacted. A few times it said they were not.\n\nOthers have tweeted they received different answers after entering the same information.\n\nThe assignment seems random. But, nevertheless, they were still asked to continue enrolling in TrustID.\n\nWhat this means is not only are none of the last names tied to your Social Security number, but there\u2019s no way to tell if you were really impacted.\n\nIt\u2019s clear Equifax\u2019s goal isn\u2019t to protect the consumer or bring them vital information. It\u2019s to get you to sign up for its revenue-generating product TrustID.\n\nEarlier it was revealed executives had sold stock in the company before going public with the leak. We also found TrustID\u2019s Terms of Service to be disturbing. The wording is such that anyone signing up for the product is barred from suing the company after.\n\nNew York attorney general Eric Schneiderman has hammered Equifax for using language meant to discourage arbitration and is asking Equifax for answers over the data breach. The company has stated since it would not bar consumers from joining breach-related lawsuits.\n\nNo doubt, those who sold company stock before publicly admitting the issues are going to face some legal trouble of their own as well.\n\nI\u2019ve since reached out to the company but so far for this story and inquiries I\u2019ve made in the last two days, I have yet to hear back.\n\nThese actions, and many others, are disgraceful, especially for a company of this size and responsibility and I truly hope Equifax feels the heat they are under for mishandling what is the largest data breach in the history of the U.S.", "sentiment": 0.011578282828282832},
{"link_title": "California could be hit by 8.2 mega-earthquake and damage would be catastrophic", "url": "http://www.latimes.com/local/lanow/la-me-ln-california-mexico-earthquake-20170908-htmlstory.html", "text": "The magnitude 8.2 earthquake that ravaged southern Mexico on Thursday was the largest to shake the country in nearly a century.\n\nLike California, Mexico is a seismically active region that has seen smaller quakes that have caused death and destruction. But Thursday\u2019s temblor is a reminder that even larger quakes \u2014 while rare \u2014 do occur.\n\nScientists say it\u2019s possible for Southern California to be hit by a magnitude 8.2 earthquake. Such a quake would be far more destructive to the Los Angeles area because the San Andreas fault runs very close to and underneath densely populated areas.\n\nThe devastating quakes that hit California over the last century were far smaller than the Thursday temblor, which Mexican authorities set at magnitude 8.2 and the U.S. Geological Survey placed at 8.1. Mexico\u2019s earthquake produced four times more energy than the great 1906 San Francisco earthquake, a magnitude 7.8, which killed 3,000 people and sparked a fire that left much of the city in ruins.\n\nSouthern California\u2019s most recent mega-quake was in 1857, also estimated to be magnitude 7.8, when the area was sparsely populated.\n\nA magnitude 8.2 earthquake would rupture the San Andreas fault from the Salton Sea \u2014 close to the Mexican border \u2014 all the way to Monterey County. The fault would rupture through counties including Los Angeles, Riverside and San Bernardino.\n\nAn 8.2 earthquake would be far worse here because the San Andreas fault runs right through areas such as the Coachella Valley \u2014 home to Palm Springs \u2014 and the San Bernardino Valley, along with the San Gabriel Mountains north of Los Angeles. The fault is about 30 miles from downtown Los Angeles.\n\nThursday\u2019s earthquake occurred in the ocean off the Mexican coast and began about 450 miles from Mexico City \u2014 and it was relatively deep, starting about 43 miles under the surface.\n\nIn Mexico, \u201cyou\u2019ve got [many] people a pretty long way aways from it,\u201d seismologist Lucy Jones said Friday. But in Southern California, \u201cwe\u2019d have a lot of people right on top of it. It would be shallow, and it runs through our backyard.\u201d\n\nA magnitude 8.2 on the San Andreas fault would cause damage in every city in Southern California, Jones has said, from Palm Springs to San Luis Obispo.\n\nSouthern California would feel even worse shaking if a magnitude 8.2 earthquake hit here than what was experienced in Mexico on Thursday. Mexico\u2019s earthquake struck under the ocean and was deep; \u201cviolent\u201d shaking \u2014 calculated as intensity 9 shaking by the USGS \u2014 struck only a relatively small part of the country that happens to be sparsely populated.\n\nThat\u2019s the same intensity that was felt in the worst-hit neighborhood in the 1994 magnitude 6.7 Northridge earthquake.\n\nEven though the Northridge and Mexico seismic events vary widely in magnitude \u2014 the Mexico earthquake Thursday produced 178 times more total energy \u2014 Angelenos also felt \u201cviolent\u201d shaking in 1994 because the Northridge earthquake struck directly underneath heavily populated areas and was extremely shallow, striking between just four and 12 miles under the surface.\n\nA magnitude 8.2 earthquake on the San Andreas would produce shaking more intense than either the Mexico or Northridge earthquakes.\n\nIt would bring intensity level 10 shaking, which is perceived by humans as \u201cextreme.\u201d Such shaking would blanket huge swaths of Southern California \u2014 an earthquake that no one alive today has experienced in this region.\n\nThe ShakeOut scenario envisioned the earthquake beginning to move the San Andreas fault at the Salton Sea close to the Mexican border, then moving rapidly to the northwest toward L.A. County.\n\nMexico City rode out Thursday\u2019s earthquake better than a devastating 1985 temblor that killed thousands of people there, in large part because the capital was so far away from the epicenter of this week\u2019s quake. The capital is about double the distance from Thursday\u2019s epicenter as it was from the earthquake that struck 32 years ago.\n\nHow you protect yourself when a quake hits might be all wrong \u00bb\n\nThe U.S. Geological Survey published a hypothetical scenario of what a magnitude 7.8 earthquake on the San Andreas fault would look like. The scenario is still a useful look to imagine what an 8.2 would do to much of Southern California. Both earthquakes would bring generally the same intensity of shaking to Los Angeles, but the 8.2 earthquake would send more intense shaking to areas farther north and west, such as Santa Barbara and San Luis Obispo.\n\nHere\u2019s what could happen if it struck at 10 a.m. on a dry, calm Thursday in November, based on an earlier interview with Jones and according to the ShakeOut report:\n\nThe death toll could be one of the worst for a natural disaster in U.S. history: nearly 1,800, about the same number of people killed in Hurricane Katrina.\n\nMore than 900 could die from fire; more than 400 from the collapse of vulnerable steel-frame buildings; more than 250 from other building damage; and more than 150 from transportation accidents, such as car crashes due to stoplights being out or broken bridges.\n\nLos Angeles County could suffer the highest death toll, more than 1,000; followed by Orange County, with more than 350 dead; San Bernardino County, with more than 250 dead; and Riverside County, with more than 70 dead. Nearly 50,000 could be injured.\n\nMain freeways to Las Vegas and Phoenix that cross the San Andreas fault would be destroyed in this scenario; Interstate 10 crosses the fault in a dozen spots, and Interstate 15 would see the roadway sliced where it crosses the fault, with one part of the roadway shifted from the other by 15 feet, Jones said.\n\nScared? Don't be. Here are tips on how to prepare \u00bb\n\n\u201cThose freeways cross the fault, and when the fault moves, they will be destroyed, period,\u201d Jones said. \u201cTo be that earthquake, it has to move that fault, and it has to break those roads.\u201d\n\nThe aqueducts that bring in 88% of Los Angeles\u2019 water supply and cross the San Andreas fault all could be damaged or destroyed, Jones said.\n\nA big threat to life would be collapsed buildings. As many as 900 unretrofitted brick buildings close to the fault could come tumbling down on occupants, pedestrians on sidewalks and even roads, crushing cars and buses in the middle of the street.\n\nFifty brittle concrete buildings housing 7,500 people could completely or partially collapse. Five high-rise steel buildings \u2014 of a type known to be seismically vulnerable \u2014 holding 5,000 people could completely collapse.\n\nSome 500,000 to 1 million people could be displaced from their homes, Jones said.\n\nSouthern California could be isolated for some time, with the region surrounded by mountains and earthquake faults. The Cajon Pass \u2014 the gap between the San Gabriel and San Bernardino mountains through which Interstate 15 is built, and the main route to Las Vegas \u2014 is also home to the San Andreas fault and a potentially explosive mix of pipelines carrying gasoline and natural gas, and overhead electricity lines.\n\nAll it would take is for the fuel line to break and a spark to create an explosion. \u201cThe explosion results in a crater,\u201d the report says.\n\nShakeOut co-author Keith Porter, research professor at the University of Colorado, Boulder, warned in a 2011 study in the journal Earthquake Spectra that under certain conditions, a magnitude 7.8 earthquake could create such a sudden interruption of high-voltage interstate transmission of electricity that \u201cpotentially all of the western U.S. could lose power.\u201d\n\nPower could be restored within hours in other states, the scenario said. But restoring power in Southern California could take several days.\n\nThere could be up to 100,000 landslides, scientists say, based off how many landslides have occurred in past magnitude 7.8 earthquakes. \u201cThe really big earthquakes \u2026 are much more destabilizing to the hillsides,\u201d Jones said.\n\nThousands could be forced to evacuate as fires spread across Southern California; 1,200 blazes could be too large to be controlled by a single fire engine company, and firefighting efforts would be hampered by traffic gridlock and a lack of water from broken pipes. Super-fires could destroy hundreds of city blocks filled with dense clusters of wood-frame homes and apartments.\n\nThe death toll could mount as hundreds of people trapped in collapsed buildings are unable to be rescued before flames burn through. Possible locations for the conflagrations include South Los Angeles, Riverside, Santa Ana and San Bernardino.\n\n\u201cIf the earthquake happens in [hot] weather ... or in a Santa Ana condition, the fires are going to become much more catastrophic. If it happens during a real rainy time, we\u2019re going to have a lot more landslides,\u201d Jones said.\n\nSeveral dams could be shaken so hard that \u201cthey would be so compromised that they would require emergency evacuation,\u201d Jones said. Even damage to just a single dam above San Bernardino could force 30,000 people out of their homes, the ShakeOut report said.\n\nA seismic warning system for the West Coast has been under development for years by the U.S. Geological Survey, the nation\u2019s lead earthquake monitoring agency. President Trump\u2019s budget would have ended the system before it launched. Officials were looking for \u201csensible and rational reductions and making hard choices to reach a balanced budget by 2027,\u201d according to the administration\u2019s proposal.\n\nBut the proposal to end the funding raised bipartisan complaints up and down the coast. Twenty-eight lawmakers in the California Legislature, including leaders from both parties, urged officials to protect the earthquake early warning system. Members of Congress from Southern California to the Canadian border say the system is crucial to public safety.\n\nIn July, a congressional committee voted to keep funding.\n\nThe earthquake early warning system works on a simple principle: The seismic waves from an earthquake travel at the speed of sound through rock \u2014 slower than today\u2019s communications systems.\n\nFor example, it would take more than a minute for a magnitude 7.8 earthquake that started at the Salton Sea to shake up Los Angeles, 150 miles away, traveling along the state\u2019s longest fault, the San Andreas.\n\nMexico got early warning before deadly earthquake struck. When will California get that system?\n\nPatience in short supply as desperation sets in among South Florida residents still in Hurricane Irma's path\n\nAfter Irma, calls for help from the Caribbean: 'The island is debris, that's all it is'\n\n6:50 p.m.: This article was updated to reflect that intensity 9, or \u201csevere,\u201d shaking has been recorded in Mexico, the same level of shaking intensity felt in the worst-hit region in the Northridge earthquake.\n\n4:45 p.m.: This article was updated with additional historical images.\n\nThis article was originally published at 3:20 p.m.", "sentiment": 0.043822519261115744},
{"link_title": "Learnuv \u2013 Learn libuv for fun and profit, a self guided workshop", "url": "https://github.com/thlorenz/learnuv", "text": "Learn uv for fun and profit, a self guided workshop to the library that powers Node.js.\n\nRead the learnuv gitbook which explains some libuv and related C language concepts that will help you complete the exercises.\n\nRequires Python 2.6 or 2.7 to be installed. If python is not in your path set the environment variable to its location. For example on Windows:\n\nlearnuv comes with build commands that you can use instead of the manual steps explained further below.\n\nNinja and Make do not work on windows except via cygwin.\n\nHighly recommended since it builds faster than any of the other options, so get ninja and do:\n\nWorks on OSX only. Highly recommended to ease debugging and code navigation.\n\nWorks on Windows only. TODO need to adapt vcbuild.bat.", "sentiment": 0.06611111111111112},
{"link_title": "Veertu \u2013 Native virtualization for macOS", "url": "https://veertu.com/veertu-desktop/", "text": "Built on top of macOS Hypervisor.Framework to run Linux and Windows VMs in a responsive and resource optimized way", "sentiment": 0.5},
{"link_title": "Elon Musk's 'Dota 2' Experiment Is Disrupting Esports in a Big Way", "url": "https://www.youtube.com/watch?v=jAu1ZsTCA64", "text": "", "sentiment": 0.0},
{"link_title": "Containerization at Pinterest", "url": "https://medium.com/@Pinterest_Engineering/containerization-at-pinterest-92295347f2f3", "text": "Over the last year the cloud management platform and site reliability engineering teams at Pinterest have been focused on moving our workload from EC2 instances to Docker containers. To date, we\u2019ve migrated more than half our stateless services, including 100 percent of our API fleet. This blog post shares our learnings and adventures throughout the process.\n\nIn recent years, Docker and container orchestration technology like Mesos and Kubernetes have become a hot topic in our industry, demonstrating clear advantages to building a container-based platform. For us, implementing a container platform could yield the following benefits:\n\nWe started our investigation, evaluation and testing in early 2016. Initially, we planned to not only migrate our workload to Docker containers but also run them in a multi-tenant way by adopting the container orchestrator technology at the same time. As the process evolved, we decided to take a phased approach. We first moved our services to Docker to free up engineering time spent on Puppet and to have an immutable infrastructure. At this time, we\u2019re in the process of adopting the container orchestration technology to better utilize the resource and leverage the open source container orchestration technology.\n\nBefore the containerization work services at Pinterest were authored and launched as illustrated as below:\n\nHowever, this process had several pain points:\n\nMigrating infrastructure is challenging for companies like Pinterest because of the scale and complexity. The variance on the programming language, technology dependencies across the technology stack, strict requirements for performance and availability and tech debt all need to be factored in.\n\nOur containerization started from a few small, non-critical applications owned by the CMP and SRE teams. Meanwhile, we started to put critical services into containers in their development and testing environments. Using this as a basis, an effort around containerization was started to bring wider adoption across applications.\n\nOne of the first applications we migrated was our main API fleet. We thought that migrating one of our largest and most complex systems early on would help us catch and solve quite a few issues right from the start. This process helped us create and solidify a more complete set of tools required by our wider infrastructure. It also lowered the risk of having to spend time rewriting tools to include other features in the future.\n\nA big concern for us early on was around performance. While the Docker network is great for development, it\u2019s widely known to negatively impact performance when running in production. This drove our decision to use the host\u2019s network and not rely on Docker networking. We ran several tests to ensure there weren\u2019t issues that would prevent our API fleet from running in a container.\n\nFor the API migration, we ensured we had a comprehensive set of metrics to compare running in a container and outside of one. This process helped us find gaps that made monitoring what was running in Docker and what was running on the host itself indistinguishable. It also helped us uncover issues caused by the conversion process from the early stages of testing and eventually gave us the confidence we needed to push forward into production. The migration process was completed in less than a month while we monitored for regressions or issues. Thankfully this was a fairly uneventful process and caused no disruptions.\n\nAfter we built the primitives using our API fleet as basis, our containerization effort continues. We\u2019ve been able to leverage the tools created to support one of our most complex applications to support the migration of other applications.\n\nOur docker platform now looks like below:\n\nHere\u2019s a more detailed look at how we\u2019re running these Docker containers:\n\nWe\u2019ve completed our first phase goal of containerizing our services. For the next phase, we\u2019re going to adopt the container orchestration and build a multi-tenant cluster to provide one unified interface for long running services and batch jobs.", "sentiment": 0.11704014939309057},
{"link_title": "James Damore (Google Memo Author) at Joe Rogan Experience", "url": "https://www.youtube.com/watch?v=uQ1JeII0eGo", "text": "", "sentiment": 0.0},
{"link_title": "Windy.com Hurricane Visualization (Update interval: 12h \u2013 14h)", "url": "https://www.windy.com/?20.015,-77.937,5", "text": "Your browser is not supported. Please use latest verion of Chrome, Firefox or Safari", "sentiment": 0.5},
{"link_title": "Safari is the new IE", "url": "http://fabiofranchino.com/blog/css-height-parent-flex-safari-issue/", "text": "Working with CSS these days is way better than few years ago thanks to the new specs as well as the commitment towards W3C standards from browser vendors.\n\nUnfortunately, it\u2019s not perfect. Indeed, we\u2019re far from living in a perfect world.\n\nI can feel a pattern everytime I work with CSS and layout: Safari is the last step, the \u201cgive me a break, eventually, I\u2019ll fix Safari issues\u201d.\n\nHere an example of what I\u2019m talking about. A simple layout with a scrollable container and a bunch of children that expand their height 100% the container size.\n\nOn Chrome and Firefox this works like a charm, but not on Safari (because it fails to calculate the height of the children elements according to the container size):\n\nSee the Pen Child height in flex parent by Fabio Franchino (@abusedmedia) on CodePen.\n\nSince it will be fixed at some point, here a screenshot taken with Safari 10.1.2 desktop version:\n\nAnd here a modified version to make it Safari compatible. You can see I had to add an additional wrapper and change the CSS making it less maintainable:\n\nSee the Pen Child height in flex parent Safari compatible by Fabio Franchino (@abusedmedia) on CodePen.\n\nAs it happened in the old days, testing against the less compliant browser is key to avoid big headaches during CSS development. You\u2019ve been warned.\n\nWant to ask something? I'd love to help you. Drop me a line! Not ready to talk? Follow me on Twitter or subscribe to my monthly recap.", "sentiment": 0.06266835016835016},
{"link_title": "List of dead and worthless cryptocurrencies", "url": "https://boingboing.net/2017/09/09/list-of-dead-and-worthless-cry.html/amp", "text": "If you want something truly, permanent, though, you can get a commemorative gold-plated bitcoin for... the grand sum of $7.88.", "sentiment": 0.5},
{"link_title": "Amazon Studios and the Challenge of a Global Hit Show Concept", "url": "https://medium.com/@6StringMerc/amazon-studios-and-the-challenge-of-a-global-hit-show-concept-57daf7ff4af0", "text": "The buzz around Amazon Studios isn\u2019t just about the shows these days\u2026\n\nWhile Amazon is pretty much a household name in much of the developed world, the goal of being a go-to Streaming Entertainment Provider is still a work in progress. With an Echo in the house, I can see the benefit of cross-over with the Amazon Prime Music Library. The convenience of just name-calling an artist like Roger Miller and getting a variety is very convenient.\n\nLast year was Living Room TV Upgrade time, and a Samsung 4K 45\" arrived, loaded with some interface programs to use via Internet: Amazon Prime Video, YouTube, Netflix, and more. Again, incredibly convenient when hooked up through some basic accounts. While still a DirecTV customer, YouTube TV is tempting at $39.99 per month, as the picture on the 4K TV is stunning for quality \u2014 specifically \u201cA Closer Look\u201d with Seth Meyers.\n\nHow often do I go over to Amazon Prime Video for Streaming Entertainment? Once every couple of months, and even then, I browse only the Free Films. For a while I did track \u201cThe Grand Tour\u201d and watch frequently, but after about Episode 6, I had enough of the reboot and have sporadically checked the rest of the season. Other than some fun B-movies like \u201cQuigley Down Under\u201d I get better material on the free weekend HBO/Cinemax or Showtime/TMC teasers that come along once every 6 months. Really.\n\nThere are recent shows I\u2019ve watched religiously, the most notable being \u201cBetter Call Saul\u201d as far as Entertainment wise. I\u2019m usually ironing a work shirt on a Sunday morning, but \u201cMeet The Press\u201d is a habit of mine for more than a decade and I like it that way. By comparison, nothing at Amazon Prime Video is compelling even in the minimal convenience sense, and this is coming from a highly engaged, tech minded creative person who likes TV and Film when done well or basically is, you know, entertaining.\n\nThere is a distinction to help new writers and/or those not familiar with all the different Amazon resources for creatives:\n\nThere is Amazon Storywriter, which is for Screenwriting and is Free.\n\nThere is Amazon Studios, the Production Company which gets a great amount of focus; it is also Free to Contact and Pitch a TV Series or Film script.\n\nThere is Amazon Prime Video, the Streaming entity discussed frequently so far, and home for both TV Shows, Films, and Original Content.\n\nThere is Amazon Video Direct, which is a Content Creator service for uploading Original Content to Amazon Prime Video; the service is Free but also has a genuine Payment Mechanism for Content Creators to get money based on views (it works).\n\nIf this was under the Adobe umbrella, it\u2019d probably get packaged as some kind of \u201cSuite\u201d and charge an annual fee to participate in. For now, the impression is that Amazon is trying to cultivate a lot of Goodwill in the up-and-coming, Global Creative Community by offering online, Free tools. Unlike the vibe I get from Facebook \u2014 one of \u201cWe\u2019ll give you tools but you can only post stuff here and even then we own it\u201d \u2014 the Amazon tools are very open to Exporting or Importing however possible.\n\nAs for my efforts, the following image should do a good job noting how frequently I\u2019ve tried to submit a Film or TV Show script to Amazon Studios through their Private channel within the past year:\n\nVariety published a very interesting Exclusive article on Amazon Studios on September 8, 2017, and it included direct Interview comments from the Executive in charge, Roy Price. The big headline is about Amazon CEO Jeff Bezos demanding a \u201cFlagship Show\u201d from Amazon Studios, and what the plan is to fulfill that mandate.\n\nWhat jumped out at me was this quote:\n\n\u2026and the sky is blue and water is wet. This is Executive Leadership? Looking at some spreadsheets, wearing an expensive suit, claiming \u201cThursday Night Football\u201d is a big programming win with a straight face, pretending to enjoy the presence of Jeff Bezos longer than 47 minutes, and blurting out the obvious pays how much per year?\n\nI\u2019m not even gunning for Roy\u2019s job and I know big-time competitor Netflix already did the heavy-lifting of data analysis for me:\n\nPeople love Adam Sandler. Okay, got it, he\u2019s like one of those rare Entertainers that people keep giving attention to for a lot of reasons. Eddie Murphy kind of fits in the category too, Film wise. So, will the Amazon Studios bet on Seth Rogen pay off the same way as Netflix and Sandler? We\u2019re talking Global appeal right?\n\nThe only way Seth Rogen has global appeal is to put him in a set of vintage stocks somewhere in L.A., put a webcam on the scene, and charge people $1,000 each for charity to throw a piece of moldy produce at him. I\u2019d watch that. Maybe even send some money if it was a telethon. Otherwise? Nah.\n\nWant a Game of Thrones level commitment from an audience? How about a Downtown Abbey level of commitment? Maybe get that sweet viral niche buzz that Stranger Things generated so quickly? There\u2019s a genuine Formula for creating a high-dollar production TV Show Concept that is flexible and general, yet keys in on a few proven elements across genres:\n\n4\u20136 Core Characters in Personal & Work Relationships\n\n+\n\nCompelling Main Location for Show\n\n+\n\nElement of Fantasy\n\n+\n\nCasting, Casting, Casting\n\n=\n\nBig Audience & Pile of Money\n\nWhile I do understand this is pretty tongue-in-cheek regarding how obvious it looks written out in such a way, but give it some play and try yourself. Like with Downtown Abbey the story is as follows:\n\nEven the Comedy Friends can slot into the mold and help explain why a plain-looking concept managed to hook so well:\n\nMain Characters are Friends + New York City + Money Not A Concern + Excellent Cast\n\nIt\u2019s the Fantasy Element, the absence of Adult Concern for Money, that enables the vibrancy of the Characters, Location, and Cast to bounce into adventures and scenarios with such ease. Think about it \u2014 the show falls apart if like every other episode Joey or Phoebe can\u2019t go out to the Coffee Shop with the group because they\u2019re broke. Simple enough, but a hard mix to get just right, in my opinion.\n\nSo, to put my Formula to the test, here\u2019s a framework for an un-developed TV Series Concept based on my creative sensibilities and perspectives on the Global Entertainment Market:\n\nA Family with Three Boys + High Class Mexico City Suburb + Extreme Soccer Talent in all Three Boys + Excellent Cast\n\nWithin this structure, leaning on Casting, the multi-generation dynamic of Parents and Boys sets the stage for the family\u2019s commitment to Soccer (Football) and the conflicts. Oldest Son injury prone, has to become a Coach. Middle Son doing great as a Goalie. Youngest Son doesn\u2019t want to be a Goalie but is being forced into it for the time being. The over-arching aim of achieving a slot on a future Junior and then Adult World Cup team drives the potential of the Fantasy Element \u2014 talent \u2014 with the Drama of Life in the Suspense of the Pursuit.\n\nMaybe Seth Rogen and recent \u201cThe Walking Dead\u201d can breathe some life into Amazon Studios in the commercial sense. Maybe my inclinations are way off, and Roy Price\u2019s data and numbers align with success in 2018 unlike in years past. From outward appearances, that doesn\u2019t sound like a terribly high bar to clear \u2014 except when balanced against 2018 being the highest expenditure year as well.\n\nThis pattern of Amazon Studios reaching into their pockets to throw money at established names is a routine by now. Initially I thought the plan was a different one\u2014 based on the number of tools and resources offered to up-and-coming Content Creators \u2014 to cultivate a \u201cMinor League\u201d of sorts to be able to know when they debut a high-dollar, high-budget, high-ad-spend new TV Show Concept, it\u2019ll be more than data, numbers, a couple meetings, money changing hands, and finger crossing. There\u2019s always risk in this business, that\u2019s the joke.\n\nIn 18 months, we\u2019ll probably have some new developments to consider. Numbers. Data. Luck. Here goes nothing, right?", "sentiment": 0.19497792423165564},
{"link_title": "Equifax's dox of America: Sign up for \u201cfree\u201d monitoring, get billed forever", "url": "https://boingboing.net/2017/09/09/to-unsubscribe-just-die.html", "text": "Equifax's dox of America: Sign up for \"free\" monitoring, get billed forever\n\nEquifax dumped dox on 143 million Americans (as well as lucky Britons and Canadians!), sat on the news for five weeks, let its execs sell millions in stock, and then unveiled an unpatched, insecure WordPress site with an abusive license agreement where you could sign up for \"free\" credit monitoring for a year, in case someone used the immortal, immutable Social Security Number that Equifax lost control over to defraud you.\n\nYour SSN is vulnerable forever, but Equifax's \"protection\" only lasts a year -- surely there's an oversight there?\n\nNot at all! To get your \"free\" monitoring from Equifax, you have to provide a credit card, and if you somehow forget to cancel your \"free\" service a year from now, Equifax will simply start billing you...forever.\n\nIf you sign up for the service, it says, \"In the event you wish to continue your membership beyond the trial period, do nothing, and your membership will automatically continue without interruption, and we will begin billing you via the payment source you provided when you signed up for the free trial.\" Let me also go to more of Equifax CEO Rick Smith from his video to consumers, where he talked about the help that the company is supposedly providing.\n\nEquifax Data Breach is a 10 out of 10 Scandal [William K. Black/The Real News]", "sentiment": 0.1982142857142857},
{"link_title": "Depression could be treated with anti-inflammatory drugs", "url": "http://www.telegraph.co.uk/science/2017/09/08/depression-physical-illness-could-treated-anti-inflammatory/", "text": "Dr Alan Carson, Reader in Neuropsychiatry, at the University of Edinburgh, said: \u201cAll psychiatric and neurological disorders are based in brain and brain is not static but structurally and functionally responsive to a range of biological, psychological and social issues.\n\n\u201cYet institutionally we use an outmoded code which separates brain disorders into psychiatric 'f' codes and neurological 'g' codes which holds back both scientific and clinical progress.\u201d\n\nStephen Buckley, Head of Information at mental health charity Mind, said more research was vital to pick apart the various causes of depression and find new treatments.\n\n\u201cWe must acknowledge a wide range of potential causes and treatments,\" he said. \"For many people, long term physical illness can cause mental health problems, such as depression. This could be because of the impact of living with the illness, the pain and discomfort or side effects of medication, among many other reasons.\n\n\"We also need to look at people\u2019s broader experiences, their lives and other challenges they face - such as a lack of access to services, experience of abuse or trauma, poor housing and exclusion, to ensure everyone with a mental health problem gets the support they need.\u201d\n\nOne promising treatment for depression on the horizon is the use of electrical stimulation to change the signals between the brain and the immune system.\n\nProf Kevin Tracey, President and CEO, of the US Feinstein Institute for Medical Research, discovered that the brain controls production of a deadly inflammatory chemical called TNF, which if released in high doses can be fatal, causing people to, literally, die of shock.\n\nHe has recently developed a electrical device which reproduces the connection and switches off the chemical. Three quarters of patients with rheumatoid arthritis recovered following trials.\n\n\u201cThis is the tip of the iceberg of a new field called bio-electric medicine,\u201d he said.\n\n\u201cThis is a new way of thinking about medicine. We\u2019re using electrons to replace drugs. This will not replace all drugs. But there will be many drugs that are either too expensive, too toxic which may be replaced by these devices.\u201d", "sentiment": 0.029763257575757568},
{"link_title": "The Prospect's Protest (A Problem)", "url": "https://capitalandgrowth.org/articles/492/the-prospects-protest-a-problem.html", "text": "", "sentiment": 0.0},
{"link_title": "Show HN: A library to record and intercept JavaScript at runtime", "url": "http://maierfelix.github.io/Iroh/", "text": "", "sentiment": 0.0},
{"link_title": "Professional drone pilots helping Houston recover", "url": "https://www.wired.com/story/houston-recovery-drones", "text": "Less than a week after the last drops of Hurricane Harvey fell, Houston is just beginning to assess the damage. At least 46 people have died. More than 30,000 houses are flooded and as many as a million vehicles waterlogged. Early estimates suggest the hurricane has inflicted $120 billion in damage on the region, making it the most expensive natural disaster in the country\u2019s history.\n\n\u201cThis is going to be a massive, massive cleanup process,\u201d Texas governor Greg Abbott told ABC\u2019s Good Morning America on Friday. \u201cThis is going to be a multiyear project for Texas to be able to dig out of this catastrophe.\u201d\n\nWhich means the drones\u2019 work has just begun. Responding to the disaster provides a major test\u2014and opportunity\u2014for the country\u2019s fast-growing network of professional UAV operators, almost exactly one year after the Federal Aviation Administration began to hand out licenses for commercial drone operation . (There are at least 2,000 licensed pilots in the Houston area alone, and some 20,300 nationwide.)\n\n\u201cThis is the one of the first big disasters where we can show how valuable drones can be,\u201d says Brandon Stark, who directs the Center of Excellence on Unmanned Aircraft System Safety at the University of California, Merced. In the coming weeks and months, they'll help locals assess damage to homes, roads, bridges, power lines, oil and gas facilities, and office buildings\u2014and determine whether it's safe to go back.\n\nAs Harvey approached, the FAA did what it usually does in emergencies: It restricted air space in the affected region. That means commercial and private aircraft, including professionally operated drones, are banned from the area until the government says otherwise. \u201cUnauthorized drone operators may prevent the response and recovery aircraft from safely doing their jobs,\u201d says Laura Brown, an FAA spokesperson.\n\nBut the FAA has found exceptions to its rule, issuing at least 43 unmanned aircraft system authorizations for groups involved in response and recovery efforts. These waivers let specially sanctioned operators fly in the otherwise verboten airspace, but operators still have to follow the basic regulations for small drones: flying below 400 feet and within the pilot's line of sight, and not over large crowds of people.\n\nOil and gas companies have claimed five of those authorizations, which they\u2019ve used to inspect their facilities, power lines, and fuel tanks. Union Pacific Railroad grabbed eight, and has had three certified operators flying DJI Mavic Pro drones in the area since Tuesday, inspecting flooded and hard-to-reach areas, like rail yards. Meanwhile, their operations team back at HQ inspects the conditions through a live feed. \u201cThis is especially useful for bridge examinations looking for track washouts and other structural changes,\u201d says Raquel Espinoza, a Union Pacific spokesperson.\n\nPilots working for Fort Bend County, to the southwest of Houston, have used drones to assess damage to roads, bridges, and water treatment plants\u2014and posted the footage online . Other local governments and agencies, including fire departments and state environmental quality officials, have worked with drone operators to identify flooding and drainage problems.\n\nParker Gyokeres, a New York-based drone pilot, arrived to volunteer in the Houston area on Sunday afternoon, as floodwaters rose. \u201cWe probably shouldn\u2019t have done it\u2014it was really stupid\u2014but it was the right thing to do,\u201d he says. He\u2019s now part of an eight-person team of professional pilots and mappers who are working with local government agencies and the Red Cross to assess the damage and work out how to respond. (They have received special authorizations from the FAA for humanitarian operations, and will not be conducting commercial work in the area, Gyokeres says.)\n\nOn Friday, the group was lugging an RV and flat-bottom boat around Katy, Texas, to the west of Houston, where they\u2019ll row out to the middle of flooded neighborhoods to run their missions. (\u201cTexas is really big,\u201d Gyokeres says.) Drones have an advantage over even helicopters in this response situation, he says, because they're cheaper, can fly lower, and don't risk pilots' (and passengers') lives as they move around in the sky.\n\nMeanwhile, insurance companies are launching their own fleets of drones, in efforts to tally up and verify claims. Allstate will use hundreds of drones in its Harvey-related deployment, its largest since it began experimenting with drone assessments a few years ago, says Justin Herndon, a spokesperson for the insurance company. \u201cWe\u2019re going to be there for a while.\u201d\n\nAllstate works with an aerial imagery company to coordinate its drone operations, which in turn contracts with freelancing drone pilots, each of whom maintain their own drone equipment. Those freelancers must not only pass their pilots\u2019 exams but complete specialized training and ensure their drones are up to par. (The DJI Phantom 4 and Mavic Pro , about $800 and up, are both acceptable models.)\n\nDrones are especially convenient for checking out insurance claims because they\u2019re fast, nimble, and can carry excellent cameras. They can relay footage back to Allstate\u2019s claim adjusters in real time, so they can get started on their work even before the drone pilot leaves the area. \u201cYou can zoom in at your desk to a single shingle and see the characteristics of that particular piece,\u201d Herndon says.\n\nPerhaps most impressive of all, unauthorized drone flyers haven't seriously run afoul of the rules. Earlier this summer, firefighters battling a blaze in Prescott, Arizona, had to ground aircraft and pull out crews for an hour after a pilot spotted an unauthorized drone in the area. (Law enforcement charged a man with 14 counts of endangerment for the incident.) Authorities fighting fires in Montana have run into similar issues .\n\nNot so in Houston: \"We've had a few reports [of misbehavior] but nothing widespread,\" the FAA says. Stay safe, Texas\u2014and let the remote control pros handle the skies.", "sentiment": 0.1431723119223119},
{"link_title": "How I dockerize my personal server", "url": "http://malexandre.fr/2017/09/09/how-i-dockerize-my-server/", "text": "To host this blog, I bought a VPS SSD 2 server on OVH. It\u2019s 7\u20ac per month, has a decent amount of storage for my current needs, and I will be able to build some apps on it. I could have gone with a classic shared hosting, but I wanted to be able to build my blog directly from my server. To do that, I dockerized everything.\n\nFirst, for a simple reason: To be able to test my configuration locally before deploying it. Docker take care of the environment, so you know you will have the same on every machine you use your docker config. By testing my nginx configuration & my build setup directly on my computer, it was easier to fix the issues, and also it was faster because of my high end processor. Once I was sure everything worked, I launched my docker-compose on the server, and I was done.\n\nSecond, it means I have a minimal installation on my server. The only things I need are Docker and Git. My whole server configuration is on GitHub, so I needed an easy way to fetch it and upate it on my server. Using git was logic, as I\u2019m already using it for versioning. If I change the configuration, a simple updates everything and I just have to , and leave my ssh session.\n\nIt also means that I can switch to another server/provider anytime I want. All I need is a server with git & Docker, and mostly all linux server can handle that. It\u2019s also easy to switch to a container service, but for now I want to have my own server.\n\nFor now, there is only my blog, so the config is pretty straightforward. I plan to had a custom Node app to be able to write my article directly one the server, commit them & rebuild the site by a simple click, but that project is not ready at all, I just planned it. To serve my blog, I used nginx. Here\u2019s my docker compose configuration:\n\nAs you can see, I already planned my blog-admin app, even though it\u2019s not ready. For nginx, I didn\u2019t even need to create my own docker image, using the official (on alpine for a smaller size) is enough. The only thing I needed was to give my own configuration & files to serve. Side note, the official docker-compose package on Debian 9 is not up to date, so I was forced to keep using the 2nd version of the . But with a simple configuration like mine, it\u2019s not a problem. I used the restart policy to make sure my nginx boots up with my system.\n\nThen, for my blog, I made my own Dockerfile. Starting from Alpine, again, I installed everything I needed, uninstalled what was only needed to build some gulp dependencies, and made own entrypoint script to build a new version of the blog everytime I launch the container. Some things are maybe too much (adding yarn makes the image bigger, and it just speeds up the build, it\u2019s not essential), and I think I could optimize the build steps with some caching, but for now it\u2019s enough. I could also just have used a scripts form my package.json, but at least like this my blog is not linked to my build configuration for Docker.\n\nWith all this configuration, I simply need to from my server, and everything is up to date.\n\nCurrently, I need my computer to write a new article on my blog. I can ssh from my phone easily, but writing an article with git is not that simple. I can write it directly on Github, but if I want to add images, it starts to be too cumbersome to really do it. That\u2019s why I want to build an app to manage everything easily, on any computer or mobile device. I know there is some solutions already out there, but I also want to keep working on more Preact & Node apps, so it\u2019s a good excuse. I also think I will build a custom app to organize board games nights with my friends, as most of them don\u2019t like Facebook or are not ready to check a slack community every once in a while. My app would be entirely usable with emails, by notification and answering to those notifications, so maybe it will be more adopted. If not, it will still be a good exercise.\n\nAnd regarding the performance on my server, for now my blog does not have a big enough reach for it to be a problem. Nginx uses only 4% of my processor, and 25% RAM, nothing critical. We\u2019ll see with time, and especially with more node app, if it will be enough, but I\u2019m pretty sure it will.", "sentiment": 0.18255781499202547},
{"link_title": "Bootstrap 4 \u2013 Wall of Browser Bugs", "url": "https://getbootstrap.com/docs/4.0/browser-bugs/", "text": "Bootstrap currently works around several outstanding browser bugs in major browsers to deliver the best cross-browser experience possible. Some bugs, like those listed below, cannot be solved by us.\n\nWe publicly list browser bugs that are impacting us here, in the hopes of expediting the process of fixing them. For information on Bootstrap\u2019s browser compatibility, see our browser compatibility docs.\n\nThere are several features specified in Web standards which would allow us to make Bootstrap more robust, elegant, or performant, but aren\u2019t yet implemented in certain browsers, thus preventing us from taking advantage of them.\n\nWe publicly list these \u201cmost wanted\u201d feature requests here, in the hopes of expediting the process of getting them implemented.", "sentiment": 0.25206043956043955},
{"link_title": "I have loved football for years, but this NFL season is making me queasy", "url": "https://www.washingtonpost.com/sports/ive-loved-the-football-for-years-but-this-nfl-season-is-making-me-queasy/2017/09/07/1e9f3eb0-9417-11e7-89fa-bb822a46da5b_story.html?utm_term=.663a044222c4", "text": "The epiphany came last weekend, and then dread followed. While shopping for soccer cleats, my 5-year-old son looked up and said, \u201cDad, the sport I really want to play and care about is football.\u201d\n\nHeart, meet stomach. It\u2019s amazing how children can crystallize your beliefs. For years, I have had these conflicting emotions about how and why I cover football \u2014 particularly the NFL \u2014 but one little boy\u2019s comment cut through the mixed feelings.\n\n\u201cNo,\u201d I thought. \u201cHell no. I\u2019ll find a way to destroy the game before you invest your mind \u2014 and definitely your body \u2014 in that misguided sport.\u201d\n\n\u201cWell, Miles, we\u2019ll see,\u201d I actually said. \u201cThat\u2019s a few years away. Maybe you can play some flag football when you get a little older.\u201d\n\nHe moved on and started obsessing over some toy. If only it were that easy for the rest of us to transition from America\u2019s favorite troubled pastime.\n\nThis isn\u2019t merely about the dangers of playing football. My son stirred some deeper thoughts. It\u2019s not that I hate the game; it\u2019s that my distrust of its stewards is at an all-time high, and the NFL leads the way with its clumsy, reckless and self-serving leadership. The problem isn\u2019t one thing. It\u2019s the whole buffet.\n\n[The making of Colin Kaepernick: The quarterback without a team is the NFL\u2019s most talked-about player]\n\nI dread the start of another NFL season. I dread it because obsession is about to drown out concern again. I dread that my work is about to help fuel this obsession. Every sport has issues, but the attention the NFL receives and the money it makes allow for blind arrogance to take over every fall and winter. The NFL has suffered some dings to its popularity, but the damage is minor thus far, barely noticeable, which means the league will go on abusing responsibility and creating an awful example for how to maintain the game.\n\nThe issues are vast and diverse. They include the NFL\u2019s ongoing failure to commit to a more responsible way to do objective research and protect its players from the effects of concussions and brain injuries caused by playing football. And the limited resources and lack of compassion given to retired athletes who wrecked their bodies to provide entertainment. And the blackballing of Colin Kaepernick and the overall NFL ambivalence toward its players\u2019 legitimate concerns about equality and social justice. And the confusing and contradictory way that the league disciplines its players, from Tom Brady to Ezekiel Elliott, which consistently shows that it is more concerned about perception than fairness or rehabilitation. And the culture of fighting the players over every dime, which leads to contentious contract negotiations even for stars such as Le\u2019Veon Bell and Aaron Donald .\n\n[The Patriots\u2019 loss to the Chiefs revealed some glaring weaknesses]\n\nAll of these grievances fall under one huge, disheartening umbrella: The NFL doesn\u2019t respect humanity. Not like it should.\n\nWhen Miles mentioned wanting to care about football, it opened my mind. As a parent, you become very alert when your child expresses an interest. More than anything, you want them to spend time on something worthwhile. But it\u2019s impossible for me to want my son to devote himself to football when there are so many indications that the game, at its highest level, doesn\u2019t care about its participants.\n\nThis is why many people have decided to boycott this season. You saw the rally to support Kaepernick in New York last month. On Change.org, there is a #NoKaepernickNoNFL petition, and more than 176,000 have signed it and pledged not to watch any games until the free agent quarterback is signed to a contract. That\u2019s a sizable number, and over a long period, the movement could become larger and make the NFL\u2019s pockets lighter. But the truth is that this season will go on without them, the league will make a ton of money, and shortsighted owners will continue to treat a considerable portion of their workforce and their fan base like they don\u2019t matter.\n\nIf my job weren\u2019t so tied to covering the NFL, I would boycott it. I would endure my kids watching reruns of \u201cPAW Patrol\u201d and \u201cPeppa Pig\u201d on Sundays. Instead, I will spend the next four months bouncing around NFL stadiums, and the good of the sport will consume me. The competition, the close games in the fourth quarter, the stunning athleticism, the teamwork, the passion and intensity \u2014 those things make the game so riveting.\n\n[Poll: Nine in 10 sports fans say NFL brain injuries are a problem, but 74 percent are still football fans]\n\nFootball is a wonderful sport if played correctly and if its stewards handle the game with care. It is physical and violent, but it avoids a savage reputation when the players are treated like valuable human beings and not disposable entertainment commodities. During his 11 years as NFL commissioner, Roger Goodell often has referenced \u201cprotecting the shield,\u201d which is his way of saying that his job is to maintain the integrity of the game. But the players hold the shield. With its actions, the NFL keeps undermining the people behind the shield.\n\nBecause of that poor example, my love-hate relationship with the NFL is tilting toward the latter. I have two sons. I don\u2019t want either of them to inherit my love for a game that treats its players like toys and its fans like breathing dollar bills.\n\nTradition can be a source of pride and ignorance. It\u2019s important to believe in and connect to something greater than you, something that has history and feels eternal, but it\u2019s foolish to do those things out of mindless habit. My children won\u2019t love the NFL simply because Mom and Dad watched a lot of games and they drifted into the obsession.\n\nIn the Brewer household, the NFL will have to earn that tradition. Daddy may be locked into a lifetime deal, but for my children, it\u2019s time to renegotiate the contract terms.\n\nFor more by Jerry Brewer, visit washingtonpost.com/brewer.", "sentiment": 0.021809897921009028},
{"link_title": "Every Single Insane Thing I've Done to Score a Reservation in Tokyo", "url": "https://www.eater.com/2017/2/21/14673238/tough-reservations-tokyo", "text": "Like everyone, I watched the documentary Jiro Dreams of Sushi, and wondered what it might be like to taste Jiro Ono\u2019s food at his subterranean restaurant in Tokyo\u2019s Ginza district.\n\nI tried everything to get a reservation. I got Japanese friends to call, but they were repeatedly told the 10-seat restaurant was fully booked. I asked concierges at the city\u2019s top hotels to help, but they needed at least two months notice, and, even then, they couldn\u2019t make any promises. One time I even stopped by the actual restaurant just before the lunch service and made pleading faces \u2014 all to no effect. I had written about Japanese food for over a decade; this was getting embarrassing. I decided the only dignified course left open to me was to snub Jiro.\n\nThen, suddenly, a tiny sliver of light appeared \u2014 a portal to that (actually quite dowdy) basement in Ginza. A Japanese friend happened to mention that he knew a regular at Jiro\u2019s counter, someone very high up in a well-known Japanese electronics company, and maybe he could try to use that connection to get me a table.\n\nI tried, for almost a full blink of the eye, to affect nonchalance, and then hurled myself at the man\u2019s ankles, refusing to let go until the reservation was confirmed.\n\nTokyo is full of fiendishly elusive reservations, places whose impenetrability makes getting a table at Eleven Madison Park look like ordering a burger at Wendy\u2019s. Indeed, these days it seems to me that, thanks to the (wholly warranted) acclaim being heaped upon the Japanese capital\u2019s high-end dining scene by leading chefs, the Michelin Guide, and even the United Nations, it is only getting more and more difficult to bag a top table in the city.\n\nI travel to Tokyo three or four times a year, and fill my trips with eating. I used to head there with a hit list of 20 or so places, and have a decent chance of dining at about 10. These days, I\u2019m lucky if I make it into two.\n\nObviously, the record number of visitors to Tokyo has put more pressure on the city\u2019s finite number of restaurant seats, but there\u2019s another reason reservations are getting harder to come by: Some restaurants are quietly banning foreign visitors from their reservation books. Those who have gone before, it seems, have screwed the chances for the rest of us, either by failing to show up, or by behaving boorishly during their meals. Other unforgivable transgressions (which all essentially boil down to ignoring Tokyo\u2019s cultural mores) might include professing a litany of allergies, dressing like a holidaying Adam Sandler, not bringing cash (which is still sometimes the only form of payment some of these places accept), or just behaving somehow inappropriately in the eyes of the hosts \u2014 blowing your nose at the table will do it.\n\nThe same undesirable parade of late, rude, no-show tourists has led several Tokyo restaurants to reject the overtures of the Michelin man. They have enough regular diners not to need the hordes of joyless box-tickers a Michelin star or two tends to attract. As one restaurateur put it to me, quite reasonably: \"We have customers who have been coming here for decades, and accommodating them is more important to us than filling every place every night.\"\n\nAnd then there are the so-called ichigen-san okotowari (\"no first-time/drop-in customers\") restaurants. In other words, they do not accept reservations from just any old schmo who rings up out of the blue and offers to pay them money in exchange for food (often, the phone number is impossible to find to begin with). You need to be a regular. But, of course, to become a regular, you need to dine there.\n\nOne of the most famous ichigen-san okotowari restaurants is Mibu, an eight-seater members-only restaurant in Ginza that\u2019s beloved by Joel Robuchon and Ferran Adria, both of whom, I\u2019m told, have been reduced to tears by the precision and beauty of chef Hiroshi Ishida\u2019s cooking. Each year, members are invited to choose one night a month on which to dine at Mibu, and they may invite seven guests along \u2014 which is how I snuck in the first time.\n\nBut even after you have exchanged meishi (business cards) with the chef and consider yourself within the circle of trust, there\u2019s still no guarantee of a table in the future. These types of restaurants are usually really small spaces. They can\u2019t rearrange tables to make room for one or two Johnny-come-latelies. Many, like Matsukawa, my favorite contemporary kaiseki ry\u014dri place, are now not only accepting guests on an introduction-only basis, but, once the introductions are made, you\u2019ve got to reserve your spot three months to the day from when you hope to dine. I don\u2019t know about you, but I barely know what I\u2019m doing tomorrow, let alone in three months\u2019 time.\n\nSo, what can a person who wants to eat at an impossible-to-get Tokyo restaurant actually do?\n\nOne obvious method is to use a concierge. This is familiar advice, but it\u2019s one of the only reliable avenues for visitors. To get the most prized reservations, you will need to stay at a prime hotel (I hear particularly good things about the concierge at the Ritz-Carlton). But even the most in-the-loop concierge can\u2019t guarantee you\u2019ll be a shoo-in. You will need to make your hotel reservation \u2014 and contact the concierge \u2014 many months in advance of your visit. You will probably have to give them your credit card details to pass on to the restaurant, a legacy of all those foreign no-shows. And, again, highly sought-after places like Sushi Saito can\u2019t magic an extra cover at the counter, no matter how expensive your hotel room. (If you\u2019re Airbnb-ing it, there are independent concierge services, but they\u2019ll cost you: One recently quoted me \"from $87\" to make one reservation, which worked out to nearly a 50 percent surcharge on my meal.)\n\nMany Tokyo restaurants also prioritize reservations made by Japanese people, rather than tourists, so one strategy that has worked for me is to make myself a friendly burden on my Japanese acquaintances. If you\u2019re lucky enough to know anyone in town, they just might connect you to a friend who knows a friend who has a coveted connection to the three-Michelin-star contemporary kapp\u014d (counter-style restaurant) of your dreams (but don\u2019t forget to bring your friend a massive gift to say thank you).\n\nAn example: I heard about a place \u2014 far outside Tokyo \u2014 which some were calling the greatest restaurant in Japan. The first tip-off came from the billionaire boss of a cartoon company with close links to a certain winsome, mouthless, pink cartoon cat. But, of course, as a non-Japanese tourist, I\u2019d have had a better shot at an invitation to tea with the Emperor. I sprang into action \u2014 if you can call sending dozens of DMs and emails \"action.\" I pulled so many strings you could have stuck a pipe in my mouth and called me Geppetto, and in the end I bagged the table via a game of five degrees of separation spanning three cities, culminating in me being personally vouched for by the chef of a restaurant in Nagoya that I had never even heard of.\n\nBut what of Jiro? The great man had just turned 90 when I ate at his restaurant, and as per his reputation, he was pretty fearsome, at one point admonishing two diners near me for some unseen transgression. Still, he couldn\u2019t have been more friendly to chat with afterward. In its brevity and atmosphere, the meal was more like a benediction than a lunch, so I would love to be able to report that the sushi was overhyped, an expensive and hard-to-get disappointment. But the truth is, it was the best sushi of my life \u2014 by a significant margin. Sometimes, those reservations really are worth selling your soul for.\n\nBut far more often, they aren\u2019t. I\u2019m a man obsessed, but high end dining in Tokyo is far from the city\u2019s only pleasure. The best advice I have to offer on this subject is this: Don\u2019t trouble yourself too much about the famous places. Instead, when you get to Tokyo, pick a weekday evening and take a train far from the center of the city in any direction (west is always good) and get off at any random station. It doesn\u2019t really matter which one. Walk away from the station for a couple of minutes, and go into the nearest izakaya.\n\nYou probably won\u2019t need a reservation. It\u2019ll be amazing.\n\nMichael Booth is the best selling, award winning author of six works of travel/food non-fiction, including The Almost Nearly Perfect People: Behind the Myth of the Scandinavian Utopia, and his latest, Super Sushi Ramen Express: One Family's Journey Through the Belly of Japan (Picador).", "sentiment": 0.15248871003800582},
{"link_title": "After the Equifax breach, how to freeze your credit to protect your identity", "url": "https://www.washingtonpost.com/news/the-switch/wp/2017/09/09/after-the-equifax-breach-heres-how-to-freeze-your-credit-to-protect-your-identity/", "text": "Consumers affected by the Equifax data breach are scrambling for ways to protect their financial lives. Some are considering Equifax's own credit-monitoring service. Others suggest freezing your credit as a better option to such services. But what does freezing your credit entail, and how easy is it to do (and undo)?\n\nIn basic terms, freezing your credit means placing restrictions on who can view your credit report. Why is this important? Well, applying for housing, checking accounts or new credit cards can all involve a credit pull by potential landlords, mortgage lenders or banks. If you prevent them from pulling your credit, it'll frustrate the fraudsters who need these organizations' approval to open fake accounts using your stolen identity.\n\n\n\nFreezing your credit comes with a $5 to $10 charge for each credit bureau. The amount of the charge depends on where you live; here's a PDF from Equifax that shows how much it might cost you. Often, victims of identity theft can freeze their credit at no charge. To get the ball rolling, visit the relevant websites of Experian, Equifax and TransUnion. You can also call Equifax (1-800-349-9960), Experian (1\u2011888\u2011397\u20113742) or TransUnion (1-888-909-8872).\n\nThe credit agencies will ask for your personal information, including your name, address, date of birth and Social Security number. Once you've supplied those and frozen your credit report, nobody except your existing lenders, or their debt collectors, will be able to see it, according to federal regulators. The only other entities that are allowed to see your credit report at this point are government agencies carrying out a search warrant or subpoena, and yourself, if you're trying to access the free credit report that is entitled to you once per year per credit bureau. (You can thank a 2003 law known as FACTA for this right. Annualcreditreport.com is the only website you'll ever see government officials recommend for this purpose.)\n\nBut what do you do once your report is frozen and you need, say, a credit card company to look at it?\n\nIn that case, you can contact the credit bureaus again and ask them to lift or \u201cthaw\u201d the freeze. To do so, you'll need a PIN that your credit bureau gave you when you enabled the freeze. The reporting agencies are required to put the thaw into effect no later than three business days after you submit the request. You can also choose to lift the freeze only for a specific amount of time, to limit your exposure. Lifting the freeze can also come with a small fee.\n\nIf you lose your PIN, you can reset it, but that will typically require you to provide proof of your identity. This poses a different type of security risk; if a criminal manages to get a copy of the required identifying documents \u2014 say, through a corporate data breach or by persuading you to give up the information voluntarily through an email phishing attack \u2014 then there isn't much standing between a determined thief and an unfrozen credit report.\n\nStill, many Americans become identity theft victims every year simply because they represent the easiest targets. Making it even a little bit harder for criminals to put your stolen identity to use could save you an enormous headache.\n\nPeople have been signing up for Equifax\u2019s help service. Here\u2019s what they were told.\n\nHow Equifax hackers might use your Social Security number to pretend they\u2019re you\n\nWhy it can take so long for companies to reveal their data breaches", "sentiment": 0.07840197083618136},
{"link_title": "Let's Not Lose Our Minds", "url": "https://medium.com/@carlzimmer/lets-not-lose-our-minds-c5dcac29e97f", "text": "I was asked to give the keynote talk at \u201cScience, Journalism, and Democracy: Grappling With A New Reality\u201d at Rockefeller University on September 6, 2017. This is what I said.\n\nWe\u2019re here at this meeting to talk about science, journalism, and democracy. So let me begin by telling you about a newspaper article on a scientific experiment, an experiment that would end up having a major influence on government policy on a vital issue.\n\nThe vital issue was food. The experiment was carried out on wheat. Some varieties of wheat are known as spring wheat. They\u2019re planted in the spring and grow soon afterwards. Winter wheat, on the other hand, is planted in the fall but does not produce its flowers till the spring. Winter wheat has the advantage of a much bigger yield. But there\u2019s a catch.\n\nA field full of winter wheat can get wiped out by something called Black Frost. This is not a creature from Game of Thrones. Black Frost is what happens when there\u2019s a sharp cold snap in winter without any snow on the ground. Winter wheat needs that layer of insulation to survive extremely low air temperatures. Spring wheat never faces that risk.\n\nThis newspaper article I want to tell you about described a new idea. What if you could plant winter wheat in the spring? On the face of it, this shouldn\u2019t work. Winter wheat actually needs the cold as a signal to prepare to flower when the soil gets warm. Plant it in the spring, and it doesn\u2019t get the signal.\n\nBut this newspaper article recounted the work of a maverick young scientist that suggested otherwise. (We journalists do have a weakness for maverick young scientists.) Just before spring planting, he germinated some winter wheat seeds and then packed them in some snow for a few days. After this artificial chill, he planted the winter wheat in the springtime soil. And they soon flowered.\n\nThe article celebrated the \u201cextraordinary scientific and economic value\u201d of the research. It promised nothing less than saving the country from a food shortage.\n\nThe article I\u2019m talking about appeared on October 8, 1929. The newspaper that published it was Pravda, the propaganda outlet of the Communist Party of the Soviet Union. And the title was \u201cOn the Sowing of Winter Cultures in Spring (The Discovery of Agronomist T.D. Lysenko).\u201d\n\nTrofim Lysenko was a little-known researcher at the time. He did his experiment in the early years of Stalin\u2019s dictatorship, when Stalin was facing dangerous food shortages across the Soviet Union. He had just responded by forcing peasants onto collectivized farms, a terrible decision that would lead over the next decade to the deaths of millions by starvation.\n\nStalin also demanded that Soviet scientists help fight the crisis by finding better crops, and find them fast. The Soviet Union at the time was home to a thriving community of geneticists who were doing pioneering work to understand the nature of genes in animals and plants. In response to the crisis, Soviet geneticists threw themselves into producing better crops through genetics. But their results were coming too slowly for Stalin.\n\nAnd then came Lysenko. Lysenko had a great backstory that fit Stalinist ideology. He wasn\u2019t one of those fussy cosmopolitan experts. He was from a peasant family. And despite having little advanced education, he was succeeding where mainstream scientists were failing. As soon as the agricultural ministry learned about Lysenko\u2019s experiment on winter wheat, they began promoting him as scientific hero.\n\nIn fact, when Lysenko first described his research at scientific conferences in early 1929, other Soviet scientists roundly dismissed it. For one thing, it was nothing new. Plant breeders had already been trying to use cold temperatures for centuries to improve plant growth. But they had little or no success.\n\nSo why should Lysenko suddenly be getting his amazing results? Lysenko\u2019s critics said he was getting nothing of the sort. He was running experiments that were so small and sloppy that they couldn\u2019t be trusted. Even in the early years of Stalin\u2019s rule, Russian scientists were still having vigorous open exchanges. That\u2019s one of the essential ingredients of science, because it allows scientists to hold each other to high standards.\n\nYou wouldn\u2019t know that from the Pravda article, though. It presented Lysenko\u2019s experiment as an open and shut case. Pravda, it\u2019s worth mentioning, was staffed by editors with little scientific training and with a lot of incentive to believe the government\u2019s claim that Lysenko was right. In their article on Lysenko, they didn\u2019t mention the scientific community\u2019s objections. More fawning press coverage soon followed. Sometimes geneticists managed to get their objections known, but the newspapers quoted government spokesmen dismissing them as bourgeois.\n\nThis complicit journalism, historians have argued, helped lift Lysenko to the highest echelons of Soviet science. As the flattering press coverage poured out, he got a huge laboratory to run more experiments. He went on to become a leader of the national efforts to grow more food.\n\nThis power went to Lysenko\u2019s head. He started making sweeping pronouncements about the nature of heredity itself. He claimed that chilling winter wheat plants did more than just allow farmers to sow them in the spring. The plants could then pass down this improved trait to their offspring.\n\nAn acquired trait in a plant could override its genes, in other words. In fact, Lysenko now said, genes didn\u2019t actually matter to life. The evidence in favor of genes, evidence that had poured in for thirty years, meant nothing to him. He brushed off the science of genetics as a \u201cbourgeois perversion.\u201d His attacks got personal, as he vilified geneticists as \u201cfly-lovers and people-haters.\u201d\n\nSoviet newspapers stuck to the same talking points. They broadcast Lysenko\u2019s amazing claims, and they attacked genetics as a myth of the capitalist west. By ignoring genetics, they claimed, Lysenko would put the Soviet Union at the top of the scientific world.\n\nGeneticists found the Soviet Union a hard place to work. Some lost their jobs. Geneticists from other parts of the world soon realized the Soviet Union was no longer a place to travel to in order to do innovative research. Hermann Muller, an American geneticist who later won the Nobel Prize, traveled to Leningrad\u2019s Institute of Genetics in 1933. He hoped to work with the scientists there. He immediately discovered that Lysenko was running roughshod over Soviet science.\n\nMuller spoke out against Lysenko in public. He even agreed to debate Lysenko in front of three thousand farmers and scientists. Muller tried to convince them that genes were real, that they stayed stable over generations. But the crowd shouted him down. They were swept away in the theater of Lysenko\u2019s performance. After the debate, Muller realized he was now in grave danger and fled the Soviet Union.\n\nHe was not overreacting. Geneticists started ending up in jail. The greatest plant scientist of the early twentieth century, Nicholai Vavilov, had supported Lysenko early in his career. But now he spoke out against Lysenko\u2019s attacks on the reality of biology. Vavilov was thrown in prison, where he starved to death in 1943.\n\nThe historian Mark Tauger has observed that some Soviet geneticists still managed to carry on their science. But they only managed to do so by disguising it \u2014 \u201cby writing research plans in ways that deceived bureaucratic censors,\u201d Trauger writes. They managed to publish some of their work in journals that used articles \u201con Lysenkoist themes as camouflage.\u201d\n\nLysenko didn\u2019t need the support of Soviet scientists for his power. Stalin was his patron. When Lysenko spoke at an agricultural conference held at the Kremlin in 1935, Stalin himself was in the audience. When Lysenko accused geneticists of being \u201csaboteurs,\u201d Stalin rose to his feet and yelled, \u201cBravo, Comrade Lysenko, bravo!\u201d\n\nSoviet newspapers provided Lysenko support as well. They repeatedly praised him with descriptions like \u201cthe distinguished son of the collective farm peasantry.\u201d And they hounded Lysenko\u2019s scientific victims. Newspapers ran hit pieces on individual geneticists, branding them as fascists. When a pair of Soviet scientists dared to publish a textbook that included Mendel, one newspaper condemned it with the headline, \u201cDrive formal genetics from higher education!\u201d\n\nLysenko kept promising that his new theory of heredity would dramatically increase Soviet crop production. But his increase never came. Stalin never asked other scientists whether Lysenko might be wrong. Instead, he continued to support Lysenko for years, damaging the Soviet agricultural economy in the process. This fiasco unfolded even as scientists in other countries were relying on genetics to develop new varieties of crops that would dramatically improve yields.\n\nMeanwhile, Lysenko ratcheted up his attacks even more. In 1948, three thousand geneticists were fired at one shot. Their laboratory stocks of fruit flies \u2014 the best tools at the time to understand animal genes \u2014 were destroyed. One of scientist shot himself in the head. When another scientist wrote a 200-page review of all the harm Lysenko was doing to biology, he was sent to a gulag for seven years.\n\nHow did Soviet newspapers respond to these events? They ran cartoons showing geneticists wearing the white hoods of the KKK.\n\nValery Soyfer, a Russian biologist and historian, has written about how much Soviet science in general suffered in these years. The ranks of its leadership filled by people with good political instincts and little scientific training. Generations of students went through universities without getting a good education in the latest advances in science.\n\nWhen Stalin died in 1953, Lysenko had amassed enough power to let him hold onto his post into the 1960s. Finally, Soviet scientists started mounting a campaign against him, which opened an investigation. Lysenko was pushed into retirement in 1965.\n\nThat investigation \u2014 along with later research \u2014 shows that Lysenko\u2019s work was meaningless. Plants have genes that control how they use signals to decide when to flower. All the evidence indicates that Lysenko accidentally picked out rare variants of winter wheat with mutations that altered their schedule. It was nothing but good old genetics. That\u2019s not to say that stress or heat or other experience couldn\u2019t possibly affect heredity in plants. Some research indicates that what is known as epigenetic inheritance may have an important role in plants. But Lysenko put a stain on the entire line of inquiry, leading to years of neglect.\n\nIt\u2019s been nearly ninety years since that Pravda article about Lysenko was published, helping to launch him on his dismal career. It\u2019s been over fifty years since he fell at last. When you hear this story, you may think, \u201cWell, that\u2019s appalling, but it happened a long time ago, and in a faraway place. It has no meaning to us today in the United States in 2017.\u201d\n\nI disagree. The things we are discussing today at this meeting \u2014 democracy, science, and journalism \u2014 are three valuable institutions that have made life in this country far better than it would be without them. They are worth defending, and worth keeping free of corruption.\n\nWe can look back over history to see how, in different places and different times, each of these pillars cracked and sometimes even fell. We should not be smug when we look back at these episodes. We should not be so arrogant as to believe we are so much smarter or nobler that we\u2019re immune from these disasters.\n\nInstead, we should look at how those problems arose, and learn how we can keep them from happening. They do not happen overnight. It takes years, and it takes a gradual shift in many people\u2019s norms and behaviors. When we look at science is today, we ought to ask, in what direction is it moving? The last thing we should want is for it to move even an inch in the direction of such terrible episodes.\n\nThe historian Timothy Snyder sums up the value of history with a short line in his new bestseller, On Tyranny: \u201cHistory can familiarize, and it can warn.\u201d\n\nIt\u2019s good to look at stories like Lysenko\u2019s and ask ourselves, what exactly appalls us about it?\n\n\u2014 A government decided that an important area of research, one that the worldwide scientific community had been working on for decades, was wrong. Instead, they embraced weak evidence to the contrary.\n\n\u2014 It ignored its own best scientists and its scientific academies.\n\n\u2014 It glamorized someone who opposed that mainstream research based on weak research, turning his meager track record into a virtue.\n\n\u2014 It forced scientists to either be political allies or opponents.\n\n\u2014 It personally condemned scientists who supported the worldwide consensus and spoke out against the government\u2019s agenda, casting them as bad people hell-bent on harming the nation.\n\n\u2014 The damage to the scientific community rippled far, and lasted for years. It showed hostility to scientists from other countries, isolating them from international partnerships. It also created an atmosphere of fear that led to self-censorship.\n\n\u2014 And by turning away from the best science, a government did harm to its country.\n\nLet us turn to the here and now. In the 1930s, climate change was barely appreciated, but today it\u2019s one of the most important scientific issues we face. For years now, the National Academy of Sciences and other groups of leading scientists have provided the U.S. government with their best analysis of what is happening as humans put billions of tons of heat-trapping gases into the air. They\u2019ve synthesized decades of research to this end.\n\nThanks largely to human activity, the Earth\u2019s surface has already warmed nearly 2 degrees since 1880, loaded the oceans with vast amounts of extra heat, lowered its pH, and raised sea levels. Glaciers are in retreat, and climate change is starting to influence extreme weather. If humans raising carbon dioxide concentrations at our torrential pace, climate scientists warn we will face much higher sea levels, and more extreme weather such as deadly heat waves. Oh, and by the way, climate change is projected to take a bite out of U.S. food production due to droughts and other impacts.\n\nThere is still a lot of debate about the precise response Earth will make to climate change. There was a lot of debate about genes in the 1930s. But we knew then that genes were real, and we know now that climate change is real. Even if some people say it isn\u2019t, our best science says it is.\n\nAnd how have our current leaders responded?\n\nOur president tweeted the climate change is a hoax concocted by China.\n\nHe has just nominated Congressman Jim Bridenstine to head NASA, an agency which has taken on an irreplaceable role in monitoring the climate and the atmosphere from space. The web site On the Issues asked Bridenstine what his position was on climate change. He said, \u201cThere is no credible scientific evidence that greenhouse gas atmospheric concentrations, including carbon dioxide, affect global climate.\u201d\n\nScott Pruitt, the man who runs the EPA, was recently asked if fossil fuels are the cause of climate change. He responded, \u201cI would not agree that it\u2019s a primary contributor to the global warming that we see.\u201d\n\nAt the Department of Energy \u2014 energy being the crucial source of human-produced carbon dioxide, and new sources of energy being essential to slowing down climate change \u2014 Secretary Rick Perry recently also said carbon dioxide is not the primary cause of climate change. He blamed it, mysteriously, on \u201cthe ocean waters and this environment that we live in.\u201d\n\nThese statements have no support from the scientific community, as you can see if you look at statements about climate change from scientific societies who study the subject, such as the American Geophysical Union and the American Meteorological Society. Neither Perry, Pruitt, or Bridenstine are scientists, I should add.\n\nWhen Hurricane Harvey struck, climate scientists spoke publicly about their research that indicates climate change will provide more moisture and energy for storms. For this utterance, the EPA attacked them.\n\n\u201cThe EPA is focused on the safety of those affected by Hurricane Harvey and providing emergency response support \u2014 not engaging in attempts to politicize an ongoing tragedy,\u201d a spokesperson told the press.\n\nThink about that: a branch of the Federal government personally vilified them as cynical sensationalists \u2014 for discussing a well-studied phenomenon that may affect countless lives in decades to come.\n\nScientists from 13 U.S. government agencies have put together a National Climate Assessment, which surveys the evidence for climate change. It paints a dire picture of the future. The report has not yet been officially released by the government. When these assessments are released every four years, it\u2019s the president\u2019s science advisor who typically signs off on them. But the new administration currently has no science advisor.\n\nOn top of that, the head of the EPA is now saying the National Climate Assessment needs to be peer-reviewed. A fact-checker would reply that the report has already been thoroughly peer reviewed by a panel of scientists. We\u2019re left to wonder what sort of peers he has in mind.\n\nSome in Congress are following suit. On August 29, Axios ran a story about Congressman Andy Biggs of Arizona, who calls climate change \u201ca discredited theory.\u201d They reported that Biggs has introduced an amendment that would wipe out the funding for the National Climate Assessment. Who needs to delay bad news if the bad news doesn\u2019t get written up in the first place?\n\nThe White House administration eliminates all climate-related research at the Department of Energy in its proposed budget. At the EPA, the Washington Post reported yesterday that final approval for research grants has been taken away from environmental experts and put in the hands of a political appointee with no environmental protection experience. According to the Post\u2019s sources, the appointee will kill any application with the \u201cthe double C-word\u201d \u2014 climate change.\n\nOne member of the transition team at the EPA got to the heart of the matter, flatly declaring: \u201cThere is no such thing as climate science.\u201d\n\nAnd how is the scientific community responding? Many participated in the March on Science. Many others have spoken out about their research. But there are other responses that have a different echo. On August 25, Nature reported things in the Department of Energy are a lot like they are in the EPA: scientists supported by the department have been asked to remove references to climate change and global warming from the descriptions of their projects.\n\nOne scientist who does this research was chillingly realistic about this situation. \u201cIf that\u2019s what it takes to keep science going for a couple of years, we will I guess play along.\u201d Just a couple years, and somehow, magically, this will all be over.\n\nWill climate scientists from other countries want to come to the United States to do research in this environment? Will they end up in staged debates in front of hostile crowds like Muller did? President Macron has invited American climate scientists to come to France. Was he just trolling us, or will we suffer a science drain from the country that has till now led the world in climate science?\n\nMost of what I just described has happened in just the last few weeks \u2014 in some cases the last few days. For science journalists starting off in their careers, I imagine the experience has been pretty overwhelming, perhaps even incomprehensible. What on Earth are they supposed to do?\n\nAs a not-so-young science writer, I would like to offer seven bits of advice \u2014 not just reporting specifically about climate in 2017, but about any other subject in science in today\u2019s cultural climate.\n\nI became a science journalist in 1990. I was an assistant editor at Discover. Andy Revkin, who now writes for ProPublica, was a senior editor at the time. Right before I started, he had just written a cover story on global warming. It was, I believe, the first cover story about global warming ever. Revkin wrote about NASA\u2019s climate scientist Jim Hansen and his testimony about climate models showing an increase in temperature due to human carbon emissions.\n\nThe story did not make huge waves. People took note of it, but it felt too abstract, too far off in the future. It did not prompt a flood of attacks on Discover. That\u2019s because we were working at the dawn of modern climate journalism.\n\nI wrote a lot about evolution at the time, and I was attacked by creationists for it. Ever since the Scopes monkey trial, evolution has been a political football. I got a first-row seat where I could watch the art of attacking science. Creationists used false ideas about balance to demand equal time for creationism. They gave creationism other names. They cherry-picked my articles to convey the opposite of what I wrote. They personally attacked biologists as out-of-touch elites who hated good people.\n\nI also started to write some pieces about climate change, because if you want to understand the history of life and its future, you have to learn about the climate in which it evolves. And I was startled to discover a new group of people who used the same tactics to cast doubt on climate science.\n\nIn 1994, for example, I wrote about paleoclimatology. For the most part, scientists were finding that, in the past, the level of carbon dioxide influenced the climate in a straightforward way. But there were odd periods. Some 440 million years ago, for example, carbon dioxide levels were 16 times higher than in the 1990s. But way back then, there was ice at the South Pole.\n\nOver a decade later, a climate denialist reached back to that article to quote-mine it for a column at the site Town Hall. \u201cAs Carl Zimmer has noted in Discover, at times in the earth\u2019s past, we\u2019ve had considerably more carbon dioxide in the air that we do today, and yet it\u2019s debatable whether the temperature was significantly warmer\u2026. Doesn\u2019t this suggest that there isn\u2019t anywhere near as much of a close relationship between greenhouse gasses like carbon dioxide and the temperature as many people seem to believe?\u201d\n\nHe quoted me, but he left out the part where I described some compelling solutions to these puzzles. The sun has been getting brighter over its long lifetime, for example. That means that it was four percent dimmer 440 million years ago. So the carbon dioxide had less radiation to trap. Such a compelling explanation would not fit his narrative.\n\nI then watched people use this sort of move over and over again to raise doubt about climate science \u2014 in columns, on the radio, on television, on the floor of Congress. Climate deniers used other familiar techniques. They kept calling for false balance. They vilified scientists, casting them as enemies who could never be trusted. Climate scientists were portrayed in a global conspiracy to control people\u2019s lives and to somehow enrich themselves with grants from the National Science Foundation.\n\nSo my first suggestion to flummoxed science journalists is: read history.\n\nNot just about Lysenko, but about recent history of science and politics and the media. For climate change in particular, read books like The Discovery of Global Warming by the historian Spencer Weart, or Merchants of Doubt by the historian Naomi Oreskes and Erik Conway. What seems new actually has deep roots.\n\nNumber two: don\u2019t give up on old-fashioned principles. Resist the simplistic notion that drastic times call for drastic measures. Remember that science journalism is a lot like science: we should try to get as close as we can to the truth about science \u2014 about its findings and its practice and its impact. Do not let your astonishing Twitter feed make you forget your job.\n\nNumber three: don\u2019t get bullied away from your principles. Don\u2019t let someone on Twitter or a TV show guilt you into doing bad reporting in the name of false balance. And don\u2019t let people push you to hide legitimate disagreements either.\n\nIf you\u2019re writing about plate tectonics, and there are geologists who have published peer-reviewed work that puts them in conflict \u2014 let\u2019s say, they\u2019re in a debate about how the Andes Mountains formed \u2014 report that debate. But don\u2019t feel guilty because someone wants equal time for their 200-page pdf online about how there is not continental drift because the Earth is hollow. Does this make your reporting biased? That\u2019s an absurd question for a science journalist. Hello, my name is Carl Zimmer, and I am pro-plate tectonics.\n\nNumber four: Always write for the public. In the United States, we thankfully have a first amendment here and aren\u2019t dominated by state-run media. But even in a free society, we must vow not to do stenography for people in power, no matter who they are, no matter what the topic.\n\nIt may be easy to keep this in mind in some cases, and harder in others. For over thirty years, governments have urged people to keep their intake of fat low. Many reporters have passed on that advice uncritically, without looking at the statistics of the underlying research. The scientific evidence behind that policy was not strong when the policy went into practice, and a growing number of studies have challenged the low-fat mantra. The latest one came out at the end of August, looking at 130,000 people. It found people with a high-carbohydrate diet were more likely to die. A diet rich in fats did not have that effect. Don\u2019t ignore this sort of solid, large-scale research because it might not agree with a public health message.\n\nNumber five: Remember that circulation managers are heroes of journalism, no less than a reporter who climbs to the top of a glacier in Greenland. They bring in the audience, in print and online. They help make journalism organizations solvent for the long term.\n\nYou can help by becoming a circulation manager yourself, or by thinking like one. People who care about science should not end up huddled around their own campfire, taking turns as speaker and audience. Building bigger audiences means trying out new kinds of media. What does each new format let you do that you couldn\u2019t do before? What audience can you reach?\n\nAlso ask yourself, why aren\u2019t you reaching more people? Are you failing to reach a big demographic swath? Climate change and other big stories of science affect everyone. Informed democracies need to know about them.\n\nBut that means acknowledging that people may not draw the conclusions from your work that you may think are obvious. And it\u2019s not because they don\u2019t understand, or because they don\u2019t have enough information. Get to know the research on the psychology of science communication \u2014 how everyone filters information about science through our identity and cultural affiliations.\n\nNumber six: remember that science is at the heart of humanity\u2019s search for truth. In the world of journalism, science writers can feel on the outside looking in. No editor-in-chief of a major newspaper that I can think of got there after being a science writer. No anchor on CNN or network news did either.\n\nBut at a time when disinformation is rampant, people look to science as something to be protected. Michael Lewis, one of our most popular and talented journalists, made that choice in his recent piece in Vanity Fair: \u201cWhy The Scariest Nuclear Threat May Be Coming From Inside The White House.\u201d\n\nHe reported how the new administration showed little interest in learning about key concerns of Department of Energy scientists \u2014 little things like making sure nuclear weapons don\u2019t blow up by accident. Instead, Lewis reported, the new administration showed much more interest in finding out who in the department went to climate change meetings and wiping out all of DOE\u2019s climate-research funding.\n\n\u201cYou can\u2019t gut the science,\u201d John MacWilliams, associate deputy secretary of the Department of Energy, told Lewis. \u201cIf you do, you are hurting the country. If you gut the core competency of the D.O.E., you gut the country.\u201d\n\nYou are better at reporting this stuff than anyone else. Let\u2019s seize this moment.\n\nFinally, number seven: Recognize that every science story can have a moral dimension, no matter how small it may seem. It\u2019s easy to think a lot of what we do is just telling entertaining stories. We get to write about pandas and electrons.\n\nBut there is a moral code hidden in every story. Most research costs taxpayers money \u2014 why was this particular study of value? Each study has to adhere to good scientific practice to have a chance of showing us something new about how the world works. Did it?\n\nAnd when a country slides into authoritarianism, it\u2019s a mistake to believe that nice little science stories can offer refuge for a journalist. To illustrate what I mean by this, I want to end my talk with another story from Russia. This one comes from the Putin era, told by the journalist Masha Gessen.\n\nGessen has been getting a lot of well-deserved attention in the past year for her essays about autocracy, the experiences Russian journalists have under Putin, and the lessons that journalists and citizens in the United States can learn from them.\n\nBut Gessen is one of us. Her portfolio includes science journalism. She was even the editor of a Russian science magazine. So as we look at other times and places to plan our actions, we can look to her experience, which she wrote about in passing in a longer piece she published last year:\n\n\u201cIn 2012, I was working as the editor-in-chief of a popular science magazine called Vokrug Sveta when Vladimir Putin, who fancies himself an explorer and a nature conservationist, took a liking to the publication. His administration launched a kind of friendly takeover of the magazine, one that the publisher could not refuse. I found myself in meetings with the Russian Geographic Society, of which Putin was the hands-on chairman. They wanted me to publish stories about their activities, most of which, as far as I could tell, were bogus. In exchange, they promised to help the magazine: at one point every school in Russia was ordered to buy a subscription (like many Kremlin orders, this one ended in naught). I felt a slow rot setting in at a magazine I loved, but I kept telling myself that I could still do a good job \u2014 and keep many fine journalists gainfully employed. Then I was asked to send a reporter to accompany Putin on his hang-gliding adventure with a migrating flock of endangered Siberian cranes. I refused \u2014 not on principle but because I was afraid that the reporter would see and describe something that would get the magazine in trouble. The publisher fired me, but then Putin called me in for a meeting and offered me my job back \u2014 legally, it wasn\u2019t his to offer, but for practical purposes it was.\n\n\u201cIn comparison to the Putin regime\u2019s major abuses of power and suppression of the opposition, the story of the cranes and my firing does not deserve a mention. All that happened as a result of the hang-gliding trip (from what I know) was that two or three of the cranes were badly injured for the sake of the president\u2019s publicity stunt, and I lost my job. But I also lost a bit of my soul and the sense of moral agency I had earned over decades of acting like my best journalist self. When Putin offered me my job back after the trip, I hesitated to say no: I loved that job, and I thought I could still edit a good magazine and keep some fine journalists employed. I didn\u2019t want to imagine what would happen the next time I was asked to cover a Putin photo op or a fake story produced by his Geographic Society, which siphoned money off like every other part his mafia state. Fortunately for me, my closest friend said, \u2018Have you lost your mind?,\u2019 by which she meant my sense of right and wrong.\u201d\n\nThe Lysenko Controversy as a Global Phenomenon: Genetics and Agriculture in the Soviet Union and Beyond. Editors: deJong-Lambert, William, Krementsov, Nikolai. Springer, 2017. Volumes 1 & 2.\n\nIngs, Simon. Stalin and the Scientists: A History of Triumph and Tragedy, 1905\u20131953. Grove/Atlantic, Inc., 2017.\n\nRoll-Hansen, Nils. The Lysenko Effect: the Politics of Science. Humanity Press, 2005.\n\nWesterman, Frank. Brother Mendel\u2019s Perfect Horse: Man and beast in an age of human warfare. Random House, 2013.", "sentiment": 0.08394805194805195},
{"link_title": "The joy of veg (2011)", "url": "https://www.theguardian.com/lifeandstyle/2011/aug/26/hugh-fearnley-whittingstall-vegetables", "text": "If you've seen my shows and read my books, you may be feeling a bit baffled to find yourself reading an article written by that notorious carnivore Hugh Fearnley-Whittingstall about the joys of eating less meat. I can appreciate that. But I really have been eating a lot less meat lately and I'm feeling almost evangelical about persuading other people to do the same.\n\nLet me be clear: I have not become a vegetarian, nor do I think I ever will. So the dialogue I'm keen to begin with other meat-eaters is not about vegetarianism, it's about vegetables. I would love to persuade you to eat more vegetables. And thereby to eat less meat \u2013 and maybe a bit less fish too. Why? To summarise, we need to eat more vegetables and less flesh because vegetables are the foods that do us the most good and our planet the least harm. Do I need to spell out the arguments to support that assertion? Is there anyone who seriously doubts it to be true? Just ask yourself if you, or anyone you know, might be in danger of eating too many vegetables. Or if you think the world might be a better, cleaner, greener place with a few more factory chicken farms or intensive pig units.\n\nWe eat too much meat in the west \u2013 too much for our own health and far too much for the welfare of the many millions of animals we raise for food. I believe that factory farming is plain wrong \u2013 environmentally and ethically. So it saddens me to say that, despite some recent significant gains in the UK on poultry and pork welfare, the problems associated with the industrial production of meat are, globally speaking, as bad as ever.\n\nI still believe in being a selective omnivore, casting a positive vote in favour of ethically produced meat and sustainably caught fish. However, I now understand that in order to eat these foods in good conscience, I have to recognise, control and impose limits on my appetite for them. To that end, over the last 18 months or so, I have undergone a sea change in the way I cook: it began as an exercise, but it became nothing short of a small revolution. It is now the case that most of the meals I eat contain no meat or fish. And I can tell you, with my hand on my heart, that I eat better than ever.\n\nI've often written about the art of making a little meat go a long way, how a few shards of bacon or a sprinkling of chorizo or some scraps of leftover chicken are a perfect way to give a lift to a big salad or add interest and texture to a vegetable soup. So why have I avoided such strategies in my bid to be less carnivorous? Why don't I allow a modest amount of meat into most things? Because it would be a cop-out, that's why! That approach, useful though it is at times, is ultimately the wrong mindset for serious change. It suggests a kind of clinging-on to meat; as if a meal is incomplete without it. That, I think, is the feeling we need to let go of.\n\nWe may be increasingly aware of the good reasons to eat less meat, but our cooking culture is still largely based around it. The idea of a fridge entirely free of sausages, bacon, chops or chicken can strike fear into the heart of many a cook. Meat is so familiar, so convenient, the easy route to something we instantly recognise as a \"proper meal\". But there is absolutely no reason why we can't embrace vegetables in the same way.\n\nAs the best vegetarian cooks know, you can produce delicious and balanced flesh-free meals without needing to somehow replace meat. We all require protein in our diets of course (though in fact we don't need a huge amount), but I've little time for veggie sausages and TVP. Instead, I'd rather exploit the vital food value and great culinary potential of pulses, nuts and grains, muddling my chickpeas, kidney beans, walnuts and quinoa with fresh leaves, crunchy roots and sun-ripened fruits: squashes, peppers, courgettes, aubergines and tomatoes, to name but five.\n\nI have to admit that, at first, it was hard to shake off the meat-lover's niggling prejudices. But I can honestly say my anxieties \u2013 about cooking without flesh being somehow less satisfying, less flavoursome or less easy \u2013 have proved groundless. I have actually found it all rather liberating. I'd go so far as to say that I've embraced a whole new way of eating \u2013 one which is more democratic than the traditional meat-and-two-veg model. Without a tyrannical piece of meat dominating the agenda, making everything else feel like a supporting act, I've found a new pleasure in cooking for myself, my family and my friends.\n\nAlthough there's sometimes a bit of peeling and chopping to be done, there are also a lot of dishes that can be prepared ahead and actually taste the better for it \u2013 soups, stews, chillies and dips among them. Veg cooking does not need to be massively time-consuming. There are all sorts of ways to make it fit into a busy life \u2013 from cooking soups and sauces in big batches and freezing them, to \"cheating\" by buying pre-prepared vegetables from the supermarket (it's not my choice to shop like that, but if it gets more of us eating more veg then I'm all for it). In any case, there are plenty of quick and cheery meat-free meals \u2013 and they don't all have to be based on pasta (though of course, that's a noble ingredient that can do your veg proud).\n\nBut to tell the truth, my tendency now is very much towards meals that give equal weight to several different dishes. Much as I enjoy generous one-pot vegetarian curries, hotpots and lasagnes, I find there's something particularly enticing about a meal made up of several \"small\" dishes, such as you get with Middle Eastern meze, Spanish tapas, Scandinavian sm\u00f6rg\u00e5sbords or Italian antipasti. Vegetable cookery lends itself to such pick-and-mix spreads. It's all so much less predictable and more fun than being a slave to meat.\n\nCivilised, and civilising, this way of eating is all about sharing; about sitting down to a kind of indoor picnic \u2013 a range of dishes that are passed around, put down, grazed, picked up and passed around again.\n\nThese little dishes often include some kind of hummus or dip: I am inordinately fond of crushed and pur\u00e9ed blends of vegetables, bound and thickened with tahini or smooth peanut butter, laced with spices, herbs, garlic and lemon juice and topped with a trickle of lovely extra-virgin rapeseed or olive oil. My current favourites are a cumin-scented roasted carrot hummus and a fantastic blended beetroot and walnut hummus. Such dips can be whizzed up in minutes, and will sit happily in the fridge for several days, to be dipped into until they're all gone. Alongside, I might serve a tray of garlicky, thyme-infused roasted vegetables, a green salad, some cold frittata or a savoury tart. There might be a bowl of simply cooked and dressed puy lentils or some really good roasted artichoke hearts from a jar.\n\nContrary to what you might think, a meal based around several dishes needn't require much work. You need only a couple of \"prepared\" things for a family meal, which you can complement with a simple hunk of good bread and a salad of ripe cherry tomatoes or a pile of scrubbed raw carrots. In any case, in my house, our meze-style dishes tend to be prepared and consumed in a sort of rolling relay from meal to meal. Leftovers from yesterday's supper are often a crucial element of today's lunch. It's a relaxed way of cooking and eating.\n\nEven if I can't tempt you into this way of serving dinner, I hope I can convince you that putting meat and fish to one side, very far from diminishing your diet in any way, can lead you into a vast new area of wonderful, wholesome dishes. Vegetable soups alone offer countless opportunities for experimentation \u2013 and if that term \"vegetable soup\" doesn't inspire you, please think again. I'm not talking about thin, anonymous broths but great, chunky, flavour-packed bowlfuls: soups like the hearty Italian ribollita, full of beans and garlic and olive oil, or the stew-like Chilean porotos granados, packed with pumpkin and sweetcorn and fragrant with oregano, or a refreshing tomato gazpacho \u2013 and any number of off-the-cuff, homemade improvisations in between.\n\nThere are all manner of \"big\" dishes you can make without meat that will satisfy your stomach and your tastebuds just as well as a roast dinner or a plate of fish and chips: saucy, sweet and spicy curries and chillies; crisply crusted pies and tarts; hearty, main-meal salads; fabulous lasagnes, souffl\u00e9s and bakes, such as a gorgeous aubergine parmigiana or a whole, stuffed squash. And I haven't even mentioned bread-based meals yet \u2013 homemade vegetable pizzas can be mind-blowing but you don't even have to get out a bag of flour in order to enjoy delicious rarebits and toasties loaded with veg, herbs and cheese.\n\nI am a better cook now than I was a year and a half ago. I feed my family better than I did before. I am far less reliant on that freezer full of meat and fish than I used to be. Our River Cottage cookery school and restaurants serve more, and more exciting, vegetable dishes than we ever have before. I enjoy my cooking, and my eating, more than ever. And that feels wonderful.\n\nUndeniably, we are faced with the very challenging question: how can we eat really well every day without contributing to global warming, the suffering of animals or the pillaging of our precious marine resources? There is one, unequivocal answer: to eat more vegetables. Addressing this issue isn't about giving anything up, it's about filling your boots: embracing a world of fabulous, fresh ingredients and finding some new and irresistible ways to cook and serve them. The crucial thing is the mental shift: after that, I predict you will find it a breeze.\n\n\u2022 River Cottage Veg Every Day! is published on 19 September by Bloomsbury at \u00a325. To order a copy for \u00a318 (including free UK mainland p&p), visit the Guardian Bookshop.", "sentiment": 0.1813472207125057},
{"link_title": "Since 1970 the number of  extreme weather events worldwide has quadrupled", "url": "https://twitter.com/TheEconomist/status/906380504953475072", "text": "", "sentiment": 0.0},
{"link_title": "The mysterious Voynich manuscript has finally been decoded", "url": "https://arstechnica.com/science/2017/09/the-mysterious-voynich-manuscript-has-finally-been-decoded/", "text": "Since its discovery in 1912, the 15th century Voynich Manuscript has been a mystery and a cult phenomenon. Full of handwriting in an unknown language or code, the book is heavily illustrated with weird pictures of alien plants, naked women, strange objects, and zodiac symbols. Now, history researcher and television writer Nicholas Gibbs appears to have cracked the code, discovering that the book is actually a guide to women's health that's mostly plagiarized from other guides of the era.\n\nGibbs writes in the Times Literary Supplement that he was commissioned by a television network to analyze the Voynich Manuscript three years ago. Because the manuscript has been entirely digitized by Yale's Beinecke Library, he could see tiny details in each page and pore over them at his leisure. His experience with medieval Latin and familiarity with ancient medical guides allowed him to uncover the first clues.\n\nAfter looking at the so-called code for a while, Gibbs realized he was seeing a common form of medieval Latin abbreviations, often used in medical treatises about herbs. \"From the herbarium incorporated into the Voynich manuscript, a standard pattern of abbreviations and ligatures emerged from each plant entry,\" he wrote. \"The abbreviations correspond to the standard pattern of words used in the Herbarium Apuleius Platonicus \u2013 aq = aqua (water), dq = decoque / decoctio (decoction), con = confundo (mix), ris = radacis / radix (root), s aiij = seminis ana iij (3 grains each), etc.\" So this wasn't a code at all; it was just shorthand. The text would have been very familiar to anyone at the time who was interested in medicine.\n\nFurther study of the herbs and images in the book reminded Gibbs of other Latin medical texts. When he consulted the Trotula and De Balneis Puteolanis, two commonly copied medieval Latin medical books, he realized that a lot of the Voynich Manuscript's text and images had been plagiarized directly from them (they, in turn, were copied in part from ancient Latin texts by Galen, Pliny, and Hippocrates). During the Middle Ages, it was very common for scribes to reproduce older texts to preserve the knowledge in them. There were no formal rules about copyright and authorship, and indeed books were extremely rare, so nobody complained.\n\nOnce he realized that the Voynich Manuscript was a medical textbook, Gibbs explained, it helped him understand the odd images in it. Pictures of plants referred to herbal medicines, and all the images of bathing women marked it out as a gynecological manual. Baths were often prescribed as medicine, and the Romans were particularly fond of the idea that a nice dip could cure all ills. Zodiac maps were included because ancient and medieval doctors believed that certain cures worked better under specific astrological signs. Gibbs even identified one image\u2014copied, of course, from another manuscript\u2014of women holding donut-shaped magnets in baths. Even back then, people believed in the pseudoscience of magnets. (The women's pseudoscience health website Goop would fit right in during the 15th century.)\n\nThe Voynich Manuscript has been reliably dated to mere decades before the invention of the printing press, so it's likely that its peculiar blend of plagiarism and curation was a dying format. Once people could just reproduce several copies of the original Trotula or De Balneis Puteolanis on a printing press, there would have been no need for scribes to painstakingly collate its information into a new, handwritten volume.\n\nGibbs concluded that it's likely the Voynich Manuscript was a customized book, possibly created for one person, devoted mostly to women's medicine. Other medieval Latin scholars will certainly want to weigh in, but the sheer mundanity of Gibbs' discovery makes it sound plausible.\n\nSee for yourself! You can look at pages from the Voynich Manuscript here.", "sentiment": 0.05884944290681995},
{"link_title": "ASP.NET Core 2.0: Action Selection", "url": "http://jackhiston.com/2017/9/9/aspnet-core-20-repository-overview-action-selection/", "text": "This article is the fourth in a series I'm dedicating to reviewing the code and design of the ASP.NET Core GitHub repository. The series tries to explain the underlying mechanisms of ASP.NET Core, and how all the code fits together to create the framework as we know it at the published date.\n\nThis article will discuss action selection within ASP.NET Core 2.0. The previous article discussed how actions and controllers are discovered within your application. This article will focus on how a specific action is selected, given a set of action descriptors found on start up.\n\nI will also try and discuss how the brand new razor pages fits into this, and why the process of action selection is the same for both MVC and razor pages.\n\nThe ActionSelector class is the central piece of infrastructure for ASP.NET Core action selection.\n\nThe ActionSelector class itself is registered with the global dependency injection system against the IActionSelector abstraction. Like all registrations in the ASP.NET Core dependency injection system, it is only added if there is no previous registration:\n\nThis means that you can easily create your own implementation of IActionSelector, and provide your own logic for selecting an action descriptor:\n\nThe IActionSelector abstraction is used by MvcAttributeRouteHandler and MvcRouteHandler. Both of these are the main registered IRouter implementations.\n\nWhen selecting an action to run, the action selector does not care whether it is a controller or razor page action, all it has is metadata described by an action descriptor. If you would like to learn more about the creation of action descriptors, see my previous article about how the action descriptor collection is populated.\n\nThe ActionSelector Class has multiple dependencies, and IActionDescriptorCollectionProvider is no doubt the most important. This dependency provides all the action descriptors that are currently registered within the system.\n\nThe action constraint cache is used to retrieve the constraint(s) on a specific action-e.g., HttpPost.\n\nA route value is the value given to a specific route key for a given action. For example, an action by default has the following route key/value pairs:\n\nWhere home and index are the controller and action names, respectively.\n\nThe ActionSelector class will take all the action descriptors provided by the action descriptor collection provider. With these, it creates it's own internal cache of mapping route keys to route values.\n\nThe action selector class does this by looping through all the actions given to it, and extracting the RouteValues for each action:\n\nFor each and every action descriptor, there will be a route value for every route key defined within the system. If the route key was not specified for a particular action, this will be filled in as null for that particular action.\n\nFor example, an action may have an Area attribute, which specifies that for the RouteKey \"area\", use the value Blog:\n\nDue to the declaration of this route value, the system will assign null to every other action descriptor for the route key \"area\".\n\nAssigning a null value to route keys is necessary for scenarios such as Razor Pages. Razor pages add a \"page\" route value. If the current context has an \"action\" route value, then it should match when the action has \"page\" set. This can be seen explained here in code comments.\n\nGoing back to the caching mechanism, The cache keys are used as an array of route values for an action, and the actual entries will be a list of action descriptors that matched those values. For example, given the action:\n\nThe route keys and value for this action will be:\n\nThe cache will store as it's key/value pair the following:\n\nThe ActionSelector class provides a way to retrieve actions based on conventional routing. The SelectCandidates method's signature of the ActionSelector class is the following:\n\nThis method retrieves a read only list of action descriptors. Each one of these action descriptors describes a particular action within the current application based on conventional routing.\n\nUsing the provided route data from the passed in RouteContext, the action selector will loop through all the registered route keys that is given from the cache-i.e. \"action\", \"controller\", \"area\" etc.:\n\nand try and retrieve the actual value matching this route key, from the RouteData given in the RouteContext.\n\nEach loop will try and receive the value associated with the key. If the value is not null, then it assigns it to the values array. This values array is then used as the key for the entries calculated in the cache:\n\nThese ordinal entries are built up from the actual route values defined from the conventional routing within your application.\n\nSo from this, the matching route values have been found, and so we are now ready to actually select an action from the selected candidates.\n\nThe cache that the action selector uses is done in a very clever way. As previously discussed, to speed up the actual selection of action candidates, it uses the actual route values as a key, as this is beneficial from the perspective of the incoming request's route data. However, the actual cache uses two dictionaries, not just one:\n\nOne dictionary is for a case-sensitive key/value lookup, and the other is case-inensitive. Why have two?\n\nIt comes down to speed. When using dictionaries, a case-sensitive key is faster than a case-insensitive key. So the code is utilising this fact.\n\nHowever, action selection is actually case-insensitive, so two actions can actually equal when one is \"Index\" and another is \"index\" for a particular route value.\n\nSo essentially, if a match is not found with a case-sensitive match first, a case-insensitive match is then used, as shown earlier.\n\nThe MvcAttributeRouteHandler is the handler for attribute routing. This does not use the \"SelectCandidates\" method to retrieve action descriptors. Attribute routing uses the AttributeRoute class to retrieve applicable action descriptors.\n\nThe AttributeRoute class, like the ActionSelector class, uses the IActionDescriptorCollectionProvider to retrieve all the action descriptors in the system:\n\nThe AttributeRoute class is only concerned with actions that have attribute route information on them. Contrast this with the ActionSelector class, that is only concerned with actions that don't have attribute routing-i.e., conventional routing.\n\nA route template is either a built-in or custom part of a URL path. For example, you may have a URL path similar to what my blog site uses:\n\nHow does this get translated into selecting a specific action?\n\nIn the above route attribute, we have defined a route template. This route template is made up of four different route parameters: year, month, day, and title. So to match this action, the URL path needs to have each of these route parameters provided.\n\nI also say that the year, month, and day all have to be integers:\n\nThe section after the colon (:) signifies an inline constraint. If the part before the first path separator (/) is not an integer, then this action will not match the URL, and so on for all the route parameters.\n\nYou can learn more about route templates in the Microsoft documentation.\n\nThe SelectBestCandidate method is called by both the MvcAttributeRouteHandler class, and the MvcRouteHandler class. They both handle matching an action to the HTTP request through attribute routing and conventional routing, respectively.\n\nBoth handlers are used by the routing system in order to handle incoming HTTP requests, and both use the ActionSelector class to achieve those goals.\n\nThe SelectBestCandidate method accepts a read only list of action descriptors. These are either sourced from the action selector itself, or through the attribute routing system.\n\nThe following code is the core of this method:\n\nThe SelectBestCandidate method is the point in time when the action constraints are evaluated. After the selection of the best candidates, each action is evaluated based on it's action constraints.\n\nAction constraints can be used to further narrow the selection of an action, when there are currently multiple candidates that match against the given route values.\n\nYou can define your own action constraints by implementing the IActionConstraint interface:\n\nThere are two pieces of functionality to implement. One is the Order property, and the other is the Accept method.\n\nThe order property defines what stage your action constraint will be evaluated. Action constraints that are of a lower order will be evaluated first.\n\nN.B., if there are two actions that match a request, and one has an action constraint, then it is deemed a more specific match, and is selected over the action with no action constraint. This is explained on the actual IActionConstraint interface in the source code.\n\nOnce the constraints are evaluated, then the results are checked, and if there is more than one matching action, then the infamous AmbiguousActionException is thrown.\n\nAction selection knows nothing about your controllers. It knows nothing about your application. All it deals with is a list of action descriptors. Action descriptors that have already been evaluated at start up.\n\nThis post went through how an action descriptor is selected based on an incoming request, and discussed the extension points of the selection process. It also discussed how attribute routing, and conventional routing, share common code, and how they differ in the sourcing of these global action descriptors.\n\nThank you for reading and I hope this helps. Please share with a friend.", "sentiment": 0.07958282569061001},
{"link_title": "France, Germany, Italy, Spain seek tax on digital giants' revenues", "url": "http://www.reuters.com/article/us-eu-tax-digital/france-germany-italy-spain-seek-tax-on-digital-giants-revenues-idUSKCN1BK0HX?feedType=RSS&feedName=technologyNews&utm_source=Twitter&utm_medium=Social&utm_campaign=Feed%3A+reuters%2FtechnologyNews+%28Reuters+Technology+News%29", "text": "FILE PHOTO The logo of the web service Amazon is pictured in this June 8, 2017 illustration photo. REUTERS/Carlos Jasso/Illustration\n\nPARIS (Reuters) - France, Germany, Italy and Spain want digital multinationals like Amazon and Google to be taxed in Europe based on their revenues, rather than only profits as now, their finance ministers said in a joint letter.\n\nFrance is leading a push to clamp down on the taxation of such companies, but has found support from other countries also frustrated at the low tax they receive under current international rules.\n\nCurrently such companies are often taxed on profits booked by subsidiaries in low-tax countries like Ireland even though the revenue originated from other EU countries.\n\n\u201cWe should no longer accept that these companies do business in Europe while paying minimal amounts of tax to our treasuries,\u201d the four ministers wrote in a letter seen by Reuters.\n\nThe letter, signed by French Finance Minister Bruno Le Maire, Wolfgang Schaeuble of Germany, Pier-Carlo Padoan of Italy and Luis de Guindos, was addressed to the EU\u2019s Estonian presidency with the bloc\u2019s executive Commission in copy.\n\nThey urged the Commission to come up with a solution creating an \u201cequalization tax\u201d on turnover that would bring taxation to the level of corporate tax in the country where the revenue was earned.\n\n\u201cThe amounts raised would aim to reflect some of what these companies should be paying in terms of corporate tax,\u201d the ministers said in the letter, first reported on by the Financial Times.\n\nLe Maire, Schaeuble, Padoan and de Guindos of Spain said they wanted to present the issue to other EU counterparts at a Sept. 15-16 meeting in Tallinn.\n\nThe EU\u2019s current Estonian presidency has scheduled a discussion at the meeting about the concept of \u201cpermanent establishment\u201d, with the aim of making it possible to tax firms where they create value, not only where they have their tax residence.\n\nFrance has stepped up pressure for EU tax rules after facing legal setbacks trying to obtain payments for taxes on activities in the country.\n\nA French court ruled in July French court ruled that Google, now part of Alphabet Inc, was not liable to pay 1.1 billion euros ($1.3 billion) in back taxes because it had no \u201cpermanent establishment\u201d in France and ran its operations there from Ireland.", "sentiment": -0.030761316872427988},
{"link_title": "Combatting Mobile Billing Fraud", "url": "https://www.iafrikan.com/2017/09/09/combat-mobile-billing-fraud-on-all-south-african-mobile-carrier-networks/", "text": "Oxygen8 South Africa has implemented the BlockFraud software across all South African mobile carrier networks to combat mobile billing fraud. BlockFraud eliminates some of the most pervasive forms of mobile carrier billing fraud.\n\n\"The proprietary technology BlockFraud has developed to combat mobile billing fraud worldwide is unsurpassed,\" said Massimo Cristini, CEO atBlockFraud.\n\nAfter implementing BlockFraud's IFrame Blocker and Block Fraud Monitor, South Africa's mobile operators will experience some of the following benefits according to Oxygen8:\n\nComplete elimination of malicious framing and malware across all South African mobile carrier networks. Identification of major ad network currently distributing malware which auto-subscribes users to services without their consent. Identification of more than 300,000 mobile devices infected with malware that were causing latency and other delays due to fake subscriptions bogging down Oxygen8 South Africa's systems. Reduced company workflow due to eradication of fraudulent activity, which in-turn reduces customer service complaints and tracking issues. Increased transparency between Oxygen8 South Africa and partners due to improved clarity regarding traffic quality.\n\nOxygen8 provides billing services to a range of companies and sectors globally. Among its clients it counts mobile networks, newspaper groups, radio and television stations, major high street retailers and banks.\n\n\"The immediate and overwhelming effectiveness of these solutions, coupled with the ease of implementation and operation, has made our decision to work with BlockFraud an obvious and vastly beneficial choice on several levels, and is something we strongly recommend for anyone who operates within the mobile carrier billing space,\" concluded Dhavelin Chetty, Operations Manager at Oxygen8 South Africa.", "sentiment": 0.05490338164251208},
{"link_title": "Learn how to use Redux step-by-step", "url": "https://github.com/happypoulp/redux-tutorial", "text": "This repository contains a step by step tutorial to help grasp flux and more specifically Redux.\n\nThe official and very exhaustive Redux documentation is available here and should be your number one source of truth regarding Redux. The present tutorial will only offer you an introduction to flux concepts through Redux use. For further or more detailed info, please refer to the Redux documentation.\n\nIt is required for you to know a bit of ES6 and ES7 (Object Spread) to correctly understand some of the examples given in this repo. Also be aware that this tutorial targets redux 3.0.2 and react-redux 4.0.0.\n\nThis tutorial is split into a sequence of javascript files that are intended to be read in order.\n\nStart with the first tutorial: Introduction\n\nBeing real js files, each example can be run to verify that it works as intended or to do your own experiments:", "sentiment": 0.275},
{"link_title": "Cassini's Grand Tour", "url": "http://www.nationalgeographic.com/science/2017/09/cassini-saturn-nasa-3d-grand-tour/", "text": "The geyser moon Enceladus shows off its fractured folds and ridges in a mosaic of Cassini images taken from March 9 to July 14, 2005. Credit: NASA/JPL/Space Science Institute\n\nEnceladus may be a tiny moon, but it has a pretty big reputation among scientists looking for life beyond Earth.\n\nSpotted by Cassini in 2005, dozens of saltwater geysers erupt from fissures in the moon\u2019s south pole. Some of the erupting material falls back to the surface like a dusting of snow, but the rest escapes into space, where it\u2019s responsible for crafting Saturn\u2019s thin, far-flung E ring.\n\nThe jets are fueled by a global sea beneath Enceladus\u2019s icy crust. When Cassini flew through and sampled the plumes, it discovered that the alien water contains all the ingredients we believe are necessary for life to exist. Chemicals in the geysers also offer hints that hydrothermal vents are active on the ocean floor. What\u2019s more, the hidden ocean has potentially existed for billions of years\u2014more than enough time for life to evolve.", "sentiment": -0.006666666666666667},
{"link_title": "Unpopular Opinion: Peak Outrage", "url": "https://www.iafrikan.com/2017/09/09/unpopular-opinion-peak-outrage/", "text": "On 2 October 2017 Twitter will have new Terms of Service (TOS) for users outside the United States of America. This, obviously, also affects Twitter users in Afrika.\n\nTwitter notified users via e-mail and as you access the app that the new TOS will affect them and that they need to accept them in order to continue using Twitter. Upon inspection, some users got alarmed and outraged by a specific section of the TOS that relates to content ownership, copyright and licensing.\n\nAlthough Twitter explicitly states in its TOS that all content posted by any user is their property, i.e. you own any and all content you post on Twitter, it does however also state the following:\n\n\"By submitting, posting or displaying Content on or through the Services, you grant us a worldwide, non-exclusive, royalty-free license (with the right to sublicense) to use, copy, reproduce, process, adapt, modify, publish, transmit, display and distribute such Content in any and all media or distribution methods (now known or later developed).\"\n\nThis, according to Twitter, gives them the right to make the user content available other companies, organizations or individuals for syndication, broadcast, distribution, promotion or publication other media and services. This portion is what seems to have had so many Twitter users outraged.\n\nBut why the outrage?\n\nSure, it sucks that Twitter can use and generate income from your tweets and content without reimbursing you. What I don't understand though about this new outrage towards Twitter's TOS is did any of the users enraged by the new TOS ever read the older TOS or even read the TOS when they signed up? It also goes without saying that you can't have a Twitter account unless you accepted (voluntarily I might add) Twitter's TOS which have had this section regarding content licensing for many years now.\n\nAre we simply getting outraged for the sake of getting outraged?\n\nAre Twitter's TOS so horrible compared to other similar platforms?\n\nIt turns out that Twitter's TOS regarding content licensing is quite similar to Facebook's.\n\nThey are also not very different to Pinterest's TOS.\n\nThey don't differ much from Instagram's TOS either.\n\nThe only TOS that are quite different from the others when it comes to content licensing is LinkedIn's (but then again who uses LinkedIn? \ud83d\ude0a)\n\nAs you can see so far, Twitter's TOS is pretty much similar to other popular social networks, answering my second question: are Twitter's TOS so horrible compared to other similar platforms?. With that settled (I hope) let's look at answering my first question: are we simply getting outraged for the sake of getting outraged?\n\nFlorent Crivello, Software Engineer at Uber, articulated it succinctly (although discussing something unrelated) when he tweeted (ironically) that \"absence of latency allows us to have knee jerk reactions on everything\u2026\".\n\nIt seems the main problem, in my opinion, is how social media, and unfortunately Twitter, allows us to send short chunks of text and receive almost immediate feedback on the transmitted message resulting with some (a large number) abandoning thought before transmitting. Unfortunately, the problem is not only a human one as I also believe that social media through its various features incentivizes such behaviour (e.g. RTs, re-shares, Likes etc.).\n\nMy main point here is that next time before you rush to your phone or keyboard to rant and express outrage, realise that perhaps you need to investigate the topic and think about what you are about to transmit a little longer. All the outrage directed at Twitter over the past couple of days could have been less had users spent an extra 5 minutes checking the old Twitter terms. Had they spent slightly more time they would have also realized that Twitter is no different from other social media platforms. More importantly, they could have realized that they actually voluntarily accepted these TOS when they signed up to Twitter, after all, Twitter is a for-profit company whose services you are not obliged to use.\n\nI guess it is appropriate to end this with a tweet (which I otherwise wouldn't be able to embed if the outraged had their way with Twitter's TOS):", "sentiment": -0.03377705627705628},
{"link_title": "Show HN: I'll mail Equifax your arbitration opt-out for free", "url": "https://unarbitrate.org/", "text": "Opting out of Equifax's binding arbitration clause requires sending a statement by mail. If you fill out the form below, I (Paul Butler) will print and mail your opt-out message for you for free. There's no catch, it's just my way of pushing back against binding arbitration.\n\nYour opt-out message has been received. Save this link to check back on the status of your message. Note: tracking links don't work yet, but save it and check back within 24 hours.\n\nIf you choose to allow me to mail the opt-out form, I will print it and mail it for you. I will not otherwise sell or distribute the data to any third party in any way, or use it for any purposes other than opting you out. If you choose to print it yourself, the data will never be transmitted over the network (the mailing address will be printed as well.)\n\nIn light of Equifax's recent security breach, they are offering a year of complimentary credit monitoring services. The media have noticed that in their Terms of Service, they include a binding arbitration clause which means you give up your right to sue them in a regular court and must instead go through an arbitration process.\n\nWhile the company has since clarified, under pressure, that the security breach is excluded from these terms, binding arbitration clauses are a growing trend that remove legal remedies like class action lawsuits from consumers. It is especially reptillian that they would have consumers give up their legal rights in the aftermath of a breach, for a product we only need because of the breach.\n\nThe arbitration clause has an out, in that you can opt-out within 30 days of signing up. However, opting-out requires sending a statement by mail, which is sure to dissuade a lot of people. In order to make opting-out as simple as opting-in, I created this site.\n\nIt's ultimately your judgement call, and you're right to be skeptical of a stranger on the internet. The best I can offer is that I have a long-established social media presence (Twitter, GitHub), and this isn't the first time I've used it to take a stance against legal strong-arming. If you decide not to trust me, I still encourage you to opt-out; if you use the Print button to mail the letter yourself the data will never leave your browser.", "sentiment": 0.1411904761904762},
{"link_title": "Why people think Germans are so efficient", "url": "http://www.bbc.com/travel/story/20170903-why-people-think-germans-are-so-efficient?tblang=english", "text": "In summer 2011, I was working at a small Berlin travel agency and facing a conundrum: my clients literally held tickets to nowhere. Their planes would be taking off shortly, but their destination \u2013 Berlin-Brandenburg Airport \u2013 would not be open to welcome them. Six summers have since come and gone, but each year the same news, almost gleeful at this point, trickles out of that massive construction site south of the capital: the project has gone billions over budget and is nowhere close to opening. So whatever happened to German efficiency?\n\nIf the overdue airport weren\u2019t enough of a hint, I\u2019ll let you in on a secret: German efficiency is a myth, with roots in religion, nationalism, enlightenment thought and a few major wars. It may have reached its pinnacle in the 20th Century, but since then it\u2019s survived as a useful stand-in for everything that confuses the world about Germans \u2013 that in spite of a war that decimated them, a wall that divided them, a currency designed to weaken them and a financial crisis that could have ended them, they still seem to come out on top.\n\nJust not where airports are concerned.\n\nYou may also be interested in: \n\n\u2013 The German village that changed the war\n\n\u2013 The country that can't choose a side\n\n\u2013 What can Poland teach us about freedom?\n\nMuch like German humour, German efficiency (or the lack of it), is often a hot topic among visitors as they marvel at trains that follow their schedules to the minute, pristine autobahns where German-produced cars seem to drive at warp speed (while getting into statistically fewer accidents), and, perhaps every foreigner\u2019s favourite gripe, citizens who wait for the walk signal before crossing the street \u2013 and admonish you if you don\u2019t do the same.\n\nYet what they mistake for German efficiency is, in many instances, a German fondness for rules \u2013 a trait that leaves foreigners equally puzzled. While rule following may help in the seamless execution of everyday tasks, it doesn\u2019t really make a difference when it comes to big, symbolic projects of national significance. Berlin culture mavens waiting for the long-extended renovation of the Staatsoper (State Opera House) would concur; so would those in Hamburg who saw costs balloon for their new philharmonic hall.\n\nSooner or later, however, talk of German efficiency always points to Prussia. Known for its militarism, nationalism and ruthless work ethic, the kingdom spanned centuries, and, at its 19th-Century peak, much of northern Germany and present-day Poland. Supposedly, while the humourless northerly Prussians were busy marching around, coaxing potatoes out of arid, infertile land, the Bavarians were happily swilling beer in warmer climes.\n\nThe gulf between the two was further widened by Prussia\u2019s adoption of Lutheran Protestantism. It was Martin Luther who had imagined a new kind of German Christianity far from the Catholic confines of the Holy Roman Empire, and his writings further shaped the image of Germans as hard-working, law-abiding and authority supporting (by no coincidence, the same characteristics Hannah Arendt would observe during the Adolf Eichmann trials when she coined the term \u2018banality of evil\u2019 to explain how Germany could have allowed Nazism).\n\nPrussia not only laid claim to these characteristics; it also helped them become national traits. Up until the mid-19th Century, Germany had been little more than a cluster of disparate kingdoms banding together once in a while for border disputes with the French or Slavs. Prussia changed all that when it fought off Napoleon III in the Franco-Prussian War (1870-71) and steered the country towards what begins to look like modern-day Germany.\n\nIn fact, according to James Hawes, author of The Shortest History of Germany (2017), it was this victory that really cemented the image of German efficiency. To the British in the early 19th Century, \u201cGermany [was] this kind of backward country\u2026 Then suddenly, seemingly overnight, they crushed the French\u2026 the premier military power in Europe. It seemed like a kind of weird, black miracle at the time.\u201d\n\nGone were the images of lush romantics and wine-drinking philosophers, of dark forests and rolling hills and lone travellers overlooking hazy vistas \u00e0 la painter Caspar David Friedrich. All of Germany \u2013 or at least the most that ever had been \u2013 was now captive to militant Prussia, and all of Europe knew the Prussians were a people to watch out for.\n\nBy the start of World War I, this was more than just a fear of the \u2018other\u2019, Hawes explained. \u201cIf you\u2019re going to make the world safe for democracy\u2026 it\u2019s handy to be able to say that your enemy is almost alien in its weirdness\u2026 its superhuman cunning.\u201d WWI propaganda posters, some of which can be viewed at Berlin\u2019s Deutsches Historisches Museum (DHM), upheld this myth, superimposing the face of the German Kaiser on a spider\u2019s body, and generally promoting the image of Germans as all-knowing, all-seeing and omnipresent.\n\nBut why does this obsession with German efficiency still exist today, even though it should have rightfully been quashed by Allied victory in 1945? Markus Hesselmann, local editor at Tagesspiegel, has an idea, though he\u2019s not too keen on admitting it: \u201cIn Britain\u2026 there is a very strange kind of fascination (I have to be very careful how I phrase this) of Nazi Germany. There is this wish to strip away all the bad things [about the Nazis] and leave only the things you respect\u2026\u201d\n\nMembers of the former Allied powers, the US and UK chief among them, like to marvel at how the Germans were hobbled by reparations after WWI yet still emerged to fight a second war \u2013 though those sanctions were partly responsible for that war. They like to believe the Wirtschaftswunder or \u2018economic miracle\u2019 of the 1950s and \u201860s was due to a ruthlessly efficient work ethic, ignoring just how much money was being pumped in to bolster West Germany against the Russians. As Hawes points out, the myth \u201chas nothing to do with history and more to do with fantasy\u201d. In making a myth of the Germans, we make a myth of ourselves.\n\nPerhaps no-one knows this better than non-German Germans; those who have settled here from elsewhere and encountered stringent rules and unending bureaucracy in daily life, even as public works like Berlin\u2019s airport languish.\n\nIn a delightfully ironic twist of fate, though, the much-maligned airport now offers guided tours. So, in addition to Berlin history museums like the DHM, brazen battle monuments like the Brandenburg Gate and the Victory Column, and the chilling Holocaust memorial, visitors can now add Germany\u2019s latest folly to their itinerary.\n\nAccording to Joseph Pearson, however, who explores German idiosyncrasies on his blog The Needle and in his upcoming book Berlin, the delayed airport shouldn\u2019t be denigrated but celebrated precisely because it contradicts the long-held myth; a sign of history correcting its course.\n\nWhen things like the airport don\u2019t work out, \u201cIt humanises the Germans; it shows that they don\u2019t fit easily into useful stereotypes that foreigners might have about them,\u201d he told me, adding, \u201cNearly every example of German inefficiency makes me as happy as it does frustrated.\u201d\n\nJoin over three million BBC Travel fans by liking us on Facebook, or follow us on Twitter and Instagram.\n\nIf you liked this story, sign up for the weekly bbc.com features newsletter called \"If You Only Read 6 Things This Week\". A handpicked selection of stories from BBC Future, Earth, Culture, Capital and Travel, delivered to your inbox every Friday.", "sentiment": 0.07517012227538544}
]